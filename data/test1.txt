FN Clarivate Analytics Web of Science
VR 1.0
PT J
AU Li, SS
   Liu, GZ
   Wei, TX
   Jia, SC
   Zhang, JW
AF Li, Sisi
   Liu, Guanzhong
   Wei, Tianxiang
   Jia, Shichao
   Zhang, Jiawan
TI EvoVis: A Visual Analytics Method to Understand the Labeling Iterations
   in Data Programming
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual analytics; model interpretation; data programming; data labeling;
   Visual analytics; model interpretation; data programming; data labeling
AB Obtaining high-quality labeled training data poses a significant bottleneck in the domain of machine learning. Data programming has emerged as a new paradigm to address this issue by converting human knowledge into labeling functions (LFs) to quickly produce low-cost probabilistic labels. To ensure the quality of labeled data, data programmers commonly iterate LFs for many rounds until satisfactory performance is achieved. However, the challenge in understanding the labeling iterations stems from interpreting the intricate relationships between data programming elements, exacerbated by their many-to-many and directed characteristics, inconsistent formats, and the large scale of data typically involved in labeling tasks. These complexities may impede the evaluation of label quality, identification of areas for improvement, and the effective optimization of LFs for acquiring high-quality labeled data. In this article, we introduce EvoVis, a visual analytics method for multi-class text labeling tasks. It seamlessly integrates relationship analysis and temporal overview to display contextual and historical information on a single screen, aiding in explaining the labeling iterations in data programming. We assessed its utility and effectiveness through case studies and user studies. The results indicate that EvoVis can effectively assist data programmers in understanding labeling iterations and improving the quality of labeled data, as evidenced by an increase of 0.16 in the average F1 score when compared to the default analysis tool.
C1 [Li, Sisi; Wei, Tianxiang; Jia, Shichao; Zhang, Jiawan] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China.
   [Liu, Guanzhong] Netflix Inc, Los Gatos, CA 95032 USA.
C3 Tianjin University; Netflix, Inc.
RP Zhang, JW (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China.
EM sisilee144144@gmail.com; guanzhongl@netflix.com;
   tianxiangwei@tju.edu.cn; jsc_se@tju.edu.cn; jwzhang@tju.edu.cn
RI Li, Sisi/IUO-7243-2023
OI Wei, Tianxiang/0000-0001-6399-176X; Zhang, Jiawan/0000-0002-0667-6744;
   Li, Sisi/0009-0002-7678-4094
FU National Natural Science Foundation of China [62172295]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62172295.
NR 40
TC 0
Z9 0
U1 1
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2025
VL 31
IS 3
BP 1802
EP 1817
DI 10.1109/TVCG.2024.3370654
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U7J0S
UT WOS:001413499200003
PM 38416617
DA 2025-03-07
ER

PT J
AU Liu, T
   Xiao, Y
   Hu, MW
   Sha, H
   Ma, SN
   Gao, BY
   Guo, SH
   Liu, Y
   Song, WT
AF Liu, Tong
   Xiao, Yi
   Hu, Mingwei
   Sha, Hao
   Ma, Shining
   Gao, Boyu
   Guo, Shihui
   Liu, Yue
   Song, Weitao
TI AudioGest: Gesture-Based Interaction for Virtual Reality Using Audio
   Devices
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Surface roughness; Rough surfaces; Pipelines; Data acquisition;
   Microphones; Dynamics; Training; Audio synthesis; gesture interaction;
   human computer interaction; virtual reality
ID HAND; HEAD
AB Current virtual reality (VR) system takes gesture interaction based on camera, handle and touch screen as one of the mainstream interaction methods, which can provide accurate gesture input for it. However, limited by application forms and the volume of devices, these methods cannot extend the interaction area to such surfaces as walls and tables. To address the above challenge, we propose AudioGest, a portable, plug-and-play system that detects the audio signal generated by finger tapping and sliding on the surface through a set of microphone devices without extensive calibration. First, an audio synthesis-recognition pipeline based on micro-contact dynamics simulation is constructed to generate modal audio synthesis from different materials and physical properties. Then the accuracy and effectiveness of the synthetic audio are verified by mixing the synthetic audio with real audio proportionally as the training sets. Finally, a series of desktop office applications are developed to demonstrate the application potential of AudioGest's scalability and versatility in VR scenarios.
C1 [Liu, Tong; Xiao, Yi; Hu, Mingwei; Sha, Hao; Ma, Shining] Beijing Inst Technol, Sch Opt & Photon, Beijing Engn Res Ctr Mixed Real & Adv Display, Beijing 100811, Peoples R China.
   [Liu, Yue; Song, Weitao] Beijing Inst Technol, Zhengzhou Res Inst, Sch Opt & Photon, Beijing Engn Res Ctr Mixed Real & Adv Display, Beijing 100811, Peoples R China.
   [Gao, Boyu] Jinan Univ, Coll Informat Sci & Technol Cyber Secur, Guangzhou 510632, Peoples R China.
   [Guo, Shihui] Xiamen Univ, Software Sch, Xiamen 361005, Peoples R China.
C3 Beijing Institute of Technology; Beijing Institute of Technology; Jinan
   University; Xiamen University
RP Liu, Y; Song, WT (corresponding author), Beijing Inst Technol, Zhengzhou Res Inst, Sch Opt & Photon, Beijing Engn Res Ctr Mixed Real & Adv Display, Beijing 100811, Peoples R China.
EM liuyue@bit.edu.cn; swt@bit.edu.cn
RI Ma, Shining/AAF-3543-2019; Hu, Mingwei/LXW-4771-2024; Gao,
   Boyu/JNE-3525-2023
OI Ma, Shining/0000-0003-2551-4027; sha, hao/0009-0009-9682-1228; Gao,
   BoYu/0000-0001-8523-2828
FU Science and technology innovation 2030-major project of "New Generation
   Artificial Intelligence" [2022ZD0115901]; National Natural Science
   Foundation of China [62332003, 61960206007]
FX This work was supported in part by the science and technology innovation
   2030-major project of "New Generation Artificial Intelligence" under
   Grant 2022ZD0115901, and in part by the National Natural Science
   Foundation of China under Grant 62332003, and Grant 61960206007.
NR 53
TC 0
Z9 0
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2025
VL 31
IS 2
BP 1569
EP 1581
DI 10.1109/TVCG.2024.3397868
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA R6W2Y
UT WOS:001392823200001
PM 38713570
DA 2025-03-07
ER

PT J
AU Wang, AZ
   Borland, D
   Peck, TC
   Wang, WY
   Gotz, D
AF Wang, Arran Zeyu
   Borland, David
   Peck, Tabitha C.
   Wang, Wenyuan
   Gotz, David
TI Causal Priors and Their Influence on Judgements of Causality in
   Visualized Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Causal inference; Perception and cognition; Causal prior; Association;
   Causality; Causal inference; Perception and cognition; Visualization;
   Visualization; Association; Causality
ID INFORMATION VISUALIZATION; RANKING VISUALIZATIONS; IMPLICIT; VERBS
AB "Correlation does not imply causation" is a famous mantra in statistical and visual analysis. However, consumers of visualizations often draw causal conclusions when only correlations between variables are shown. In this paper, we investigate factors that contribute to causal relationships users perceive in visualizations. We collected a corpus of concept pairs from variables in widely used datasets and created visualizations that depict varying correlative associations using three typical statistical chart types. We conducted two MTurk studies on (1) preconceived notions on causal relations without charts, and (2) perceived causal relations with charts, for each concept pair. Our results indicate that people make assumptions about causal relationships between pairs of concepts even without seeing any visualized data. Moreover, our results suggest that these assumptions constitute causal priors that, in combination with visualized association, impact how data visualizations are interpreted. The results also suggest that causal priors may lead to over- or under-estimation in perceived causal relations in different circumstances, and that those priors can also impact users' confidence in their causal assessments. In addition, our results align with prior work, indicating that chart type may also affect causal inference. Using data from the studies, we develop a model to capture the interaction between causal priors and visualized associations as they combine to impact a user's perceived causal relations. In addition to reporting the study results and analyses, we provide an open dataset of causal priors for 56 specific concept pairs that can serve as a potential benchmark for future studies. We also suggest remaining challenges and heuristic-based guidelines to help designers improve visualization design choices to better support visual causal inference.
C1 [Wang, Arran Zeyu; Wang, Wenyuan; Gotz, David] Univ North Carolina Chapel Hill UNC, Chapel Hill, NC 27599 USA.
   [Borland, David] UNC, RENCI, Chapel Hill, NC USA.
   [Peck, Tabitha C.] Davidson Coll, Davidson, NC USA.
C3 University of North Carolina; University of North Carolina Chapel Hill;
   Davidson College
RP Wang, AZ (corresponding author), Univ North Carolina Chapel Hill UNC, Chapel Hill, NC 27599 USA.
EM zeyuwang@cs.unc.edu; borland@renci.org; tapeck@davidson.edu;
   vaapad@live.unc.edu; gotz@unc.edu
RI Peck, Tabitha/AAH-2032-2021
OI Borland, David/0000-0002-0162-4080; Peck, Tabitha/0000-0002-3667-7713;
   Gotz, David/0000-0002-6424-7374; Wang, Wenyuan/0000-0001-8765-6675
FU National Science Foundation [2211845]
FX We thank the reviewers for their insightful comments. This material is
   based upon work supported by the National Science Foundation under Grant
   No. 2211845.
NR 65
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2025
VL 31
IS 1
BP 765
EP 775
DI 10.1109/TVCG.2024.3456381
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N9Y7G
UT WOS:001367808800004
PM 39255145
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Calandra, D
   Lamberti, F
AF Calandra, Davide
   Lamberti, Fabrizio
TI A Testbed for Studying Cybersickness and its Mitigation in Immersive
   Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Prevention and mitigation; Visualization; Measurement; Dynamics; Task
   analysis; Standards; Taxonomy; Cybersickness; testbed; virtual reality;
   evaluation; virtual environments; simulator sickness; taxonomy
ID MOTION SICKNESS; LOCOMOTION
AB Cybersickness (CS) represents one of the oldest problems affecting Virtual Reality (VR) technology. In an attempt to resolve or at least limit this form of discomfort, an increasing number of mitigation techniques have been proposed by academic and industrial researchers. However, the validation of such techniques is often carried out without grounding on a common methodology, making the comparison between the various works in the state of the art difficult. To address this issue, the present article proposes a novel testbed for studying CS in immersive VR and, in particular, methods to mitigate it. The testbed consists of four virtual scenarios, which have been designed to elicit CS in a targeted and predictable manner. The scenarios, grounded on available literature, support the extraction of objective metrics about user's performance. The testbed additionally integrates an experimental protocol that employs standard questionnaires as well as measurements typically adopted in state-of-the-art practice to assess levels of CS and other subjective aspects regarding User Experience. The article shows a possible use case of the testbed, concerning the evaluation of a CS mitigation technique that is compared with the absence of mitigation as baseline condition.
C1 [Calandra, Davide; Lamberti, Fabrizio] Politecn Torino, Dept Control & Comp Engn DAUIN, I-10129 Turin, Italy.
C3 Polytechnic University of Turin
RP Calandra, D (corresponding author), Politecn Torino, Dept Control & Comp Engn DAUIN, I-10129 Turin, Italy.
EM davide.calandra@polito.it; fabrizio.lamberti@polito.it
RI Lamberti, Fabrizio/I-9153-2012; Calandra, Davide/ABG-3277-2020
OI Calandra, Davide/0000-0003-0449-5752
FU VR@POLITO initiative
FX This work was supported by VR@POLITO initiative.
NR 56
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2024
VL 30
IS 12
BP 7788
EP 7805
DI 10.1109/TVCG.2024.3448203
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K8D1L
UT WOS:001346124800018
PM 39172606
OA hybrid
DA 2025-03-07
ER

PT J
AU Pooryousef, V
   Cordeil, M
   Besançon, L
   Bassed, R
   Dwyer, T
AF Pooryousef, Vahid
   Cordeil, Maxime
   Besancon, Lonni
   Bassed, Richard
   Dwyer, Tim
TI Collaborative Forensic Autopsy Documentation and Supervised Report
   Generation Using a Hybrid Mixed-Reality Environment and Generative AI
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Forensic autopsy; report generation; documentation; documentation; mixed
   reality; mixed reality; generative AI; generative AI; mixed reality;
   generative AI
ID AUGMENTED REALITY; HOLOLENS; IMPACT; TOOL
AB Forensic investigation is a complex procedure involving experts working together to establish cause of death and report findings to legal authorities. While new technologies are being developed to provide better post-mortem imaging capabilities-including mixed-reality (MR) tools to support 3D visualisation of such data-these tools do not integrate seamlessly into their existing collaborative workflow and report authoring process, requiring extra steps, e.g. to extract imagery from the MR tool and combine with physical autopsy findings for inclusion in the report. Therefore, in this work we design and evaluate a new forensic autopsy report generation workflow and present a novel documentation system using hybrid mixed-reality approaches to integrate visualisation, voice and hand interaction, as well as collaboration and procedure recording. Our preliminary findings indicate that this approach has the potential to improve data management, aid reviewability, and thus, achieve more robust standards. Further, it potentially streamlines report generation and minimise dependency on external tools and assistance, reducing autopsy time and related costs. This system also offers significant potential for education.
C1 [Pooryousef, Vahid; Bassed, Richard; Dwyer, Tim] Monash Univ, Melbourne, Australia.
   [Cordeil, Maxime] Univ Queensland, St Lucia, Australia.
   [Besancon, Lonni] Linkoping Univ, Linkoping, Sweden.
   [Bassed, Richard] Victorian Inst Forens Med, Southbank, Australia.
C3 Monash University; University of Queensland; Linkoping University
RP Pooryousef, V (corresponding author), Monash Univ, Melbourne, Australia.
EM vahid.pooryousef@monash.edu; m.cordeil@uq.edu.au;
   lonni.besancon@gmail.com; richard.bassed@monash.edu;
   tim.dwyer@monash.edu
OI Pooryousef, Vahid/0000-0003-4258-4502; Dwyer, Tim/0000-0002-9076-9571;
   Bassed, Richard/0000-0001-5473-055X; Cordeil, Maxime/0000-0002-9732-4874
FU VIFM; Knut and Alice Wallenberg Foundation [KAW 2019.0024]
FX The authors wish to thank all participants from VIFM, and University of
   Toronto. This research was supported by the VIFM funding as well as the
   Knut and Alice Wallenberg Foundation (grant KAW 2019.0024).
NR 82
TC 0
Z9 0
U1 6
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7452
EP 7462
DI 10.1109/TVCG.2024.3456212
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300043
PM 39250385
OA Green Published
DA 2025-03-07
ER

PT J
AU Wang, ZJ
   Wu, J
   Fan, RZ
   Ke, W
   Wang, LL
AF Wang, Zijun
   Wu, Jian
   Fan, Runze
   Ke, Wei
   Wang, Lili
TI VPRF: Visual Perceptual Radiance Fields for Foveated Image Synthesis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Rendering (computer graphics); Neural radiance field; Visualization;
   Sensitivity; Training; Three-dimensional displays; Image reconstruction;
   Virtual reality; Foveated rendering; Visual perceptual; Contrast
   sensitivity
AB Neural radiance fields (NeRF) has achieved revolutionary breakthrough in the novel view synthesis task for complex 3D scenes. However, this new paradigm struggles to meet the requirements for real-time rendering and high perceptual quality in virtual reality. In this paper, we propose VPRF, a novel visual perceptual based radiance fields representation method, which for the first time integrates the visual acuity and contrast sensitivity models of human visual system (HVS) into the radiance field rendering framework. Initially, we encode both the appearance and visual sensitivity information of the scene into our radiance field representation. Then, we propose a visual perceptual sampling strategy, allocating computational resources according to the HVS sensitivity of different regions. Finally, we propose a sampling weight-constrained training scheme to ensure the effectiveness of our sampling strategy and improve the representation of the radiance field based on the scene content. Experimental results demonstrate that our method renders more efficiently, with higher PSNR and SSIM in the foveal and salient regions compared to the state-of-the-art FoV-NeRF. The results of the user study confirm that our rendering results exhibit high-fidelity visual perception.
C1 [Wang, Lili] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Wang, Lili] Peng Cheng Lab, Shengzhen, Peoples R China.
   [Wang, Lili] Beihang Univ, Beijing Adv Innovat Ctr Biomed Engn, Beijing, Peoples R China.
   [Wang, Zijun; Wu, Jian; Fan, Runze] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Ke, Wei] Macau Polytech Univ, Fac Appl Sci, Macau, Peoples R China.
C3 Beihang University; Beihang University; Beihang University; Macao
   Polytechnic University
RP Wang, LL (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.; Wang, LL (corresponding author), Peng Cheng Lab, Shengzhen, Peoples R China.; Wang, LL (corresponding author), Beihang Univ, Beijing Adv Innovat Ctr Biomed Engn, Beijing, Peoples R China.
EM 892710638@qq.com; lanayawj@buaa.edu.cn; by2106131@buaa.edu.cn;
   wke@mpu.edu.mo; wanglily@buaa.edu.cn
RI jiang, jun/GWC-9329-2022; Ke, Wei/LOS-3255-2024
OI Wu, Jian/0000-0002-3863-8814; Ke, Wei/0000-0003-0952-0961
FU National Natural Science Foundation of China [61932003, 62372026];
   Beijing Science and Technology Plan [Z221100007722004]; National Key RD
   plan [2019YFC1521102]
FX This work is supported by the National Natural Science Foundation of
   China through Projects 61932003 and 62372026, Beijing Science and
   Technology Plan Project Z221100007722004, and the National Key R&D plan
   2019YFC1521102.
NR 39
TC 0
Z9 0
U1 11
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7183
EP 7192
DI 10.1109/TVCG.2024.3456205
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300019
PM 39259633
DA 2025-03-07
ER

PT J
AU Battle, L
   Ottley, A
AF Battle, Leilani
   Ottley, Alvitta
TI What Do We Mean When We Say "Insight"? A Formal Synthesis of Existing
   Theory
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data models; Data mining; Data and knowledge visualization;
   Visualization techniques and methodologies
ID VISUAL ANALYTICS; VISUALIZATION; MODEL; DESIGN; TASKS; GRAMMAR; SYSTEM;
   QUERY
AB Researchers have derived many theoretical models for specifying users' insights as they interact with a visualization system. These representations are essential for understanding the insight discovery process, such as when inferring user interaction patterns that lead to insight or assessing the rigor of reported insights. However, theoretical models can be difficult to apply to existing tools and user studies, often due to discrepancies in how insight and its constituent parts are defined. This article calls attention to the consistent structures that recur across the visualization literature and describes how they connect multiple theoretical representations of insight. We synthesize a unified formalism for insights using these structures, enabling a wider audience of researchers and developers to adopt the corresponding models. Through a series of theoretical case studies, we use our formalism to compare and contrast existing theories, revealing interesting research challenges in reasoning about a user's domain knowledge and leveraging synergistic approaches in data mining and data management research.
C1 [Battle, Leilani] Univ Washington, Seattle, WA 98195 USA.
   [Ottley, Alvitta] Washington Univ St Louis, St Louis, MO 63130 USA.
C3 University of Washington; University of Washington Seattle; Washington
   University (WUSTL)
RP Battle, L (corresponding author), Univ Washington, Seattle, WA 98195 USA.
EM leibatt@cs.washington.edu; alvitta@wustl.edu
OI Battle, Leilani/0000-0003-3870-636X; Ottley, Alvitta/0000-0002-9485-276X
FU National Science Foundation (NSF) [2141506, 2142977, 2118201]
FX This work was supported by the National Science Foundation (NSF) under
   Grants 2141506, 2142977, and 2118201.
NR 78
TC 3
Z9 3
U1 6
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6075
EP 6088
DI 10.1109/TVCG.2023.3326698
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000031
PM 37874712
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Lavoué,É
   Villenave, S
   Serna, A
   Didier, C
   Baert, P
   Lavoué, G
AF Lavoue, Elise
   Villenave, Sophie
   Serna, Audrey
   Didier, Clementine
   Baert, Patrick
   Lavoue, Guillaume
TI Influence of Scenarios and Player Traits on Flow in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual reality; player traits; flow; scenario; game mechanics; Virtual
   reality; player traits; flow; scenario; game mechanics
ID SELF-DETERMINATION; ENVIRONMENTS; PERSONALITY; TECHNOLOGY; EXPERIENCE;
   IMMERSION; EQUATION; GAMES
AB Many studies have investigated how interpersonal differences between users influence their experience in Virtual Reality (VR) and it is now well recognized that user's subjective experiences and responses to the same VR environment can vary widely. In this study, we focus on player traits, which correspond to users' preferences for game mechanics, arguing that players react differently when experiencing VR scenarios. We developed three scenarios in the same VR environment that rely on different game mechanics, and evaluate the influence of the scenarios, the player traits and the time of practice of the VR environment on users' perceived flow. Our results show that 1) the type of scenario has an impact on specific dimensions of flow; 2) the scenarios have different effects on flow depending on the order they are performed, the flow preconditions being stronger when performed at last; 3) almost all dimensions of flow are influenced by the player traits, these influences depending on the scenario, 4) the Aesthetic trait has the most influences in the three scenarios. We finally discuss the findings and limitations of the present study that we believe have strong implications for the design of scenarios in VR experiences.
C1 [Lavoue, Elise] Univ Jean Moulin Lyon 3, IAE Lyon Sch Management, CNRS, LIRIS,UMR5205,INSA Lyon, F-69621 Villeurbanne, France.
   [Villenave, Sophie] Univ Lyon, Cent Lyon, CNRS, LIRIS,UMR5205,ENISE, F-42023 St Etienne, France.
   [Serna, Audrey] Univ Lyon, INSA Lyon, LIRIS, CNRS,UMR5205, F-69621 Villeurbanne, France.
   [Didier, Clementine] Univ Lyon, Cent Lyon, CNRS, LTDS,ENISE, F-69130 Ecully, France.
C3 Institut National des Sciences Appliquees de Lyon - INSA Lyon; Centre
   National de la Recherche Scientifique (CNRS); Universite Jean Moulin
   Lyon 3; Centre National de la Recherche Scientifique (CNRS); Institut
   National des Sciences Appliquees de Lyon - INSA Lyon; Institut National
   des Sciences Appliquees de Lyon - INSA Lyon; Centre National de la
   Recherche Scientifique (CNRS); Ecole Centrale de Lyon; Centre National
   de la Recherche Scientifique (CNRS)
RP Lavoué,É (corresponding author), Univ Jean Moulin Lyon 3, IAE Lyon Sch Management, CNRS, LIRIS,UMR5205,INSA Lyon, F-69621 Villeurbanne, France.
EM elise.lavoue@univ-lyon3.fr; fsophie.villenave@enise.fr;
   audrey.serna@insa-lyon.fr; clementine.didier@enise.fr;
   patrick.baert@enise.fr; guillaume.lavoue@enise.fr
OI Lavoue, Guillaume/0000-0003-3988-6702; Villenave,
   Sophie/0000-0002-6152-9800; SERNA, Audrey/0000-0003-1468-9761;
   Helfenstein-Didier, Clementine/0000-0002-3197-601X
FU LIRIS Laboratory; Agence nationale de la recherche [ANR-22-CE31-0023-03]
FX This work was supported in part by LIRIS Laboratory and in part by
   RENFORCE under Grant ANR-22-CE31-0023-03 project financed by the Agence
   nationale de la recherche.
NR 55
TC 0
Z9 0
U1 11
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6208
EP 6221
DI 10.1109/TVCG.2023.3332261
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000045
PM 37956017
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Scully-Allison, C
   Lumsden, I
   Williams, K
   Bartels, J
   Taufer, M
   Brink, S
   Bhatele, A
   Pearce, O
   Isaacs, KE
AF Scully-Allison, Connor
   Lumsden, Ian
   Williams, Katy
   Bartels, Jesse
   Taufer, Michela
   Brink, Stephanie
   Bhatele, Abhinav
   Pearce, Olga
   Isaacs, Katherine E.
TI Design Concerns for Integrated Scripting and Interactive Visualization
   in Notebook Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computational notebooks; exploratory data analysis; hybrid
   visualization-scripting; interactive data analysis; visualization
   design; Computational notebooks; exploratory data analysis; hybrid
   visualization-scripting; interactive data analysis; visualization design
ID TOOLS
AB Interactive visualization can support fluid exploration but is often limited to predetermined tasks. Scripting can support a vast range of queries but may be more cumbersome for free-form exploration. Embedding interactive visualization in scripting environments, such as computational notebooks, provides an opportunity to leverage the strengths of both direct manipulation and scripting. We investigate interactive visualization design methodology, choices, and strategies under this paradigm through a design study of calling context trees used in performance analysis, a field which exemplifies typical exploratory data analysis workflows with Big Data and hard to define problems. We first produce a formal task analysis assigning tasks to graphical or scripting contexts based on their specificity, frequency, and suitability. We then design a notebook-embedded interactive visualization and validate it with intended users. In a follow-up study, we present participants with multiple graphical and scripting interaction modes to elicit feedback about notebook-embedded visualization design, finding consensus in support of the interaction model. We report and reflect on observations regarding the process and design implications for combining visualization and scripting in notebooks.
C1 [Scully-Allison, Connor; Isaacs, Katherine E.] Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.
   [Lumsden, Ian] Univ Tennessee, Knoxville, TN 37996 USA.
   [Taufer, Michela] Univ Tennessee, Dept Elect Engn & Comp Sci, Knoxville, TN 37996 USA.
   [Williams, Katy] Davidson Coll, Math & Comp Sci, Davidson, NC 28035 USA.
   [Bartels, Jesse] Rincon Res, Tucson, AZ 85711 USA.
   [Brink, Stephanie; Pearce, Olga] Larence Livermore Natl Lab, Livermore, CA 94550 USA.
   [Bhatele, Abhinav] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
   [Bhatele, Abhinav] Univ Maryland, Parallel Software & Syst Grp, College Pk, MD 20742 USA.
C3 Utah System of Higher Education; University of Utah; University of
   Tennessee System; University of Tennessee Knoxville; University of
   Tennessee System; University of Tennessee Knoxville; Davidson College;
   University System of Maryland; University of Maryland College Park;
   University System of Maryland; University of Maryland College Park
RP Scully-Allison, C (corresponding author), Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.
EM cscullyallison@sci.utah.edu; ilumsden@utk.edu; kawilliams@davidson.edu;
   jessebartels@email.arizona.edu; taufer@utk.edu; brink2@llnl.gov;
   bhatele@cs.umd.edu; pearce8@llnl.gov; kisaacs@sci.utah.edu
OI Lumsden, Ian/0000-0003-0009-5487; Taufer, Michela/0000-0002-0031-6377;
   Bhatele, Abhinav/0000-0003-3069-3701; Bartels,
   Jesse/0009-0000-1859-3404; Pearce, Olga/0000-0002-1904-9627; Williams,
   Katy/0000-0003-0864-1446; Isaacs, Katherine/0000-0002-9947-928X; Brink,
   Stephanie/0000-0002-1458-8453
FU U.S. Department of Energy by Lawrence Livermore National Laboratory
   [DE-AC52-07NA27344]; United States Department of Defense through DTIC
   [FA8075-14-D-002-007]; National Science Foundation [NSF IIS-1844573,
   IIS-2324465]; Department of Energy [DE-SC0022044, DE-SC0024635,
   LLNL-JRNL-859074]; U.S. Department of Energy (DOE) [DE-SC0022044,
   DE-SC0024635] Funding Source: U.S. Department of Energy (DOE)
FX This work was supported in part by the U.S. Department of Energy by
   Lawrence Livermore National Laboratory under Grant DE-AC52-07NA27344, in
   part the United States Department of Defense through DTIC under Grant
   FA8075-14-D-002-007, in part by National Science Foundation under Grants
   NSF IIS-1844573 and IIS-2324465, and in part by the Department of Energy
   under Grants DE-SC0022044 and DE-SC0024635 and LLNL-JRNL-859074.
NR 56
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6572
EP 6585
DI 10.1109/TVCG.2024.3354561
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000012
PM 38236684
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Wei, GS
   Pan, H
   Zhuang, SJ
   Zhou, YF
   Li, CJ
AF Wei, Guangshun
   Pan, Hao
   Zhuang, Shaojie
   Zhou, Yuanfeng
   Li, Changjian
TI iPUNet: Iterative Cross Field Guided Point Cloud Upsampling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Arbitrary ratios; cross field; flow-guided upsampling; iterative
   refinement; point cloud; sharp feature alignment
AB Point clouds acquired by 3D scanning devices are often sparse, noisy, and non-uniform, causing a loss of geometric features. To facilitate the usability of point clouds in downstream applications, given such input, we present a learning-based point upsampling method, i.e., iPUNet, which generates dense and uniform points at arbitrary ratios and better captures sharp features. To generate feature-aware points, we introduce cross fields that are aligned to sharp geometric features by self-supervision to guide point generation. Given cross field defined frames, we enable arbitrary ratio upsampling by learning at each input point a local parameterized surface. The learned surface consumes the neighboring points and 2D tangent plane coordinates as input, and maps onto a continuous surface in 3D where arbitrary ratios of output points can be sampled. To solve the non-uniformity of input points, on top of the cross field guided upsampling, we further introduce an iterative strategy that refines the point distribution by moving sparse points onto the desired continuous 3D surface in each iteration. Within only a few iterations, the sparse points are evenly distributed and their corresponding dense samples are more uniform and better capture geometric features. Through extensive evaluations on diverse scans of objects and scenes, we demonstrate that iPUNet is robust to handle noisy and non-uniformly distributed inputs, and outperforms state-of-the-art point cloud upsampling methods.
C1 [Wei, Guangshun; Zhuang, Shaojie; Zhou, Yuanfeng] Shandong Univ, Sch Software, Jinan 250101, Shandong, Peoples R China.
   [Pan, Hao] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Li, Changjian] Univ Edinburgh, Sch Informat, Edinburgh EH8 9AB, Scotland.
C3 Shandong University; Microsoft; Microsoft China; Microsoft Research
   Asia; University of Edinburgh
RP Zhou, YF (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Shandong, Peoples R China.
EM guangshunwei@gmail.com; haopan@microsoft.com; sjzhuang@mail.sdu.edu.cn;
   yfzhou@sdu.edu.cn; changjian.li@ed.ac.uk
RI Zhou, Yuanfeng/AAT-4670-2020; Zhuang, Shaojie/JEF-3178-2023
OI Li, Changjian/0000-0003-0448-4957; Wei, Guangshun/0000-0002-6045-4392;
   PAN, Hao/0000-0003-3628-9777; Zhuang, Shaojie/0000-0003-3973-3925
FU School of Informatics; National Natural Science Foundation of China
   [U1909210, 62172257]
FX The work of Changjian Li was supported in part by the start-up grant
   from the School of Informatics. This work was supported in part by the
   National Natural Science Foundation of China under Grants U1909210 and
   62172257. Recommended for acceptance by Z. Deng.
NR 44
TC 0
Z9 0
U1 6
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6089
EP 6103
DI 10.1109/TVCG.2023.3324924
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000022
PM 37856273
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Yang, MX
   Svirsky, Y
   Cheng, ZL
   Sharf, A
AF Yang, Mingxin
   Svirsky, Yonatan
   Cheng, Zhanglin
   Sharf, Andrei
TI Self-Supervised Fragment Alignment With Gaps
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Generative adversarial networks; Neural networks;
   Visualization; Training; Noise measurement; Image restoration; Fragment
   registration; fragment alignment; deep learning
AB Image alignment and registration methods typically rely on visual correspondences across common regions and boundaries to guide the alignment process. Without them, the problem becomes significantly more challenging. Nevertheless, in real world, image fragments may be corrupted with no common boundaries and little or no overlap. In this work, we address the problem of learning the alignment of image fragments with gaps (i.e., without common boundaries or overlapping regions). Our setting is unsupervised, having only the fragments at hand with no ground truth to guide the alignment process. This is usually the situation in the restoration of unique archaeological artifacts such as frescoes and mosaics. Hence, we suggest a self-supervised approach utilizing self-examples which we generate from the existing data and then feed into an adversarial neural network. Our idea is that available information inside fragments is often sufficiently rich to guide their alignment with good accuracy. Following this observation, our method splits the initial fragments into sub-fragments yielding a set of aligned pieces. Thus, sub-fragmentation allows exposing new alignment relations and revealing inner structures and feature statistics. In fact, the new sub-fragments construct true and false alignment relations between fragments. We feed this data to a spatial transformer GAN which learns to predict the alignment between fragments gaps. We test our technique on various synthetic datasets as well as large scale frescoes and mosaics. Results demonstrate our method's capability to learn the alignment of deteriorated image fragments in a self-supervised manner, by examining inner image statistics for both synthetic and real data.
C1 [Yang, Mingxin; Cheng, Zhanglin] Chinese Acad Sci, Shenzhen Inst Adv Technol SIAT, Shenzhen Key Lab Visual Comp & Analyt VisuCA, Shenzhen 518055, Peoples R China.
   [Svirsky, Yonatan; Sharf, Andrei] Bengurion Univ, Beer Sheva 84105, Israel.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS
RP Cheng, ZL (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol SIAT, Shenzhen Key Lab Visual Comp & Analyt VisuCA, Shenzhen 518055, Peoples R China.
EM yangmingxin18@mails.ucas.edu.cn; svirskyy@post.bgu.ac.il;
   zhanglin.cheng@gmail.com; asharf@gmail.com
RI Cheng, Zhanglin/AAP-1760-2021; Sharf, Andrei/F-1370-2012
OI Sharf, Andrei/0000-0002-3963-4508; Cheng, Zhanglin/0000-0002-3360-2679
FU NSFC [U21A20515, 61972388]; National Key R&D Program of China
   [2022ZD0160801]; Shenzhen Science and Technology Program
   [GJHZ20210705141402008]
FX This work was supported in part by NSFC under Grants U21A20515 and
   61972388, in part by National Key R&D Program of China under Grant
   2022ZD0160801, and in part by Shenzhen Science and Technology Program
   under Grant GJHZ20210705141402008.
NR 53
TC 0
Z9 0
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6235
EP 6246
DI 10.1109/TVCG.2023.3330859
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000024
PM 37938966
DA 2025-03-07
ER

PT J
AU Zhang, Y
   Liu, JS
   Lai, CF
   Zhou, Y
   Chen, SM
AF Zhang, Yang
   Liu, Jisheng
   Lai, Chufan
   Zhou, Yuan
   Chen, Siming
TI Interpreting High-Dimensional Projections With Capacity
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Distortion; Measurement; Distortion measurement; Data visualization;
   Statistical analysis; Clustering algorithms; Visual analytics;
   Dimensional reduction; interactive visual exploration; projection
   algorithm evaluation
ID VISUAL ANALYSIS; VISUALIZATION; TOPOLOGY
AB Dimensionality reduction (DR) algorithms are diverse and widely used for analyzing high-dimensional data. Various metrics and tools have been proposed to evaluate and interpret the DR results. However, most metrics and methods fail to be well generalized to measure any DR results from the perspective of original distribution fidelity or lack interactive exploration of DR results. There is still a need for more intuitive and quantitative analysis to interactively explore high-dimensional data and improve interpretability. We propose a metric and a generalized algorithm-agnostic approach based on the concept of capacity to evaluate and analyze the DR results. Based on our approach, we develop a visual analytic system HiLow for exploring high-dimensional data and projections. We also propose a mixed-initiative recommendation algorithm that assists users in interactively DR results manipulation. Users can compare the differences in data distribution after the interaction through HiLow. Furthermore, we propose a novel visualization design focusing on quantitative analysis of differences between high and low-dimensional data distributions. Finally, through user study and case studies, we validate the effectiveness of our approach and system in enhancing the interpretability of projections and analyzing the distribution of high and low-dimensional data.
C1 [Zhang, Yang; Zhou, Yuan; Chen, Siming] Fudan Univ, Sch Data Sci, Shanghai 200437, Peoples R China.
   [Liu, Jisheng] Fudan Univ, Sch Math Sci, Shanghai, Peoples R China.
   [Lai, Chufan] Peking Univ, Beijing 100871, Peoples R China.
C3 Fudan University; Fudan University; Peking University
RP Chen, SM (corresponding author), Fudan Univ, Sch Data Sci, Shanghai 200437, Peoples R China.
EM yang_zhang21@m.fudan.edu.cn; 19307130217@fudan.edu.cn;
   chufan.lai.1990@gmail.com; yuanzhou@fudan.edu.cn;
   simingchen@fudan.edu.cn
RI Chen, Siming/AAK-1874-2020
FU National Natural Science Foundation of China (NSFC) [62202105]; Shanghai
   Municipal Science and Technology Major Project [2018SHZDZX01,
   2021SHZDZX0103]; Sailing Program [21YF1402900]; ZJLab;  [21ZR1403300]
FX This work was supported in part by the National Natural Science
   Foundation of China (NSFC) under Grant 62202105, in part by Shanghai
   Municipal Science and Technology Major Project under Grants
   2018SHZDZX01, and 2021SHZDZX0103, in part by General Program under Grant
   21ZR1403300,in part by Sailing Program under Grant 21YF1402900, and in
   part by ZJLab.
NR 52
TC 0
Z9 0
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6038
EP 6055
DI 10.1109/TVCG.2023.3324851
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000013
PM 37938967
DA 2025-03-07
ER

PT J
AU Miura, S
   Fukumoto, R
   Okamura, N
   Fujie, MG
   Sugano, S
AF Miura, Satoshi
   Fukumoto, Ryota
   Okamura, Naomi
   Fujie, Masakatsu G.
   Sugano, Shigeki
TI Visual Illusion Created by a Striped Pattern Through Augmented Reality
   for the Prevention of Tumbling on Stairs
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Stairs; Foot; Visualization; Augmented reality; Trajectory; Resists;
   Training; Human-computer interaction; user interfaces; virtual and
   augmented reality; virtual device interfaces
ID MINIMUM FOOT CLEARANCE; BLURRING VISION; FALLS; RISK; AGE; WALKING;
   MODEL; GAIT; CLASSIFICATION; PREDICTION
AB A fall on stairs can be a dangerous accident. An important indicator of falling risk is the foot clearance, which is the height of the foot when ascending stairs or the distance of the foot from the step when descending. We developed an augmented reality system with a holographic lens using a visual illusion to improve the foot clearance on stairs. The system draws a vertical striped pattern on the stair riser as the participant ascends the stairs to create the illusion that the steps are higher than the actual steps, and draws a horizontal striped pattern on the stair tread as the participant descends the stairs to create the illusion of narrower stairs. We experimentally evaluated the accuracy of the system and fitted a model to determine the appropriate stripe thickness. Finally, participants ascended and descended stairs before, during, and after using the augmented reality system. The foot clearance significantly improved, not only while the participants used the system but also after they used the system compared with before.
C1 [Miura, Satoshi] Tokyo Inst Technol, Dept Mech Engn, Meguro Ku, Tokyo 1528550, Japan.
   [Fukumoto, Ryota] Waseda Univ, Dept Modern Mech Engn, Tokyo 1698050, Japan.
C3 Institute of Science Tokyo; Tokyo Institute of Technology; Waseda
   University
RP Miura, S (corresponding author), Tokyo Inst Technol, Dept Mech Engn, Meguro Ku, Tokyo 1528550, Japan.
EM miura.s.aj@m.titech.ac.jp; fukuryo@toki.waseda.jp;
   n-okamura@toki.waseda.jp; mgfujie@waseda.jp; sugano@waseda.jp
OI Miura, Satoshi/0000-0001-5402-8074
FU JSPS KAKENHI [21K18075]; Murata Foundation; Hattori Foundation; Takano
   Foundation; Suzuki Foundation; Nakajima Foundation
FX This work was supported in part by JSPS KAKENHI under Grant 21K18075, in
   part by the Murata Foundation, in part by the Hattori Foundation, in
   part by the Takano Foundation, in part by the Suzuki Foundation; and in
   part by the Nakajima Foundation.
NR 51
TC 0
Z9 0
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5466
EP 5477
DI 10.1109/TVCG.2023.3295425
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400007
PM 37450363
DA 2025-03-07
ER

PT J
AU Wu, HS
   Ma, ZH
   Wu, WL
   Liu, XT
   Li, CZ
   Wen, ZK
AF Wu, Huisi
   Ma, Ziheng
   Wu, Wenliang
   Liu, Xueting
   Li, Chengze
   Wen, Zhenkun
TI Shading-Guided Manga Screening From Reference
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Line drawing shading; manga screening; screentone generation
ID IMAGE; QUALITY
AB Manga screening is a critical process in manga production, which still requires intensive labor and cost. Existing manga screening methods either generate simple dotted screentones only or rely on color information and manual hints during screentone selection. Due to the large domain gap between line drawings and screened manga, and the difficulties in generating high-quality, properly selected and shaded screentones, even state-of-the-art deep learning methods cannot convert line drawings to screened manga well. Besides, ambiguity exists in the screening process since different artists may screen differently for the same line drawing. In this article, we propose to introduce shaded line drawing as the intermediate counterpart of the screened manga so that the manga screening task can be decomposed into two sub-tasks, generating shading from a line drawing and replacing shading with proper screentones. The reference image is adopted to resolve the ambiguity issue and provides options and controls on the generated screened manga. We proposed a reference-based shading generation network and a reference-based screentone generation module to achieve the two sub-tasks individually. We conduct extensive visual and quantitative experiments to verify the effectiveness of our system. Results and statistics show that our method outperforms existing methods on the manga screening task.
C1 [Wu, Huisi; Ma, Ziheng; Wu, Wenliang; Liu, Xueting; Wen, Zhenkun] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Guangdong, Peoples R China.
   [Li, Chengze] Caritas Inst Higher Educ, Sch Comp & Informat Sci, Hong Kong 999077, Peoples R China.
C3 Shenzhen University; Saint Francis University Hong Kong
RP Wu, HS (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Guangdong, Peoples R China.
EM hswu@szu.edu.cn; 2070276193@email.szu.edu.cn;
   wenliangwu2019@email.szu.edu.cn; xtliu@szu.edu.cn; czli@cihe.edu.hk;
   wenzk@szu.edu.cn
RI ; Li, Chengze/AAU-7168-2021
OI Wu, Huisi/0000-0002-0399-9089; Ma, Ziheng/0000-0002-2628-553X; Li,
   Chengze/0000-0002-1519-750X
FU National Natural Science Foundation of China [61973221, 62002232,
   62273241]; Natural Science Foundation of Guangdong Province, China
   [2019A1515011165]; Major Project of the New Generation of Artificial
   Intelligence [2018AAA0102900]
FX No Statement Available
NR 62
TC 0
Z9 0
U1 2
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4941
EP 4954
DI 10.1109/TVCG.2023.3282223
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400012
PM 37267131
DA 2025-03-07
ER

PT J
AU Qiu, JX
   Yin, ZX
   Cheng, MM
   Ren, B
AF Qiu, Jiaxiong
   Yin, Ze-Xin
   Cheng, Ming-Ming
   Ren, Bo
TI NeRC: Rendering Planar Caustics by Learning Implicit Neural
   Representations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Caustics; centering calibration; photon mapping; sine activation; volume
   rendering
ID GLOBAL ILLUMINATION
AB Caustics are challenging light transport effects for photo-realistic rendering. Photon mapping techniques play a fundamental role in rendering caustics. However, photon mapping methods render single caustics under the stationary light source in a fixed scene view. They require significant storage and computing resources to produce high-quality results. In this paper, we propose efficiently rendering more diverse caustics of a scene with the camera and the light source moving. We present a novel learning-based volume rendering approach with implicit representations for our proposed task. Considering the variety of materials and textures of planar caustic receivers, we decompose the output appearance into two components: the diffuse and specular parts with a probabilistic module. Unlike NeRF, we construct weights for rendering each component from the implicit signed distance function (SDF). Moreover, we introduce the centering calibration and the sine activation function to improve the performance of the color prediction network. Extensive experiments on the synthetic and real-world datasets illustrate that our method achieves much better performance than baselines in the quantitative and qualitative comparison, for rendering caustics in novel views with the dynamic light source. Especially, our method outperforms the baseline on the temporal consistency across frames.
C1 [Qiu, Jiaxiong; Yin, Ze-Xin; Cheng, Ming-Ming; Ren, Bo] Nankai Univ, Coll Comp Sci, VCIP, Tianjin 300000, Peoples R China.
C3 Nankai University
RP Ren, B (corresponding author), Nankai Univ, Coll Comp Sci, VCIP, Tianjin 300000, Peoples R China.
EM qiujiaxiong727@gmail.com; Zexin.Yin.cn@gmail.com; cmm@nankai.edu.cn;
   rb@nankai.edu.cn
RI Qiu, Jiaxiong/KDM-8471-2024; Cheng, Ming-Ming/A-2527-2009
OI Qiu, Jiaxiong/0000-0002-6065-7296; Cheng, Ming-Ming/0000-0001-5550-8758;
   Yin, Ze-Xin/0009-0004-1478-7868
FU National Key Research and Development Program of China [2018AAA0100400];
   NSFC [61922046, 62132012]
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2018AAA0100400, in part by the NSFC under
   Grant 61922046, and in part by the NSFC under Grant 62132012.
NR 36
TC 0
Z9 0
U1 5
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4339
EP 4348
DI 10.1109/TVCG.2023.3259382
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700072
PM 37030762
DA 2025-03-07
ER

PT J
AU Rogha, M
   Sah, S
   Karduni, A
   Markant, D
   Dou, WW
AF Rogha, Milad
   Sah, Subham
   Karduni, Alireza
   Markant, Douglas
   Dou, Wenwen
TI The Impact of Elicitation and Contrasting Narratives on Engagement,
   Recall and Attitude Change With News Articles Containing Data
   Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Market research; Visualization; Uncertainty; Data
   models; Correlation; Attitude control; Belief elicitation; visual
   elicitation; data visualization; contrasting narratives
ID COGNITION; BIASES; POWER
AB News articles containing data visualizations play an important role in informing the public on issues ranging from public health to politics. Recent research on the persuasive appeal of data visualizations suggests that prior attitudes can be notoriously difficult to change. Inspired by an NYT article, we designed two experiments to evaluate the impact of elicitation and contrasting narratives on attitude change, recall, and engagement. We hypothesized that eliciting prior beliefs leads to more elaborative thinking that ultimately results in higher attitude change, better recall, and engagement. Our findings revealed that visual elicitation leads to higher engagement in terms of feelings of surprise. While there is an overall attitude change across all experiment conditions, we did not observe a significant effect of belief elicitation on attitude change. With regard to recall error, while participants in the draw trend elicitation exhibited significantly lower recall error than participants in the categorize trend condition, we found no significant difference in recall error when comparing elicitation conditions to no elicitation. In a follow-up study, we added contrasting narratives with the purpose of making the main visualization (communicating data on the focal issue) appear strikingly different. Compared to the results of Study 1, we found that contrasting narratives improved engagement in terms of surprise and interest but interestingly resulted in higher recall error and no significant change in attitude. We discuss the effects of elicitation and contrasting narratives in the context of topic involvement and the strengths of temporal trends encoded in the data visualization.
C1 [Rogha, Milad; Sah, Subham; Dou, Wenwen] Univ North Carolina Charlotte, Dept Comp Sci, Charlotte, NC 28223 USA.
   [Markant, Douglas] Univ North Carolina Charlotte, Dept Psychol Sci, Charlotte, NC 28223 USA.
   [Karduni, Alireza] Simon Fraser Univ, Sch Interact Arts & Technol, Burnaby, BC V5A 1S6, Canada.
C3 University of North Carolina; University of North Carolina Charlotte;
   University of North Carolina; University of North Carolina Charlotte;
   Simon Fraser University
RP Rogha, M (corresponding author), Univ North Carolina Charlotte, Dept Comp Sci, Charlotte, NC 28223 USA.
EM mrogha@charlotte.edu; ssah1@uncc.edu; alireza116@gmail.com;
   dmarkant@charlotte.edu; wdou1@uncc.edu
OI Karduni, Alireza/0000-0001-9719-7513; Rogha, Milad/0000-0002-1464-2157;
   Sah, Subham/0009-0007-9446-6731; Markant, Douglas/0000-0003-0568-2648;
   Dou, Wenwen/0000-0003-0319-9484
FU NSF [CNS-1747785, CNS-2323795]
FX This work was supported in part by the NSF under Grant CNS-1747785 and
   by the NFS under Grant CNS-2323795.
NR 42
TC 1
Z9 1
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4375
EP 4389
DI 10.1109/TVCG.2024.3355884
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700015
PM 38241101
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Lu, M
   Zeng, XF
   Lanir, J
   Sun, XQ
   Li, GZ
   Cohen-Or, D
   Huang, H
AF Lu, Min
   Zeng, Xiangfang
   Lanir, Joel
   Sun, Xiaoqin
   Li, Guozheng
   Cohen-Or, Daniel
   Huang, Hui
TI Sticky Links: Encoding Quantitative Data of Graph Edges
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Encoding; Layout; Shape; Clutter; Graph drawing;
   Uncertainty; Graph visualization; edge drawing; quantitative encoding
ID OF-THE-ART; VISUALIZATION; LAYOUT
AB Visually encoding quantitative information associated with graph links is an important problem in graph visualization. A conventional approach is to vary the thickness of lines to encode the strength of connections in node-link diagrams. In this paper, we present Sticky Links, a novel visual encoding method that draws graph links with stickiness. Taking the metaphor of links with glues, sticky links represent connection strength using spiky shapes, ranging from two broken spikes for weak connections to connected lines for strong connections. We conducted a controlled user study to compare the efficiency and aesthetic appeal of stickiness with conventional thickness encoding. Our results show that stickiness enables more effective and expressive quantitative encoding while maintaining the perception of node connectivity. Participants also found sticky links to be more aesthetic and less visually cluttering than conventional thickness encoding. Overall, our findings suggest that sticky links offer a promising alternative to conventional methods for encoding quantitative information in graphs.
C1 [Lu, Min; Zeng, Xiangfang; Sun, Xiaoqin; Huang, Hui] Shenzhen Univ, Shenzhen 518060, Peoples R China.
   [Lanir, Joel] Univ Haifa, IL-3498838 H_efa, Israel.
   [Li, Guozheng] Beijing Inst Technol, Beijing 100811, Peoples R China.
   [Cohen-Or, Daniel] Tel Aviv Univeristy, IL-6997801 Tel Aviv, Israel.
C3 Shenzhen University; University of Haifa; Beijing Institute of
   Technology
RP Huang, H (corresponding author), Shenzhen Univ, Shenzhen 518060, Peoples R China.
EM lumin.vis@gmail.com; xiangfangzeng15@gmail.com; ylanir@is.haifa.il;
   ssharonqin@gmail.com; guozheng.li@bit.edu.cn; cohenor@gmail.com;
   hhzhiyan@gmail.com
RI Huang, Hui/JGB-1049-2023
OI Li, Guozheng/0000-0001-6663-6712; Huang, Hui/0000-0003-3212-0544
FU NSFC
FX No Statement Available
NR 70
TC 0
Z9 0
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2024
VL 30
IS 6
BP 2968
EP 2980
DI 10.1109/TVCG.2024.3388562
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WC8Z6
UT WOS:001252775500013
PM 38648150
DA 2025-03-07
ER

PT J
AU Wang, JC
   Ma, J
   Zhou, Z
   Xie, X
   Zhang, H
   Wu, YC
   Qu, HM
AF Wang, Jiachen
   Ma, Ji
   Zhou, Zheng
   Xie, Xiao
   Zhang, Hui
   Wu, Yingcai
   Qu, Huamin
TI TacPrint: Visualizing the Biomechanical Fingerprint in Table Tennis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Biomechanics; Sports; Fingerprint recognition; Biological system
   modeling; Data visualization; Machine learning; Feature extraction; Data
   transformation; biomechanical data; machine learning
ID INTERACTIVE VISUALIZATION; RACKET SPORTS; PRESSURE; VIDEO
AB Table tennis is a sport that demands high levels of technical proficiency and body coordination from players. Biomechanical fingerprints can provide valuable insights into players' habitual movement patterns and characteristics, allowing them to identify and improve technical weaknesses. Despite the potential, few studies have developed effective methods for generating such fingerprints. To address this gap, we propose TacPrint, a framework for generating a biomechanical fingerprint for each player. TacPrint leverages machine learning techniques to extract comprehensive features from biomechanics data collected by inertial measurement units (IMU) and employs the attention mechanism to enhance model interpretability. After generating fingerprints, TacPrint provides a visualization system to facilitate the exploration and investigation of these fingerprints. In order to validate the effectiveness of the framework, we designed an experiment to evaluate the model's performance and conducted a case study with the system. The results of our experiment demonstrated the high accuracy and effectiveness of the model. Additionally, we discussed the potential of TacPrint to be extended to other sports.
C1 [Wang, Jiachen; Ma, Ji; Wu, Yingcai] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
   [Zhou, Zheng; Xie, Xiao; Zhang, Hui] Zhejiang Univ, Dept Sports Sci, Hangzhou 310027, Peoples R China.
   [Qu, Huamin] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
C3 Zhejiang University; Zhejiang University; Hong Kong University of
   Science & Technology
RP Xie, X (corresponding author), Zhejiang Univ, Dept Sports Sci, Hangzhou 310027, Peoples R China.
EM wangjiachen@zju.edu.cn; zjumaji@zju.edu.cn; zheng.zhou@zju.edu.cn;
   xxie@zju.edu.cn; zhang_hui@zju.edu.cn; ycwu@zju.edu.cn;
   huamin@cse.ust.hk
RI 张, 智浩/KIC-8136-2024; Wang, Jiachen/KIK-8161-2024
OI , Hui/0000-0003-0601-3905; Wang, Jiachen/0000-0001-9630-9958; Ma,
   Ji/0000-0002-9018-4047
FU NSFC
FX No Statement Available
NR 66
TC 0
Z9 0
U1 9
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2024
VL 30
IS 6
BP 2955
EP 2967
DI 10.1109/TVCG.2024.3388555
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WC8Z6
UT WOS:001252775500010
PM 38619948
DA 2025-03-07
ER

PT J
AU Garzotto, F
   Gianotti, M
   Patti, A
   Pentimalli, F
   Vona, F
AF Garzotto, Franca
   Gianotti, Mattia
   Patti, Alberto
   Pentimalli, Francesca
   Vona, Francesco
TI Empowering Persons with Autism Through Cross-Reality and Conversational
   Agents
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Human computer interaction; Accessibility technologies; Interactive
   learning environments; Virtual reality; Augmented reality;
   Conversational agents
ID AUGMENTED REALITY; ADOLESCENTS; STUDENTS; ADULTS
AB Autism Spectrum Disorder is a neurodevelopmental condition that can affect autonomy and independence. Our research explores the integration of Cross-Reality and Conversational Agents for Autistic persons to improve ability and confidence in everyday life situations. We combine two technologies of the Virtual-Real continuum. User experiences unfold from the simulation of tasks in VR to the execution of similar tasks supported by AR in the real world. A speech-based Conversational Agent is integrated with both VR and AR. It provides contextualized help, promotes generalization, and stimulates users to apply what they learned in the virtual space. The paper presents the approach and describes an empirical study involving 17 young Autistic persons.
C1 [Garzotto, Franca; Gianotti, Mattia; Patti, Alberto; Pentimalli, Francesca; Vona, Francesco] Politecn Milan, Dept Elect Informat & Bioengn, Milan, Italy.
C3 Polytechnic University of Milan
RP Garzotto, F (corresponding author), Politecn Milan, Dept Elect Informat & Bioengn, Milan, Italy.
EM franca.garzotto@polimi.it; mattia.gianotti@polimi.it;
   alberto.patti@polimi.it; francesca.pentimalli@polimi.it;
   francesco.vona@polimi.it
RI Garzotto, Franca/AAQ-7886-2020; Gianotti, Mattia/AAC-2326-2022
OI Patti, Alberto/0000-0002-6871-7417; Vona, Francesco/0000-0003-4558-4989;
   GARZOTTO, Franca/0000-0003-4905-7166; Gianotti,
   Mattia/0000-0001-6035-3367; Pentimalli, Francesca/0000-0002-5052-0560
FU TIM Foundation Program "Liberi di comunicare"
FX No Statement Available
NR 38
TC 1
Z9 1
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2591
EP 2601
DI 10.1109/TVCG.2024.3372110
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400047
PM 38437092
DA 2025-03-07
ER

PT J
AU Ceneda, D
   Collins, C
   El-Assady, M
   Miksch, S
   Tominski, C
   Arleo, A
AF Ceneda, Davide
   Collins, Christopher
   El-Assady, Mennatallah
   Miksch, Silvia
   Tominski, Christian
   Arleo, Alessio
TI A Heuristic Approach for Dual Expert/End-User Evaluation of Guidance in
   Visual Analytics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Guidance; heuristics; evaluation; visual analytics
ID VISUALIZATION; DESIGN; TASKS
AB Guidance can support users during the exploration and analysis of complex data. Previous research focused on characterizing the theoretical aspects of guidance in visual analytics and implementing guidance in different scenarios. However, the evaluation of guidance-enhanced visual analytics solutions remains an open research question. We tackle this question by introducing and validating a practical evaluation methodology for guidance in visual analytics. We identify eight quality criteria to be fulfilled and collect expert feedback on their validity. To facilitate actual evaluation studies, we derive two sets of heuristics. The first set targets heuristic evaluations conducted by expert evaluators. The second set facilitates end-user studies where participants actually use a guidance-enhanced system. By following such a dual approach, the different quality criteria of guidance can be examined from two different perspectives, enhancing the overall value of evaluation studies. To test the practical utility of our methodology, we employ it in two studies to gain insight into the quality of two guidance-enhanced visual analytics solutions, one being a work-in-progress research prototype, and the other being a publicly available visualization recommender system. Based on these two evaluations, we derive good practices for conducting evaluations of guidance in visual analytics and identify pitfalls to be avoided during such studies.
C1 [Ceneda, Davide; Miksch, Silvia; Arleo, Alessio] TU Wien, Vienna, Austria.
   [Collins, Christopher] Ontario Tech Univ, Oshawa, ON, Canada.
   [El-Assady, Mennatallah] Swiss Fed Inst Technol, AI Ctr, Zurich, Switzerland.
   [Tominski, Christian] Univ Rostock, VAC Inst, Rostock, Germany.
C3 Technische Universitat Wien; Swiss Federal Institutes of Technology
   Domain; ETH Zurich; University of Rostock
RP Ceneda, D (corresponding author), TU Wien, Vienna, Austria.
EM davide.ceneda@tuwien.ac.at; christopher.collins@ontariotechu.ca;
   melassady@ai.ethz.ch; miksch@ifs.tuwien.ac.at;
   christian.tominski@uni-rostock.de; alessio.arleo@tuwien.ac.at
RI Tominski, Christian/H-6388-2019; Ceneda, Davide/HTT-2753-2023; Collins,
   Christopher/AAJ-6345-2020; Arleo, Alessio/IRZ-8036-2023
OI Tominski, Christian/0000-0001-7704-355X; Ceneda,
   Davide/0000-0003-1198-567X; Arleo, Alessio/0000-0003-2008-3651; Miksch,
   Silvia/0000-0003-4427-5703; Collins, Christopher/0000-0002-4520-7000;
   El-Assady, Mennatallah/0000-0001-8526-2613
FU Vienna Science and Technology Fund (WWTF)
FX No Statement Available
NR 55
TC 2
Z9 2
U1 2
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 997
EP 1007
DI 10.1109/TVCG.2023.3327152
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500077
PM 37903044
OA hybrid, Green Published, Green Submitted
DA 2025-03-07
ER

PT J
AU Hao, JN
   Shi, Q
   Ye, YL
   Zeng, W
AF Hao, Jianing
   Shi, Qing
   Ye, Yilin
   Zeng, Wei
TI <i>TimeTuner:</i> Diagnosing Time Representations for Time-Series
   Forecasting with Counterfactual Explanations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Time-series forecasting; counterfactual explanation; visual analytics
ID STATE
AB Deep learning (DL) approaches are being increasingly used for time-series forecasting, with many efforts devoted to designing complex DL models. Recent studies have shown that the DL success is often attributed to effective data representations, fostering the fields of feature engineering and representation learning. However, automated approaches for feature learning are typically limited with respect to incorporating prior knowledge, identifying interactions among variables, and choosing evaluation metrics to ensure that the models are reliable. To improve on these limitations, this paper contributes a novel visual analytics framework, namely TimeTuner, designed to help analysts understand how model behaviors are associated with localized correlations, stationarity, and granularity of time-series representations. The system mainly consists of the following two-stage technique: We first leverage counterfactual explanations to connect the relationships among time-series representations, multivariate features and model predictions. Next, we design multiple coordinated views including a partition-based correlation matrix and juxtaposed bivariate stripes, and provide a set of interactions that allow users to step into the transformation selection process, navigate through the feature space, and reason the model performance. We instantiate TimeTuner with two transformation methods of smoothing and sampling, and demonstrate its applicability on real-world time-series forecasting of univariate sunspots and multivariate air pollutants. Feedback from domain experts indicates that our system can help characterize time-series representations and guide the feature engineering processes.
C1 [Hao, Jianing; Shi, Qing; Ye, Yilin; Zeng, Wei] Hong Kong Univ Sci & Technol Guangzhou, Guangzhou, Peoples R China.
   [Ye, Yilin; Zeng, Wei] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology (Guangzhou); Hong Kong
   University of Science & Technology
RP Zeng, W (corresponding author), Hong Kong Univ Sci & Technol Guangzhou, Guangzhou, Peoples R China.
EM jhao768@connect.hkust-gz.edu.cn; brantqshi@hkust-gz.edu.cn;
   yyebd@connect.hkust-gz.edu.cn; weizeng@hkust-gz.edu.cn
RI Ye, Yilin/GRR-8394-2022
FU National Natural Science Foundation of China
FX No Statement Available
NR 67
TC 1
Z9 1
U1 9
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 1183
EP 1193
DI 10.1109/TVCG.2023.3327389
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500034
PM 37883273
DA 2025-03-07
ER

PT J
AU Hung, SH
   Zhang, Y
   Zhang, E
AF Hung, Shih-Hsuan
   Zhang, Yue
   Zhang, Eugene
TI Global Topology of 3D Symmetric Tensor Fields
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Tensors; Structural rings; Three-dimensional displays; Topology; Stress;
   Feature extraction; Data visualization; Tensor field visualization; 3D
   symmetric tensor fields; global tensor field topology; topological
   graphs; degenerate curves; neutral surfaces; wedges and trisectors
ID SURFACES; VISUALIZATION; EXTRACTION; LINES
AB There have been recent advances in the analysis and visualization of 3D symmetric tensor fields, with a focus on the robust extraction of tensor field topology. However, topological features such as degenerate curves and neutral surfaces do not live in isolation. Instead, they intriguingly interact with each other. In this paper, we introduce the notion of topological graph for 3D symmetric tensor fields to facilitate global topological analysis of such fields. The nodes of the graph include degenerate curves and regions bounded by neutral surfaces in the domain. The edges in the graph denote the adjacency information between the regions and degenerate curves. In addition, we observe that a degenerate curve can be a loop and even a knot and that two degenerate curves (whether in the same region or not) can form a link. We provide a definition and theoretical analysis of individual degenerate curves in order to help understand why knots and links may occur. Moreover, we differentiate between wedges and trisectors, thus making the analysis more detailed about degenerate curves. We incorporate this information into the topological graph. Such a graph can not only reveal the global structure in a 3D symmetric tensor field but also allow two symmetric tensor fields to be compared. We demonstrate our approach by applying it to solid mechanics and material science data sets.
C1 [Hung, Shih-Hsuan; Zhang, Yue; Zhang, Eugene] Oregon State Univ, Sch Elect Engn & Comp Sci, Corvalis, OR 97331 USA.
C3 Oregon State University
RP Hung, SH (corresponding author), Oregon State Univ, Sch Elect Engn & Comp Sci, Corvalis, OR 97331 USA.
EM hungsh@oregonstate.edu; zhangyue@oregonstate.edu;
   zhange@eecs.oregonstate.edu
OI Zhang, Eugene/0000-0003-4752-3119; Zhang, Yue/0000-0002-8467-2781
FU NSF
FX No Statement Available
NR 38
TC 0
Z9 0
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 1282
EP 1291
DI 10.1109/TVCG.2023.3326933
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500058
PM 37874708
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Setlur, V
   Correll, M
   Satyanarayan, A
   Tory, M
AF Setlur, Vidya
   Correll, Michael
   Satyanarayan, Arvind
   Tory, Melanie
TI Heuristics for Supporting Cooperative Dashboard Design
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Oral communication; Data visualization; Guidelines; Visualization;
   Maintenance engineering; Surveys; Grounding; Gricean maxims; interactive
   visualization; conversation initiation; grounding; turn-taking; repair
   and refinement
ID ORGANIZATION
AB Dashboards are no longer mere static displays of metrics; through functionality such as interaction and storytelling, they have evolved to support analytic and communicative goals like monitoring and reporting. Existing dashboard design guidelines, however, are often unable to account for this expanded scope as they largely focus on best practices for visual design. In contrast, we frame dashboard design as facilitating an analytical conversation: a cooperative, interactive experience where a user may interact with, reason about, or freely query the underlying data. By drawing on established principles of conversational flow and communication, we define the concept of a cooperative dashboard as one that enables a fruitful and productive analytical conversation, and derive a set of 39 dashboard design heuristics to support effective analytical conversations. To assess the utility of this framing, we asked 52 computer science and engineering graduate students to apply our heuristics to critique and design dashboards as part of an ungraded, opt-in homework assignment. Feedback from participants demonstrates that our heuristics surface new reasons dashboards may fail, and encourage a more fluid, supportive, and responsive style of dashboard design. Our approach suggests several compelling directions for future work, including dashboard authoring tools that better anticipate conversational turn-taking, repair, and refinement and extending cooperative principles to other analytical workflows.
C1 [Setlur, Vidya; Correll, Michael] Tableau Res, Palo Alto, CA 94306 USA.
   [Satyanarayan, Arvind] MIT CSAIL, Cambridge, MA 02139 USA.
   [Tory, Melanie] Northeastern Univ, Boston, MA USA.
C3 Massachusetts Institute of Technology (MIT); Northeastern University
RP Setlur, V (corresponding author), Tableau Res, Palo Alto, CA 94306 USA.
EM vsetlur@tableau.com; m.correll@northeastern.edu; arvindsatya@mit.edu;
   m.tory@northeastern.edu
OI Setlur, Vidya/0000-0003-3722-406X; Satyanarayan,
   Arvind/0000-0001-5564-635X
FU NSF
FX No Statement Available
NR 100
TC 4
Z9 4
U1 4
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 370
EP 380
DI 10.1109/TVCG.2023.3327158
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500019
PM 37883257
OA hybrid, Green Submitted
DA 2025-03-07
ER

PT J
AU dos Anjos, RK
   Walton, D
   Aksit, K
   Friston, S
   Swapp, D
   Steed, A
   Ritschel, T
AF dos Anjos, Rafael Kuffner
   Walton, David
   Aksit, Kaan
   Friston, Sebastian
   Swapp, David
   Steed, Anthony
   Ritschel, Tobias
TI Metameric Inpainting for Image Warping
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Real-time systems; Visualization; Rendering (computer graphics); Neural
   networks; Image color analysis; Task analysis; Visual perception;
   Inpainting; warping; perception; real-time rendering
AB Image-warping, a per-pixel deformation of one image into another, is an essential component in immersive visual experiences such as virtual reality or augmented reality. The primary issue with image warping is disocclusions, where occluded (and hence unknown) parts of the input image would be required to compose the output image. We introduce a new image warping method, Metameric image inpainting - an approach for hole-filling in real-time with foundations in human visual perception. Our method estimates image feature statistics of disoccluded regions from their neighbours. These statistics are inpainted and used to synthesise visuals in real-time that are less noticeable to study participants, particularly in peripheral vision. Our method offers speed improvements over the standard structured image inpainting methods while improving realism over colour-based inpainting such as push-pull. Hence, our work paves the way towards future applications such as depth image-based rendering, 6-DoF 360 rendering, and remote render-streaming.
C1 [dos Anjos, Rafael Kuffner] Univ Leeds, Leeds LS2 9JT, W Yorkshire, England.
   [Walton, David; Aksit, Kaan; Friston, Sebastian; Swapp, David; Steed, Anthony; Ritschel, Tobias] UCL, London WC1E 6BT, England.
C3 University of Leeds; University of London; University College London
RP dos Anjos, RK (corresponding author), Univ Leeds, Leeds LS2 9JT, W Yorkshire, England.
EM r.kuffnerdosanjos@leeds.ac.uk; david.walton.13@ucl.ac.uk;
   kunguz@gmail.com; Sebastian.friston@ucl.ac.uk; d.swapp@ucl.ac.uk;
   a.steed@ucl.ac.uk; t.ritschel@ucl.ac.uk
RI Aksit, Kaan/AAY-6704-2020
OI Walton, David/0000-0001-5879-9714; Swapp, David/0000-0002-9335-8663;
   Kuffner dos Anjos, Rafael/0000-0002-2616-7541; Steed,
   Anthony/0000-0001-9034-3020; AKSIT, KAAN/0000-0002-5934-5500
FU EPSRC/UKRI project [EP/T01346X/1]; EPSRC [EP/T01346X/1] Funding Source:
   UKRI
FX This work was supported by EPSRC/UKRI project under Grant EP/T01346X/1.
NR 53
TC 0
Z9 0
U1 1
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5511
EP 5522
DI 10.1109/TVCG.2022.3216712
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300048
PM 36279345
OA Green Submitted, Green Accepted
DA 2025-03-07
ER

PT J
AU Liu, YY
   Long, CJ
   Zhang, ZX
   Liu, BK
   Zhang, Q
   Yin, BC
   Yang, X
AF Liu, Yuanyuan
   Long, Chengjiang
   Zhang, Zhaoxuan
   Liu, Bokai
   Zhang, Qiang
   Yin, Baocai
   Yang, Xin
TI Explore Contextual Information for 3D Scene Graph Generation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Feature extraction; Three-dimensional displays; Task analysis;
   Visualization; Skeleton; Cognition; Message passing; Context
   exploration; graph skeleton; scene graph generation; scene understanding
ID NETWORK
AB 3D scene graph generation (SGG) has been of high interest in computer vision. Although the accuracy of 3D SGG on coarse classification and single relation label has been gradually improved, the performance of existing works is still far from being perfect for fine-grained and multi-label situations. In this article, we propose a framework fully exploring contextual information for the 3D SGG task, which attempts to satisfy the requirements of fine-grained entity class, multiple relation labels, and high accuracy simultaneously. Our proposed approach is composed of a Graph Feature Extraction module and a Graph Contextual Reasoning module, achieving appropriate information-redundancy feature extraction, structured organization, and hierarchical inferring. Our approach achieves superior or competitive performance over previous methods on the 3DSSG dataset, especially on the relationship prediction sub-task.
C1 [Liu, Yuanyuan; Zhang, Zhaoxuan; Liu, Bokai; Yang, Xin] Dalian Univ Technol, Dept Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
   [Long, Chengjiang] Meta Real Labs, Burlingame, CA 94010 USA.
   [Zhang, Qiang; Yin, Baocai] Dalian Univ Technol, Dept Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
   [Zhang, Qiang] Dalian Univ, Minist Educ, Key Lab Adv Design & Intelligent Comp, Dalian 116622, Peoples R China.
   [Yin, Baocai] Beijing Univ Technol, Fac Informat Technol, Beijing Inst Artificial Intelligence, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100124, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology; Dalian
   University; Beijing University of Technology
RP Yang, X (corresponding author), Dalian Univ Technol, Dept Elect Informat & Elect Engn, Dalian 116024, Peoples R China.
EM Liuyy990415@gmail.com; clong1@meta.com; zhangzx@mail.dlut.edu.cn;
   liubokai2021@mail.dlut.edu.cn; zhangq@dlut.edu.cn; ybc@dlut.edu.cn;
   xinyang@dlut.edu.cn
RI Zhang, Qiang/IWU-5000-2023
OI Zhang, Qiang/0000-0003-3776-9799; , Liu/0000-0002-7326-9655; Zhang,
   Zhaoxuan/0000-0002-9366-859X
FU National Key Research and Development Program of China [2022ZD0210500,
   2021ZD0112400, 2018AAA0102003]; National Natural Science Foundation of
   China [61972067, U21A20491, U1908214]; Innovation Technology Funding of
   Dalian [2020JJ26GX036]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grants 2022ZD0210500, 2021ZD0112400,
   and 2018AAA0102003, in part by the National Natural Science Foundation
   of China under Grant 61972067/U21A20491/U1908214, and in part by the
   Innovation Technology Funding of Dalian unde Grant 2020JJ26GX036.
NR 68
TC 6
Z9 6
U1 5
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5556
EP 5568
DI 10.1109/TVCG.2022.3219451
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300051
PM 36367917
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Huang, F
   Liu, C
   Hsiao, KW
   Kuo, YM
   Chu, HK
   Yang, YL
AF Huang, Fei
   Liu, Chen
   Hsiao, Kai-Wen
   Kuo, Ying-Miao
   Chu, Hung-Kuo
   Yang, Yong-Liang
TI Image-Based OA-Style Paper Pop-Up Design via Mixed-Integer Programming
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Shape; Stability analysis; Computational
   modeling; Geometry; Solid modeling; Optimization; Origami architecture;
   paper pop-up; image-based design; foldable structure; mixed-integer
   programming
ID CARD DESIGN; DEFORMATION; SURFACE
AB Origami architecture (OA) is a fascinating papercraft that involves only a piece of paper with cuts and folds. Interesting geometric structures 'pop up' when the paper is opened. However, manually designing such a physically valid 2D paper pop-up plan is challenging since fold lines must jointly satisfy hard spatial constraints. Existing works on automatic OA-style paper pop-up design all focused on how to generate a pop-up structure that approximates a given target 3D model. This article presents the first OA-style paper pop-up design framework that takes 2D images instead of 3D models as input. Our work is inspired by the fact that artists often use 2D profiles to guide the design process, thus benefited from the high availability of 2D image resources. Due to the lack of 3D geometry information, we perform novel theoretic analysis to ensure the foldability and stability of the resultant design. Based on a novel graph representation of the paper pop-up plan, we further propose a practical optimization algorithm via mixed-integer programming that jointly optimizes the topology and geometry of the 2D plan. We also allow the user to interactively explore the design space by specifying constraints on fold lines. Finally, we evaluate our framework on various images with interesting 2D shapes. Experiments and comparisons exhibit both the efficacy and efficiency of our framework.
C1 [Huang, Fei; Yang, Yong-Liang] Univ Bath, Dept Comp Sci, Bath BA2 7AY, England.
   [Liu, Chen] Meta Facebook Real Labs, Redmond, WA 98052 USA.
   [Hsiao, Kai-Wen; Kuo, Ying-Miao; Chu, Hung-Kuo] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 300, Taiwan.
C3 University of Bath; National Tsing Hua University
RP Yang, YL (corresponding author), Univ Bath, Dept Comp Sci, Bath BA2 7AY, England.
EM huangfei037@hotmail.com; chenliu@wustl.edu; kevin30112@gmail.com;
   jollytreeskuo@gmail.com; hkchu@cs.nthu.edu.tw; y.yang@cs.bath.ac.uk
OI Huang, Fei/0000-0001-5928-4018; Yang, Yong-Liang/0000-0002-8071-5756
FU CAMERA; RCUK Centre for the Analysis of Motion, Entertainment Research
   and Applications [EP/M023281/1, EP/T022523/1]; Ministry of Science and
   Technology of Taiwan [110-2221-E-007-061-MY3, 110-2221-E-007-060-MY3];
   EPSRC [EP/T022523/1, EP/M023281/1] Funding Source: UKRI
FX This work was supported in part by CAMERA, the RCUK Centre for the
   Analysis of Motion, Entertainment Research and Applications under Grants
   EP/M023281/1 and EP/T022523/1, and in part by the Ministry of Science
   and Technology of Taiwan under Grants 110-2221-E-007-061-MY3 and
   110-2221-E-007-060-MY3, and a gift from Adobe.
NR 51
TC 2
Z9 2
U1 1
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2023
VL 29
IS 10
BP 4269
EP 4283
DI 10.1109/TVCG.2022.3189569
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8ZW3
UT WOS:001060356200017
PM 35802544
OA Green Published
DA 2025-03-07
ER

PT J
AU Lin, ZH
   Gu, X
   Li, S
   Hu, ZM
   Wang, GP
AF Lin, Zehui
   Gu, Xiang
   Li, Sheng
   Hu, Zhiming
   Wang, Guoping
TI Intentional Head-Motion Assisted Locomotion for Reducing Cybersickness
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cybersickness; head motion; locomotion; presence; rotation; translation;
   velocity
ID WALKING-IN-PLACE; VIRTUAL-REALITY; SICKNESS; NAVIGATION
AB We present an efficient locomotion technique that can reduce cybersickness through aligning the visual and vestibular induced self-motion illusion. Our locomotion technique stimulates proprioception consistent with the visual sense by intentional head motion, which includes both the head's translational movement and yaw rotation. A locomotion event is triggered by the hand-held controller together with an intended physical head motion simultaneously. Based on our method, we further explore the connections between the level of cybersickness and the velocity of self motion through a series of experiments. We first conduct Experiment 1 to investigate the cybersickness induced by different translation velocities using our method and then conduct Experiment 2 to investigate the cybersickness induced by different angular velocities. Our user studies from these two experiments reveal a new finding on the correlation between translation/angular velocities and the level of cybersickness. The cybersickness is greatest at the lowest velocity using our method, and the statistical analysis also indicates a possible U-shaped relation between the translation/angular velocity and cybersickness degree. Finally, we conduct Experiment 3 to evaluate the performances of our method and other commonly-used locomotion approaches, i.e., joystick-based steering and teleportation. The results show that our method can significantly reduce cybersickness compared with the joystick-based steering and obtain a higher presence compared with the teleportation. These advantages demonstrate that our method can be an optional locomotion solution for immersive VR applications using commercially available HMD suites only.
C1 [Lin, Zehui; Gu, Xiang; Li, Sheng; Hu, Zhiming; Wang, Guoping] Peking Univ, Sch Comp Sci, Beijing 100871, Peoples R China.
C3 Peking University
RP Li, S (corresponding author), Peking Univ, Sch Comp Sci, Beijing 100871, Peoples R China.
EM zehui@pku.edu.cn; gu_xiang@pku.edu.cn; lisheng@pku.edu.cn;
   jimmyhu@pku.edu.cn; wgp@pku.edu.cn
RI wang, guoping/KQU-3394-2024
OI Hu, Zhiming/0000-0002-5105-9753; Li, Sheng/0000-0002-8901-2184; Gu,
   Xiang/0000-0002-8527-9113
FU National Key Ramp;D Program of China [2021YFF0500901]; National Natural
   Science Foundation of China [62172013, 61631001]
FX This work was supported in part by the National Key R & amp;D Program of
   China under Grant 2021YFF0500901 and in part by the National Natural
   Science Foundation of China under Grants 62172013 and 61631001.
NR 73
TC 0
Z9 0
U1 1
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2023
VL 29
IS 8
BP 3458
EP 3471
DI 10.1109/TVCG.2022.3160232
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L3CU5
UT WOS:001022080200002
PM 35298380
DA 2025-03-07
ER

PT J
AU Ebner, C
   Mohr, P
   Langlotz, T
   Peng, YF
   Schmalstieg, D
   Wetzstein, G
   Kalkofen, D
AF Ebner, Christoph
   Mohr, Peter
   Langlotz, Tobias
   Peng, Yifan
   Schmalstieg, Dieter
   Wetzstein, Gordon
   Kalkofen, Denis
TI Off-Axis Layered Displays: Hybrid Direct-View/Near-Eye Mixed Reality
   with Focus Cues
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Two dimensional displays; Resists; Stereo
   image processing; Prototypes; Image color analysis; Lenses; Layered
   display; Direct-view; Near-eye; Mixed Reality; Focus cues
AB This work introduces off-axis layered displays, the first approach to stereoscopic direct-view displays with support for focus cues. Off-axis layered displays combine a head-mounted display with a traditional direct-view display for encoding a focal stack and thus, for providing focus cues. To explore the novel display architecture, we present a complete processing pipeline for the real-time computation and post-render warping of off-axis display patterns. In addition, we build two prototypes using a head-mounted display in combination with a stereoscopic direct-view display, and a more widely available monoscopic direct-view display. In addition we show how extending off-axis layered displays with an attenuation layer and with eye-tracking can improve image quality. We thoroughly analyze each component in a technical evaluation and present examples captured through our prototypes.
C1 [Ebner, Christoph; Mohr, Peter; Schmalstieg, Dieter; Kalkofen, Denis] Graz Univ Technol, Graz, Austria.
   [Langlotz, Tobias] Univ Otago, Dunedin, New Zealand.
   [Peng, Yifan] Univ Hong Kong, Hong Kong, Peoples R China.
   [Wetzstein, Gordon] Stanford Univ, Stanford, CA USA.
   [Kalkofen, Denis] Flinders Univ S Australia, Bedford Pk, Australia.
C3 Graz University of Technology; University of Otago; University of Hong
   Kong; Stanford University; Flinders University South Australia
RP Ebner, C (corresponding author), Graz Univ Technol, Graz, Austria.
EM christoph.ebner@icg.tugraz.at; tobias.langlotz@otago.ac.nz;
   evanpeng@hku.hk; gordonwz@stanford.edu; kalkofen@icg.tugraz.at
RI Peng, Yifan/M-1605-2016; Langlotz, Tobias/LKN-4608-2024
OI Langlotz, Tobias/0000-0003-1275-2026; Ebner,
   Christoph/0000-0002-1449-4780
FU Snap Inc.; Hong Kong UGC Early Career Scheme Fund [27212822]; Marsden
   Fund Council [MFP-UOO2124]
FX This work was partially sponsored by Snap Inc., the Hong Kong UGCEarly
   Career Scheme Fund (27212822), and the Marsden Fund Councilfrom
   Government funding (grant no. MFP-UOO2124).
NR 58
TC 2
Z9 2
U1 1
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2816
EP 2825
DI 10.1109/TVCG.2023.3247077
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D1AQ2
UT WOS:000966122800001
PM 37027729
OA hybrid
DA 2025-03-07
ER

PT J
AU Mullen, JF Jr
   Manocha, D
AF Mullen Jr, James F. F.
   Manocha, Dinesh
TI PACE: Data-Driven Virtual Agent Interaction in Dense and Cluttered
   Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Measurement; Solid modeling; Animation;
   Videos; Geometry; Affordances; Embodied agents; Virtual humans; Human
   Factors
AB We present PACE, a novel method for modifying motion-captured virtual agents to interact with and move throughout dense, cluttered 3D scenes. Our approach changes a given motion sequence of a virtual agent as needed to adjust to the obstacles and objects in the environment. We first take the individual frames of the motion sequence most important for modeling interactions with the scene and pair them with the relevant scene geometry, obstacles, and semantics such that interactions in the agents motion match the affordances of the scene (e.g., standing on a floor or sitting in a chair). We then optimize the motion of the human by directly altering the high-DOF pose at each frame in the motion to better account for the unique geometric constraints of the scene. Our formulation uses novel loss functions that maintain a realistic flow and natural-looking motion. We compare our method with prior motion generating techniques and highlight the benefits of our method with a perceptual study and physical plausibility metrics. Human raters preferred our method over the prior approaches. Specifically, they preferred our method 57.1% of the time versus the state-of-the-art method using existing motions, and 81.0% of the time versus a state-of-the-art motion synthesis method. Additionally, our method performs significantly higher on established physical plausibility and interaction metrics. Specifically, we outperform competing methods by over 1.2% in terms of the non-collision metric and by over 18% in terms of the contact metric. We have integrated our interactive system with Microsoft HoloLens and demonstrate its benefits in real-world indoor scenes. Our project website is available at https://gamma.mnd.edu/pace/
C1 [Mullen Jr, James F. F.; Manocha, Dinesh] Univ Maryland, College Pk, MD 20742 USA.
C3 University System of Maryland; University of Maryland College Park
RP Mullen, JF Jr (corresponding author), Univ Maryland, College Pk, MD 20742 USA.
EM mullenj@umd.edu; dmanocha@umd.edu
OI Mullen, James/0000-0002-4117-1741; Manocha, Dinesh/0000-0001-7047-9801
FU National Science Foundation Graduate Research Fellowship Program [DGE
   1840340]; ARO [W911NF2110026]; U.S. Army Cooperative Agreement
   [W911NF2120076]; U.S. Department of Defense (DOD) [W911NF2110026]
   Funding Source: U.S. Department of Defense (DOD)
FX This material is based upon work supported by the National Science
   Foundation Graduate Research Fellowship Program under Grant No. DGE
   1840340. It is also supported by ARO Grant W911NF2110026 and U.S. Army
   Cooperative Agreement W911NF2120076. Any opinions, findings, and
   conclusions or recommendations expressed in this material are those of
   the author(s) and do not necessarily reflect the views of these funding
   agencies.
NR 72
TC 0
Z9 0
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2536
EP 2546
DI 10.1109/TVCG.2023.3247054
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D1CF8
UT WOS:000966164400001
PM 37027697
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Wang, JL
   Shi, RK
   Zheng, WX
   Xie, WJ
   Kao, DMN
   Liang, HN
AF Wang, Jialin
   Shi, Rongkai
   Zheng, Wenxuan
   Xie, Weijie
   Kao, Dominic
   Liang, Hai-Ning
TI Effect of Frame Rate on User Experience, Performance, and Simulator
   Sickness in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Games; User experience; Visualization; Task analysis; Monitoring;
   Hardware; Virtual reality
AB The refresh rate of virtual reality (VR) head-mounted displays (HMDs) has been growing rapidly in recent years because of the demand to provide higher frame rate content as it is often linked with a better experience. Today's HMDs come with different refresh rates ranging from 20Hz to 180Hz, which determines the actual maximum frame rate perceived by users' naked eyes. VR users and content developers often face a choice because having high frame rate content and the hardware that supports it comes with higher costs and other trade-offs (such as heavier and bulkier HMDs). Both VR users and developers can choose a suitable frame rate if they are aware of the benefits of different frame rates in user experience, performance, and simulator sickness (SS). To our knowledge, limited research on frame rate in VR HMDs is available. In this paper, we aim to fill this gap and report a study with two VR application scenarios that compared four of the most common and highest frame rates currently available (60, 90, 120, and 180 frames per second (fps)) to explore their effect on users' experience, performance, and SS symptoms. Our results show that 120fps is an important threshold for VR. After 120fps, users tend to feel lower SS symptoms without a significant negative effect on their experience. Higher frame rates (e.g., 120 and 180fps) can ensure better user performance than lower rates. Interestingly, we also found that at 60fps and when users are faced with fast-moving objects, they tend to adopt a strategy to compensate for the lack of visual details by predicting or filling the gaps to try to meet the performance needs. At higher fps, users do not need to follow this compensatory strategy to meet the fast response performance requirements.
C1 [Wang, Jialin; Shi, Rongkai; Zheng, Wenxuan; Xie, Weijie; Liang, Hai-Ning] Xian Jiaotong Liverpool Univ, Dept Comp, Suzhou, Peoples R China.
   [Kao, Dominic] Purdue Univ, Dept Comp & Informat Technol, W Lafayette, IN USA.
C3 Xi'an Jiaotong-Liverpool University; Purdue University System; Purdue
   University
RP Liang, HN (corresponding author), Xian Jiaotong Liverpool Univ, Dept Comp, Suzhou, Peoples R China.
EM haining.liang@xjtlu.edu.cn
RI Wang, Jialin/KFS-9745-2024
OI Wang, Jialin/0000-0002-1990-1293; Liang, Hai-Ning/0000-0003-3600-8955
FU Key Program Special Fund at XJTLU [KSF-A-03]; National Natural Science
   Foundation of China [62272396]; XJTLU Research Development Fund
   [RDF-17-01-54, RDF-19-02-47]
FX The authors thank the participants who volunteered their time to join
   the experiment. We also thank the reviewers whose insightful comments
   and suggestions helped improve our paper. This work was partially funded
   by the Key Program Special Fund at XJTLU (#KSF-A-03), National Natural
   Science Foundation of China (#62272396), and XJTLU Research Development
   Fund (#RDF-17-01-54; #RDF-19-02-47).
NR 43
TC 36
Z9 37
U1 5
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2478
EP 2488
DI 10.1109/TVCG.2023.3247057
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C9ZH7
UT WOS:000965410300001
PM 37027727
DA 2025-03-07
ER

PT J
AU Lehman, SM
   Elezovikj, S
   Ling, HB
   Tan, CC
AF Lehman, Sarah M.
   Elezovikj, Semir
   Ling, Haibin
   Tan, Chiu C.
TI ARCHIE plus plus : A Cloud-Enabled Framework for Conducting AR System
   Testing in the Wild
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Testing; Usability; System testing; Market research; User experience;
   Task analysis; System performance; Augmented reality; testing and
   debugging; mobile applications; human-centered computing
ID AUGMENTED REALITY; USABILITY; EDUCATION
AB In this paper, we present ARCHIE++, a testing framework for conducting AR system testing and collecting user feedback in the wild. Our system addresses challenges in AR testing practices by aggregating usability feedback data (collected in situ) with system performance data from that same time period. These data packets can then be leveraged to identify edge cases encountered by testers during unconstrained usage scenarios. We begin by presenting a set of current trends in performing human testing of AR systems, identified by reviewing a selection of recent work from leading conferences in mixed reality, human factors, and mobile and pervasive systems. From the trends, we identify a set of challenges to be faced when attempting to adopt these practices to testing in the wild. These challenges are used to inform the design of our framework, which provides a cloud-enabled and device-agnostic way for AR systems developers to improve their knowledge of environmental conditions and to support scalability and reproducibility when testing in the wild. We then present a series of case studies demonstrating how ARCHIE++ can be used to support a range of AR testing scenarios, and demonstrate the limited overhead of the framework through a series of evaluations. We close with additional discussion on the design and utility of ARCHIE++ under various edge conditions.
C1 [Lehman, Sarah M.; Elezovikj, Semir; Tan, Chiu C.] Temple Univ, Philadelphia, PA 19122 USA.
   [Ling, Haibin] SUNY Stony Brook, Stony Brook, NY 11794 USA.
C3 Pennsylvania Commonwealth System of Higher Education (PCSHE); Temple
   University; State University of New York (SUNY) System; Stony Brook
   University
RP Lehman, SM (corresponding author), Temple Univ, Philadelphia, PA 19122 USA.
EM smlehman@temple.edu; semir@temple.edu; hling@cs.stonybrook.edu;
   cctan@temple.edu
OI Ling, Haibin/0000-0003-4094-8413; Lehman, Sarah/0000-0002-9466-0688
FU NSF [2006665, 2128350, 2128187]
FX Dr. Haibin Ling's work was supported in part by NSF under Grants 2006665
   , 2128350, and 2128187.
NR 49
TC 2
Z9 2
U1 1
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2023
VL 29
IS 4
BP 2102
EP 2116
DI 10.1109/TVCG.2022.3141029
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D9DT5
UT WOS:000971666900015
PM 34990364
DA 2025-03-07
ER

PT J
AU Miyatake, Y
   Hiraki, T
   Iwai, D
   Sato, K
AF Miyatake, Yamato
   Hiraki, Takefumi
   Iwai, Daisuke
   Sato, Kosuke
TI HaptoMapping: Visuo-Haptic Augmented Reality by Embedding
   User-Imperceptible Tactile Display Control Signals in a Projected Image
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Haptic interfaces; Visualization; Spatiotemporal phenomena; Vibrations;
   Visible light communication; Synchronization; Surface roughness;
   Visuo-haptic display; high-speed projection; social haptics
ID PERCEPTION
AB This article proposes HaptoMapping, a projection-based visuo-haptic augmented reality (VHAR) system, that can render visual and haptic content independently and present consistent visuo-haptic sensations on physical surfaces. HaptoMapping controls wearable haptic displays by embedded control signals that are imperceptible to the user in projected images using a pixel-level visible light communication technique. The prototype system is comprised of a high-speed projector and three types of haptic devices-finger worn, stylus, and arm mounted. The finger-worn and stylus devices present vibrotactile sensations to a user's fingertips. The arm-mounted device presents stroking sensations on a user's forearm using arrayed actuators with a synchronized hand projection mapping. We identified that the developed system's maximum latency of haptic from visual sensations was 93.4 ms. We conducted user studies on the latency perception of our VHAR system. The results revealed that the developed haptic devices can present haptic sensations without user-perceivable latencies, and the visual-haptic latency tolerance of our VHAR system was 100, 159, 500 ms for the finger-worn, stylus, and arm-mounted devices, respectively. Another user study with the arm-mounted device discovered that the visuo-haptic stroking system maintained both continuity and pleasantness when the spacing between each substrate was relatively sparse, such as 20 mm, and significantly improved both the continuity and pleasantness at 80 and 150 mm/s when compared to the haptic only stroking system. Lastly, we introduced four potential applications in daily scenes. Our system methodology allows for a wide range of VHAR application design without concern for latency and misalignment effects.
C1 [Miyatake, Yamato; Iwai, Daisuke; Sato, Kosuke] Osaka Univ, Suita, Osaka 5650871, Japan.
   [Hiraki, Takefumi] Univ Tsukuba, Tsukuba, Ibaraki 3058577, Japan.
C3 Osaka University; University of Tsukuba
RP Miyatake, Y (corresponding author), Osaka Univ, Suita, Osaka 5650871, Japan.
EM miyatake@sens.sys.es.osaka-u.ac.jp; hiraki@slis.tsukuba.ac.jp;
   daisuke.iwai@sys.es.osaka-u.ac.jp; sato@sys.es.osaka-u.ac.jp
RI Iwai, Daisuke/R-8174-2019
OI Sato, Kosuke/0000-0003-1429-9990; Iwai, Daisuke/0000-0002-3493-5635;
   Miyatake, Yamato/0000-0003-1241-0114; Hiraki,
   Takefumi/0000-0002-5767-3607
FU JST ACT-X [JPMJAX190O]; JST PRESTO [JPMJPR19J2]; JSPSKAKENHI
   [JP15H05925, JP20H05958]
FX This work was supported in part by JST ACT-X under Grant JPMJAX190O,in
   part by JST PRESTO under Grant JPMJPR19J2, and in part by JSPSKAKENHI
   under Grants JP15H05925 and JP20H05958, Japan.
NR 62
TC 6
Z9 6
U1 1
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2023
VL 29
IS 4
BP 2005
EP 2019
DI 10.1109/TVCG.2021.3136214
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D9DT5
UT WOS:000971666900008
PM 34965211
OA hybrid
DA 2025-03-07
ER

PT J
AU Lan, J
   Zhou, Z
   Wang, JC
   Zhang, H
   Xie, X
   Wu, YC
AF Lan, Ji
   Zhou, Zheng
   Wang, Jiachen
   Zhang, Hui
   Xie, Xiao
   Wu, Yingcai
TI SimuExplorer: Visual Exploration of Game Simulation in Table Tennis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Sports; Markov processes; Visualization; Analytical models; Tools; Task
   analysis; Software; Sports visualization; game simulation; model
   interpretation; etc
ID INTERACTIVE VISUALIZATION; ANALYTICS; MOVEMENT; MODEL; VIDEO
AB We propose SimuExplorer, a visualization system to help analysts explore how player behaviors impact scoring rates in table tennis. Such analysis is indispensable for analysts and coaches, who aim to formulate training plans that can help players improve. However, it is challenging to identify the impacts of individual behaviors, as well as to understand how these impacts are generated and accumulated gradually over the course of a game. To address these challenges, we worked closely with experts who work for a top national table tennis team to design SimuExplorer. The SimuExplorer system integrates a Markov chain model to simulate individual and cumulative impacts of particular behaviors. It then provides flow and matrix views to help users visualize and interpret these impacts. We demonstrate the usefulness of the system with case studies and expert interviews. The experts think highly of the system and have obtained insights into players' behaviors using it.
C1 [Lan, Ji; Wang, Jiachen; Wu, Yingcai] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
   [Zhou, Zheng; Zhang, Hui; Xie, Xiao] Zhejiang Univ, Dept Sport Sci, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Wu, YC (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.; Xie, X (corresponding author), Zhejiang Univ, Dept Sport Sci, Hangzhou 310027, Zhejiang, Peoples R China.
EM lanjizju@zju.edu.cn; zhouzhengzju@zju.edu.cn; wangjiachen@zju.edu.cn;
   zhang_hui@zju.edu.cn; xxie@zju.edu.cn; ycwu@zju.edu.cn
RI Wang, Jiachen/KIK-8161-2024; 张, 智浩/KIC-8136-2024; LAN, JI/M-2006-2018
OI LAN, JI/0000-0002-8658-8620; Wang, Jiachen/0000-0001-9630-9958; ,
   Hui/0000-0003-0601-3905
FU NSFC [62072400]; Zhejiang Provincial Natural Science Foundation
   [LR18F020001]; Collaborative Innovation Center of Artificial
   Intelligence by MOE; Zhejiang Provincial Government (ZJU); Key Research
   Projectof Zhejiang Lab [2021KE0AC02]
FX The work was supported by NSFC under Grant 62072400 Zhejiang Provincial
   Natural Science Foundation under Grant LR18F020001 and the Collaborative
   Innovation Center of Artificial Intelligence by MOE and Zhejiang
   Provincial Government (ZJU). This work was also supported by the Key
   Research Projectof Zhejiang Lab under Grant 2021KE0AC02
NR 54
TC 2
Z9 2
U1 5
U2 26
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2023
VL 29
IS 3
BP 1719
EP 1732
DI 10.1109/TVCG.2021.3130422
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8N3OW
UT WOS:000925059900009
PM 34818191
DA 2025-03-07
ER

PT J
AU Nguyen, DB
   Wu, PR
   Monico, RO
   Chen, GN
AF Nguyen, Duong B.
   Wu, Panruo
   Monico, Rodolfo Ostilla
   Chen, Guoning
TI Dynamic Mode Decomposition for Large-Scale Coherent Structure Extraction
   in Shear Flows
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Flow visualization; shear flows; dynamic mode decomposition
ID PROPER ORTHOGONAL DECOMPOSITION; OF-THE-ART; VISUALIZATION
AB Large-scale structures have been observed in many shear flows which are the fluid generated between two surfaces moving with different velocity. A better understanding of the physics of the structures (especially large-scale structures) in shear flows will help explain a diverse range of physical phenomena and improve our capability of modeling more complex turbulence flows. Many efforts have been made in order to capture such structures; however, conventional methods have their limitations, such as arbitrariness in parameter choice or specificity to certain setups. To address this challenge, we propose to use Multi-Resolution Dynamic Mode Decomposition (mrDMD), for large-scale structure extraction in shear flows. In particular, we show that the slow motion DMD modes are able to reveal large-scale structures in shear flows that also have slow dynamics. In most cases, we find that the slowest DMD mode and its reconstructed flow can sufficiently capture the large-scale dynamics in the shear flows, which leads to a parameter-free strategy for large-scale structure extraction. Effective visualization of the large-scale structures can then be produced with the aid of the slowest DMD mode. To speed up the computation of mrDMD, we provide a fast GPU-based implementation. We also apply our method to some non-shear flows that need not behave quasi-linearly to demonstrate the limitation of our strategy of using the slowest DMD mode. For non-shear flows, we show that multiple modes from different levels of mrDMD may be needed to sufficiently characterize the flow behavior.
C1 [Nguyen, Duong B.; Wu, Panruo; Monico, Rodolfo Ostilla; Chen, Guoning] Univ Houston, Houston, TX 77004 USA.
C3 University of Houston System; University of Houston
RP Nguyen, DB (corresponding author), Univ Houston, Houston, TX 77004 USA.
EM duongnguyenbinh@gmail.com; pwu7@uh.edu; rostilla@central.uh.edu;
   chengu@cs.uh.edu
OI Wu, Panruo/0000-0003-1859-3580; Chen, Guoning/0000-0003-0581-6415
FU NSF [IIS 1553329, OAC 2102761]
FX This work was in part supported by NSF under Grants IIS 1553329 and OAC
   2102761.
NR 57
TC 5
Z9 6
U1 2
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2023
VL 29
IS 2
BP 1531
EP 1544
DI 10.1109/TVCG.2021.3124729
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7M2HO
UT WOS:000906475100019
PM 34727033
DA 2025-03-07
ER

PT J
AU Burns, A
   Xiong, C
   Franconeri, S
   Cairo, A
   Mahyar, N
AF Burns, Alyxander
   Xiong, Cindy
   Franconeri, Steven
   Cairo, Alberto
   Mahyar, Narges
TI Designing With Pictographs: Envision Topics Without Sacrificing
   Understanding
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Taxonomy; Graphics; Bars; Task
   analysis; COVID-19; Infographics; pictographs; design; graph
   comprehension; understanding; casual sensemaking
ID PERCEPTUAL FLUENCY; VISUALIZATION; INFOGRAPHICS; RECALL; GRAPH
AB Past studies have shown that when a visualization uses pictographs to encode data, they have a positive effect on memory, engagement, and assessment of risk. However, little is known about how pictographs affect one's ability to understand a visualization, beyond memory for values and trends. We conducted two crowdsourced experiments to compare the effectiveness of using pictographs when showing part-to-whole relationships. In Experiment 1, we compared pictograph arrays to more traditional bar and pie charts. We tested participants' ability to generate high-level insights following Bloom's taxonomy of educational objectives via 6 free-response questions. We found that accuracy for extracting information and generating insights did not differ overall between the two versions. To explore the motivating differences between the designs, we conducted a second experiment where participants compared charts containing pictograph arrays to more traditional charts on 5 metrics and explained their reasoning. We found that some participants preferred the way that pictographs allowed them to envision the topic more easily, while others preferred traditional bar and pie charts because they seem less cluttered and faster to read. These results suggest that, at least in simple visualizations depicting part-to-whole relationships, the choice of using pictographs has little influence on sensemaking and insight extraction. When deciding whether to use pictograph arrays, designers should consider visual appeal, perceived comprehension time, ease of envisioning the topic, and clutteredness.
C1 [Burns, Alyxander; Mahyar, Narges] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
   [Xiong, Cindy; Franconeri, Steven] Northwestern Univ, Dept Psychol, Evanston, IL 60208 USA.
   [Cairo, Alberto] Univ Miami, Sch Commun, Coral Gables, FL 33146 USA.
C3 University of Massachusetts System; University of Massachusetts Amherst;
   Northwestern University; University of Miami
RP Burns, A (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.
EM alyxanderbur@cs.umass.edu; cxiong@u.northwestern.edu;
   franconeri@northwestern.edu; a.cairo@miami.edu; nmahyar@cs.umass.edu
OI Franconeri, Steven/0000-0001-5244-9764; Xiong Bearfield,
   Cindy/0000-0002-1451-4083; Burns, Alyxander/0000-0002-2784-3011
NR 72
TC 9
Z9 9
U1 2
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4515
EP 4530
DI 10.1109/TVCG.2021.3092680
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400040
PM 34170828
DA 2025-03-07
ER

PT J
AU Wang, QW
   Chen, ZT
   Wang, Y
   Qu, HM
AF Wang, Qianwen
   Chen, Zhutian
   Wang, Yong
   Qu, Huamin
TI A Survey on ML4VIS: Applying Machine Learning Advances to Data
   Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Pipelines; Human computer interaction; Task
   analysis; Guidelines; Data mining; Analytical models; ML4VIS; machine
   learning; data visualization; survey
ID OF-THE-ART; GRAPH LAYOUTS; GENERATION; DESIGN; IMAGES; CHARTS; MODEL
AB Inspired by the great success of machine learning (ML), researchers have applied ML techniques to visualizations to achieve a better design, development, and evaluation of visualizations. This branch of studies, known as ML4VIS, is gaining increasing research attention in recent years. To successfully adapt ML techniques for visualizations, a structured understanding of the integration of ML4VIS is needed. In this article, we systematically survey 88 ML4VIS studies, aiming to answer two motivating questions: "what visualization processes can be assisted by ML?" and "how ML techniques can be used to solve visualization problems? "This survey reveals seven main processes where the employment of ML techniques can benefit visualizations: Data Processing4VIS, Data-VIS Mapping, Insight Communication, Style Imitation, VIS Interaction, VIS Reading, and User Profiling. The seven processes are related to existing visualization theoretical models in an ML4VIS pipeline, aiming to illuminate the role of ML-assisted visualization in general visualizations. Meanwhile, the seven processes are mapped into main learning tasks in ML to align the capabilities of ML with the needs in visualization. Current practices and future opportunities of ML4VIS are discussed in the context of the ML4VIS pipeline and the ML-VIS mapping. While more studies are still needed in the area of ML4VIS, we hope this article can provide a stepping-stone for future exploration. A web-based interactive browser of this survey is available at https://ml4vis.github.io.
C1 [Wang, Qianwen] Harvard Univ, Cambridge, MA 02138 USA.
   [Chen, Zhutian] Univ Calif San Diego, La Jolla, CA 92093 USA.
   [Wang, Yong] Singapore Management Univ, Singapore 188065, Singapore.
   [Qu, Huamin] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn CSE, Hong Kong, Peoples R China.
C3 Harvard University; University of California System; University of
   California San Diego; Singapore Management University; Hong Kong
   University of Science & Technology
RP Wang, QW (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.
EM qianwen_wang@hms.harvard.edu; zhutian@ucsd.edu; yongwang@smu.edu.sg;
   huamin@cse.ust.hk
RI Wang, Qianwen/GRJ-9435-2022; Wang, Yong/HKF-3903-2023
OI Wang, Yong/0000-0002-0092-0793
FU Hong Kong Themebased Research Scheme [T41-709/17N]; Singapore Ministry
   of Education (MOE) Academic Research Fund (AcRF) Tier 1
   [20-C220-SMU-011]
FX This work was supported in part by Hong Kong Themebased Research Scheme
   under Grant T41-709/17N and the Singapore Ministry of Education (MOE)
   Academic Research Fund (AcRF) Tier 1 under Grant 20-C220-SMU-011. The
   authors would like to thank all the anonymous reviewers for their
   constructive comments.
NR 148
TC 46
Z9 50
U1 3
U2 31
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 5134
EP 5153
DI 10.1109/TVCG.2021.3106142
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400081
PM 34437063
OA Green Accepted, Green Submitted
DA 2025-03-07
ER

PT J
AU Kumpf, A
   Stumpfegger, J
   Härtl, PF
   Westermann, R
AF Kumpf, Alexander
   Stumpfegger, Josef
   Haertl, Patrick Fabian
   Westermann, Ruediger
TI Visual Analysis of Multi-Parameter Distributions Across Ensembles of 3D
   Fields
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Three-dimensional displays; Uncertainty; Rendering
   (computer graphics); Visual analytics; Market research; Isosurfaces;
   Ensemble visualization; multi-parameter visualization; 3D rendering;
   distribution comparison; parallel coordinates
ID VISUALIZATION; UNCERTAINTY; EXPLORATION
AB For an ensemble of 3D multi-parameter fields, we present a visual analytics workflow to analyse whether and which parts of a selected multi-parameter distribution is present in all ensemble members. Supported by a parallel coordinate plot, a multi-parameter brush is applied to all ensemble members to select data points with similar multi-parameter distribution. By a combination of spatial sub-division and a covariance analysis of partitioned sub-sets of data points, a tight partition in multi-parameter space with reduced number of selected data points is obtained. To assess the representativeness of the selected multi-parameter distribution across the ensemble, we propose a novel extension of violin plots that can show multiple parameter distributions simultaneously. We investigate the visual design that effectively conveys (dis-)similarities in multi-parameter distributions, and demonstrate that users can quickly comprehend parameter-specific differences regarding distribution shape and representativeness from a side-by-side view of these plots. In a 3D spatial view, users can analyse and compare the spatial distribution of selected data points in different ensemble members via interval-based isosurface raycasting. In two real-world application cases we show how our approach is used to analyse the multi-parameter distributions across an ensemble of 3D fields.
C1 [Kumpf, Alexander; Stumpfegger, Josef; Haertl, Patrick Fabian; Westermann, Ruediger] Tech Univ Munich TUM, Comp Graph & Visualizat Grp, D-80333 Munich, Germany.
C3 Technical University of Munich
RP Kumpf, A (corresponding author), Tech Univ Munich TUM, Comp Graph & Visualizat Grp, D-80333 Munich, Germany.
EM alexander.kumpf@tum.de; josefStumpfegger@outlook.de;
   patrick.haertl@tum.de; westermann@tum.de
OI Westermann, Rudiger/0000-0002-3394-0731; Stumpfegger,
   Josef/0000-0002-2553-184X
FU Transregional Collaborative Research Center SFB/TRR 165 Waves - German
   Research Foundation (DFG)
FX This research has been done within the subproject B5 of the
   Transregional Collaborative Research Center SFB/TRR 165 Waves to Weather
   funded by the German Research Foundation (DFG).
NR 69
TC 3
Z9 3
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2022
VL 28
IS 10
BP 3530
EP 3545
DI 10.1109/TVCG.2021.3061925
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4G5UL
UT WOS:000849261100016
PM 33625986
DA 2025-03-07
ER

PT J
AU Ren, B
   He, W
   Li, CF
   Chen, X
AF Ren, Bo
   He, Wei
   Li, Chen-Feng
   Chen, Xu
TI Incompressibility Enforcement for Multiple-Fluid SPH Using Deformation
   Gradient
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Strain; Computational modeling; Mathematical model; Visualization;
   Solids; Volume measurement; Tensors; Compressible and incompressible
   fluids; multiple-fluid simulation; smoothed particle hydrodynamics;
   deformation gradient
ID PARTICLE; SIMULATION
AB To maintain incompressibility in SPH fluid simulations is important for visual plausibility. However, it remains an outstanding challenge to enforce incompressibility in such recent multiple-fluid simulators as the mixture-model SPH framework. To tackle this problem, we propose a novel incompressible SPH solver, where the compressibility of fluid is directly measured by the deformation gradient. By disconnecting the incompressibility of fluid from the conditions of constant density and divergence-free velocity, the new incompressible SPH solver is applicable to both single- and multiple-fluid simulations. The proposed algorithm can be readily integrated into existing incompressible SPH frameworks developed for single-fluid, and is fully parallelizable on GPU. Applied to multiple-fluid simulations, the new incompressible SPH scheme significantly improves the visual effects of the mixture-model simulation, and it also allows exploitation for artistic controlling.
C1 [Ren, Bo; He, Wei; Chen, Xu] Nankai Univ, Coll Comp Sci, Tianjin 300071, Peoples R China.
   [Li, Chen-Feng] Swansea Univ, Swansea SA2 8PP, W Glam, Wales.
C3 Nankai University; Swansea University
RP Ren, B (corresponding author), Nankai Univ, Coll Comp Sci, Tianjin 300071, Peoples R China.
EM rb@nankai.edu.cn; 2120190362@mail.nankai.edu.cn; c.f.li@swansea.ac.uk;
   cnarutox@mail.nankai.edu.cn
RI ren, bo/IST-0814-2023; He, Wei/ADP-7281-2022; Li, Chenfeng/AFQ-6554-2022
OI Chen, Xu/0000-0002-2632-7576; Li, Chenfeng/0000-0003-0441-211X
FU National Key R&D Program of China [2017YFB1002701]
FX This work was supported by the National Key R&D Program of China under
   Grant 2017YFB1002701. The authors would also like to thank all the
   anonymous reviewers for the inspiring suggestions.
NR 27
TC 5
Z9 6
U1 5
U2 29
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2022
VL 28
IS 10
BP 3417
EP 3427
DI 10.1109/TVCG.2021.3062643
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4G5UL
UT WOS:000849261100008
PM 33646953
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Li, YQ
   Wang, CA
   Hong, J
   Zhu, J
   Guo, J
   Wang, J
   Guo, YW
   Wang, WP
AF Li, Yuanqi
   Wang, Chuan
   Hong, Jing
   Zhu, Jie
   Guo, Jie
   Wang, Jue
   Guo, Yanwen
   Wang, Wenping
TI Video Vectorization via Bipartite Diffusion Curves Propagation and
   Optimization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image color analysis; Optimization; Image resolution; Geometry; Image
   reconstruction; Spatial coherence; Feature extraction; Video;
   vectorization; diffusion curve
ID IMAGE VECTORIZATION; ROBUST; COLOR
AB We propose a new video vectorization approach for converting videos in the raster format to vector representation with the benefits of resolution independence and compact storage. Through classifying extracted curves in each video frame into salient ones and non-salient ones, we introduce a novel bipartite diffusion curves (BDCs) representation in order to preserve both important image features such as sharp boundaries and regions with smooth color variation. This bipartite representation allows us to propagate non-salient curves across frames such that the propagation, in conjunction with geometry optimization and color optimization of salient curves, ensures the preservation of fine details within each frame and across different frames, and meanwhile, achieves good spatial-temporal coherence. Thorough experiments on a variety of videos show that our method is capable of converting videos to the vector representation with low reconstruction errors, low computational cost, and fine details, demonstrating our superior performance over the state of the art. We also show that, when used for video upsampling, our method produces results comparable to video super-resolution.
C1 [Li, Yuanqi; Wang, Chuan; Hong, Jing; Zhu, Jie; Guo, Jie; Guo, Yanwen] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210000, Jiangsu, Peoples R China.
   [Wang, Jue] Megvii Technol, Beijing 100000, Peoples R China.
   [Wang, Wenping] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Nanjing University; University of Hong Kong
RP Guo, J; Guo, YW (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210000, Jiangsu, Peoples R China.
EM yuanqili@smail.nju.edu.cn; wangchuan@megvii.com;
   hongjing@smail.nju.edu.cn; magickuang@126.com; guojie@nju.edu.cn;
   wangjue@megvii.com; ywguo@nju.edu.cn; wenping@cs.hku.hk
RI Wang, Jue/GVU-0480-2022
OI Li, Yuanqi/0000-0003-4100-7471; Wang, Jue/0000-0002-3641-3136
FU National Natural Science Foundation of China [62032011, 61772257,
   61972194]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62032011, 61772257, and 61972194. J.
   Guo and Y. Guo are the co-corresponding authors.
NR 47
TC 2
Z9 2
U1 1
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2022
VL 28
IS 9
BP 3265
EP 3276
DI 10.1109/TVCG.2021.3061131
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3K0HP
UT WOS:000833767700015
PM 33621178
DA 2025-03-07
ER

PT J
AU Zhang, L
   Guo, JW
   Xiao, J
   Zhang, XP
   Yan, DM
AF Zhang, Long
   Guo, Jianwei
   Xiao, Jun
   Zhang, Xiaopeng
   Yan, Dong-Ming
TI Blending Surface Segmentation and Editing for 3D Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Solid modeling; Shape; Surface morphology;
   Clustering algorithms; Trajectory; Partitioning algorithms; Mesh
   segmentation; structure recovery; superfacets; rolling-ball blending
   surface; Markov random field
ID MESH SEGMENTATION; STRUCTURE RECOVERY; RECONSTRUCTION; EXTRACTION
AB Recognizing and fitting shape primitives from underlying 3D models are key components of many computer graphics and computer vision applications. Although a vast number of structural recovery methods are available, they usually fail to identify blending surfaces, which corresponds to small transitional regions among relatively large primary patches. To address this issue, we present a novel approach for automatic segmentation and surface fitting with accurate geometric parameters from 3D models, especially mechanical parts. Overall, we formulate the structural segmentation as a Markov random field (MRF) labeling problem. In contrast to existing techniques, we first propose a new clustering algorithm to build superfacets by incorporating 3D local geometric information. This algorithm extracts the general quadric and rolling-ball blending regions, and improves the robustness of further segmentation. Next, we apply a specially designed MRF framework to efficiently partition the original model into different meaningful patches of known surface types by defining the multilabel energy function on the superfacets. Furthermore, we present an iterative optimization algorithm based on skeleton extraction to fit rolling-ball blending patches by recovering the parameters of the rolling center trajectories and ball radius. Experiments on different complex models demonstrate the effectiveness and robustness of the proposed method, and the superiority of our method is also verified through comparisons with state-of-the-art approaches. We further apply our algorithm in applications such as mesh editing by changing the radius of the rolling balls.
C1 [Zhang, Long; Guo, Jianwei; Xiao, Jun; Zhang, Xiaopeng; Yan, Dong-Ming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
   [Guo, Jianwei; Zhang, Xiaopeng; Yan, Dong-Ming] Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Automation, CAS
RP Xiao, J (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.
EM zzlzlzl001@gmail.com; jianwei.guo@nlpr.ia.ac.cn; xiaojun@ucas.ac.cn;
   Xiaopeng.Zhang@ia.ac.cn; yandongming@gmail.com
OI Guo, Jianwei/0000-0002-3376-1725; Yan, Dong-Ming/0000-0003-2209-2404;
   Xiao, Jun/0000-0002-1799-3948; Zhang, Long/0000-0003-0096-403X; ZHANG,
   Xiaopeng/0000-0002-0092-6474
FU National Key RD Program [2018YFB2100602]; National Natural Science
   Foundation of China [61802406, 61772523, U2003109]; Beijing Natural
   Science Foundation [L182059]; Key Research Program of Frontier Sciences
   CAS [QYZDY-SSW-SYS004]; Strategic Priority Research Program of CAS
   [XDA23090304]; Youth Innovation Promotion Association of CAS [Y201935];
   Open Project Programof State Key Laboratory of Virtual Reality
   Technology and Systems Beihang University [VRLAB2019B02]; Alibaba Group
   through Alibaba Innovative Research Program; Fundamental Research Funds
   for the Central Universities
FX The authors would like to thank anonymous reviewer for their valuable
   comments. This work was supported in part by the National Key R&D
   Program under Grant 2018YFB2100602, the National Natural Science
   Foundation of China under Grants 61802406, 61772523, U2003109, Beijing
   Natural Science Foundation under Grant L182059, the Key Research Program
   of Frontier Sciences CAS under Grant QYZDY-SSW-SYS004, the Strategic
   Priority Research Program of CAS under Grant XDA23090304, the Youth
   Innovation Promotion Association of CAS under Grant Y201935, Open
   Project Programof State Key Laboratory of Virtual Reality Technology and
   Systems Beihang University under Grant VRLAB2019B02, the Alibaba Group
   through Alibaba Innovative Research Program, and the Fundamental
   Research Funds for the Central Universities. Long Zhang and Jianwei Guo
   are joint first authors with equal contribution
NR 75
TC 9
Z9 9
U1 4
U2 31
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2022
VL 28
IS 8
BP 2879
EP 2894
DI 10.1109/TVCG.2020.3045450
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2P6BI
UT WOS:000819823600006
PM 33332272
DA 2025-03-07
ER

PT J
AU Chen, SY
   Zhang, JQ
   Gao, L
   He, Y
   Xia, SH
   Shi, M
   Zhang, FL
AF Chen, Shu-Yu
   Zhang, Jia-Qi
   Gao, Lin
   He, Yue
   Xia, Shihong
   Shi, Min
   Zhang, Fang-Lue
TI Active Colorization for Cartoon Line Drawings
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image color analysis; Image segmentation; Semantics; Feature extraction;
   Shape; Machine learning; Animation; Active learning; line drawing
   colorization; region matching
AB In the animation industry, the colorization of raw sketch images is a vitally important but very time-consuming task. This article focuses on providing a novel solution that semiautomatically colorizes a set of images using a single colorized reference image. Our method is able to provide coherent colors for regions that have similar semantics to those in the reference image. An active-learning-based framework is used to match local regions, followed by mixed-integer quadratic programming (MIQP) which considers the spatial contexts to further refine the matching results. We efficiently utilize user interactions to achieve high accuracy in the final colorized images. Experiments show that our method outperforms the current state-of-the-art deep learning based colorization method in terms of color coherency with the reference image. The region matching framework could potentially be applied to other applications, such as color transfer.
C1 [Chen, Shu-Yu; Gao, Lin; He, Yue; Xia, Shihong] Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.
   [Chen, Shu-Yu; Gao, Lin; He, Yue; Xia, Shihong] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
   [Zhang, Jia-Qi; Shi, Min] North China Elect Power Univ, Beijing 102206, Peoples R China.
   [Zhang, Fang-Lue] Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington 6012, New Zealand.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; North China Electric Power University; Victoria University
   Wellington
RP Gao, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.
EM chenshuyu@ict.ac.cn; zhangjiaqi@ncepu.edu.cn; gaolin@ict.ac.cn;
   heyue@ict.ac.cn; xsh@ict.ac.cn; shi_min@ncepu.edu.cn;
   fanglue.zhang@ecs.vuw.ac.nz
RI Zhang, Jianming/JHU-1268-2023; Gao, Lin/JNF-0375-2023
OI jiaqi, zhang/0000-0002-8482-3666
FU Royal Society Newton Advanced Fellowship [NAF\R2\192151]; National
   Natural Science Foundation of China [61872440]; CCF-Tencent Open Fund;
   Tencent AI Lab Rhino-Bird Focused Research Program [JR202024]; Beijing
   Municipal Natural Science Foundation [L182016]; Youth Innovation
   Promotion Association CAS; Victoria Early-Career Research Excellence
   Award
FX This work was supported by Royal Society Newton Advanced Fellowship (No.
   NAFnR2n192151), National Natural Science Foundation of China (No.
   61872440), CCF-Tencent Open Fund, Tencent AI Lab Rhino-Bird Focused
   Research Program (No. JR202024), Beijing Municipal Natural Science
   Foundation (No. L182016), Youth Innovation Promotion Association CAS and
   Victoria Early-Career Research Excellence Award. The author would like
   to thank Simon Finnie (CMIC, Victoria University ofWellington) for
   proofreading the article. Shu-Yu Chen and Jia-Qi Zhang contributed
   equally to this work.
NR 25
TC 16
Z9 16
U1 4
U2 31
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2022
VL 28
IS 2
BP 1198
EP 1208
DI 10.1109/TVCG.2020.3009949
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XY1KL
UT WOS:000736740300001
PM 32746275
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Trepkowski, C
   Marquardt, A
   Eibich, TD
   Shikanai, Y
   Maiero, J
   Kiyokawa, K
   Kruijff, E
   Schöning, J
   König, P
AF Trepkowski, Christina
   Marquardt, Alexander
   Eibich, Tom David
   Shikanai, Yusuke
   Maiero, Jens
   Kiyokawa, Kiyoshi
   Kruijff, Ernst
   Schoening, Johannes
   Koenig, Peter
TI Multisensory Proximity and Transition Cues for Improving Target
   Awareness in Narrow Field of View Augmented Reality Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Task analysis; Three-dimensional displays; Augmented
   reality; Urban areas; Working environment noise; Search problems;
   Augmented reality; view management; guidance; multisensory cues;
   performance; situation awareness
ID SITUATION AWARENESS; SEARCH PERFORMANCE; AUDITORY CUES; LOCALIZATION;
   ATTENTION; TACTILE; OBJECTS; RESOLUTION; MOVEMENTS; WARNINGS
AB Augmented reality applications allow users to enrich their real surroundings with additional digital content. However, due to the limited field of view of augmented reality devices, it can sometimes be difficult to become aware of newly emerging information inside or outside the field of view. Typical visual conflicts like clutter and occlusion of augmentations occur and can be further aggravated especially in the context of dense information spaces. In this article, we evaluate how multisensory cue combinations can improve the awareness for moving out-of-view objects in narrow field of view augmented reality displays. We distinguish between proximity and transition cues in either visual, auditory or tactile manner. Proximity cues are intended to enhance spatial awareness of approaching out-of-view objects while transition cues inform the user that the object just entered the field of view. In study 1, user preference was determined for 6 different cue combinations via forced-choice decisions. In study 2, the 3 most preferred modes were then evaluated with respect to performance and awareness measures in a divided attention reaction task. Both studies were conducted under varying noise levels. We show that on average the Visual-Tactile combination leads to 63% and Audio-Tactile to 65% faster reactions to incoming out-of-view augmentations than their Visual-Audio counterpart, indicating a high usefulness of tactile transition cues. We further show a detrimental effect of visual and audio noise on performance when feedback included visual proximity cues. Based on these results, we make recommendations to determine which cue combination is appropriate for which application.
C1 [Trepkowski, Christina; Marquardt, Alexander; Eibich, Tom David; Maiero, Jens; Kruijff, Ernst] Bonn Rhein Sieg Univ Appl Sci, D-53757 St Augustin, Germany.
   [Shikanai, Yusuke; Kiyokawa, Kiyoshi] Nara Inst Sci & Technol, Ikoma, Nara 6300192, Japan.
   [Kruijff, Ernst] Simon Fraser Univ, Burnaby, BC V5A 1S6, Canada.
   [Schoening, Johannes] Univ St Gallen, CH-9000 St Gallen, Switzerland.
   [Koenig, Peter] Osnabruck Univ, D-49074 Osnabruck, Germany.
C3 Hochschule Bonn Rhein Sieg; Nara Institute of Science & Technology;
   Simon Fraser University; University of St Gallen; University Osnabruck
RP Trepkowski, C; Marquardt, A (corresponding author), Bonn Rhein Sieg Univ Appl Sci, D-53757 St Augustin, Germany.
EM christina.trepkowski@h-brs.de; alexander.marquardt@h-brs.de;
   tom-david.eibich@h-brs.de; shikanai327327@gmail.com;
   jens.maiero@h-brs.de; kiyo@is.naist.jp; ernst.kruijff@h-brs.de;
   johannes.schoening@unisg.ch; pkoenig@uos.de
RI Schöning, Johannes/AAE-9585-2020; König, Peter/ABB-2380-2020
OI Kruijff, Ernst/0000-0003-1625-0955; Eibich, Tom
   David/0000-0003-4309-9904; Konig, Peter/0000-0003-3654-5267; Shikanai,
   Yusuke/0000-0001-8322-5827; Marquardt, Alexander/0000-0002-2844-0804;
   Schoning, Johannes/0000-0002-8823-4607
NR 124
TC 8
Z9 9
U1 1
U2 24
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2022
VL 28
IS 2
BP 1342
EP 1362
DI 10.1109/TVCG.2021.3116673
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA XY1KL
UT WOS:000736740300011
PM 34591771
DA 2025-03-07
ER

PT J
AU Fujiwara, T
   Wei, XH
   Zhao, J
   Ma, KL
AF Fujiwara, Takanori
   Wei, Xinhai
   Zhao, Jian
   Ma, Kwan-Liu
TI Interactive Dimensionality Reduction for Comparative Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Principal component analysis; Visualization; Optimization; Task
   analysis; Dimensionality reduction; Tools; Libraries; Dimensionality
   reduction; discriminant analysis; contrastive learning; comparative
   analysis; interpretability; visual analytics
ID DISCRIMINANT-ANALYSIS; VISUAL ANALYTICS; VISUALIZATION; CRITERION
AB Finding the similarities and differences between groups of datasets is a fundamental analysis task. For high-dimensional data, dimensionality reduction (DR) methods are often used to find the characteristics of each group. However, existing DR methods provide limited capability and flexibility for such comparative analysis as each method is designed only for a narrow analysis target, such as identifying factors that most differentiate groups. This paper presents an interactive DR framework where we integrate our new DR method, called ULCA (unified linear comparative analysis), with an interactive visual interface. ULCA unifies two DR schemes, discriminant analysis and contrastive learning, to support various comparative analysis tasks. To provide flexibility for comparative analysis, we develop an optimization algorithm that enables analysts to interactively refine ULCA results. Additionally, the interactive visualization interface facilitates interpretation and refinement of the ULCA results. We evaluate ULCA and the optimization algorithm to show their efficiency as well as present multiple case studies using real-world datasets to demonstrate the usefulness of this framework.
C1 [Fujiwara, Takanori; Ma, Kwan-Liu] Univ Calif Davis, Davis, CA 95616 USA.
   [Wei, Xinhai; Zhao, Jian] Univ Waterloo, Waterloo, ON, Canada.
C3 University of California System; University of California Davis;
   University of Waterloo
RP Fujiwara, T (corresponding author), Univ Calif Davis, Davis, CA 95616 USA.
EM tfujiwara@ucdavis.edu; x67wei@uwaterloo.ca; jianzhao@uwaterloo.ca;
   klma@ucdavis.edu
RI Fujiwara, Takanori/AAY-5045-2020
OI Fujiwara, Takanori/0000-0002-6382-2752
FU U.S. National Science Foundation [IIS-1741536]; U.S. National Institute
   of Standards and Technology [70NANB20H197]; Natural Sciences and
   Engineering Research Council of Canada
FX The authors wish to thank Dr. Tzu-Ping Liu at the University of Taipei
   and Samuel Fuller at UC Davis for their guidance for the dataset used in
   Case Study 1. This work is supported in part by the U.S. National
   Science Foundation through grant IIS-1741536, the U.S. National
   Institute of Standards and Technology through grant 70NANB20H197, and
   the Natural Sciences and Engineering Research Council of Canada.
NR 98
TC 21
Z9 23
U1 4
U2 22
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 758
EP 768
DI 10.1109/TVCG.2021.3114807
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA XW3DW
UT WOS:000735505300009
PM 34591765
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Parsons, P
AF Parsons, Paul
TI Understanding Data Visualization Design Practice
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Human computer interaction; Design methodology;
   Visualization; Production; Complexity theory; Scholarships; Design
   practice; data visualization; design methods; design process;
   research-practice relationships
ID INFORMATION VISUALIZATION
AB Professional roles for data visualization designers are growing in popularity, and interest in relationships between the academic research and professional practice communities is gaining traction. However, despite the potential for knowledge sharing between these communities, we have little understanding of the ways in which practitioners design in real-world, professional settings. Inquiry in numerous design disciplines indicates that practitioners approach complex situations in ways that are fundamentally different from those of researchers. In this work, I take a practice-led approach to understanding visualization design practice on its own terms. Twenty data visualization practitioners were interviewed and asked about their design process, including the steps they take, how they make decisions, and the methods they use. Findings suggest that practitioners do not follow highly systematic processes, but instead rely on situated forms of knowing and acting in which they draw from precedent and use methods and principles that are determined appropriate in the moment. These findings have implications for how visualization researchers understand and engage with practitioners, and how educators approach the training of future data visualization designers.
C1 [Parsons, Paul] Purdue Univ, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University
RP Parsons, P (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.
EM parsonsp@purdue.edu
RI Parsons, Paul/C-4190-2013
FU NSF [1755957]; Direct For Computer & Info Scie & Enginr; Div Of
   Information & Intelligent Systems [1755957] Funding Source: National
   Science Foundation
FX The author would like to thank students from the DVC Lab, including
   Ya-Hsin Hung, Ali Baigelenov, Erica Chadwell, Connor Schrank, Ian Carr,
   and Labonno Zaman. The author would also like to thank Colin M. Gray for
   stimulating discussions about design theory, and the anonymous reviewers
   from two venues for providing helpful suggestions on earlier drafts.
   Many thanks are due to the participants in this study, who devoted their
   time to make this research possible. This work was supported in part by
   a grant from NSF (#1755957).
NR 90
TC 10
Z9 12
U1 9
U2 58
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 665
EP 675
DI 10.1109/TVCG.2021.3114959
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000074
PM 34596554
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Wang, YF
   Peng, TQ
   Lu, HH
   Wang, HR
   Xie, X
   Qu, HM
   Wu, YC
AF Wang, Yifang
   Peng, Tai-Quan
   Lu, Huihua
   Wang, Haoren
   Xie, Xiao
   Qu, Huamin
   Wu, Yingcai
TI Seek for Success: A Visualization Approach for Understanding the
   Dynamics of Academic Careers
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Engineering profession; Data visualization; Social factors; Visual
   analytics; Collaboration; Sociology; Sequences; Career Analysis;
   Academic Profiles; Science of Science; Publication Data; Citation Data;
   Sequence Analysis
ID VISUAL ANALYTICS; SCIENCE
AB How to achieve academic career success has been a long-standing research question in social science research. With the growing availability of large-scale well-documented academic profiles and career trajectories, scholarly interest in career success has been reinvigorated, which has emerged to be an active research domain called the Science of Science (i.e., SciSci). In this study, we adopt an innovative dynamic perspective to examine how individual and social factors will influence career success over time. We propose ACSeeker, an interactive visual analytics approach to explore the potential factors of success and how the influence of multiple factors changes at different stages of academic careers. We first applied a Multi-factor Impact Analysis framework to estimate the effect of different factors on academic career success over time. We then developed a visual analytics system to understand the dynamic effects interactively. A novel timeline is designed to reveal and compare the factor impacts based on the whole population. A customized career line showing the individual career development is provided to allow a detailed inspection. To validate the effectiveness and usability of ACSeeker, we report two case studies and interviews with a social scientist and general researchers.
C1 [Wang, Yifang; Lu, Huihua; Wang, Haoren; Wu, Yingcai] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.
   [Wang, Yifang; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Peng, Tai-Quan] Michigan State Univ, E Lansing, MI 48824 USA.
   [Lu, Huihua; Wang, Haoren; Wu, Yingcai] Zhejiang Univ, Dept Sport Sci, Hangzhou, Peoples R China.
C3 Zhejiang University; Hong Kong University of Science & Technology;
   Michigan State University; Zhejiang University
EM yifang.wang@connect.ust.hk; pengtaiq@msu.edu; huihualu@zju.edu.cn;
   haorenwang@zju.edu.cn; xxie@zju.edu.cn; huamin@cse.ust.hk;
   ycwu@zju.edu.cn
RI zhang, hui/GXH-6098-2022; Wang, Yifang/GXH-9767-2022; Peng,
   Tai-Quan/B-3176-2011
OI Wang, Yifang/0000-0001-6267-9440; Peng, Tai-Quan/0000-0002-2588-7491
FU Hong Kong Theme-based Research Scheme [T44-707/16-N]; HKSAR Research
   Grants Council [GRF 11505119]; NSFC [62072400]; Zhejiang Provincial
   Natural Science Foundation [LR18F020001]; Collaborative Innovation
   Center of Artificial Intelligence by MOE; Zhejiang Provincial Government
   (ZJU)
FX This research was supported in part by Hong Kong Theme-based Research
   Scheme grant T44-707/16-N, GRF 11505119 from HKSAR Research Grants
   Council, NSFC (62072400), Zhejiang Provincial Natural Science Foundation
   (LR18F020001), and the Collaborative Innovation Center of Artificial
   Intelligence by MOE and Zhejiang Provincial Government (ZJU).
NR 72
TC 10
Z9 12
U1 2
U2 22
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 475
EP 485
DI 10.1109/TVCG.2021.3114790
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA XU0IB
UT WOS:000733959000056
PM 34587034
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Feldstein, IT
   Ellis, SR
AF Feldstein, Ilja T.
   Ellis, Stephen R.
TI A Simple Video-Based Technique for Measuring Latency in Virtual Reality
   or Teleoperation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Delays; Synchronization; Virtual environments; Visualization; Hardware;
   Cameras; End-to-end latency; frame counting; head-mounted display; HMD;
   human-computer interaction; immersive virtual environment; latency
   measurement; measurement techniques; performance analysis and design
   aids; system delay; virtual reality; VR
ID DELAY; ENVIRONMENTS; SENSE
AB Designers of virtual reality (VR) systems are aware of the need to minimize delays between the user's tracked physical actions and the consequent displayed actions in the virtual environment. Such delays, also referred to as end-to-end latency, are known to degrade user performance and even cause simulator sickness. Though a wide variety of hardware and software design strategies have been used to reduce delays, techniques for measuring and minimizing latency continue to be needed since transmission and switching delays are likely to continue to introduce new sources of latency, especially in wireless mobile environments. This article describes a convenient low-cost technique for measuring end-to-end latencies using a human evaluator and an ordinary consumer camera (e.g., cell phone camera). Since the technique does not depend upon the use of specialized hardware and software, it differs from other methods in that it can easily be used to measure latencies of systems in the specific hardware and software configuration and the relevant performance environments. The achievable measurement accuracy was assessed in an experimental trial. Results indicate a measurement uncertainty below 10 ms. Some refinements to the technique are discussed, which may further reduce the measurement uncertainty to approximately 1 ms.
C1 [Feldstein, Ilja T.] Harvard Med Sch, Dept Oyhthalmol, Boston, MA 02111 USA.
   [Ellis, Stephen R.] NASA, Ames Res Ctr, Moffett Field, CA 94035 USA.
C3 Harvard University; Harvard Medical School; National Aeronautics & Space
   Administration (NASA); NASA Ames Research Center
RP Feldstein, IT (corresponding author), Harvard Med Sch, Dept Oyhthalmol, Boston, MA 02111 USA.
EM feldstein@meei.harvard.edu; ellisstephenr3@gmail.com
FU Fulbright Program; Studienstiftung des Deutschen Volkes
FX The authors would like to express their gratitude to Florian F. R.
   Hanel, who recorded the latency sequences used in the demonstration
   presented here and participated in meaningful discussions. Gratitude is
   also owed to Cecile Boudot for her extensive support with the
   manuscript's diagrams. Ilja T. Feldstein thanks the Fulbright Program
   and the Studienstiftung des Deutschen Volkes for funding his research.
NR 71
TC 4
Z9 7
U1 0
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2021
VL 27
IS 9
BP 3611
EP 3625
DI 10.1109/TVCG.2020.2980527
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TS3CP
UT WOS:000679532000002
PM 32175865
DA 2025-03-07
ER

PT J
AU Raji, M
   Duncan, J
   Hobson, T
   Huang, J
AF Raji, Mohammad
   Duncan, Jeremiah
   Hobson, Tanner
   Huang, Jian
TI Dataless Sharing of Interactive Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Software; Videos; Visualization; Tools; Graphical
   user interfaces; Containers; Shareable visualizations; visualization
   recordings; user interaction
AB Interactive visualization has become a powerful insight-revealing medium. However, the close dependency of interactive visualization on its data inhibits its shareability. Users have to choose between the two extremes of (i) sharing non-interactive dataless formats such as images and videos, or (ii) giving access to their data and software to others with no control over how the data will be used. In this work, we fill the gap between the two extremes and present a new system, called Loom. Loom captures interactive visualizations as standalone dataless objects. Users can interact with Loom objects as if they still have the original software and data that created those visualizations. Yet, Loom objects are completely independent and can therefore be shared online without requiring the data or the visualization software. Loom objects are efficient to store and use, and provide privacy preserving mechanisms. We demonstrate Loom's efficacy with examples of scientific visualization using Paraview, information visualization using Tableau, and journalistic visualization from New York Times.
C1 [Raji, Mohammad; Duncan, Jeremiah; Hobson, Tanner; Huang, Jian] Univ Tennessee, Knoxville, TN 37996 USA.
   [Huang, Jian] Univ Tennessee, Dept Elect Engn & Comp Sci, Knoxville, TN 37996 USA.
C3 University of Tennessee System; University of Tennessee Knoxville;
   University of Tennessee System; University of Tennessee Knoxville
RP Raji, M (corresponding author), Univ Tennessee, Knoxville, TN 37996 USA.
EM mahmadza@vols.utk.edu; jdunca51@vols.utk.edu; thobson2@vols.utk.edu;
   huangj@utk.edu
OI Duncan, Jeremiah/0000-0003-1296-0802; Hobson, Tanner/0000-0002-6269-7881
FU US National Science Foundation [CNS-1629890]; USDI-NPS [P14AC01485]
FX The authors would like to thank the anonymous reviewers of this and
   previous versions of the manuscript for their valuable comments and
   suggestions. The authors were supported in part by US National Science
   Foundation Award CNS-1629890, and USDI-NPS P14AC01485.
NR 46
TC 4
Z9 5
U1 1
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2021
VL 27
IS 9
BP 3656
EP 3669
DI 10.1109/TVCG.2020.2984708
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TS3CP
UT WOS:000679532000005
PM 32746250
OA Bronze
DA 2025-03-07
ER

PT J
AU Alderson, T
   Mahdavi-Amiri, A
   Samavati, F
AF Alderson, Troy
   Mahdavi-Amiri, Ali
   Samavati, Faramarz
TI RIAS: Repeated Invertible Averaging for Surface Multiresolution of
   Arbitrary Degree
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Splines (mathematics); Topology; Tensors; Smoothing methods;
   Terminology; IP networks; Indexes; Geometric modeling; subdivision;
   multiresolution
ID REVERSING SUBDIVISION RULES; B-SPLINE SURFACES; CATMULL-CLARK;
   ALGORITHMS; CURVES
AB In this article, we introduce two local surface averaging operators with local inverses and use them to devise a method for surface multiresolution (subdivision and reverse subdivision) of arbitrary degree. Similar to previous works by Stam, Zorin, and Schroder that achieved forward subdivision only, our averaging operators involve only direct neighbours of a vertex, and can be configured to generalize B-Spline multiresolution to arbitrary topology surfaces. Our subdivision surfaces are hence able to exhibit C-d continuity at regular vertices (for arbitrary values of d) and appear to exhibit C-1 continuity at extraordinary vertices. Smooth reverse and non-uniform subdivisions are additionally supported.
C1 [Alderson, Troy; Samavati, Faramarz] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
   [Mahdavi-Amiri, Ali] Simon Fraser Univ Burnaby, GrUVi Lab, Burnaby, BC V5A 1S6, Canada.
C3 University of Calgary; Simon Fraser University
RP Alderson, T (corresponding author), Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
EM tfalders@ucalgary.ca; amahdavi@sfu.ca; samavati@ucalgary.ca
OI Alderson, Troy/0000-0001-5528-8652
FU NSERC CRD
FX This research was supported by an NSERC CRD with the auhors' industrial
   collaborator, Global Grid Systems (formerly the PYXIS innovation).
NR 26
TC 1
Z9 1
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2021
VL 27
IS 8
BP 3546
EP 3557
DI 10.1109/TVCG.2020.2972877
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TC7PK
UT WOS:000668831500015
PM 32070958
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Liu, YL
   Zheng, CW
   Xu, F
   Tong, X
   Guo, BN
AF Liu, Yilong
   Zheng, Chengwei
   Xu, Feng
   Tong, Xin
   Guo, Baining
TI Data-Driven 3D Neck Modeling and Animation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Neck; Larynx; Facial animation; Strain; Solid modeling;
   Three-dimensional displays; Neck modeling; neck animation; speech-driven
   animation
ID RECONSTRUCTION
AB In this article, we present a data-driven approach for modeling and animation of 3D necks. Our method is based on a new neck animation model that decomposes the neck animation into local deformation caused by larynx motion and global deformation driven by head poses, facial expressions, and speech. A skinning model is introduced for modeling local deformation and underlying larynx motions, while the global neck deformation caused by each factor is modeled by its corrective blendshape set, respectively. Based on this neck model, we introduce a regression method to drive the larynx motion and neck deformation from speech. Both the neck model and the speech regressor are learned from a dataset of 3D neck animation sequences captured from different identities. Our neck model significantly improves the realism of facial animation and allows users to easily create plausible neck animations from speech and facial expressions. We verify our neck model and demonstrate its advantages in 3D neck tracking and animation.
C1 [Liu, Yilong; Zheng, Chengwei; Xu, Feng; Guo, Baining] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Tong, Xin; Guo, Baining] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Tsinghua University; Microsoft; Microsoft China; Microsoft Research Asia
RP Xu, F (corresponding author), Tsinghua Univ, Beijing 100084, Peoples R China.
EM liuyilong.thu@gmail.com; l1l11012@qq.com; xufeng2003@gmail.com;
   xtong@microsoft.com; bainguo@microsoft.com
RI Zheng, Chengwei/HPC-8073-2023
OI Zheng, Chengwei/0000-0002-3657-0297; Tong, Xin/0000-0001-8788-2453; Liu,
   Yilong/0000-0001-8964-8679
FU National Key R&D Program of China [2018YFA0704000]; NSFC [61822111,
   61727808, 61671268]; Beijing Natural Science Foundation [JQ19015,
   L182052]
FX This work was supported by the National Key R&D Program of China
   2018YFA0704000, the NSFC (No.61822111, 61727808, 61671268) and Beijing
   Natural Science Foundation (JQ19015, L182052).
NR 44
TC 6
Z9 7
U1 2
U2 30
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2021
VL 27
IS 7
BP 3226
EP 3237
DI 10.1109/TVCG.2020.2967036
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SK0PE
UT WOS:000655924400010
PM 31944979
DA 2025-03-07
ER

PT J
AU Williams, NL
   Bera, A
   Manocha, D
AF Williams, Niall L.
   Bera, Aniket
   Manocha, Dinesh
TI ARC: Alignment-based Redirection Controller for Redirected Walking in
   Complex Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual Reality; Locomotion; Redirected Walking; Redirection
   Controllers; Steering Algorithms; Alignment
ID INTERFACE
AB We present a novel redirected walking controller based on alignment that allows the user to explore large and complex virtual environments, while minimizing the number of collisions with obstacles in the physical environment. Our alignment-based redirection controller. ARC, steers the user such that their proximity to obstacles in the physical environment matches the proximity to obstacles in the virtual environment as closely as possible. To quantify a controller's performance in complex environments, we introduce a new metric. Complexity Ratio (CR), to measure the relative environment complexity and characterize the difference in navigational complexity between the physical and virtual environments. Through extensive simulation-based experiments, we show that ARC significantly outperforms current state-of-the-art controllers in its ability to steer the user on a collision-free path. We also show through quantitative and qualitative measures of performance that our controller is robust in complex environments with many obstacles. Our method is applicable to arbitrary environments and operates without any user input or parameter tweaking, aside from the layout of the environments. We have implemented our algorithm on the Oculus Quest head-mounted display and evaluated its performance in environments with varying complexity. Our project website is available at https://gamma.umd.edu/arc/.
C1 [Williams, Niall L.; Bera, Aniket; Manocha, Dinesh] Univ Maryland, College Pk, MD 20742 USA.
C3 University System of Maryland; University of Maryland College Park
RP Williams, NL (corresponding author), Univ Maryland, College Pk, MD 20742 USA.
EM niallw@umd.edu; ab@umd.edu; dm@umd.edu
RI ; Bera, Aniket/AAS-7402-2020
OI Williams, Niall/0000-0002-0273-883X; Bera, Aniket/0000-0002-0182-6985
FU ARO [W911NF1910069, W911NF1910315]; Intel
FX The authors wish to thank Tabitha Peck for her helpful comments on
   statistical analysis, and the reviewers for their insightful
   suggestions. This work was supported in part by ARO under Grants
   W911NF1910069 and W911NF1910315, and in part by Intel.
NR 43
TC 42
Z9 44
U1 1
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2021
VL 27
IS 5
BP 2535
EP 2544
DI 10.1109/TVCG.2021.3067781
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR5YU
UT WOS:000661120200004
PM 33750709
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Xu, PF
   Yan, GH
   Fu, HB
   Igarashi, T
   Tai, CL
   Huang, H
AF Xu, Pengfei
   Yan, Guohang
   Fu, Hongbo
   Igarashi, Takeo
   Tai, Chiew-Lan
   Huang, Hui
TI Global Beautification of 2D and 3D Layouts With Interactive Ambiguity
   Resolution
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Layout; Tools; Three-dimensional displays; Two dimensional displays;
   User interfaces; Visualization; Global beautification; layout editing;
   snapping; alignment; ambiguity resolution; gestural interface
AB Specifying precise relationships among graphic elements is often a time-consuming process with traditional alignment tools. Automatic beautification of roughly designed layouts can provide a more efficient solution but often lead to undesired results due to ambiguity problems. To facilitate ambiguity resolution in layout beautification, we present a novel user interface for visualizing and editing inferred relationships through an automatic global layout beautification process. First, our interface provides a preview of the beautified layout with inferred constraints without directly modifying an input layout. In this way, the user can easily keep refining beautification results by interactively repositioning and/or resizing elements in the input layout. Second, we present a gestural interface for editing automatically inferred constraints by directly interacting with the visualized constraints via simple gestures. Our technique is applicable to both 2D and 3D global layout beautification, supported by efficient system implementation that provides instant user feedback. Our user study validates that our tool is capable of creating, editing, and refining layouts of graphic elements, and is significantly faster than the standard snap-dragging or command-based alignment tools for both 2D and 3D layout tasks.
C1 [Xu, Pengfei; Yan, Guohang; Huang, Hui] Shenzhen Univ, Visual Comp Res Ctr VCC, Guangdong Lab Artificial Intelligence & Digital E, Shenzhen 518060, Guangdong, Peoples R China.
   [Fu, Hongbo] City Univ Hong Kong, Kowloon, Hong Kong, Peoples R China.
   [Igarashi, Takeo] Univ Tokyo, Tokyo 1138654, Japan.
   [Tai, Chiew-Lan] Hong Kong Univ Sci & Technol, Kowloon, Hong Kong, Peoples R China.
C3 Shenzhen University; Guangming Laboratory; City University of Hong Kong;
   University of Tokyo; Hong Kong University of Science & Technology
RP Xu, PF (corresponding author), Shenzhen Univ, Visual Comp Res Ctr VCC, Guangdong Lab Artificial Intelligence & Digital E, Shenzhen 518060, Guangdong, Peoples R China.
EM xupengfei.cg@gmail.com; guohang.yan@gmail.com; hongbofu@cityu.edu.hk;
   takeo@acm.org; taicl@cs.ust.hk; hhzhiyan@gmail.com
RI Huang, Hui/JGB-1049-2023; Igarashi, Takeo/ITT-5921-2023
OI FU, Hongbo/0000-0002-0284-726X; Huang, Hui/0000-0003-3212-0544; Xu,
   Pengfei/0000-0003-4770-4374
FU NSFC [61602310, 61761146002, 61861130365]; GD Higher Education
   Innovation Key Program [2018KZDXM058]; GD Science and Technology Program
   [2015A030312015]; Shenzhen Innovation Program [JCYJ20170302154106666];
   Research Grants Council of HKSAR [HKUST16210718]; LHTD [20170003];
   Center for Applied Computing and Interactive Media (ACIM) of School of
   Creative Media, CityU; National Engineering Laboratory for Big Data
   System Computing Technology, SZU
FX The authors would like to thank the reviewers for their constructive
   comments and the user study participants for their time. This work was
   supported in parts by NSFC (61602310, 61761146002, 61861130365), GD
   Higher Education Innovation Key Program (2018KZDXM058), GD Science and
   Technology Program (2015A030312015), Shenzhen Innovation Program
   (JCYJ20170302154106666), LHTD (20170003), the Research Grants Council of
   HKSAR (HKUST16210718), a Gift from Adobe, the Center for Applied
   Computing and Interactive Media (ACIM) of School of Creative Media,
   CityU, and the National Engineering Laboratory for Big Data System
   Computing Technology, SZU.
NR 37
TC 5
Z9 6
U1 4
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2021
VL 27
IS 4
BP 2355
EP 2368
DI 10.1109/TVCG.2019.2954321
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO8XL
UT WOS:000623420400008
PM 31751244
OA Bronze
DA 2025-03-07
ER

PT J
AU Cannavò, A
   Calandra, D
   Pratticò, FG
   Gatteschi, V
   Lamberti, F
AF Cannavo, Alberto
   Calandra, Davide
   Prattico, F. Gabriele
   Gatteschi, Valentina
   Lamberti, Fabrizio
TI An Evaluation Testbed for Locomotion in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Legged locomotion; Virtual environments; Usability;
   Hardware; Three-dimensional displays; Virtual reality; virtual
   environments; locomotion; performance; user experience; requirements;
   evaluation; testbed
ID ENVIRONMENTS; WALKING; TRAVEL
AB A common operation performed in Virtual Reality (VR) environments is locomotion. Although real walking can represent a natural and intuitive way to manage displacements in such environments, its use is generally limited by the size of the area tracked by the VR system (typically, the size of a room) or requires expensive technologies to cover particularly extended settings. A number of approaches have been proposed to enable effective explorations in VR, each characterized by different hardware requirements and costs, and capable to provide different levels of usability and performance. However, the lack of a well-defined methodology for assessing and comparing available approaches makes it difficult to identify, among the various alternatives, the best solutions for selected application domains. To deal with this issue, this article introduces a novel evaluation testbed which, by building on the outcomes of many separate works reported in the literature, aims to support a comprehensive analysis of the considered design space. An experimental protocol for collecting objective and subjective measures is proposed, together with a scoring system able to rank locomotion approaches based on a weighted set of requirements. Testbed usage is illustrated in a use case requesting to select the technique to adopt in a given application scenario.
C1 [Cannavo, Alberto; Calandra, Davide; Prattico, F. Gabriele; Gatteschi, Valentina; Lamberti, Fabrizio] Politecn Torino, Dipartimento Automat & Informat, GRAINS GRAph & INtelligent Syst Grp, I-10129 Turin, Italy.
C3 Polytechnic University of Turin
RP Lamberti, F (corresponding author), Politecn Torino, Dipartimento Automat & Informat, GRAINS GRAph & INtelligent Syst Grp, I-10129 Turin, Italy.
EM alberto.cannavo@polito.it; davide.calandra@polito.it;
   filippogabriele.prattico@polito.it; valentina.gatteschi@polito.it;
   fabrizio.lamberti@polito.it
RI Cannavò, Alberto/AAK-4076-2021; Lamberti, Fabrizio/I-9153-2012;
   Gatteschi, Valentina/C-5375-2014; Prattico, Filippo
   Gabriele/AAY-7120-2020; Calandra, Davide/ABG-3277-2020
OI Gatteschi, Valentina/0000-0001-6075-6430; Prattico, Filippo
   Gabriele/0000-0001-7606-8552; Calandra, Davide/0000-0003-0449-5752;
   Cannavo, Alberto/0000-0002-6884-9268
FU VR@POLITOinitiative
FX This work has been supported by VR@POLITOinitiative.
NR 71
TC 25
Z9 25
U1 1
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2021
VL 27
IS 3
BP 1871
EP 1889
DI 10.1109/TVCG.2020.3032440
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA QA9EF
UT WOS:000613744500001
PM 33079670
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zellmann, S
   Schulze, JP
   Lang, U
AF Zellmann, Stefan
   Schulze, Juergen P.
   Lang, Ulrich
TI Binned <i>k</i>-d Tree Construction for Sparse Volume Data on Multi-Core
   and GPU Systems
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Scientific Visualization; Sparse Data; Direct Volume Rendering; k-d
   Tree; Parallel and GPGPU Computing
AB While k-d trees are known to be effective for spatial indexing of sparse 3-d volume data, full reconstruction, e.g. due to changes to the alpha transfer function during rendering, is usually a costly operation with this hierarchical data structure. In a recent publication we showed how to port a clever state of the art k-d tree construction algorithm to a multi-core CPU architecture and by means of thorough optimization we were able to obtain interactive reconstruction rates for moderately sized to large data sets. The construction scheme is based on maintaining partial summed-volume tables that fit in the L1 cache of the multi-core CPU and that allow for fast occupancy queries. In this work we propose a GPU implementation of the parallel k-d tree construction algorithm and compare it with the original multi-core CPU implementation. We conduct a thorough comparative study that outlines performance and scalability of our implementation.
C1 [Zellmann, Stefan; Lang, Ulrich] Univ Cologne, Chair Comp Sci, D-50923 Cologne, Germany.
   [Schulze, Juergen P.] Univ Calif San Diego, Dept Comp Sci, La Jolla, CA 92093 USA.
C3 University of Cologne; University of California System; University of
   California San Diego
RP Zellmann, S (corresponding author), Univ Cologne, Chair Comp Sci, D-50923 Cologne, Germany.
EM zellman@uni-koeln.de; jschulze@ucsd.edu; lang@uni-koeln.de
RI Müller, Thomas/AAD-3910-2019
OI Lang, Ulrich/0000-0001-7166-0805; Zellmann, Stefan/0000-0003-2880-9090
NR 42
TC 6
Z9 9
U1 2
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2021
VL 27
IS 3
BP 1904
EP 1915
DI 10.1109/TVCG.2019.2938957
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QA9EF
UT WOS:000613744500003
PM 31494550
DA 2025-03-07
ER

PT J
AU Zhu, KX
   He, XW
   Li, S
   Wang, HA
   Wang, GP
AF Zhu, Kuixin
   He, Xiaowei
   Li, Sheng
   Wang, Hongan
   Wang, Guoping
TI Shallow Sand Equations: Real-Time Height Field Simulation of Dry
   Granular Flows
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Mathematical model; Computational modeling; Solids; Real-time systems;
   Media; Friction; Solid modeling; Shallow sand equations; dry granular
   media; real-time simulation
AB Granular media is the second-most-manipulated substance on Earth, second only to water. However, simulation of granular media is still challenging due to the complexity of granular materials and the large number of discrete solid particles. As we know, dry granular materials could form a hybrid state between a fluid and a solid, therefore we propose a two-layer model and divide the simulation domain into a dilute layer, where granules can move freely as a fluid, and a dense layer, where granules act more like a solid. Motivated by the shallow water equations, we derive a set of shallow sand equations for modeling dry granular flows by depth-integrating three-dimensional governing equations along its vertical direction. Unlike previous methods for simulating a 2D granular media, our model does not restrict the depth of the granular media to be shallow anymore. To allow efficient fluid-solid interactions, we also present a ray casting algorithm for one-way solid-fluid coupling. Finally, we introduce a particle-tracking method to improve the visual representation. Our method can be efficiently implemented based on a height field and is fully compatible with modern GPUs, therefore allows us to simulate large-scale dry granular flows in real time.
C1 [Zhu, Kuixin] Peking Univ, ShenZhen Grad Sch, Sch Elect & Comp Engn, Haidian 100871, Peoples R China.
   [He, Xiaowei] Chinese Acad Sci, Inst Software, Key Lab CS, Beijing 100864, Peoples R China.
   [Li, Sheng] Peking Univ, Sch Elect Engn & Comp Sci, Haidian 100871, Peoples R China.
   [Wang, Guoping] Peking Univ, Haidian 100871, Peoples R China.
   [Wang, Hongan] Chinese Acad Sci, Inst Software, Beijing 100864, Peoples R China.
C3 Peking University; Chinese Academy of Sciences; Institute of Software,
   CAS; Peking University; Peking University; Chinese Academy of Sciences;
   Institute of Software, CAS
RP He, XW (corresponding author), Chinese Acad Sci, Inst Software, Key Lab CS, Beijing 100864, Peoples R China.
EM zkx@pku.edu.cn; xiaowei@iscas.ac.cn; lisheng@pku.edu.cn;
   hongan@iscas.ac.cn; wgp@pku.edu.cn
RI wang, guoping/KQU-3394-2024
OI Li, Sheng/0000-0002-8901-2184; he, xiao wei/0000-0002-8870-2482
FU National Key R&D Program of China [2017YFB1002700]; National Natural
   Science Foundation of China [6187070657, 61632003]; Youth Innovation
   Promotion Association, CAS [2019109]; Key Research Program of Frontier
   Sciences, CAS [QYZDY-SSW-JSC041]
FX We would like to thank anonymous reviewers for their constructive
   comments, Liyou Xu and Deyang Wu for helping prepare final examples. The
   project was supported by the National Key R&D Program of China
   (No.2017YFB1002700), the National Natural Science Foundation of China
   (No. 6187070657, 61632003), Youth Innovation Promotion Association, CAS
   (No. 2019109) and Key Research Program of Frontier Sciences, CAS (No.
   QYZDY-SSW-JSC041).
NR 47
TC 3
Z9 4
U1 1
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2021
VL 27
IS 3
BP 2073
EP 2084
DI 10.1109/TVCG.2019.2944172
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QA9EF
UT WOS:000613744500015
PM 31567091
DA 2025-03-07
ER

PT J
AU McDonald, T
   Usher, W
   Morrical, N
   Gyulassy, A
   Petruzza, S
   Federer, F
   Angelucci, A
   Pascucci, V
AF McDonald, Torin
   Usher, Will
   Morrical, Nate
   Gyulassy, Attila
   Petruzza, Steve
   Federer, Frederick
   Angelucci, Alessandra
   Pascucci, Valerio
TI Improving the Usability of Virtual Reality Neuron Tracing with
   Topological Elements
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual reality; Virtual Reality; Morse-Smale Complex; Semi-automatic
   Neuron Tracing
ID VISUAL ANALYSIS; VISUALIZATION; PERSISTENCE; DESIGN; TOOL
AB Researchers in the field of connectomics are working to reconstruct a map of neural connections in the brain in order to understand at a fundamental level how the brain processes information. Constructing this wiring diagram is done by tracing neurons through high-resolution image stacks acquired with fluorescence microscopy imaging techniques. While a large number of automatic tracing algorithms have been proposed, these frequently rely on local features in the data and fail on noisy data or ambiguous cases, requiring time-consuming manual correction. As a result, manual and semi-automatic tracing methods remain the state-of-the-art for creating accurate neuron reconstructions. We propose a new semi-automatic method that uses topological features to guide users in tracing neurons and integrate this method within a virtual reality (VR) framework previously used for manual tracing. Our approach augments both visualization and interaction with topological elements, allowing rapid understanding and tracing of complex morphologies. In our pilot study, neuroscientists demonstrated a strong preference for using our tool over prior approaches, reported less fatigue during tracing, and commended the ability to better understand possible paths and alternatives. Quantitative evaluation of the traces reveals that users' tracing speed increased, while retaining similar accuracy compared to a fully manual approach.
C1 [McDonald, Torin; Usher, Will; Morrical, Nate; Gyulassy, Attila; Pascucci, Valerio] Unit Utah, SCI Inst, Provo, UT 84097 USA.
   [Petruzza, Steve] Univ Utah, Univ Utah Slaw, SCI Inst, Provo, UT USA.
   [Federer, Frederick; Angelucci, Alessandra] Univ Utah, Moran Ese Inst, Provo, UT USA.
C3 Utah System of Higher Education; University of Utah; Utah System of
   Higher Education; University of Utah; Utah System of Higher Education;
   University of Utah
RP McDonald, T (corresponding author), Unit Utah, SCI Inst, Provo, UT 84097 USA.
EM torin@sci.utah.edu
RI pascucci, Valerio/GXF-0616-2022; Petruzza, Steve/LCD-7133-2024
OI Petruzza, Steve/0000-0002-2649-1595; pascucci,
   valerio/0000-0002-8877-2042
FU NSF OAC [1842042, 1941085]; NSF CMMI [1629660]; LLNL LDRD project
   [SI-20-001]; Department of Energy (DoE); National Nuclear Security
   Administration (NNSA) [DE-NA0002375]; Exascale Computing Project [17
   -SC-20-SC]; DoE by Lawrence Livermore National Laboratory
   [DE-AC52-07NA27344]; NIH [ROl EY026812, ROl EY019743, BRAIN U01
   NS099702, EY014800]; NSF [IOS 1755431, EAGER 1649923]; University of
   Utah Neuroscience Initiative; Research to Prevent Blindness, Inc.;
   University of Utah; Directorate For Engineering; Div Of Civil,
   Mechanical, & Manufact Inn [1629660] Funding Source: National Science
   Foundation
FX The authors would like to thank Pavol Klacansky for assistance in the
   initial integration of the tool. This work was funded in part by NSF OAC
   awards 1842042, 1941085, NSF CMMI awards 1629660, LLNL LDRD project
   SI-20-001. This material is based in part upon work supported by the
   Department of Energy (DoE), National Nuclear Security Administration
   (NNSA), under award DE-NA0002375. This research was supported in part by
   the Exascale Computing Project (17 -SC-20-SC), a collaborative effort of
   the DoE Office of Science and the NNSA. This work was performed in part
   under the auspices of the DoE by Lawrence Livermore National Laboratory
   under contract DE-AC52-07NA27344. This work is supported by in part by
   grants from the NIH (ROl EY026812, ROl EY019743, BRAIN U01 NS099702),
   the NSF (IOS 1755431, EAGER 1649923), and The University of Utah
   Neuroscience Initiative, to A.A., a grant from Research to Prevent
   Blindness, Inc. and a core grant from the NIH (EY014800) to the
   Department of Ophthalmology, University of Utah.
NR 69
TC 8
Z9 8
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 744
EP 754
DI 10.1109/TVCG.2020.3030363
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100060
PM 33055032
OA Green Accepted, Green Submitted
DA 2025-03-07
ER

PT J
AU Monadjemi, S
   Garnett, R
   Ottley, A
AF Monadjemi, Shayan
   Garnett, Roman
   Ottley, Alvitta
TI Competing Models: Inferring Exploration Patterns and Information
   Relevance via Bayesian Model Selection
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE User Interaction Modeling; Bayesian Machine Learning
ID VISUAL EXPLORATION; PROVENANCE; COMMUNICATION; VISUALIZATION; DESIGN
AB Analyzing interaction data provides an opportunity to learn about users, uncover their underlying goals, and create intelligent visualization systems. The first step for intelligent response in visualizations is to enable computers to infer user goals and strategies through observing their interactions with a system. Researchers have proposed multiple techniques to model users, however, their frameworks often depend on the visualization design, interaction space, and dataset. Due to these dependencies, many techniques do not provide a general algorithmic solution to user exploration modeling. In this paper, we construct a series of models based on the dataset and pose user exploration modeling as a Bayesian model selection problem where we maintain a belief over numerous competing models that could explain user interactions. Each of these competing models represent an exploration strategy the user could adopt during a session. The goal of our technique is to make high-level and in-depth inferences about the user by observing their low-level interactions. Although our proposed idea is applicable to various probabilistic model spaces, we demonstrate a specific instance of encoding exploration patterns as competing models to infer information relevance. We validate our technique's ability to infer exploration bias, predict future interactions, and summarize an analytic session using user study datasets. Our results indicate that depending on the application, our method outperforms established baselines for bias detection and future interaction prediction. Finally, we discuss future research directions based on our proposed modeling paradigm and suggest how practitioners can use this method to build intelligent visualization systems that understand users' goals and adapt to improve the exploration process.
C1 [Monadjemi, Shayan; Garnett, Roman; Ottley, Alvitta] Washington Univ, St Louis, MO 63110 USA.
C3 Washington University (WUSTL)
RP Monadjemi, S (corresponding author), Washington Univ, St Louis, MO 63110 USA.
EM monadjemi@wustl.edu; garnett@wustl.edu; alvitta@wustl.edu
RI Garnett, Roman/N-9894-2014
FU National Science Foundation [1755734, 1845434, 1940224]; Div Of
   Information & Intelligent Systems; Direct For Computer & Info Scie &
   Enginr [1845434] Funding Source: National Science Foundation; Div Of
   Information & Intelligent Systems; Direct For Computer & Info Scie &
   Enginr [1755734] Funding Source: National Science Foundation; Office of
   Advanced Cyberinfrastructure (OAC); Direct For Computer & Info Scie &
   Enginr [1940224] Funding Source: National Science Foundation
FX The authors wish to thank Sunwoo Ha for assisting in data preparation.
   Lane Harrison, Mi Feng, and Adam Kern for sharing their data. Emily Wall
   for her conversation on the bias detection metric. This material is
   based upon work supported by the National Science Foundation under grant
   numbers 1755734, 1845434, and 1940224.
NR 44
TC 8
Z9 10
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 412
EP 421
DI 10.1109/TVCG.2020.3030430
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100029
PM 33052859
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Tricoche, X
   Schlei, W
   Howell, KC
AF Tricoche, Xavier
   Schlei, Wayne
   Howell, Kathleen C.
TI Extraction and Visualization of Poincare Map Topology for Spacecraft
   Trajectory Design
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Trajectory planning and design; Poincare map; dynamical systems;
   topology extraction; invariant manifolds; chaos; visual analysis
ID COHERENT STRUCTURES; REPRESENTATIONS; MANIFOLDS; SURFACES; ORBITS
AB Mission designers must study many dynamical models to plan a low-cost spacecraft trajectory that satisfies mission constraints. They routinely use Poincare maps to search for a suitable path through the interconnected web of periodic orbits and invariant manifolds found in multi-body gravitational systems. This paper is concerned with the extraction and interactive visual exploration of this structural landscape to assist spacecraft trajectory planning. We propose algorithmic solutions that address the specific challenges posed by the characterization of the topology in astrodynamics problems and allow for an effective visual analysis of the resulting information. This visualization framework is applied to the circular restricted three-body problem (CR3BP), where it reveals novel periodic orbits with their relevant invariant manifolds in a suitable format for interactive transfer selection. Representative design problems illustrate how spacecraft path planners can leverage our topology visualization to fully exploit the natural dynamics pathways for energy-efficient trajectory designs.
C1 [Tricoche, Xavier] Purdue Univ, Comp Sci, W Lafayette, IN 47907 USA.
   [Schlei, Wayne] JHU Appl Phys Lab, W Lafayette, IN USA.
   [Howell, Kathleen C.] Purdue Univ, Aeronaut & Asronaut, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University; Purdue University System;
   Purdue University
RP Tricoche, X (corresponding author), Purdue Univ, Comp Sci, W Lafayette, IN 47907 USA.
EM xmt@purdue.edu; Wayne.Schlei@jhuapl.edu; howell@purdue.edu
OI Tricoche, Xavier/0000-0002-1688-3106
FU NSF CAREER Award [1150000]; Direct For Computer & Info Scie & Enginr;
   Office of Advanced Cyberinfrastructure (OAC) [1150000] Funding Source:
   National Science Foundation
FX This research was supported in part by NSF CAREER Award #1150000. This
   work also benefited from a gift by Intel. The authors are grateful to
   Rune and Barbara Eliasen for their support funding the Eliasen
   Visualization Laboratory at Purdue University. Also, the authors wish to
   acknowledge Visualization Sciences Group (an F.E.I. company and the
   developers of Avizo (R)) for implementation assistance with the
   visualization tools employed in this work.
NR 31
TC 8
Z9 9
U1 4
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 765
EP 774
DI 10.1109/TVCG.2020.3030402
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100062
PM 33048716
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Buttussi, F
   Chittaro, L
AF Buttussi, Fabio
   Chittaro, Luca
TI Locomotion in Place in Virtual Reality: A Comparative Evaluation of
   Joystick, Teleport, and Leaning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Tracking; Usability; Task analysis; Legged locomotion; Head-mounted
   displays; Virtual environments; Immersive virtual reality; locomotion
   techniques; teleport; joystick; leaning; user study; comparative
   evaluation
ID ENVIRONMENTS; TRAVEL
AB Recent VR head-mounted displays for consumers feature 3-DOF or 6-DOF head tracking. However, position tracking (when available) is limited to a small area. Moreover, in small or cluttered physical spaces, users can safely experience VR only by staying in place, standing or seated. Different locomotion techniques have been proposed to allow users to explore virtual environments by staying in place. Two in-place locomotion techniques, frequently employed in the literature and in consumer applications, are based on joystick and teleport. Some authors explored leaning with the aim of proposing a more natural in-place locomotion technique. However, more research is needed to understand the effects of the three techniques, since no user study thoroughly compared them all together on a variety of fundamental aspects. Therefore, this paper presents a comparative evaluation with 75 users, assessing the effects of the three techniques on performance, sickness, presence, usability, and different aspects of comfort. Performance of teleport was better than the other techniques, and performance of leaning was better than joystick. Teleport also caused less nausea than the other techniques. Unexpectedly, no significant differences were found for presence. Teleport received a higher usability score than the other techniques. Finally, the techniques had different effects on comfort that we discuss in detail.
C1 [Buttussi, Fabio; Chittaro, Luca] Univ Udine, Human Comp Interact Lab, Dept Math Comp Sci & Phys, I-33100 Udine, Italy.
C3 University of Udine
RP Buttussi, F (corresponding author), Univ Udine, Human Comp Interact Lab, Dept Math Comp Sci & Phys, I-33100 Udine, Italy.
EM fabio.buttussi@uniud.it; luca.chittaro@uniud.it
RI ; Buttussi, Fabio/AAD-9210-2020
OI CHITTARO, Luca/0000-0001-5975-4294; Buttussi, Fabio/0000-0003-0868-3638
NR 35
TC 73
Z9 78
U1 1
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN 1
PY 2021
VL 27
IS 1
BP 125
EP 136
DI 10.1109/TVCG.2019.2928304
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OY4TT
UT WOS:000594242000010
PM 31329560
DA 2025-03-07
ER

PT J
AU Nsonga, B
   Niemann, M
   Fröhlich, J
   Staib, J
   Gumhold, S
   Scheuermann, G
AF Nsonga, Baldwin
   Niemann, Martin
   Frohlich, Jochen
   Staib, Joachim
   Gumhold, Stefan
   Scheuermann, Gerik
TI Detection and Visualization of Splat and Antisplat Events in Turbulent
   Flows
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Feature extraction; Data visualization; Visualization; Stress; Heat
   transfer; Strain; Solids; Flow visualization; visualization techniques
   and methodologies
ID DIRECT NUMERICAL-SIMULATION; LARGE-EDDY SIMULATION; BACKWARD-FACING
   STEP; FREE-SURFACE; HEAT-TRANSFER; COHERENT STRUCTURES; BOUNDARY-LAYER;
   SHEAR-LAYER; EVOLUTION; WALL
AB Splat and antisplat events are a widely found phenomenon in three-dimensional turbulent flow fields. Splats are observed when fluid locally impinges on an impermeable surface transferring energy from the normal component to the tangential velocity components, while antisplats relate to the inverted situation. These events affect a variety of flow properties, such as the transfer of kinetic energy between velocity components and the transfer of heat, so that their investigation can provide new insight into these issues. Here, we propose the first Lagrangian method for the detection of splats and antisplats as features of an unsteady flow field. Our method utilizes the concept of strain tensors on flow-embedded flat surfaces to extract disjoint regions in which splat and antisplat events of arbitrary scale occur. We validate the method with artificial flow fields of increasing complexity. Subsequently, the method is used to analyze application data stemming from a direct numerical simulation of the turbulent flow over a backward facing step. Our results show that splat and antisplat events can be identified efficiently and reliably even in such a complex situation, demonstrating that the new method constitutes a well-suited tool for the analysis of turbulent flows.
C1 [Nsonga, Baldwin; Scheuermann, Gerik] Univ Leipzig, Inst Comp Sci, Augustuspl 10, D-04109 Leipzig, Germany.
   [Staib, Joachim; Gumhold, Stefan] Tech Univ Dresden, Inst Software & Multimedia Technol, Nothnitzer Str 46, D-01187 Dresden, Germany.
   [Niemann, Martin; Frohlich, Jochen] Tech Univ Dresden, Inst Fluid Mech, George Bahr Str 3c, D-01062 Dresden, Germany.
C3 Leipzig University; Technische Universitat Dresden; Technische
   Universitat Dresden
RP Nsonga, B (corresponding author), Univ Leipzig, Inst Comp Sci, Augustuspl 10, D-04109 Leipzig, Germany.
EM nsonga@informatik.uni-leipzig.de; martin.niemann@tu-dresden.de;
   jochen.froehlich@tu-dresden.de; joachim.staib@tu-dresden.de;
   stefan.gumhold@tu-dresden.de; scheuermann@informatik.uni-leipzig.de
RI ; Frohlich, Jochen/B-4275-2010
OI Scheuermann, Gerik/0000-0001-5200-8870; Frohlich,
   Jochen/0000-0003-1653-5686; Nsonga, Baldwin/0000-0002-0651-952X
FU German Federal Ministry of Education and Research within the project
   Competence Center for Scalable Data Services and Solutions (ScaDS)
   Dresden/Leipzig [BMBF 01IS14014B]; DFG [FR1593/15-1]
FX Computation time for the simulations was provided by ZIH Dresden and is
   gratefully acknowledged. This work was partially funded by the German
   Federal Ministry of Education and Research within the project Competence
   Center for Scalable Data Services and Solutions (ScaDS) Dresden/Leipzig
   (BMBF 01IS14014B) and by DFG through FR1593/15-1.
NR 41
TC 8
Z9 8
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV 1
PY 2020
VL 26
IS 11
BP 3147
EP 3162
DI 10.1109/TVCG.2019.2920157
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CK
UT WOS:000574745100002
PM 31170076
DA 2025-03-07
ER

PT J
AU Nuñez-Garcia, M
   Bernardino, G
   Alarcón, F
   Caixal, G
   Mont, L
   Camara, O
   Butakoff, C
AF Nunez-Garcia, Marta
   Bernardino, Gabriel
   Alarcon, Francisco
   Caixal, Gala
   Mont, Llis
   Camara, Oscar
   Butakoff, Constantine
TI Fast Quasi-Conformal Regional Flattening of the Left Atrium
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Two dimensional displays; Three-dimensional displays; Shape; Veins;
   Magnetic resonance; Sociology; Statistics; Left atrium; two-dimensional
   map; conformal flattening; regional flattening
ID MAP; REGISTRATION; VOLTAGE; GAPS
AB Two-dimensional representation of 3D anatomical structures is a simple and intuitive way for analysing patient information across populations and image modalities. While cardiac ventricles, especially the left ventricle, have an established standard representation (bull's eye plot), the 2D depiction of the left atrium (LA) remains challenging due to its sub-structural complexity including the pulmonary veins (PV) and the left atrial appendage (LAA). Quasi-conformal flattening techniques, successfully applied to cardiac ventricles, require additional constraints in the case of the LA to place the PV and LAA in the same geometrical 2D location for different cases. Some registration-based methods have been proposed but surface registration is time-consuming and prone to errors when the geometries are very different. We propose a novel atrial flattening methodology where a 2D standardised map of the LA is obtained quickly and without errors related to registration. The LA is divided into five regions which are then mapped to their analogue two-dimensional regions. 67 human left atria from magnetic resonance images (MRI) were studied to derive a population-based template representing the averaged relative locations of the PVs and LAA. The clinical application of our methodology is illustrated on different use cases including the integration of MRI and electroanatomical data.
C1 [Nunez-Garcia, Marta; Bernardino, Gabriel; Camara, Oscar; Butakoff, Constantine] Univ Pompeu Fabra, Dept Informat & Commun Technol, Physense, Barcelona 08002, Spain.
   [Alarcon, Francisco; Caixal, Gala; Mont, Llis] Univ Barcelona, Hosp Clin, Unitat Fibrillacio Auricular UFA, Dept Cardiol, Barcelona 08007, Spain.
   [Alarcon, Francisco; Caixal, Gala; Mont, Llis] Inst Invest Biomed August Pi & Sunyer IDIBAPS, Barcelona 08036, Spain.
C3 Pompeu Fabra University; University of Barcelona; Hospital Clinic de
   Barcelona; University of Barcelona; Hospital Clinic de Barcelona;
   IDIBAPS
RP Nuñez-Garcia, M (corresponding author), Univ Pompeu Fabra, Dept Informat & Commun Technol, Physense, Barcelona 08002, Spain.
EM marta.nunez@upf.edu; gabriel.bernardino@upf.edu; FALARCON@clinic.cat;
   GCAIXAL@clinic.cat; lmont@clinic.cat; oscar.camara@upf.edu;
   constantine.butakoff@upf.edu
RI Butakoff, Constantine/E-8644-2016; Nuñez-Garcia, Marta/LSJ-9823-2024;
   Bernardino, Gabriel/ADK-2205-2022; Camara, Oscar/I-4710-2015
OI Bernardino, Gabriel/0000-0001-8741-2566; Camara,
   Oscar/0000-0002-5125-6132; Nunez-Garcia, Marta/0000-0001-9349-3059
FU Spanish Ministry of Economy and Competitiveness [DPI2015-71640-R];
   Fundacio LaMarato de TV3 [20154031]; European Union Horizon 2020
   Programme for Research and Innovation [642676]; Marie Curie Actions
   (MSCA) [642676] Funding Source: Marie Curie Actions (MSCA)
FX This study was partially funded by the Spanish Ministry of Economy and
   Competitiveness (DPI2015-71640-R), by the "Fundacio LaMarato de TV3" (no
   20154031) and by European Union Horizon 2020 Programme for Research and
   Innovation, under grant agreement No. 642676 (CardioFunXion).
NR 32
TC 7
Z9 7
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG. 1
PY 2020
VL 26
IS 8
BP 2591
EP 2602
DI 10.1109/TVCG.2020.2966702
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MG6BL
UT WOS:000546115000004
PM 31944978
OA Green Submitted, Green Accepted
DA 2025-03-07
ER

PT J
AU Ratcliff, J
   Supikov, A
   Alfaro, S
   Azuma, R
AF Ratcliff, Joshua
   Supikov, Alexey
   Alfaro, Santiago
   Azuma, Ronald
TI ThinVR: Heterogeneous microlens arrays for compact, 180 degree FOV VR
   near-eye displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 27th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 22-26, 2020
CL Atlanta, GA
SP IEEE, IEEE Comp Soc
DE Lenses; Microoptics; Prototypes; Optical imaging; Optical diffraction;
   Optical distortion; Computational display; lenslets; wide field of view;
   head-worn display
ID DEPTH; LIGHT
AB Today's Virtual Reality (VR) displays are dramatically better than the head-worn displays offered 30 years ago, but today's displays remain nearly as bulky as their predecessors in the 1980's. Also, almost all consumer VR displays today provide 90-110 degrees field of view (FOV), which is much smaller than the human visual system's FOV which extends beyond 180 degrees horizontally. In this paper, we propose ThinVR as a new approach to simultaneously address the bulk and limited FOV of head-worn VR displays. ThinVR enables a head-worn VR display to provide 180 degrees horizontal FOV in a thin, compact form factor. Our approach is to replace traditional large optics with a curved microlens array of custom-designed heterogeneous lenslets and place these in front of a curved display. We found that heterogeneous optics were crucial to make this approach work, since over a wide FOV, many lenslets are viewed off the central axis. We developed a custom optimizer for designing custom heterogeneous lenslets to ensure a sufficient eyebox while reducing distortions. The contribution includes an analysis of the design space for curved microlens arrays, implementation of physical prototypes, and an assessment of the image quality, eyebox, FOV, reduction in volume and pupil swim distortion. To our knowledge, this is the first work to demonstrate and analyze the potential for curved, heterogeneous microlens arrays to enable compact, wide FOV head-worn VR displays.
C1 [Ratcliff, Joshua; Supikov, Alexey; Alfaro, Santiago; Azuma, Ronald] Intel Labs, Santa Clara, CA 95054 USA.
C3 Intel Corporation; Intel USA
RP Ratcliff, J (corresponding author), Intel Labs, Santa Clara, CA 95054 USA.
EM joshua.j.ratcliff@intel.com; alexei.soupikov@intel.com;
   santiago.alfaro@intel.com; ronald.t.azuma@intel.com
NR 31
TC 46
Z9 48
U1 3
U2 50
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2020
VL 26
IS 5
BP 1981
EP 1990
DI 10.1109/TVCG.2020.2973064
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA LA1WU
UT WOS:000523746000017
PM 32070971
DA 2025-03-07
ER

PT J
AU Zenner, A
   Makhsadov, A
   Klingner, S
   Liebemann, D
   Kruger, A
AF Zenner, Andre
   Makhsadov, Akhmajon
   Klingner, Soren
   Liebemann, David
   Kruger, Antonio
TI Immersive Process Model Exploration in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 27th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 22-26, 2020
CL Atlanta, GA
SP IEEE, IEEE Comp Soc
DE Virtual reality; Business; Virtual reality; multi-sensory feedback;
   passive haptics; immersion; business process models; immersive data
   analysis
ID WALKING
AB In many professional domains, relevant processes are documented as abstract process models, such as event-driven process chains (EPCs). EPCs are traditionally visualized as 2D graphs and their size varies with the complexity of the process. While process modeling experts are used to interpreting complex 2D EPCs, in certain scenarios such as, for example, professional training or education, also novice users inexperienced in interpreting 2D EPC data are facing the challenge of learning and understanding complex process models. To communicate process knowledge in an effective yet motivating and interesting way, we propose a novel virtual reality (VR) interface for non-expert users. Our proposed system turns the exploration of arbitrarily complex EPCs into an interactive and multi-sensory VR experience. It automatically generates a virtual 3D environment from a process model and lets users explore processes through a combination of natural walking and teleportation. Our immersive interface leverages basic gamification in the form of a logical walkthrough mode to motivate users to interact with the virtual process. The generated user experience is entirely novel in the field of immersive data exploration and supported by a combination of visual, auditory, vibrotactile and passive haptic feedback. In a user study with $\mathrm{N}=27$ novice users, we evaluate the effect of our proposed system on process model understandability and user experience, while comparing it to a traditional 2D interface on a tablet device. The results indicate a tradeoff between efficiency and user interest as assessed by the UEQ novelty subscale, while no significant decrease in model understanding performance was found using the proposed VR interface. Our investigation highlights the potential of multi-sensory VR for less time-critical professional application domains, such as employee training, communication, education, and related scenarios focusing on user interest.
C1 [Zenner, Andre; Makhsadov, Akhmajon; Klingner, Soren; Liebemann, David; Kruger, Antonio] German Res Ctr Artificial Intelligence DFKI, Saarland Informat Campus, Saarbrucken, Germany.
C3 German Research Center for Artificial Intelligence (DFKI)
RP Zenner, A (corresponding author), German Res Ctr Artificial Intelligence DFKI, Saarland Informat Campus, Saarbrucken, Germany.
EM andre.zenner@dfki.de; akhmajon.makhsadov@dfki.de;
   soren.klingner@dfki.de; david.liebemann@dfki.de; antonio.kruger@dfki.de
OI Zenner, Andre/0000-0003-3386-1635; Kruger, Antonio/0000-0002-8055-8367
FU German Federal Ministry of Education and Research (BMBF) [01IS17043]
FX We thank Scheer Holding for supporting this project and all participants
   of the study for their time and feedback. This research was funded
   inpart by the German Federal Ministry of Education and Research (BMBF)
   under grant number 01IS17043 (projectViRUX).
NR 46
TC 22
Z9 22
U1 2
U2 47
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2020
VL 26
IS 5
BP 2104
EP 2114
DI 10.1109/TVCG.2020.2973476
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA LA1WU
UT WOS:000523746000029
PM 32070982
DA 2025-03-07
ER

PT J
AU Han, J
   Tao, J
   Wang, CL
AF Han, Jun
   Tao, Jun
   Wang, Chaoli
TI FlowNet: A Deep Learning Framework for Clustering and Selection of
   Streamlines and Stream Surfaces
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Surface reconstruction; Feature extraction;
   Data visualization; Analytical models; Flow visualization; streamlines;
   stream surfaces; deep learning; autoencoder; feature descriptor;
   clustering; selection
AB For effective flow visualization, identifying representative flow lines or surfaces is an important problem which has been studied. However, no work can solve the problem for both lines and surfaces. In this paper, we present FlowNet, a single deep learning framework for clustering and selection of streamlines and stream surfaces. Given a collection of streamlines or stream surfaces generated from a flow field data set, our approach converts them into binary volumes and then employs an autoencoder to learn their respective latent feature descriptors. These descriptors are used to reconstruct binary volumes for error estimation and network training. Once converged, the feature descriptors can well represent flow lines or surfaces in the latent space. We perform dimensionality reduction of these feature descriptors and cluster the projection results accordingly. This leads to a visual interface for exploring the collection of flow lines or surfaces via clustering, filtering, and selection of representatives. Intuitive user interactions are provided for visual reasoning of the collection with ease. We validate and explain our deep learning framework from multiple perspectives, demonstrate the effectiveness of FlowNet using several flow field data sets of different characteristics, and compare our approach against state-of-the-art streamline and stream surface selection algorithms.
C1 [Han, Jun; Tao, Jun; Wang, Chaoli] Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
C3 University of Notre Dame
RP Han, J (corresponding author), Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
EM jhan5@nd.edu; jtao1@nd.edu; chaoli.wang@nd.edu
RI Wang, Chaoli/AAJ-5173-2020
OI Han, Jun/0000-0002-7286-062X
FU U.S. National Science Foundation [IIS-1455886, CNS1629914, DUE-1833129];
   NVIDIA GPU Grant Program
FX This research was supported in part by the U.S. National Science
   Foundation through grants IIS-1455886, CNS1629914, and DUE-1833129, and
   the NVIDIA GPU Grant Program. The authors would like to thank the
   anonymous reviewers for their insightful comments.
NR 41
TC 63
Z9 81
U1 0
U2 35
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2020
VL 26
IS 4
BP 1732
EP 1744
DI 10.1109/TVCG.2018.2880207
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KU2OG
UT WOS:000519547200009
PM 30418910
OA Bronze
DA 2025-03-07
ER

PT J
AU He, WB
   Guo, HQ
   Shen, HW
   Peterka, T
AF He, Wenbin
   Guo, Hanqi
   Shen, Han-Wei
   Peterka, Tom
TI <i>e</i>FESTA: Ensemble Feature Exploration with Surface Density
   Estimates
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Feature extraction; Uncertainty; Estimation; Isosurfaces; Visualization;
   Computational modeling; Density estimation; ensemble data visualization;
   uncertainty visualization; feature exploration
ID INTERACTIVE VISUALIZATION; NONPARAMETRIC MODELS; UNCERTAINTY;
   VARIABILITY; GLYPHS; FIELDS; FLOW
AB We propose surface density estimate (SDE) to model the spatial distribution of surface features-isosurfaces, ridge surfaces, and streamsurfaces-in 3D ensemble simulation data. The inputs of SDE computation are surface features represented as polygon meshes, and no field datasets are required (e.g., scalar fields or vector fields). The SDE is defined as the kernel density estimate of the infinite set of points on the input surfaces and is approximated by accumulating the surface densities of triangular patches. We also propose an algorithm to guide the selection of a proper kernel bandwidth for SDE computation. An ensemble Feature Exploration method based on Surface densiTy EstimAtes (eFESTA) is then proposed to extract and visualize the major trends of ensemble surface features. For an ensemble of surface features, each surface is first transformed into a density field based on its contribution to the SDE, and the resulting density fields are organized into a hierarchical representation based on the pairwise distances between them. The hierarchical representation is then used to guide visual exploration of the density fields as well as the underlying surface features. We demonstrate the application of our method using isosurface in ensemble scalar fields, Lagrangian coherent structures in uncertain unsteady flows, and streamsurfaces in ensemble fluid flows.
C1 [He, Wenbin; Shen, Han-Wei] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
   [Guo, Hanqi; Peterka, Tom] Argonne Natl Lab, Math & Comp Sci Div, Lemont, IL 60439 USA.
C3 University System of Ohio; Ohio State University; United States
   Department of Energy (DOE); Argonne National Laboratory
RP He, WB (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
EM he.495@osu.edu; hguo@anl.gov; shen.94@osu.edu; tpeterka@mcs.anl.gov
RI Shen, Han-wei/A-4710-2012; Guo, Hanqi/AAL-1929-2021; Guo,
   Hanqi/ADW-4234-2022
OI Guo, Hanqi/0000-0001-7776-1834
FU NSF [IIS-1250752, IIS-1065025]; US Department of Energy [DESC0007444,
   DE-DC0012495]
FX This work was supported in part by NSF grants IIS-1250752, IIS-1065025,
   and US Department of Energy grants DESC0007444, DE-DC0012495, program
   manager Lucy Nowell.
NR 71
TC 8
Z9 10
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2020
VL 26
IS 4
BP 1716
EP 1731
DI 10.1109/TVCG.2018.2879866
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KU2OG
UT WOS:000519547200008
PM 30418881
OA Bronze
DA 2025-03-07
ER

PT J
AU Cui, WW
   Zhang, XY
   Wang, Y
   Huang, H
   Chen, B
   Fang, L
   Zhang, HD
   Lou, JG
   Zhang, DM
AF Cui, Weiwei
   Zhang, Xiaoyu
   Wang, Yun
   Huang, He
   Chen, Bei
   Fang, Lei
   Zhang, Haidong
   Lou, Jian-Guan
   Zhang, Dongmei
TI Text-to-Viz: Automatic Generation of Infographics from
   Proportion-Related Natural Language Statements
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization for the masses; infographic; automatic visualization,
   presentation and dissemination
ID NARRATIVE VISUALIZATION; DESIGN; RECOGNITION
AB Combining data content with visual embellishments, infographics can effectively deliver messages in an engaging and memorable manner. Various authoring tools have been proposed to facilitate the creation of infographics. However, creating a professional infographic with these authoring tools is still not an easy task, requiring much time and design expertise. Therefore, these tools are generally not attractive to casual users, who are either unwilling to take time to learn the tools or lacking in proper design expertise to create a professional infographic. In this paper, we explore an alternative approach: to automatically generate infographics from natural language statements. We first conducted a preliminary study to explore the design space of infographics. Based on the preliminary study, we built a proof-of-concept system that automatically converts statements about simple proportion-related statistics to a set of infographics with pre-designed styles. Finally, we demonstrated the usability and usefulness of the system through sample results, exhibits, and expert reviews.
C1 [Cui, Weiwei; Zhang, Xiaoyu; Wang, Yun; Huang, He; Chen, Bei; Fang, Lei; Zhang, Haidong; Lou, Jian-Guan; Zhang, Dongmei] Microsoft Res Asia, Beijing, Peoples R China.
   [Zhang, Xiaoyu] Univ Calif Davis, ViDi Res Grp, Davis, CA 95616 USA.
C3 Microsoft Research Asia; Microsoft; Microsoft China; University of
   California System; University of California Davis
RP Cui, WW (corresponding author), Microsoft Res Asia, Beijing, Peoples R China.
EM weiweicu@microsoft.com; xybzhang@ucdavis.edu; wangyun@microsoft.com;
   rayhuang@microsoft.com; beichen@microsoft.com; leifa@microsoft.com;
   haizhang@microsoft.com; jlou@microsoft.com; dongmeiz@microsoft.com
RI Huang, He/LIG-6876-2024; Fang, Lei/Q-4913-2019; Zhang,
   Haidong/J-9302-2019; Zhang, Xiaoyu/HGD-2946-2022; zhang,
   dongmei/B-8011-2013
OI Fang, Lei/0000-0003-2510-1281; Zhang, Xiaoyu/0000-0002-8057-3997
NR 70
TC 71
Z9 86
U1 0
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 906
EP 916
DI 10.1109/TVCG.2019.2934785
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100084
PM 31478860
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Huang, ZS
   Zhao, Y
   Chen, W
   Gao, SJ
   Yu, KJ
   Xu, WX
   Tang, MJ
   Zhu, MF
   Xu, ML
AF Huang, Zhaosong
   Zhao, Ye
   Chen, Wei
   Gao, Shengjie
   Yu, Kejie
   Xu, Weixia
   Tang, Mingjie
   Zhu, Minfeng
   Xu, Mingliang
TI A Natural-language-based Visual Query Approach of Uncertain Human
   Trajectories
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Natural-language-based Visual Query; Spatial Uncertaity; Trajectory
   Exploration
ID MOVEMENT; PATTERNS; MOBILITY; EXPLORATION; ANALYTICS; AGGREGATION;
   BEHAVIOR
AB Visual querying is essential for interactively exploring massive trajectory data. However, the data uncertainty imposes profound challenges to fulfill advanced analytics requirements. On the one hand, many underlying data does not contain accurate geographic coordinates, e.g., positions of a mobile phone only refer to the regions (i.e., mobile cell stations) in which it resides, instead of accurate GPS coordinates. On the other hand, domain experts and general users prefer a natural way, such as using a natural language sentence, to access and analyze massive movement data. In this paper, we propose a visual analytics approach that can extract spatial-temporal constraints from a textual sentence and support an effective query method over uncertain mobile trajectory data. It is built up on encoding massive, spatially uncertain trajectories by the semantic information of the POls and regions covered by them, and then storing the trajectory documents in text database with an effective indexing scheme. The visual interface facilitates query condition specification, situation-aware visualization, and semantic exploration of large trajectory data. Usage scenarios on real-world human mobility datasets demonstrate the effectiveness of our approach.
C1 [Huang, Zhaosong; Chen, Wei; Gao, Shengjie; Yu, Kejie; Xu, Weixia; Zhu, Minfeng] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.
   [Zhao, Ye] Kent State Univ, Dept Comp Sci, Kent, OH 44242 USA.
   [Tang, Mingjie] Ant Financial, San Mateo, CA USA.
   [Xu, Mingliang] Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450000, Peoples R China.
C3 Zhejiang University; University System of Ohio; Kent State University;
   Kent State University Salem; Kent State University Kent; Zhengzhou
   University
RP Chen, W (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.; Xu, ML (corresponding author), Zhengzhou Univ, Sch Informat Engn, Zhengzhou 450000, Peoples R China.
EM zhaosong_huang@zju.edu.cn; zhao@cs.ketu.edu; chenwei@cad.zju.edu.cn;
   gaoshengjie@zju.edu.cn; ykjage@gmail.com; xuweixia96@gmail.com;
   tangrock@gmail.com; minfeng_zhu@zju.edu.cn; iexumingliang@zzu.edu.cn
RI Zhu, Minfeng/R-6788-2019; Chen, Wei/AAR-9817-2020
OI Zhu, Minfeng/0000-0002-6711-3099
FU National Natural Science Foundation of China [U1609217, 61772456,
   61761136020]; U.S. NSF [1535031, 1535081, 1739491]
FX This work was supported in part by National Natural Science Foundation
   of China (U1609217, 61772456, 61761136020). Y. Zhao's work was supported
   in part by the U.S. NSF grants 1535031, 1535081, and 1739491.
NR 80
TC 18
Z9 23
U1 2
U2 26
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 1256
EP 1266
DI 10.1109/TVCG.2019.2934671
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100116
PM 31443013
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Khayat, M
   Karimzadeh, M
   Zhao, JQ
   Ebert, DS
AF Khayat, Mosab
   Karimzadeh, Morteza
   Zhao, Jieqiong
   Ebert, David S.
TI VASSL: A Visual Analytics Toolkit for Social Spambot Labeling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Spambot; Labeling; Detection; Visual Analytics; Social Media Annotation
ID BOX PLOT; REDUCTION; RISE
AB Social media platforms are filled with social spambots. Detecting these malicious accounts is essential, yet challenging, as they continually evolve to evade detection techniques. In this article, we present VASSL, a visual analytics system that assists in the process of detecting and labeling spambots. Our tool enhances the performance and scalability of manual labeling by providing multiple connected views and utilizing dimensionality reduction, sentiment analysis and topic modeling, enabling insights for the identification of spambots. The system allows users to select and analyze groups of accounts in an interactive manner, which enables the detection of spambots that may not be identified when examined individually. We present a user study to objectively evaluate the performance of VASSL users, as well as capturing subjective opinions about the usefulness and the ease of use of the tool.
C1 [Khayat, Mosab; Zhao, Jieqiong; Ebert, David S.] Purdue Univ, W Lafayette, IN 47907 USA.
   [Karimzadeh, Morteza] Univ Colorado, Purdue Univ, Boulder, CO 80309 USA.
C3 Purdue University System; Purdue University; University of Colorado
   System; University of Colorado Boulder
RP Khayat, M (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.
EM mkhayat@purdue.edu; karimzadeh@colorado.edu; zhao413@purdue.edu;
   ebertd@purdue.edu
RI Zhao, Jieqiong/HLH-8586-2023; Karimzadeh, Morteza/AAE-8300-2020
OI Zhao, Jieqiong/0000-0002-4303-7722; Karimzadeh,
   Morteza/0000-0002-6498-1763; Ebert, David/0000-0001-6177-1296
NR 46
TC 14
Z9 17
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 874
EP 883
DI 10.1109/TVCG.2019.2934266
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100081
PM 31425086
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ma, YX
   Xie, TK
   Li, JD
   Maciejewski, R
AF Ma, Yuxin
   Xie, Tiankai
   Li, Jundong
   Maciejewski, Ross
TI Explaining Vulnerabilities to Adversarial Machine Learning through
   Visual Analytics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Adversarial machine learning; data poisoning; visual analytics
ID SECURITY; CLASSIFICATION; RECOGNITION; EXPLORATION; PROGRESS
AB Machine learning models are currently being deployed in a variety of real-world applications where model predictions are used to make decisions about healthcare, bank loans, and numerous other critical tasks. As the deployment of artificial intelligence technologies becomes ubiquitous, it is unsurprising that adversaries have begun developing methods to manipulate machine learning models to their advantage. While the visual analytics community has developed methods for opening the black box of machine learning models, little work has focused on helping the user understand their model vulnerabilities in the context of adversarial attacks. In this paper, we present a visual analytics framework for explaining and exploring model vulnerabilities to adversarial attacks. Our framework employs a multi-faceted visualization scheme designed to support the analysis of data poisoning attacks from the perspective of models, data instances, features, and local structures. We demonstrate our framework through two case studies on binary classifiers and illustrate model vulnerabilities with respect to varying attack strategies.
C1 [Ma, Yuxin; Xie, Tiankai; Maciejewski, Ross] Arizona State Univ, Sch Comp Informat & Decis Syst Engn, Tempe, AZ 85287 USA.
   [Li, Jundong] Univ Virginia, Dept Elect & Comp Engn, Charlottesville, VA 22903 USA.
C3 Arizona State University; Arizona State University-Tempe; University of
   Virginia
RP Ma, YX (corresponding author), Arizona State Univ, Sch Comp Informat & Decis Syst Engn, Tempe, AZ 85287 USA.
EM yuxinma@asu.edu; txie21@asu.edu; jl6qk@virginia.edu; rmacieje@asu.edu
RI Xie, Tiankai/JWA-3076-2024; MA, Yuxin/AAG-8630-2020
FU U.S.Department of Homeland Security [2017-ST-061-QA0001]
FX This work was supported by the U.S.Department of Homeland Security under
   Grant Award 2017-ST-061-QA0001. The views and conclusions contained in
   this document are those of the authors and should not be interpreted as
   necessarily representing the official policies, either expressed or
   implied, of the U.S. Department of Homeland Security.
NR 70
TC 45
Z9 57
U1 1
U2 31
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 1075
EP 1085
DI 10.1109/TVCG.2019.2934631
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100100
PM 31478859
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Binyahib, R
   Peterka, T
   Larsen, M
   Ma, KL
   Childs, H
AF Binyahib, Roba
   Peterka, Tom
   Larsen, Matthew
   Ma, Kwan-Liu
   Childs, Hank
TI A Scalable Hybrid Scheme for Ray-Casting of Unstructured Volume Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Volume rendering; parallel visualization; large scale visualization
ID OF-THE-ART
AB We present an algorithm for parallel volume rendering that is a hybrid between classical object order and image order techniques. The algorithm operates on unstructured grids (and structured ones), and thus can deal with block boundaries interleaving in complex ways. It also deals effectively with cases that are prone to load imbalance, i.e., cases where cell sizes differ dramatically, either because of the nature of the input data, or because of the effects of the camera transformation. The algorithm divides work over resources such that each phase of its processing is bounded in the amount of computation it can perform. We demonstrate its efficacy through a series of studies, varying over camera position, data set size, transfer function, image size, and processor count. At its biggest, our experiments scaled up to 8,192 processors and operated on data sets with more than one billion cells. In total, we find that our hybrid algorithm performs well in all cases. This is because our algorithm naturally adapts its computation based on workload, and can operate like either an object order technique or an image order technique in scenarios where those techniques are efficient.
C1 [Binyahib, Roba; Childs, Hank] Univ Oregon, Eugene, OR 97403 USA.
   [Peterka, Tom] Argonne Natl Lab, 9700 S Cass Ave, Argonne, IL 60439 USA.
   [Larsen, Matthew] Lawrence Livermore Natl Lab, Livermore, CA 94550 USA.
   [Ma, Kwan-Liu] Univ Calif Davis, Davis, CA 95616 USA.
C3 University of Oregon; United States Department of Energy (DOE); Argonne
   National Laboratory; United States Department of Energy (DOE); Lawrence
   Livermore National Laboratory; University of California System;
   University of California Davis
RP Binyahib, R (corresponding author), Univ Oregon, Eugene, OR 97403 USA.
EM roba@cs.uoregon.edu; tpeterka@mcs.anl.gov; larsen30@llnl.gov;
   ma@cs.ucdavis.edu; hank@cs.uoregon.edu
RI Binyahib, Roba/AAZ-8480-2020
OI Ma, Kwan-Liu/0000-0001-8086-0366; Binyahib, Roba/0000-0001-5164-0751
FU Advanced Scientific Computing Research, Office of Science, U.S.
   Department of Energy [DE-AC02-06CH11357]; DOE Early Career Award
   [DE-SC0010652]; U.S. Department of Energy by Lawrence Livermore National
   Laboratory [DE-AC52-07NA27344 (LLNL-JRNL-74473)]; U.S. Department of
   Energy (DOE) [DE-SC0010652] Funding Source: U.S. Department of Energy
   (DOE)
FX This work was supported by Advanced Scientific Computing Research,
   Office of Science, U.S. Department of Energy, under Contract
   DE-AC02-06CH11357, Program Manager Lucy Nowell. Hank Childs is grateful
   for support from the DOE Early Career Award, Contract No. DE-SC0010652,
   Program Manager Lucy Nowell. Some of this work performed under the
   auspices of the U.S. Department of Energy by Lawrence Livermore National
   Laboratory under Contract DE-AC52-07NA27344 (LLNL-JRNL-74473). Thanks
   for Dr. Christoph Garth at the University of Kaiserslautern for
   providing the Ice dataset, which was originally generated by Markus
   Rtten(markus.ruetten@dlr.de) at the German Aerospace Research Center,
   Gttingen.
NR 27
TC 15
Z9 16
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2019
VL 25
IS 7
BP 2349
EP 2361
DI 10.1109/TVCG.2018.2833113
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IA8WG
UT WOS:000469838700002
PM 29994004
OA Green Submitted, Bronze
DA 2025-03-07
ER

PT J
AU Zhang, L
   Zheng, QZ
   Huang, H
AF Zhang, Lei
   Zheng, Qing-Zhuo
   Huang, Hua
TI Intrinsic Motion Stability Assessment for Video Stabilization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Stabilization; non-reference; assessment; geodesic curvature
AB This paper presents a novel algorithm for assessing the motion stability of a video after stabilization. The assessment works in a non-reference manner that directly measures the intrinsic smoothness of the video motion path. Specifically, the motion path is cast as a curve embedded in the Lie group of homographies, and its smoothness is mathematically characterized by the intrinsic geodesic curvature. A bundle of paths are adopted to handle spatially variant motions through the frames. Then, we compute the weighted curvature for a holistic assessment on the motion stability. Other factors related to video stabilization, e.g., distortion and cropping, are also investigated as supplement. We collect 160 shaky video clips and their stabilized results for verification, and the experimental evidence shows the effectiveness of our algorithm in good correlation with human subjective judgements.
C1 [Zhang, Lei; Zheng, Qing-Zhuo; Huang, Hua] Beijing Inst Technol, Beijing Key Lab Intelligent Informat Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
C3 Beijing Institute of Technology
RP Huang, H (corresponding author), Beijing Inst Technol, Beijing Key Lab Intelligent Informat Technol, Sch Comp Sci, Beijing 100081, Peoples R China.
EM cgzhanglei@gmail.com; zhengqingzhuo@sina.com; huahuang@bit.edu.cn
RI Huang, Hua/M-9684-2013
OI Huang, Hua/0000-0003-2587-1702
FU National Natural Science Foundation of China [61772069, 61425013]
FX The authors would like to thank the anonymous reviewers for their
   helpful comments. This work was supported by the National Natural
   Science Foundation of China under Grant 61772069 and Grant 61425013.
NR 30
TC 14
Z9 17
U1 1
U2 28
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2019
VL 25
IS 4
BP 1681
EP 1692
DI 10.1109/TVCG.2018.2817209
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HN6TU
UT WOS:000460319500004
PM 29993778
DA 2025-03-07
ER

PT J
AU Wagner, M
   Slijepcevic, D
   Horsak, B
   Rind, A
   Zeppelzauer, M
   Aigner, W
AF Wagner, Markus
   Slijepcevic, Djordje
   Horsak, Brian
   Rind, Alexander
   Zeppelzauer, Matthias
   Aigner, Wolfgang
TI KAVAGait: Knowledge-Assisted Visual Analytics for Clinical Gait Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Design study; interface design; knowledge generation;
   knowledge-assisted; visualization; visual analytics; gait analysis
ID PRINCIPAL COMPONENT ANALYSIS; GROUND REACTION FORCE; DESIGN; PATTERNS;
   VISUALIZATION; CLUSTER
AB In 2014, more than 10 million people in the US were affected by an ambulatory disability. Thus, gait rehabilitation is a crucial part of health care systems. The quantification of human locomotion enables clinicians to describe and analyze a patient's gait performance in detail and allows them to base clinical decisions on objective data. These assessments generate a vast amount of complex data which need to be interpreted in a short time period. We conducted a design study in cooperation with gait analysis experts to develop a novel Knowledge-Assisted Visual Analytics solution for clinical Gait analysis (KAVAGait). KAVAGait allows the clinician to store and inspect complex data derived during clinical gait analysis. The system incorporates innovative and interactive visual interface concepts, which were developed based on the needs of clinicians. Additionally, an explicit knowledge store (EKS) allows externalization and storage of implicit knowledge from clinicians. It makes this information available for others, supporting the process of data inspection and clinical decision making. We validated our system by conducting expert reviews, a user study, and a case study. Results suggest that KAVAGait is able to support a clinician during clinical practice by visualizing complex gait data and providing knowledge of other clinicians.
C1 [Wagner, Markus; Slijepcevic, Djordje; Rind, Alexander; Zeppelzauer, Matthias; Aigner, Wolfgang] St Polten Univ Appl Sci, Inst Creativen Media Techol, A-3100 St Polten, Austria.
   [Horsak, Brian] St Polten Univ Appl Sci, Dept Hlth Sci, A-3100 St Polten, Austria.
   [Wagner, Markus; Slijepcevic, Djordje; Rind, Alexander; Zeppelzauer, Matthias; Aigner, Wolfgang] TU Wien, A-1040 Vienna, Austria.
C3 St. Polten University of Applied Sciences; St. Polten University of
   Applied Sciences; Technische Universitat Wien
RP Wagner, M (corresponding author), St Polten Univ Appl Sci, Inst Creativen Media Techol, A-3100 St Polten, Austria.; Wagner, M (corresponding author), TU Wien, A-1040 Vienna, Austria.
EM markus.wagner@fhstp.ac.at; djordje.slijepcevic@fhstp.ac.at;
   brian.horsak@fhstp.ac.at; alexander.rind@fhstp.ac.at;
   matthias.zeppelzauer@fhstp.ac.at; wolfgang.aigner@fhstp.ac.at
RI Slijepcevic, Djordje/KEI-4093-2024; Wagner, Markus/HZM-6101-2023;
   Horsak, Brian/HZJ-8906-2023
OI Horsak, Brian/0000-0002-9296-3212; Zeppelzauer,
   Matthias/0000-0003-0413-4746; Wagner, Markus/0000-0002-6619-6494;
   Slijepcevic, Djordje/0000-0002-2295-7466; Rind,
   Alexander/0000-0001-8788-4600
FU Austrian Science Fund (FWF) [P25489-N23, P27975-NBL]; NFB -Lower
   Austrian Research and Education Company; Provincial Government of Lower
   Austria, Dep. of Science and Research [LSC14-005]; Austrian Science Fund
   (FWF) [P27975, P25489] Funding Source: Austrian Science Fund (FWF)
FX This work was supported by theAustrian Science Fund (FWF): P25489-N23
   ("KAVA-Time"), P27975-NBL ("VisOnFire") and by the NFB -Lower Austrian
   Research and Education Company and the Provincial Government of Lower
   Austria, Dep. of Science and Research ("IntelliGait" LSC14-005). Cordial
   thanks to Marianne Worisch, Christina Niederer, and Niklas Thur for
   their support and to Tarique Siragy for proofreading. Special thanks to
   all focus group members and test participants volunteering in this
   project.
NR 65
TC 24
Z9 27
U1 1
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2019
VL 25
IS 3
BP 1528
EP 1542
DI 10.1109/TVCG.2017.2785271
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HK3NZ
UT WOS:000457824500008
PM 29993807
OA Green Submitted, hybrid
DA 2025-03-07
ER

PT J
AU Yuan, MZ
   Gao, L
   Fu, HB
   Xia, SH
AF Yuan, Ming-Ze
   Gao, Lin
   Fu, Hongbo
   Xia, Shihong
TI Temporal Upsampling of Depth Maps Using a Hybrid Camera
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Hybrid camera; scene flow estimation; depth upsampling
ID RECONSTRUCTION; PERSPECTIVE; CAPTURE
AB In recent years, consumer-level depth cameras have been adopted for various applications. However, they often produce depth maps at only a moderately high frame rate (approximately 30 frames per second), preventing them from being used for applications such as digitizing human performance involving fast motion. On the other hand, low-cost, high-frame-rate video cameras are available. This motivates us to develop a hybrid camera that consists of a high-frame-rate video camera and a low-frame-rate depth camera and to allow temporal interpolation of depth maps with the help of auxiliary color images. To achieve this, we develop a novel algorithm that reconstructs intermediate depth maps and estimates scene flow simultaneously. We test our algorithm on various examples involving fast, non-rigid motions of single or multiple objects. Our experiments show that our scene flow estimation method is more precise than a tracking-based method and the state-of-the-art techniques.
C1 [Yuan, Ming-Ze; Gao, Lin; Xia, Shihong] Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.
   [Yuan, Ming-Ze] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
   [Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; City University of Hong Kong
RP Gao, L; Xia, SH (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.
EM yuanmingze@ict.ac.cn; gaolin@ict.ac.cn; hongbofu@cityu.edu.hk;
   xsh@ict.ac.cn
RI Gao, Lin/JNF-0375-2023
OI WANG - demo, Tom/0000-0002-8885-9287; FU, Hongbo/0000-0002-0284-726X
FU National Natural Science Foundation of China [61502453, 61772499,
   61611130215]; Royal Society-Newton Mobility Grant [IE150731]; Science
   and Technology Service Network Initiative of Chinese Academy of Sciences
   [KFJ-STS-ZDTP-017]; Knowledge Innovation Program of the Institute of
   Computing Technology of the Chinese Academy of Sciences [ICT20166040];
   Hong Kong Research Grants Council [CityU CityU 11237116]; ACIM-SCM
FX Thiswork was supported by grants from the National Natural Science
   Foundation of China (No. 61502453, No. 61772499 and No. 61611130215),
   Royal Society-Newton Mobility Grant (No. IE150731), the Science and
   Technology Service Network Initiative of Chinese Academy of Sciences
   (No. KFJ-STS-ZDTP-017), the Knowledge Innovation Program of the
   Institute of Computing Technology of the Chinese Academy of Sciences
   under Grant No. ICT20166040, the Hong Kong Research Grants Council (No.
   CityU CityU 11237116) and ACIM-SCM.
NR 65
TC 10
Z9 11
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2019
VL 25
IS 3
BP 1591
EP 1602
DI 10.1109/TVCG.2018.2812879
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HK3NZ
UT WOS:000457824500012
PM 29993604
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Günther, T
   Theisel, H
AF Guenther, Tobias
   Theisel, Holger
TI Objective Vortex Corelines of Finite-sized Objects in Fluid Flows
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Vortex extraction; inertial particles; objectivity; vortex coreline
ID TIME LYAPUNOV EXPONENTS; INERTIAL PARTICLES; VORTICES; VISUALIZATION;
   DYNAMICS; MOTION; IDENTIFICATION; ALIGNMENT; FEATURES; CRITERIA
AB Vortices are one of the most-frequently studied phenomena in fluid flows. The center of the rotating motion is called the vortex coreline and its successful detection strongly depends on the choice of the reference frame. The optimal frame moves with the center of the vortex, which incidentally makes the observed fluid flow steady and thus standard vortex coreline extractors such as Sujudi-Haimes become applicable. Recently, an objective optimization framework was proposed that determines a near-steady reference frame for tracer particles. In this paper, we extend this technique to the detection of vortex corelines of inertial particles. An inertial particle is a finite-sized object that is carried by a fluid flow. In contrast to the usual tracer particles, they do not move tangentially with the flow, since they are subject to gravity and exhibit mass-dependent inertia. Their particle state is determined by their position and own velocity, which makes the search for the optimal frame a high-dimensional problem. We demonstrate in this paper that the objective detection of an inertial vortex coreline can be reduced in 2D to a critical point search in 2D. For 3D flows, however, the vortex coreline criterion remains a parallel vectors condition in 6D. To detect the vortex corelines we propose a recursive subdivision approach that is tailored to the underlying structure of the 6D vectors. The resulting algorithm is objective, and we demonstrate the vortex coreline extraction in a number of 2D and 3D vector fields.
C1 [Guenther, Tobias] Swiss Fed Inst Technol, Comp Graph Lab, Zurich, Switzerland.
   [Theisel, Holger] Univ Magdeburg, Visual Comp Grp, Magdeburg, Germany.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich; Otto von
   Guericke University
RP Günther, T (corresponding author), Swiss Fed Inst Technol, Comp Graph Lab, Zurich, Switzerland.
EM tobias.guenther@inf.ethz.ch; theisel@ovgu.de
OI Gunther, Tobias/0000-0002-3020-0930
NR 81
TC 6
Z9 7
U1 1
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 956
EP 966
DI 10.1109/TVCG.2018.2864828
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000091
PM 30130219
DA 2025-03-07
ER

PT J
AU Chakravarthula, P
   Dunn, D
   Aksit, K
   Fuchs, H
AF Chakravarthula, Praneeth
   Dunn, David
   Aksit, Kaan
   Fuchs, Henry
TI FocusAR: Auto-focus Augmented Reality Eyeglasses for both Real World and
   Virtual Imagery
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 17th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 16-20, 2018
CL Munich, GERMANY
SP IEEE, IEEE Comp Soc, IEEE VGTC, ACM SIGCHI, Mozilla, Apple, Intel, DAQRI, PTC, Amazon, Facebook, Qualcomm, Umajin, Disney Res, Univ S Australia Ventures Pty Ltd, REFLEKT, Occipital, Envisage AR, KHRONOS Grp, TUM, ETH Zurich
DE Augmented Reality; Displays; Auto-focus; Focus accommodation;
   Prescription correction
ID LIQUID LENS; ACCOMMODATION; ABERRATION
AB We describe a system which corrects dynamically for the focus of the real world surrounding the near-eye display of the user and simultaneously the internal display for augmented synthetic imagery, with an aim of completely replacing the user prescription eyeglasses. The ability to adjust focus for both real and virtual stimuli will be useful for a wide variety of users, but especially for users over 40 years of age who have limited accommodation range. Our proposed solution employs a tunable-focus lens for dynamic prescription vision correction, and a varifocal internal display for setting the virtual imagery at appropriate spatially registered depths. We also demonstrate a proof of concept prototype to verify our design and discuss the challenges to building an auto-focus augmented reality eyeglasses for both real and virtual.
C1 [Chakravarthula, Praneeth; Dunn, David; Fuchs, Henry] Univ N Carolina, Chapel Hill, NC 27599 USA.
   [Aksit, Kaan] NVIDIA Res, Santa Clara, CA USA.
C3 University of North Carolina; University of North Carolina Chapel Hill
RP Chakravarthula, P (corresponding author), Univ N Carolina, Chapel Hill, NC 27599 USA.
EM cpk@cs.unc.edu; dunn@unc.edu; kaksit@nvidia.com; fuchs@cs.unc.edu
RI Aksit, Kaan/AAY-6704-2020; Chakravarthula, Praneeth Kumar/AFZ-2211-2022
OI AKSIT, KAAN/0000-0002-5934-5500
FU BeingTogether Centre; National Research Foundation, Prime Minister's
   Office, Singapore under its International Research Centres in Singapore
   Funding Initiative
FX The authors thank Madhumita Mahadevan for fruitful discussions, and Jim
   Mahaney for helping with the physical setup. This research is supported
   in part by the BeingTogether Centre, a collaboration between Nanyang
   Technological University (NTU) Singapore and University of North
   Carolina (UNC) at Chapel Hill. The BeingTogether Centre is supported by
   the National Research Foundation, Prime Minister's Office, Singapore
   under its International Research Centres in Singapore Funding
   Initiative.
NR 41
TC 56
Z9 64
U1 1
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2018
VL 24
IS 11
BP 2906
EP 2916
DI 10.1109/TVCG.2018.2868532
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GZ0TM
UT WOS:000449077900009
PM 30207958
DA 2025-03-07
ER

PT J
AU Golodetz, S
   Cavallari, T
   Lord, NA
   Prisacariu, VA
   Murray, DW
   Torr, PHS
AF Golodetz, Stuart
   Cavallari, Tommaso
   Lord, Nicholas A.
   Prisacariu, Victor A.
   Murray, David W.
   Torr, Philip H. S.
TI Collaborative Large-Scale Dense 3D Reconstruction with Online
   Inter-Agent Pose Optimisation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 17th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 16-20, 2018
CL Munich, GERMANY
SP IEEE, IEEE Comp Soc, IEEE VGTC, ACM SIGCHI, Mozilla, Apple, Intel, DAQRI, PTC, Amazon, Facebook, Qualcomm, Umajin, Disney Res, Univ S Australia Ventures Pty Ltd, REFLEKT, Occipital, Envisage AR, KHRONOS Grp, TUM, ETH Zurich
DE Collaborative; large-scale; dense 3D reconstruction; inter-agent
   relocalisation; pose graph optimisation
ID REAL-TIME; SIMULTANEOUS LOCALIZATION
AB Reconstructing dense, volumetric models of real-world 3D scenes is important for many tasks, but capturing large scenes can take significant time, and the risk of transient changes to the scene goes up as the capture time increases. These are good reasons to want instead to capture several smaller sub-scenes that can be joined to make the whole scene. Achieving this has traditionally been difficult: joining sub-scenes that may never have been viewed from the same angle requires a high-quality camera relocaliser that can cope with novel poses, and tracking drift in each sub-scene can prevent them from being joined to make a consistent overall scene. Recent advances, however, have significantly improved our ability to capture medium-sized sub-scenes with little to no tracking drift: real-time globally consistent reconstruction systems can close loops and re-integrate the scene surface on the fly, whilst new visual-inertial odometry approaches can significantly reduce tracking drift during live reconstruction. Moreover, high-quality regression forest-based relocalisers have recently been made more practical by the introduction of a method to allow them to be trained and used online. In this paper, we leverage these advances to present what to our knowledge is the first system to allow multiple users to collaborate interactively to reconstruct dense, voxel-based models of whole buildings using only consumer-grade hardware, a task that has traditionally been both time-consuming and dependent on the availability of specialised hardware. Using our system, an entire house or lab can be reconstructed in under half an hour and at a far lower cost than was previously possible.
C1 [Golodetz, Stuart; Cavallari, Tommaso; Lord, Nicholas A.; Prisacariu, Victor A.; Murray, David W.; Torr, Philip H. S.] Univ Oxford, Oxford, England.
   [Golodetz, Stuart; Cavallari, Tommaso; Lord, Nicholas A.] FiveAI Ltd, Cambridge, England.
C3 University of Oxford
RP Golodetz, S (corresponding author), Univ Oxford, Oxford, England.; Golodetz, S (corresponding author), FiveAI Ltd, Cambridge, England.
EM stuart@five.ai; tommaso.cavallari@five.ai; nick@five.ai;
   victor@robots.ox.ac.uk; dwm@robots.ox.ac.uk; phst@robots.ox.ac.uk
RI Golodetz, Stuart/ABE-4971-2020
OI Cavallari, Tommaso/0000-0003-2490-5341
FU Innovate UK/CCAV project [103700]; ERC [ERC-2012-AdG 321162-HELIOS];
   EPSRC [EP/M013774/1]; EPSRC/MURI [EP/N019474/1]; EPSRC; EPSRC
   [EP/I001107/2, EP/N019474/1, EP/M013774/1] Funding Source: UKRI; ISCF
   [103700] Funding Source: UKRI
FX This work was supported by Innovate UK/CCAV project 103700
   (Street-Wise), the EPSRC, ERC grant ERC-2012-AdG 321162-HELIOS, EPSRC
   grant Seebibyte EP/M013774/1 and EPSRC/MURI grant EP/N019474/1. We would
   also like to thank Manar Marzouk and Maria Anastasia Tsitsigkou for
   their help with the collaborative dataset collection, and Oscar Rahnama,
   Tom Joy, Daniela Massiceti, Nantas Nardelli, Mohammad Najafi and Adnane
   Boukhayma for their help with the experiments.
NR 54
TC 41
Z9 50
U1 0
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2018
VL 24
IS 11
BP 2895
EP 2905
DI 10.1109/TVCG.2018.2868533
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GZ0TM
UT WOS:000449077900008
PM 30334761
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Centin, M
   Signoroni, A
AF Centin, Marco
   Signoroni, Alberto
TI Mesh Denoising with (Geo) Metric Fidelity
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Mesh denoising; high-fidelity 3D modeling; feature preservation;
   saliency-driven geometry processing; scale-invariance; surface normal
   diffusion; geometric aliasing
ID VARIATIONAL-PROBLEMS; DIFFUSION
AB Working with noisy meshes and aiming at providing high-fidelity 3D object models without tampering the metric quality of the acquisitions, we propose a mesh denoising technique that, through a normal-diffusion process guided by a curvature saliency map, is able to preserve and emphasize the natural object features, concurrently allowing the introduction of a bound on the maximum distance from the original model. Moreover, both the position of the mesh vertices and the edge orientations are optimized through a tailored geometric-aliasing correction. Thanks to an efficiently parallelized procedure, we are able to process even large models almost instantly with a parameter configuration that does not depend on the scale of the object. An essential survey on mesh denoising is also presented which is functional to the definition of a common framework where to set up our solutions and the related technical and experimental comparisons. The proposed results prove the effectiveness of our method, especially on the challenging target application profiles. Where competing techniques tend to inappropriately recover sharp edges while deforming the surrounding geometry or, on the contrary, to oversmooth shallow features, our method protects and enhances the natural object features and effectively reduces scanning noise on the smooth parts, while guaranteeing the prescribed metric-fidelity to the input model.
C1 [Centin, Marco; Signoroni, Alberto] Univ Brescia, Dept Informat Engn, Via Branze 38, I-25123 Brescia, Italy.
C3 University of Brescia
RP Signoroni, A (corresponding author), Univ Brescia, Dept Informat Engn, Via Branze 38, I-25123 Brescia, Italy.
EM marco.centin@gmail.com; alberto.signoroni@unibs.it
RI Signoroni, Alberto/U-6789-2019
OI SIGNORONI, Alberto/0000-0002-8383-3766
NR 52
TC 22
Z9 27
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2018
VL 24
IS 8
BP 2380
EP 2396
DI 10.1109/TVCG.2017.2731771
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GL6DJ
UT WOS:000437269000010
PM 28749353
DA 2025-03-07
ER

PT J
AU Dal Col, A
   Valdivia, P
   Petronetto, F
   Dias, F
   Silva, CT
   Nonato, LG
AF Dal Col, Alcebiades
   Valdivia, Paola
   Petronetto, Fabiano
   Dias, Fabio
   Silva, Claudio T.
   Gustavo Nonato, L.
TI Wavelet-Based Visual Analysis of Dynamic Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Dynamic networks; spectral graph wavelets; visual analytics
ID GRAPH WAVELETS; TIME; PATTERNS
AB Dynamic networks naturally appear in a multitude of applications from different fields. Analyzing and exploring dynamic networks in order to understand and detect patterns and phenomena is challenging, fostering the development of new methodologies, particularly in the field of visual analytics. In this work, we propose a novel visual analytics methodology for dynamic networks, which relies on the spectral graph wavelet theory. We enable the automatic analysis of a signal defined on the nodes of the network, making viable the robust detection of network properties. Specifically, we use a fast approximation of a graph wavelet transform to derive a set of wavelet coefficients, which are then used to identify activity patterns on large networks, including their temporal recurrence. The coefficients naturally encode the spatial and temporal variations of the signal, leading to an efficient and meaningful representation. This methodology allows for the exploration of the structural evolution of the network and their patterns over time. The effectiveness of our approach is demonstrated using usage scenarios and comparisons involving real dynamic networks.
C1 [Dal Col, Alcebiades; Valdivia, Paola; Gustavo Nonato, L.] Univ Sao Paulo, Inst Math & Comp Sci, BR-13566590 Sao Carlos, SP, Brazil.
   [Valdivia, Paola] French Inst Res Comp Sci & Automat, F-1275589 Paris, France.
   [Petronetto, Fabiano] Univ Fed Espirito Santo, BR-29075910 Vitoria, ES, Brazil.
   [Dias, Fabio; Silva, Claudio T.] NYU, New York, NY 10003 USA.
C3 Universidade de Sao Paulo; Universidade Federal do Espirito Santo; New
   York University
RP Dal Col, A (corresponding author), Univ Sao Paulo, Inst Math & Comp Sci, BR-13566590 Sao Carlos, SP, Brazil.
EM alcebiades_dalcol@usp.br; paolalv@icmc.usp.br; fabiano.carmo@ufes.br;
   fabio.dias@nyu.edu; csilva@nyu.edu; gnonato@icmc.usp.br
RI Petronetto, Fabiano/ABC-6521-2020; Dias, Fábio/I-3534-2019; Nonato,
   Luis/D-5782-2011; Valdivia, Paola/AAE-5059-2020; Petronetto,
   Fabiano/I-1742-2016
OI Petronetto, Fabiano/0000-0003-1940-5406; Valdivia,
   Paola/0000-0002-8627-1316; Dal Col, Alcebiades/0000-0002-1376-2229;
   Dias, Fabio/0000-0002-8781-583X
FU Sao Paulo Research Foundation (FAPESP) [2011/22749-8, 2013/14089-3,
   2014/12815-1, 2015/03330-7, 2016/04391-2]; CAPES; Fundacao de Amparo a
   Pesquisa do Estado de Sao Paulo (FAPESP) [14/12815-1, 13/14089-3,
   15/03330-7, 16/04391-2] Funding Source: FAPESP
FX Grants 2011/22749-8, 2013/14089-3, 2014/12815-1, 2015/03330-7,
   2016/04391-2 Sao Paulo Research Foundation (FAPESP). The views expressed
   are those of the authors and do not reflect the official policy or
   position of the Sao Paulo Research Foundation. Alcebiades acknowledges
   CAPES (Brazilian government agency for postgraduate studies) for the
   financial support to the present research work. The authors wish to
   thank Dr. Harish Doraiswamy for the assistance with the Taxi dataset.
NR 43
TC 22
Z9 24
U1 0
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2018
VL 24
IS 8
BP 2456
EP 2469
DI 10.1109/TVCG.2017.2746080
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GL6DJ
UT WOS:000437269000015
PM 28866594
DA 2025-03-07
ER

PT J
AU Bovet, S
   Debarba, HG
   Herbelin, B
   Molla, E
   Boulic, R
AF Bovet, Sidney
   Debarba, Henrique Galvan
   Herbelin, Bruno
   Molla, Eray
   Boulic, Ronan
TI The Critical Role of Self-Contact for Embodiment in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 25th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 18-22, 2018
CL Reutlingen, GERMANY
SP IEEE
DE Virtual Reality; Avatar; Embodiment; Agency; Body Ownership;
   Self-contact
ID MOTOR-PERFORMANCE; CONSCIOUSNESS; PRINCIPLES; OWNERSHIP; HAND
AB With the broad range of motion capture devices available on the market, it is now commonplace to directly control the limb movement of an avatar during immersion in a virtual environment. Here, we study how the subjective experience of embodying a full-body controlled avatar is influenced by motor alteration and self-contact mismatches. Self-contact is in particular a strong source of passive haptic feedback and we assume it to bring a clear benefit in terms of embodiment. For evaluating this hypothesis, we experimentally manipulate self-contacts and the virtual hand displacement relatively to the body. We introduce these body posture transformations to experimentally reproduce the imperfect or incorrect mapping between real and virtual bodies, with the goal of quantifying the limits of acceptance for distorted mapping on the reported body ownership and agency. We first describe how we exploit egocentric coordinate representations to perform a motion capture ensuring that real and virtual hands coincide whenever the real hand is in contact with the body. Then, we present a pilot study that focuses on quantifying our sensitivity to visuo-tactile mismatches. The results are then used to design our main study with two factors, offset (for self-contact) and amplitude (for movement amplification). Our main result shows that subjects' embodiment remains important, even when an artificially amplified movement of the hand was performed, but provided that correct self-contacts are ensured.
C1 [Bovet, Sidney; Debarba, Henrique Galvan; Molla, Eray] Ecole Polytech Fed Lausanne, Lausanne, Switzerland.
   [Bovet, Sidney] Logitech, Lausanne, Switzerland.
   [Debarba, Henrique Galvan] Artanim Fdn, Meyrin, Switzerland.
   [Herbelin, Bruno] Ecole Polytech Fed Lausanne, Cognit Neurosci Lab, Lausanne, Switzerland.
   [Boulic, Ronan] Ecole Polytech Fed Lausanne, Immers Interact Res Grp, Lausanne, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; Ecole Polytechnique
   Federale de Lausanne; Logitech International S.A.; Swiss Federal
   Institutes of Technology Domain; Ecole Polytechnique Federale de
   Lausanne; Swiss Federal Institutes of Technology Domain; Ecole
   Polytechnique Federale de Lausanne
RP Bovet, S (corresponding author), Logitech, Lausanne, Switzerland.
EM sidney.bovet@alumni.epfl.ch; henrique.debarba@artanim.ch;
   bruno.herbelin@epfl.ch; eraymolla@gmail.com; ronan.boulic@epfl.ch
RI Galvan Debarba, Henrique/D-8081-2015
OI Galvan Debarba, Henrique/0000-0003-2090-9409; Herbelin,
   Bruno/0000-0003-4570-5146
FU Hasler fundation [16033]; SNFS grant [200020-159968]; Swiss National
   Science Foundation (SNF) [200020_159968] Funding Source: Swiss National
   Science Foundation (SNF)
FX We wish to thank the anonymous reviewers for their comments. This work
   was supported by the Hasler fundation with grant 16033 and partially
   supported by the SNFS grant 200020-159968.
NR 38
TC 42
Z9 46
U1 0
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2018
VL 24
IS 4
BP 1428
EP 1436
DI 10.1109/TVCG.2018.2794658
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FZ6BS
UT WOS:000427682500005
PM 29543161
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Schmitz, P
   Hildebrandt, J
   Valdez, AC
   Kobbelt, L
   Ziefle, M
AF Schmitz, Patric
   Hildebrandt, Julian
   Valdez, Andre Calero
   Kobbelt, Leif
   Ziefle, Martina
TI You Spin my Head Right Round: Threshold of Limited Immersion for
   Rotation Gains in Redirected Walking
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 25th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 18-22, 2018
CL Reutlingen, GERMANY
SP IEEE
DE Virtual reality; redirected walking; rotation gain; perceptual
   threshold; immersion; cybersickness
ID VIRTUAL ENVIRONMENTS; CYBERSICKNESS
AB In virtual environments, the space that can be explored by real walking is limited by the size of the tracked area. To enable unimpeded walking through large virtual spaces in small real-world surroundings, redirection techniques are used. These unnoticeably manipulate the user's virtual walking trajectory. It is important to know how strongly such techniques can be applied without the user noticing the manipulation-or getting cybersick. Previously, this was estimated by measuring a detection threshold (DT) in highly-controlled psychophysical studies, which experimentally isolate the effect but do not aim for perceived immersion in the context of VR applications. While these studies suggest that only relatively low degrees of manipulation are tolerable, we claim that, besides establishing detection thresholds, it is important to know when the user's immersion breaks. We hypothesize that the degree of unnoticed manipulation is significantly different from the detection threshold when the user is immersed in a task. We conducted three studies: a) to devise an experimental paradigm to measure the threshold of limited immersion (TLI), b) to measure the TLI for slowly decreasing and increasing rotation gains, and c) to establish a baseline of cybersickness for our experimental setup. For rotation gains greater than 1.0, we found that immersion breaks quite late after the gain is detectable. However, for gains lesser than 1.0, some users reported a break of immersion even before established detection thresholds were reached. Apparently, the developed metric measures an additional quality of user experience. This article contributes to the development of effective spatial compression methods by utilizing the break of immersion as a benchmark for redirection techniques.
C1 [Schmitz, Patric; Kobbelt, Leif] Rhein Westfal TH Aachen, Visual Comp Inst, Aachen, Germany.
   [Hildebrandt, Julian; Valdez, Andre Calero; Ziefle, Martina] Rhein Westfal TH Aachen, HCIC, Aachen, Germany.
C3 RWTH Aachen University; RWTH Aachen University
RP Schmitz, P (corresponding author), Rhein Westfal TH Aachen, Visual Comp Inst, Aachen, Germany.
EM patric.schmitz@cs.rwth-aachen.de; hildebrandt@comm.rwth-aachen.de;
   calero-valdez@comm.rwth-aachen.de; kobbelt@cs.rwth-aachen.de;
   ziefle@comm.rwth-aachen.de
RI Schmitz, Patric/ABG-9091-2020; Calero Valdez, André/AAJ-3310-2021;
   Calero Valdez, Andre/E-8359-2012
OI Calero Valdez, Andre/0000-0002-6214-1461; Kobbelt,
   Leif/0000-0002-7880-9470
FU strategy funds of the RWTH Aachen University within the project house
   "ICT Foundations of a Digitized Industry, Economy, and Society"
FX Authors would like to thank Sarah Hilker, Luca Liener, Christian Mattes,
   Kaspar Scharf, Kevin Wegener as well as Pascal Fries for their help in
   the experimental work. Also, thanks to the participants for their
   willingness to spend their time in novel VR environments. This work has
   been funded by the strategy funds of the RWTH Aachen University within
   the project house "ICT Foundations of a Digitized Industry, Economy, and
   Society".
NR 48
TC 55
Z9 57
U1 1
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2018
VL 24
IS 4
BP 1623
EP 1632
DI 10.1109/TVCG.2018.2793671
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FZ6BS
UT WOS:000427682500025
PM 29543179
DA 2025-03-07
ER

PT J
AU Zirr, T
   Dachsbacher, C
AF Zirr, Tobias
   Dachsbacher, Carsten
TI Memory-Efficient On-the-Fly Voxelization and Rendering of Particle Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Surface extraction; interactive particle visualization; ray tracing
ID SURFACE RECONSTRUCTION; SIMULATION; MODEL
AB In this paper we present a novel GPU-friendly real-time voxelization technique for rendering homogeneous media that is defined by particles, e.g., fluids obtained from particle-based simulations such as Smoothed Particle Hydrodynamics (SPH). Our method computes view-adaptive binary voxelizations with on-the-fly compression of a tiled perspective voxel grid, achieving higher resolutions than previous approaches. It allows for interactive generation of realistic images, enabling advanced rendering techniques such as ray casting-based refraction and reflection, light scattering and absorption, and ambient occlusion. In contrast to previous methods, it does not rely on preprocessing such as expensive, and often coarse, scalar field conversion or mesh generation steps. Our method directly takes unsorted particle data as input. It can be further accelerated by identifying fully populated simulation cells during simulation. The extracted surface can be filtered to achieve smooth surface appearance. Finally, we provide a new scheme for accelerated ray casting inside the voxelization.
C1 [Zirr, Tobias; Dachsbacher, Carsten] Karlsruhe Inst Technol, D-76131 Karlsruhe, Germany.
C3 Helmholtz Association; Karlsruhe Institute of Technology
RP Zirr, T (corresponding author), Karlsruhe Inst Technol, D-76131 Karlsruhe, Germany.
EM tobias.zirr@kit.edu; dachsbacher@kit.edu
FU Heidelberg Institute for Theoretical Studies, H-ITS gGmbH
FX We thank the Computer Graphics group at the University of Freiburg for
   providing us with the dambreak dataset. The first author is funded by a
   grant of the Heidelberg Institute for Theoretical Studies, H-ITS gGmbH.
NR 35
TC 3
Z9 4
U1 0
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2018
VL 24
IS 2
BP 1155
EP 1166
DI 10.1109/TVCG.2017.2656897
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR8BW
UT WOS:000419299900011
PM 28129162
DA 2025-03-07
ER

PT J
AU Chen, YZ
   Xu, PP
   Ren, L
AF Chen, Yuanzhe
   Xu, Panpan
   Ren, Liu
TI Sequence Synopsis: Optimize Visual Summary of Temporal Event Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Time Series Data; Data Transformation and Representation; Visual
   Knowledge Representation; Visual Analytics
ID EXPLORATION; VISUALIZATION; ANALYTICS; PATTERNS
AB Event sequences analysis plays an important role in many application domains such as customer behavior analysis, electronic health record analysis and vehicle fault diagnosis. Real-world event sequence data is often noisy and complex with high event cardinality, making it a challenging task to construct concise yet comprehensive overviews for such data. In this paper. we propose a novel visualization technique based on the minimum description length (MDL) principle to construct a coarse-level overview of event sequence data while balancing the information loss in it. The method addresses a fundamental trade-off in visualization design: reducing visual clutter vs. increasing the information content in a visualization. The method enables simultaneous sequence clustering and pattern extraction and is highly tolerant to noises such as missing or additional events in the data. Based on this approach we propose a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. We demonstrate the usability and effectiveness of our approach through case studies with two real-world datasets. One dataset showcases a new application domain for event sequence visualization, i.e., fault development path analysis in vehicles for predictive maintenance. We also discuss the strengths and limitations of the proposed method based on user feedback.
C1 [Chen, Yuanzhe] Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
   [Xu, Panpan; Ren, Liu] Bosch Res North Amer, Palo Alto, CA USA.
C3 Hong Kong University of Science & Technology
RP Chen, YZ (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
EM ychench@ust.hk; panpan.xu@us.bosch.com; liu.ren@us.bosch.com
FU RGC GRF [16208514]
FX We would like to thank Kelsey Hoggard for supporting the video editing.
   We would also like to thank the VAST reviewers for their valuable
   comments. This work is also supported by RGC GRF 16208514.
NR 57
TC 69
Z9 86
U1 0
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 45
EP 55
DI 10.1109/TVCG.2017.2745083
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400007
PM 28885154
DA 2025-03-07
ER

PT J
AU Pezzotti, N
   Höllt, T
   van Gemert, J
   Lelieveldt, BPF
   Eisemann, E
   Vilanova, A
AF Pezzotti, Nicola
   Hollt, Thomas
   van Gemert, Jan
   Lelieveldt, Boudewijn P. F.
   Eisemann, Elmar
   Vilanova, Anna
TI DeepEyes: Progressive Visual Analytics for Designing Deep Neural
   Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Progressive visual analytics; deep neural networks; machine learning
AB Deep neural networks are now rivaling human accuracy in several pattern recognition problems. Compared to traditional classifiers, where features are handcrafted, neural networks learn increasingly complex features directly from the data. Instead of handcrafting the features, it is now the network architecture that is manually engineered. The network architecture parameters such as the number of layers or the number of filters per layer and their interconnections are essential for good performance. Even though basic design guidelines exist, designing a neural network is an iterative trial-and-error process that takes days or even weeks to perform due to the large datasets used for training. In this paper. we present DeepEyes, a Progressive Visual Analytics system that supports the design of neural networks during training. We present novel visualizations, supporting the identification of layers that learned a stable set of patterns and, therefore, are of interest for a detailed analysis. The system facilitates the identification of problems, such as superfluous filters or layers, and information that is not being captured by the network. We demonstrate the effectiveness of our system through multiple use cases, showing how a trained network can be compressed, reshaped and adapted to different problems.
C1 [Pezzotti, Nicola; Hollt, Thomas; van Gemert, Jan; Lelieveldt, Boudewijn P. F.; Eisemann, Elmar; Vilanova, Anna] Delft Univ Technol, Intelligent Syst Dept, Delft, Netherlands.
   [Lelieveldt, Boudewijn P. F.] Leiden Univ, Div Image Proc, Dept Radiol, Med Ctr, Leiden, Netherlands.
C3 Delft University of Technology; Leiden University - Excl LUMC; Leiden
   University; Leiden University Medical Center (LUMC)
RP Pezzotti, N (corresponding author), Delft Univ Technol, Intelligent Syst Dept, Delft, Netherlands.
RI Lelieveldt, Boudewijn/B-6501-2008
OI Lelieveldt, Boudewijn/0000-0001-8269-7603; /0000-0001-8125-1650
NR 50
TC 121
Z9 150
U1 0
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 98
EP 108
DI 10.1109/TVCG.2017.2744358
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400012
PM 28866543
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Bruneton, E
AF Bruneton, Eric
TI A Qualitative and Quantitative Evaluation of 8 Clear Sky Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Clear sky; atmospheric scattering; model; measurements; evaluation
ID RADIATIVE-TRANSFER; SCATTERING; ALGORITHM; REGION
AB We provide a qualitative and quantitative evaluation of eight clear sky models used in Computer Graphics. We compare the models with each other as well as with measurements and with a reference model from the physics community. After a short summary of the physics of the problem, we present the measurements and the reference model, and how we "invert" it to get the model parameters. We then give an overview of each CG model, and detail its scope, its algorithmic complexity, and its results using the same parameters as in the reference model. We also compare the models with a perceptual study. Our quantitative results confirm that the less simplifications and approximations are used to solve the physical equations, the more accurate are the results. We conclude with a discussion of the advantages and drawbacks of each model, and how to further improve their accuracy.
C1 [Bruneton, Eric] Orange Labs, Caen, France.
   [Bruneton, Eric] INRIA, EVASION team, Paris, France.
   [Bruneton, Eric] Google, Mountain View, CA 94043 USA.
C3 Orange SA; Inria; Google Incorporated
RP Bruneton, E (corresponding author), Google, Mountain View, CA 94043 USA.
NR 28
TC 20
Z9 22
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2017
VL 23
IS 12
BP 2641
EP 2655
DI 10.1109/TVCG.2016.2622272
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6YZ
UT WOS:000414393700013
PM 28362610
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Wang, YW
   Liu, YB
   Heidrich, W
   Dai, QH
AF Wang, Yuwang
   Liu, Yebin
   Heidrich, Wolfgang
   Dai, Qionghai
TI The Light Field Attachment: Turning a DSLR into a Light Field Camera
   Using a Low Budget Camera Ring
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Light field; super-resolution; computational imaging
AB We propose a concept for a lens attachment that turns a standard DSLR camera and lens into a light field camera. The attachment consists of eight low-resolution, low-quality side cameras arranged around the central high-quality SLR lens. Unlike most existing light field camera architectures, this design provides a high-quality 2D image mode, while simultaneously enabling a new high-quality light field mode with a large camera baseline but little added weight, cost, or bulk compared with the base DSLR camera. From an algorithmic point of view, the high-quality light field mode is made possible by a new light field super-resolution method that first improves the spatial resolution and image quality of the side cameras and then interpolates additional views as needed. At the heart of this process is a super-resolution method that we call iterative Patch-And Depth-based Synthesis (iPADS), which combines patch-based and depth-based synthesis in a novel fashion. Experimental results obtained for both real captured data and synthetic data confirm that our method achieves substantial improvements in super-resolution for side-view images as well as the high-quality and view-coherent rendering of dense and high-resolution light fields.
C1 [Wang, Yuwang; Liu, Yebin; Dai, Qionghai] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
   [Heidrich, Wolfgang] King Abdullah Univ Sci & Technol, Visual Comp Ctr, Thuwal 23955, Saudi Arabia.
C3 Tsinghua University; King Abdullah University of Science & Technology
RP Wang, YW (corresponding author), Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
EM wang-yw14@mails.tsinghua.edu.cn; liuyebin@tsinghua.edu.cn;
   wolfgang.heidrich@kaust.edu.sa; qhdai@tsinghua.edu.cn
RI Dai, Qionghai/ABD-5298-2021; Liu, Yebin/L-7393-2019
FU National key foundation for exploring scientific instrument
   [2013YQ140517]; National NSF of China [61522111, 61531014]
FX Yebin Liu is the corresponding author. This work was supported by the
   National key foundation for exploring scientific instrument No.
   2013YQ140517, the National NSF of China grant No. 61522111 and No.
   61531014.
NR 42
TC 44
Z9 49
U1 1
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2017
VL 23
IS 10
BP 2357
EP 2364
DI 10.1109/TVCG.2016.2628743
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FG0VB
UT WOS:000409496700013
PM 28113941
OA Bronze
DA 2025-03-07
ER

PT J
AU Liu, SS
   Maljovec, D
   Wang, B
   Bremer, PT
   Pascucci, V
AF Liu, Shusen
   Maljovec, Dan
   Wang, Bei
   Bremer, Peer-Timo
   Pascucci, Valerio
TI Visualizing High-Dimensional Data: Advances in the Past Decade
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Taxonomy; high-dimensional data; multidimensional data; visualization;
   data models; computational modeling
ID TOPOLOGY-BASED VISUALIZATION; MORSE-SMALE COMPLEXES; OF-THE-ART;
   MULTIVARIATE DATA; FLOW VISUALIZATION; POINT CLOUDS; MULTIDIMENSIONAL
   DATA; PERSISTENT HOMOLOGY; DATA EXPLORATION; FIBER SURFACES
AB Massive simulations and arrays of sensing devices, in combination with increasing computing resources, have generated large, complex, high-dimensional datasets used to study phenomena across numerous fields of study. Visualization plays an important role in exploring such datasets. We provide a comprehensive survey of advances in high-dimensional data visualization that focuses on the past decade. We aim at providing guidance for data practitioners to navigate through a modular view of the recent advances, inspiring the creation of new visualizations along the enriched visualization pipeline, and identifying future opportunities for visualization research.
C1 [Liu, Shusen; Maljovec, Dan; Wang, Bei; Pascucci, Valerio] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
   [Bremer, Peer-Timo] Lawrence Livermore Natl Lab, Livermore, CA 94550 USA.
C3 Utah System of Higher Education; University of Utah; United States
   Department of Energy (DOE); Lawrence Livermore National Laboratory
RP Liu, SS (corresponding author), Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
EM shusenl@sci.utah.edu; maljovec@sci.utah.edu; beiwang@sci.utah.edu;
   bremer5@llnl.gov; pascucci@sci.utah.edu
RI pascucci, Valerio/GXF-0616-2022; Wang, Bei/ABH-7125-2022; Liu,
   Shusen/AAS-7784-2020
OI pascucci, valerio/0000-0002-8877-2042; Wang, Bei/0000-0002-9240-0700;
   Bremer, Peer-Timo/0000-0003-4107-3831
FU US DOE by LLNL [DE-AC52-07NA27344., LLNL-CONF-658933]; US National
   Science Foundation [IIS-1513616, 0904631, DE-EE0004449, DE-NA0002375,
   DE-SC0007446, DE-SC0010498]; NSG [IIS-1045032]; NSF [EFTACI-0906379];
   DOE/NEUP [120341]; DOE/Codesign [P01180734]; U.S. Department of Energy
   (DOE) [DE-SC0010498, DE-SC0007446] Funding Source: U.S. Department of
   Energy (DOE); Direct For Computer & Info Scie & Enginr [1314813] Funding
   Source: National Science Foundation; Div Of Information & Intelligent
   Systems [1314813] Funding Source: National Science Foundation; Div Of
   Information & Intelligent Systems; Direct For Computer & Info Scie &
   Enginr [1654221, 1513616, 1314896] Funding Source: National Science
   Foundation; Office of Advanced Cyberinfrastructure (OAC); Direct For
   Computer & Info Scie & Enginr [0904631] Funding Source: National Science
   Foundation
FX This work was performed in part under the auspices of the US DOE by LLNL
   under Contract DE-AC52-07NA27344., LLNL-CONF-658933. This work is also
   supported in part by US National Science Foundation IIS-1513616, US
   National Science Foundation 0904631, DE-EE0004449, DE-NA0002375,
   DE-SC0007446, DE-SC0010498, NSG IIS-1045032, NSF EFTACI-0906379,
   DOE/NEUP 120341, DOE/Codesign P01180734.
NR 268
TC 216
Z9 250
U1 4
U2 67
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2017
VL 23
IS 3
BP 1249
EP 1268
DI 10.1109/TVCG.2016.2640960
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM8CR
UT WOS:000395539300010
PM 28113321
OA Bronze
DA 2025-03-07
ER

PT J
AU van Goethem, A
   Staals, F
   Löffler, M
   Dykes, J
   Speckmann, B
AF van Goethem, Arthur
   Staals, Frank
   Loffler, Maarten
   Dykes, Jason
   Speckmann, Bettina
TI Multi-Granular Trend Detection for Time-Series Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Interactive Exploration; Trend Detection; Time Series
ID ENSEMBLE; VISUALIZATION
AB Time series (such as stock prices) and ensembles (such as model runs for weather forecasts) are two important types of one-dimensional time-varying data. Such data is readily available in large quantities but visual analysis of the raw data quickly becomes infeasible, even for moderately sized data sets. Trend detection is an effective way to simplify time-varying data and to summarize salient information for visual display and interactive analysis. We propose a geometric model for trend-detection in one-dimensional time-varying data, inspired by topological grouping structures for moving objects in two-or higher-dimensional space. Our model gives provable guarantees on the trends detected and uses three natural parameters: granularity, support-size, and duration. These parameters can be changed on-demand. Our system also supports a variety of selection brushes and a time-sweep to facilitate refined searches and interactive visualization of (sub-)trends. We explore different visual styles and interactions through which trends, their persistence, and evolution can be explored.
C1 [van Goethem, Arthur; Speckmann, Bettina] TU Eindhoven, Eindhoven, Netherlands.
   [Staals, Frank] Aarhus Univ, MADALGO, DK-8000 Aarhus C, Denmark.
   [Loffler, Maarten] Univ Utrecht, NL-3508 TC Utrecht, Netherlands.
   [Dykes, Jason] City Univ London, London, England.
C3 Eindhoven University of Technology; Aarhus University; Utrecht
   University; City St Georges, University of London; City, University of
   London
RP van Goethem, A (corresponding author), TU Eindhoven, Eindhoven, Netherlands.
EM a.i.v.goethem@tue.nl; fstaals@cs.au.dk; m.loffler@uu.nl;
   J.Dykes@city.ac.uk; b.speckmann@tue.nl
RI Löffler, Maarten/KBC-8616-2024; Dykes, Jason/AAD-6067-2021
OI Speckmann, Bettina/0000-0002-8514-7858
FU Netherlands Organisation for Scientific Research (NWO) [612.001.102,
   639.023.208]; Danish National Research Foundation [DNRF84]
FX This research started at the workshop "Geometric Algorithms in the
   Field" at the Lorentz Center. A.v.G. and B.S. are supported by the
   Netherlands Organisation for Scientific Research (NWO) under project no.
   612.001.102, and 639.023.208 respectively. F.S. is supported by the
   Danish National Research Foundation under grant nr. DNRF84.
NR 38
TC 13
Z9 14
U1 4
U2 24
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 661
EP 670
DI 10.1109/TVCG.2016.2598619
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600069
PM 27875181
OA Green Submitted, Green Accepted
DA 2025-03-07
ER

PT J
AU Wang, JL
   Fang, T
   Su, QK
   Zhu, SY
   Liu, JB
   Cai, SN
   Tai, CL
   Quan, L
AF Wang, Jinglu
   Fang, Tian
   Su, Qingkun
   Zhu, Siyu
   Liu, Jingbo
   Cai, Shengnan
   Tai, Chiew-Lan
   Quan, Long
TI Image-Based Building Regularization Using Structural Linear Features
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Modeling packages; reconstruction
ID SURFACE RECONSTRUCTION
AB Reconstructed building models using stereo-based methods inevitably suffer from noise, leading to the lack of regularity which is characterized by straightness of structural linear features and smoothness of homogeneous regions. We leverage the structural linear features embedded in the mesh to construct a novel surface scaffold structure for model regularization. The regularization comprises two iterative stages: (1) the linear features are semi-automatically proposed from images by exploiting photometric and geometric clues jointly; (2) the scaffold topology represented by spatial relations among the linear features is optimized according to data fidelity and topological rules, then the mesh is refined by adjusting itself to the consolidated scaffold. Our method has two advantages. First, the proposed scaffold representation is able to concisely describe semantic building structures. Second, the scaffold structure is embedded in the mesh, which can preserve the mesh connectivity and avoid stitching or intersecting surfaces in challenging cases. We demonstrate that our method can enhance structural characteristics and suppress irregularities in the building models robustly in some challenging datasets. Moreover, the regularization can significantly improve the results of general applications such as simplification and non-photorealistic rendering.
C1 [Wang, Jinglu; Fang, Tian; Su, Qingkun; Zhu, Siyu; Liu, Jingbo; Cai, Shengnan; Tai, Chiew-Lan; Quan, Long] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology
RP Wang, JL (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Kowloon, Hong Kong, Peoples R China.
EM jwangae@cse.ust.hk; tianft@cse.ust.hk; qsuaa@cse.ust.hk;
   szhu@cse.ust.hk; jingbo@cse.ust.hk; scaiad@cse.ust.hk; taicl@cse.ust.hk;
   quan@cse.ust.hk
RI Wang, Jinglu/JVN-3859-2024
OI Wang, Jinglu/0000-0002-3222-6579; Zhu, Siyu/0000-0003-0293-0044
FU RGC-GRF [16208614, 618711, 16209514, 619611, ITC-PSKL12EG02]
FX The authors would like to thank the anonymous reviewers for their
   constructive comments, Zhaohua Li for her help in the early version of
   the paper, Zhexi Wang and Shiwei Li for their valuable discussion, and
   Shauna Dalton for proofreading the final version. This work was
   supported by RGC-GRF 16208614, 618711, 16209514, 619611, and
   ITC-PSKL12EG02.
NR 41
TC 13
Z9 14
U1 2
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2016
VL 22
IS 6
BP 1760
EP 1772
DI 10.1109/TVCG.2015.2461163
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DO0MX
UT WOS:000377474100012
DA 2025-03-07
ER

PT J
AU Wu, J
   Dick, C
   Westermann, R
AF Wu, Jun
   Dick, Christian
   Westermann, Ruediger
TI A System for High-Resolution Topology Optimization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Topology optimization; 3D printing; finite element analysis; multigrid
ID CODE WRITTEN; DESIGN
AB A key requirement in 3D fabrication is to generate objects with individual exterior shapes and their interior being optimized to application-specific force constraints and low material consumption. Accomplishing this task is challenging on desktop computers, due to the extreme model resolutions that are required to accurately predict the physical shape properties, requiring memory and computational capacities going beyond what is currently available. Moreover, fabrication-specific constraints need to be considered to enable printability. To address these challenges, we present a scalable system for generating 3D objects using topology optimization, which allows to efficiently evolve the topology of high-resolution solids towards printable and light-weight-high-resistance structures. To achieve this, the system is equipped with a high-performance GPU solver which can efficiently handle models comprising several millions of elements. A minimum thickness constraint is built into the optimization process to automatically enforce printability of the resulting shapes. We further shed light on the question how to incorporate geometric shape constraints, such as symmetry and pattern repetition, in the optimization process. We analyze the performance of the system and demonstrate its potential by a variety of different shapes such as interior structures within closed surfaces, exposed support structures, and surface models.
C1 [Wu, Jun; Dick, Christian; Westermann, Ruediger] Tech Univ Munich, Comp Graph & Visualizat Grp, D-80290 Munich, Germany.
C3 Technical University of Munich
RP Wu, J; Dick, C; Westermann, R (corresponding author), Tech Univ Munich, Comp Graph & Visualizat Grp, D-80290 Munich, Germany.
EM jun.wu@tum.de; dick@tum.de; westermann@tum.de
RI Wu, Jun/L-2487-2017
OI Wu, Jun/0000-0003-4237-1806
FU European Union under the ERC [291372]; European Research Council (ERC)
   [291372] Funding Source: European Research Council (ERC)
FX We thank the reviewers for their constructive suggestions, Lin Lu and
   Yuan Wei for providing the comparison data, Niels Aage and Ole Sigmund
   for helpful discussions, and Florian Reichl and Matthaus G. Chajdas for
   image and video rendering. This work was partially supported by the
   European Union under the ERC Advanced Grant 291372 SaferVis-Uncertainty
   Visualization for Reliable Data Discovery.
NR 54
TC 118
Z9 129
U1 2
U2 39
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2016
VL 22
IS 3
BP 1195
EP 1208
DI 10.1109/TVCG.2015.2502588
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE2DC
UT WOS:000370435700003
PM 26600063
DA 2025-03-07
ER

PT J
AU Byska, J
   Le Muzic, M
   Gröller, ME
   Viola, I
   Kozlíková, B
AF Byska, Jan
   Le Muzic, Mathieu
   Groller, M. Eduard
   Viola, Ivan
   Kozlikova, Barbora
TI AnimoAminoMiner: Exploration of Protein Tunnels and their Properties in
   Molecular Dynamics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Protein; tunnel; molecular dynamics; aggregation; interaction
ID VISUALIZATION; CHANNELS; CAVER; MOLE; TOOL
AB In this paper we propose a novel method for the interactive exploration of protein tunnels. The basic principle of our approach is that we entirely abstract from the 3D/4D space the simulated phenomenon is embedded in. A complex 3D structure and its curvature information is represented only by a straightened tunnel centerline and its width profile. This representation focuses on a key aspect of the studied geometry and frees up graphical estate to key chemical and physical properties represented by surrounding amino acids. The method shows the detailed tunnel profile and its temporal aggregation. The profile is interactively linked with a visual overview of all amino acids which are lining the tunnel over time. In this overview, each amino acid is represented by a set of colored lines depicting the spatial and temporal impact of the amino acid on the corresponding tunnel. This representation clearly shows the importance of amino acids with respect to selected criteria. It helps the biochemists to select the candidate amino acids for mutation which changes the protein function in a desired way. The AnimoAminoMiner was designed in close cooperation with domain experts. Its usefulness is documented by their feedback and a case study, which are included.
C1 [Byska, Jan; Kozlikova, Barbora] Masaryk Univ, CS-60177 Brno, Czech Republic.
   [Le Muzic, Mathieu; Groller, M. Eduard; Viola, Ivan; Kozlikova, Barbora] TU Wien, Vienna, Austria.
   [Groller, M. Eduard; Viola, Ivan] Univ Bergen, N-5020 Bergen, Norway.
C3 Masaryk University Brno; Technische Universitat Wien; University of
   Bergen
RP Byska, J (corresponding author), Masaryk Univ, CS-60177 Brno, Czech Republic.
EM xbyska@fi.muni.cz; mathieu@cg.tuwien.ac.at; groeller@cg.tuwien.ac.at;
   viola@cg.tuwien.ac.at; kozlikova@fi.muni.cz
RI Byška, Jan/AAX-6507-2020; Kozlikova, Barbora/G-3890-2014; Gröller,
   Eduard/AAH-2111-2020; Viola, Ivan/O-8944-2014
OI Viola, Ivan/0000-0003-4248-6574; Kozlikova, Barbora/0000-0003-0045-0872
FU Vienna Science and Technology Fund (WWTF) [VRG11-010]; OeAD ICM; 
   [MSMT-1492/2015-1];  [CZ 17/2015]
FX This work was supported through grants from the Vienna Science and
   Technology Fund (WWTF) through project VRG11-010 and the OeAD ICM and
   MSMT-1492/2015-1 through project CZ 17/2015.
NR 25
TC 20
Z9 22
U1 0
U2 34
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 747
EP 756
DI 10.1109/TVCG.2015.2467434
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400080
PM 26529726
DA 2025-03-07
ER

PT J
AU Dutta, S
   Shen, HW
AF Dutta, Soumya
   Shen, Han-Wei
TI Distribution Driven Extraction and Tracking of Features for Time-varying
   Data Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Gaussian mixture model (GMM); Incremental learning; Feature extraction
   and tracking; Time-varying data analysis
ID FLOW VISUALIZATION
AB Effective analysis of features in time-varying data is essential in numerous scientific applications. Feature extraction and tracking are two important tasks scientists rely upon to get insights about the dynamic nature of the large scale time-varying data. However, often the complexity of the scientific phenomena only allows scientists to vaguely define their feature of interest. Furthermore. such features can have varying motion patterns and dynamic evolution over time. As a result, automatic extraction and tracking of features becomes a non-trivial task. In this work, we investigate these issues and propose a distribution driven approach which allows us to construct novel algorithms for reliable feature extraction and tracking with high confidence in the absence of accurate feature definition. We exploit two key properties of an object, motion and similarity to the target feature, and fuse the information gained from them to generate a robust feature-aware classification field at every time step. Tracking of features is done using such classified fields which enhances the accuracy and robustness of the proposed algorithm. The efficacy of our method is demonstrated by successfully applying it on several scientific data sets containing a wide range of dynamic time-varying features.
C1 [Dutta, Soumya; Shen, Han-Wei] Ohio State Univ, GRAVITY Grp, Columbus, OH 43210 USA.
C3 University System of Ohio; Ohio State University
RP Dutta, S (corresponding author), Ohio State Univ, GRAVITY Grp, Columbus, OH 43210 USA.
EM dutta.33@osu.edu; hwshen@cse.ohio-state.edu
RI Dutta, Soumya/AAN-2212-2020; Shen, Han-wei/A-4710-2012
OI Dutta, Soumya/0000-0001-5030-9979
FU NSF [IIS- 1250752, IIS- 1065025]; US Department of Energy [DE-
   SC0007444, DE- DC0012495]
FX This work was supported in part by NSF grants IIS- 1250752, IIS-
   1065025, and US Department of Energy grants DE- SC0007444, DE-
   DC0012495, program manager Lucy Nowell.
NR 44
TC 32
Z9 41
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 837
EP 846
DI 10.1109/TVCG.2015.2467436
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400089
PM 26529731
DA 2025-03-07
ER

PT J
AU Jang, SJ
   Elmqvist, N
   Ramani, K
AF Jang, Sujin
   Elmqvist, Niklas
   Ramani, Karthik
TI Motion Flow: Visual Abstraction and Aggregation of Sequential Patterns
   in Human Motion Tracking Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Human motion visualization; interactive clustering; motion tracking
   data; expert reviews; user study
ID VISUALIZATION; ALGORITHM; TREE
AB Pattern analysis of human motions, which is useful in many research areas, requires understanding and comparison of different styles of motion patterns. However, working with human motion tracking data to support such analysis poses great challenges. In this paper, we propose Motion Flow, a visual analytics system that provides an effective overview of various motion patterns based on an interactive flow visualization. This visualization formulates a motion sequence as transitions between static poses, and aggregates these sequences into a tree diagram to construct a set of motion patterns. The system also allows the users to directly reflect the context of data and their perception of pose similarities in generating representative pose states. We provide local and global controls over the partition-based clustering process. To support the users in organizing unstructured motion data into pattern groups, we designed a set of interactions that enables searching for similar motion sequences from the data, detailed exploration of data subsets, and creating and modifying the group of motion patterns. To evaluate the usability of Motion Flow, we conducted a user study with six researchers with expertise in gesture-based interaction design. They used Motion Flow to explore and organize unstructured motion tracking data. Results show that the researchers were able to easily learn how to use Motion Flow, and the system effectively supported their pattern analysis activities, including leveraging their perception and domain knowledge.
C1 [Jang, Sujin; Ramani, Karthik] Purdue Univ, W Lafayette, IN 47907 USA.
   [Elmqvist, Niklas] Univ Maryland, College Pk, MD 20742 USA.
C3 Purdue University System; Purdue University; University System of
   Maryland; University of Maryland College Park
RP Jang, SJ (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.
EM jang64@purdue.edu; elm@umd.edu; ramani@purdue.edu
RI Jang, Sujin/LXV-4457-2024
OI Jang, Sujin/0000-0002-2723-5606; Elmqvist, Niklas/0000-0001-5805-5301
FU NSF from CMMI [1235232]; CPS [1329979]; Purdue School of Mechanical
   Engineering; Directorate For Engineering; Div Of Civil, Mechanical, &
   Manufact Inn [1329979] Funding Source: National Science Foundation
FX This work was partially supported by the NSF Award No.1235232 from CMMI
   and 1329979 from CPS, as well as the Donald W. Feddersen Chaired
   Professorship from Purdue School of Mechanical Engineering. Any
   opinions, findings, and conclusions or recommendations expressed in this
   material are those of the authors and do not necessarily reflect the
   views of the sponsors.
NR 46
TC 22
Z9 24
U1 1
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 21
EP 30
DI 10.1109/TVCG.2015.2468292
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400007
PM 26529685
DA 2025-03-07
ER

PT J
AU Kieffer, S
   Dwyer, T
   Marriott, K
   Wybrow, M
AF Kieffer, Steve
   Dwyer, Tim
   Marriott, Kim
   Wybrow, Michael
TI HOLA: Human-like Orthogonal Network Layout
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Graph layout; orthogonal layout; automatic layout algorithms;
   user-generated layout; graph-drawing aesthetics
ID GRAPH LAYOUTS; VISUALIZATION; ALGORITHMS; DIAGRAMS; FEATURES; SYSTEM
AB Over the last 50 years a wide variety of automatic network layout algorithms have been developed. Some are fast heuristic techniques suitable for networks with hundreds of thousands of nodes while others are multi-stage frameworks for higher-quality layout of smaller networks. However, despite decades of research currently no algorithm produces layout of comparable quality to that of a human. We give a new "human-centred" methodology for automatic network layout algorithm design that is intended to overcome this deficiency. User studies are first used to identify the aesthetic criteria algorithms should encode, then an algorithm is developed that is informed by these criteria and finally, a follow-up study evaluates the algorithm output. We have used this new methodology to develop an automatic orthogonal network layout method, HOLA, that achieves measurably better (by user study) layout than the best available orthogonal layout algorithm and which produces layouts of comparable quality to those produced by hand.
C1 [Kieffer, Steve; Dwyer, Tim; Marriott, Kim; Wybrow, Michael] Monash Univ, Clayton, Vic 3800, Australia.
   [Kieffer, Steve; Marriott, Kim] NICTA Victoria, West Melbourne, Vic, Australia.
C3 Monash University
RP Kieffer, S (corresponding author), Monash Univ, Clayton, Vic 3800, Australia.
EM steve.kieffer@monash.edu; tim.dwyer@monash.edu; kim.marriott@monash.edu;
   michael.wybrow@monash.edu
OI Wybrow, Michael/0000-0001-5536-7780; Dwyer, Tim/0000-0002-9076-9571
FU Australian Research Council [DP140100077]
FX We acknowledge the support of the Australian Research Council through
   Discovery Project DP140100077
NR 32
TC 51
Z9 61
U1 1
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 349
EP 358
DI 10.1109/TVCG.2015.2467451
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400040
PM 26390483
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Sacha, D
   Senaratne, H
   Kwon, BC
   Ellis, G
   Keim, DA
AF Sacha, Dominik
   Senaratne, Hansi
   Kwon, Bum Chul
   Ellis, Geoffrey
   Keim, Daniel A.
TI The Role of Uncertainty, Awareness, and Trust in Visual Analytics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Visual Analytics; Knowledge Generation; Uncertainty Measures and
   Propagation; Trust Building; Human Factors
ID VISUALIZATION; MODEL; REPRESENTATIONS; SUPPORT; DESIGN
AB Visual analytics supports humans in generating knowledge from large and often complex datasets. Evidence is collected, collated and cross-linked with our existing knowledge. In the process, a myriad of analytical and visualisation techniques are employed to generate a visual representation of the data These often introduce their own uncertainties, in addition to the ones inherent in the data and these propagated and compounded uncertainties can result in impaired decision making. The user's confidence or trust in the results depends on the extent of user's awareness of the underlying uncertainties generated on the system side. This paper unpacks the uncertainties that propagate through visual analytics systems; illustrates how human's perceptual and cognitive biases influence the user's awareness of such uncertainties, and how this affects the user's trust building. The knowledge generation model for visual analytics is used to provide a terminology and framework to discuss the consequences of these aspects in knowledge construction and though examples, machine uncertainty is compared to human trust measures with provenance. Furthermore, guidelines for the design of uncertainty-aware systems are presented that can aid the user in better decision making.
C1 [Sacha, Dominik; Senaratne, Hansi; Kwon, Bum Chul; Ellis, Geoffrey; Keim, Daniel A.] Univ Konstanz, Data Anal & Visualisat Grp, Constance, Germany.
C3 University of Konstanz
RP Sacha, D (corresponding author), Univ Konstanz, Data Anal & Visualisat Grp, Constance, Germany.
EM dominik.sacha@uni-konstanz.de; hansi.senaratne@uni-konstanz.de;
   bum.kwon@uni-konstanz.de; geoffrey.ellis@uni-konstanz.de;
   daniel.keim@uni-konstanz.de
RI Keim, Daniel/X-7749-2019
OI Senaratne, Hansi/0000-0001-8444-2196
FU EU project Visual Analytics for Sense-making in Criminal Intelligence
   Analysis (VALCRI) [FP7-SEC-2013-608142, SPP 1335]
FX This work was supported by the EU project Visual Analytics for
   Sense-making in Criminal Intelligence Analysis (VALCRI) under grant
   number FP7-SEC-2013-608142 and the SPP 1335 project Visual Analysis on
   Movement and Event Data in Spatiotemporal Context.
NR 83
TC 233
Z9 260
U1 3
U2 57
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 240
EP 249
DI 10.1109/TVCG.2015.2467591
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400029
PM 26529704
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Weiss, K
   Lindstrom, P
AF Weiss, Kenneth
   Lindstrom, Peter
TI Adaptive Multi linear Tensor Product Wavelets
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Multi linear interpolation; adaptive wavelets; multiresolution models;
   octrees; continuous reconstruction
ID DIAMOND HIERARCHIES; MESH REFINEMENT; RECONSTRUCTION; TRANSFORMS
AB Many foundational visualization techniques including isosurfacing, direct volume rendering and texture mapping rely on piecewise multilinear interpolation over the cells of a mesh. However, there has not been much focus within the visualization community on techniques that efficiently generate and encode globally continuous functions defined by the union of multilinear cells. Wavelets provide a rich context for analyzing and processing complicated datasets. In this paper, we exploit adaptive regular refinement as a means of representing and evaluating functions described by a subset of their nonzero wavelet coefficients. We analyze the dependencies involved in the wavelet transform and describe how to generate and represent the coarsest adaptive mesh with nodal function values such that the inverse wavelet transform is exactly reproduced via simple interpolation (subdivision) over the mesh elements. This allows for an adaptive, sparse representation of the function with on-demand evaluation at any point in the domain. We focus on the popular wavelets formed by tensor products of linear B-splines, resulting in an adaptive, nonconforming but crack-free quadtree (2D) or octree (3D) mesh that allows reproducing globally continuous functions via multilinear interpolation over its cells.
C1 [Weiss, Kenneth; Lindstrom, Peter] Lawrence Livermore Natl Lab, Livermore, CA 94550 USA.
C3 United States Department of Energy (DOE); Lawrence Livermore National
   Laboratory
RP Weiss, K (corresponding author), Lawrence Livermore Natl Lab, Livermore, CA 94550 USA.
EM kweiss@llnl.gov; pl@llnl.gov
OI Weiss, Kenneth/0000-0001-6649-8022; Lindstrom, Peter/0000-0003-3817-4199
FU U.S. Department of Energy by Lawrence Livermore National Laboratory
   [DE-AC52-07NA27344]; Office of Advanced Scientific Computing Research
FX This work was performed under the auspices of the U.S. Department of
   Energy by Lawrence Livermore National Laboratory under Contract
   DE-AC52-07NA27344, and was supported by the Office of Advanced
   Scientific Computing Research. We wish to thank the reviewers for their
   valuable feedback and the various data providers for making their data
   sets freely available.
NR 50
TC 6
Z9 6
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 985
EP 994
DI 10.1109/TVCG.2015.2467412
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400104
PM 26529742
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kumar, A
   Garg, S
   Dutta, S
AF Kumar, Atul
   Garg, Siddharth
   Dutta, Soumya
TI Uncertainty-Aware Deep Neural Representations for Visual Analysis of
   Vector Field Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Uncertainty; Data visualization; Vectors; Predictive models; Estimation;
   Data models; Computational modeling; Implicit Neural Network; Monte
   Carlo Dropout; Deep Ensemble; Vector Field; Visualization; Deep Learning
ID CRITICAL-POINTS; FLOW; VISUALIZATION; SEGMENTATION; STREAMLINES
AB The widespread use of Deep Neural Networks (DNNs) has recently resulted in their application to challenging scientific visualization tasks. While advanced DNNs demonstrate impressive generalization abilities, understanding factors like prediction quality, confidence, robustness, and uncertainty is crucial. These insights aid application scientists in making informed decisions. However, DNNs lack inherent mechanisms to measure prediction uncertainty, prompting the creation of distinct frameworks for constructing robust uncertainty-aware models tailored to various visualization tasks. In this work, we develop uncertainty-aware implicit neural representations to model steady-state vector fields effectively. We comprehensively evaluate the efficacy of two principled deep uncertainty estimation techniques: (1) Deep Ensemble and (2) Monte Carlo Dropout, aimed at enabling uncertainty-informed visual analysis of features within steady vector field data. Our detailed exploration using several vector data sets indicate that uncertainty-aware models generate informative visualization results of vector field features. Furthermore, incorporating prediction uncertainty improves the resilience and interpretability of our DNN model, rendering it applicable for the analysis of non-trivial vector field data sets.
C1 [Kumar, Atul; Dutta, Soumya] IIT Kanpur, Dept Comp Sci & Engn, Kanpur, India.
   [Garg, Siddharth] IIT Kanpur, Elect Engn Dept, Kanpur, India.
C3 Indian Institute of Technology System (IIT System); Indian Institute of
   Technology (IIT) - Kanpur; Indian Institute of Technology System (IIT
   System); Indian Institute of Technology (IIT) - Kanpur
RP Dutta, S (corresponding author), IIT Kanpur, Dept Comp Sci & Engn, Kanpur, India.
EM soumya.cvpr@gmail.com
OI Dutta, Soumya/0000-0001-5030-9979
NR 79
TC 0
Z9 0
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2025
VL 31
IS 1
BP 1343
EP 1353
DI 10.1109/TVCG.2024.3456360
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N3C7E
UT WOS:001363163600001
PM 39250384
DA 2025-03-07
ER

PT J
AU Gao, XH
   Yang, Y
   Xie, ZY
   Du, SY
   Sun, ZQ
   Wu, Y
AF Gao, Xuehao
   Yang, Yang
   Xie, Zhenyu
   Du, Shaoyi
   Sun, Zhongqian
   Wu, Yang
TI GUESS: GradUally Enriching SyntheSis for Text-Driven Human Motion
   Generation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Three-dimensional displays; Generators; Visualization;
   Skeleton; Feature extraction; Electronic mail; Coarse-to-fine
   generation; deep generative model; human motion synthesis; latent
   conditional diffusion
AB In this article, we propose a novel cascaded diffusion-based generative framework for text-driven human motion synthesis, which exploits a strategy named GradUally Enriching SyntheSis (GUESS as its abbreviation). The strategy sets up generation objectives by grouping body joints of detailed skeletons in close semantic proximity together and then replacing each of such joint group with a single body-part node. Such an operation recursively abstracts a human pose to coarser and coarser skeletons at multiple granularity levels. With gradually increasing the abstraction level, human motion becomes more and more concise and stable, significantly benefiting the cross-modal motion synthesis task. The whole text-driven human motion synthesis problem is then divided into multiple abstraction levels and solved with a multi-stage generation framework with a cascaded latent diffusion model: an initial generator first generates the coarsest human motion guess from a given text description; then, a series of successive generators gradually enrich the motion details based on the textual description and the previous synthesized results. Notably, we further integrate GUESS with the proposed dynamic multi-condition fusion mechanism to dynamically balance the cooperative effects of the given textual condition and synthesized coarse motion prompt in different generation stages. Extensive experiments on large-scale datasets verify that GUESS outperforms existing state-of-the-art methods by large margins in terms of accuracy, realisticness, and diversity.
C1 [Gao, Xuehao] Xi An Jiao Tong Univ, Control Sci & Engn, Xian 710049, Peoples R China.
   [Yang, Yang] Xi An Jiao Tong Univ, Sch Elect & Informat Engn, Xian 710049, Peoples R China.
   [Du, Shaoyi] Xi An Jiao Tong Univ, Inst Artificial Intelligence & Robot, Xian 710049, Peoples R China.
   [Xie, Zhenyu] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou 510275, Peoples R China.
   [Sun, Zhongqian; Wu, Yang] Tencent AI Lab, Shenzhen 518057, Peoples R China.
C3 Xi'an Jiaotong University; Xi'an Jiaotong University; Xi'an Jiaotong
   University; Sun Yat Sen University; Tencent
RP Wu, Y (corresponding author), Tencent AI Lab, Shenzhen 518057, Peoples R China.
EM gaoxuehao.xjtu@gmail.com; yyang@mail.xjtu.edu.cn;
   xiezhy6@mail2.sysu.edu.cn; dushaoyi@gmail.com; sallensun@tencent.com;
   dylanywu@tencent.com
RI Xie, Zhenyu/AAA-5933-2021
OI Yang, Yang/0000-0001-8687-4427; Gao, Xuehao/0000-0003-3168-5770; Du,
   Shaoyi/0000-0002-7092-0596; /0000-0003-1812-6085; Xie,
   Zhenyu/0000-0001-9207-1014
FU National Key Research and Development Program of China [2018AAA0102500]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0102500. Recommended for
   acceptance by S.-H. Lee.
NR 45
TC 1
Z9 1
U1 8
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2024
VL 30
IS 12
BP 7518
EP 7530
DI 10.1109/TVCG.2024.3352002
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K8D1L
UT WOS:001346124800023
PM 38224502
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Hu, ZM
   Yin, ZM
   Haeufle, D
   Schmitt, S
   Bulling, A
AF Hu, Zhiming
   Yin, Zheming
   Haeufle, Daniel
   Schmitt, Syn
   Bulling, Andreas
TI HOIMotion: Forecasting Human Motion During Human-Object Interactions
   Using Egocentric 3D Object Bounding Boxes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Forecasting; Feature extraction; Three-dimensional displays; Solid
   modeling; Brain modeling; Predictive models; Augmented reality; Human
   motion forecasting; human-object interaction; graph convolutional
   network; augmented reality
AB We present HOIMotion - a novel approach for human motion forecasting during human-object interactions that integrates information about past body poses and egocentric 3D object bounding boxes. Human motion forecasting is important in many augmented reality applications but most existing methods have only used past body poses to predict future motion. HOIMotion first uses an encoder-residual graph convolutional network (GCN) and multi-layer perceptrons to extract features from body poses and egocentric 3D object bounding boxes, respectively. Our method then fuses pose and object features into a novel pose-object graph and uses a residual-decoder GCN to forecast future body motion. We extensively evaluate our method on the Aria digital twin (ADT) and MoGaze datasets and show that HOIMotion consistently outperforms state-of-the-art methods by a large margin of up to 8.7% on ADT and 7.2% on MoGaze in terms of mean per joint position error. Complementing these evaluations, we report a human study (N=20) that shows that the improvements achieved by our method result in forecasted poses being perceived as both more precise and more realistic than those of existing methods. Taken together, these results reveal the significant information content available in egocentric 3D object bounding boxes for human motion forecasting and the effectiveness of our method in exploiting this information.
C1 [Hu, Zhiming; Yin, Zheming; Schmitt, Syn] Univ Stuttgart, Stuttgart, Germany.
   [Haeufle, Daniel] Heidelberg Univ, Heidelberg, Germany.
   [Haeufle, Daniel] Univ Tubingen, Tubingen, Germany.
   [Haeufle, Daniel] Ctr Bion Intelligence Tuebingen Stuttgart BITS, Stuttgart, Germany.
C3 University of Stuttgart; Ruprecht Karls University Heidelberg; Eberhard
   Karls University of Tubingen
RP Hu, ZM (corresponding author), Univ Stuttgart, Stuttgart, Germany.
EM zhiming.hu@vis.uni-stuttgart.de; daniel.haeufle@ziti.uni-heidelberg.de;
   st178328@stud.uni-stuttgart.de; schmitt@simtech.uni-stuttgart.de;
   andreas.bulling@vis.uni-stuttgart.de
RI Schmitt, Syn/D-3136-2011
OI Schmitt, Syn/0000-0002-7768-8961; Hu, Zhiming/0000-0002-5105-9753
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under
   Germany's Excellence Strategy [EXC 2075 - 390740016]
FX This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German
   Research Foundation) under Germany's Excellence Strategy - EXC 2075 -
   390740016.
NR 46
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7375
EP 7385
DI 10.1109/TVCG.2024.3456152
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300038
PM 39255111
DA 2025-03-07
ER

PT J
AU Yu, JX
   Nai, WZ
   Sun, XY
AF Yu, Jiaxin
   Nai, Weizhi
   Sun, Xiaoying
TI Accurate Raycasting Selection With Rotation Gesture Using a 6-DOF
   Tracking Device
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; 6-DOF; Three-dimensional displays; Jitter; Target
   tracking; User interfaces; Aerospace electronics; pen; raycasting; Vive;
   target selection
ID DESIGN
AB A six degrees of freedom (6-DOF) controller is a commonly used input device in three-dimensional user interface (3DUI) applications. However, for the fundamental task of target selection in 3D space, the selection accuracy decreases owing to the Heisenberg effect during the manipulation of the 6-DOF controller. Based on the pointing action of a 6-DOF device, we establish the mathematical model for raycasting and analyze the possibility of using a rotation gesture to select a target. This study proposes a target selection method using the axial rotation of the user's wrist, which reduces the negative impact of a discrete input for triggering the selection on accuracy. The detection model can identify the start time of the user's rotation action. The custom designed control display gain (CD gain) function can maintain the ray's stability during the rotation gesture. This method was verified using a 6-DOF pen and Vive controller in three experiments. The results show that the accuracy of the proposed rotation gesture-based raycasting target selection method is superior to that of the traditional button-pressing method, and it can be integrated into existing tracking systems for further application.
C1 [Yu, Jiaxin; Nai, Weizhi; Sun, Xiaoying] Jilin Univ, Dept Commun Engn, Changchun 130025, Peoples R China.
C3 Jilin University
RP Sun, XY (corresponding author), Jilin Univ, Dept Commun Engn, Changchun 130025, Peoples R China.
EM jxyu18@mails.jlu.edu; naiwz@jlu.edu.cn; sunxy@jlu.edu.cn
OI Yu, Jiaxin/0000-0002-0297-4605; Nai, Weizhi/0000-0002-2447-1814
FU National Key Research and Development Program of China [2016YFB1001304]
FX This work was supported by the National Key Research and Development
   Program of China under Grant 2016YFB1001304.
NR 37
TC 0
Z9 0
U1 1
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6104
EP 6117
DI 10.1109/TVCG.2023.3324373
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000003
PM 37831579
DA 2025-03-07
ER

PT J
AU Kim, H
   Kim, J
   Han, Y
   Hong, H
   Kwon, OS
   Park, YW
   Elmqvist, N
   Ko, S
   Kwon, BC
AF Kim, Hwiyeon
   Kim, Joohee
   Han, Yunha
   Hong, Hwajung
   Kwon, Oh-Sang
   Park, Young-Woo
   Elmqvist, Niklas
   Ko, Sungahn
   Kwon, Bum Chul
TI Towards Visualization Thumbnail Designs That Entice Reading Data-Driven
   Articles
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data journalism; data-driven storytelling; online news; visualization;
   thumbnail images; data stories
ID VISUAL EMBELLISHMENTS; PICTURE; RECOGNITION; MEMORY
AB As online news increasingly include data journalism, there is a corresponding increase in the incorporation of visualization in article thumbnail images. However, little research exists on the design rationale for visualization thumbnails, such as resizing, cropping, simplifying, and embellishing charts that appear within the body of the associated article. Therefore, in this paper we aim to understand these design choices and determine what makes a visualization thumbnail inviting and interpretable. To this end, we first survey visualization thumbnails collected online and discuss visualization thumbnail practices with data journalists and news graphics designers. Based on the survey and discussion results, we then define a design space for visualization thumbnails and conduct a user study with four types of visualization thumbnails derived from the design space. The study results indicate that different chart components play different roles in attracting reader attention and enhancing reader understandability of the visualization thumbnails. We also find various thumbnail design strategies for effectively combining the charts' components, such as a data summary with highlights and data labels, and a visual legend with text labels and Human Recognizable Objects (HROs), into thumbnails. Ultimately, we distill our findings into design implications that allow effective visualization thumbnail designs for data-rich news articles. Our work can thus be seen as a first step toward providing structured guidance on how to design compelling thumbnails for data stories.
C1 [Kim, Hwiyeon; Kim, Joohee; Han, Yunha; Kwon, Oh-Sang; Park, Young-Woo; Ko, Sungahn] UNIST, Ulsan 44919, South Korea.
   [Hong, Hwajung] Korea Adv Inst Sci & Technol, Daejeon 34141, South Korea.
   [Elmqvist, Niklas] Univ Maryland, College Pk, MD 20742 USA.
   [Kwon, Bum Chul] IBM Res, Cambridge, MA 10598 USA.
C3 Ulsan National Institute of Science & Technology (UNIST); Korea Advanced
   Institute of Science & Technology (KAIST); University System of
   Maryland; University of Maryland College Park; International Business
   Machines (IBM); IBM USA
RP Ko, S (corresponding author), UNIST, Ulsan 44919, South Korea.
EM hwiyeon.d@gmail.com; jkim17@unist.ac.kr; diana438@unist.ac.kr;
   hwajung@kaist.ac.kr; oskwon@unist.ac.kr; ywpark@unist.ac.kr;
   elm@umd.edu; sako@unist.ac.kr; bumchul.kwon@us.ibm.com
OI Ko, Sungahn/0000-0002-7410-5652; Kwon, Bum Chul/0000-0002-9391-6274;
   Kim, Joohee/0000-0002-0745-2339; Park, Young-Woo/0000-0003-2257-9394;
   Elmqvist, Niklas/0000-0001-5805-5301
FU Korean National Research Foundation (NRF) [2021R1A2C1004542]; Korea
   Health Industry Development Institute(KHIDI) - Ministry of Health &
   Welfare, Republic of Korea [HI22C0646]; Institute of Information &
   Communications Technology Planning & Evaluation (IITP) [2020-0-01336];
   UNIST - Korea government (MSIT)
FX This work was supported in part by the Korean National Research
   Foundation (NRF) under Grant 2021R1A2C1004542, in part by a grant of the
   Korea Health Technology R&D Project through the Korea Health Industry
   Development Institute(KHIDI), funded by the Ministry of Health &
   Welfare, Republic of Korea under Grant HI22C0646, and in part by the
   Institute of Information & Communications Technology Planning &
   Evaluation (IITP) under Grant 2020-0-01336-Artificial Intelligence
   Graduate School Program, UNIST, funded by the Korea government(MSIT).
   Recommended for acceptance by R. Chang.
NR 78
TC 1
Z9 1
U1 2
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4825
EP 4840
DI 10.1109/TVCG.2023.3278304
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400043
PM 37216254
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Liu, Z
   Zhao, YW
   Zhan, SJ
   Liu, YY
   Chen, RJ
   He, Y
AF Liu, Zheng
   Zhao, Yaowu
   Zhan, Sijing
   Liu, Yuanyuan
   Chen, Renjie
   He, Ying
TI PCDNF: Revisiting Learning-Based Point Cloud Denoising via Joint Normal
   Filtering
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Noise reduction; Point cloud compression; Task analysis; Noise
   measurement; Feature extraction; Three-dimensional displays; Computer
   architecture; Point cloud denoising; normal filtering; 3D deep learning;
   point cloud processing
AB Point cloud denoising is a fundamental and challenging problem in geometry processing. Existing methods typically involve direct denoising of noisy input or filtering raw normals followed by point position updates. Recognizing the crucial relationship between point cloud denoising and normal filtering, we re-examine this problem from a multitask perspective and propose an end-to-end network called PCDNF for joint normal filtering-based point cloud denoising. We introduce an auxiliary normal filtering task to enhance the network's ability to remove noise while preserving geometric features more accurately. Our network incorporates two novel modules. First, we design a shape-aware selector to improve noise removal performance by constructing latent tangent space representations for specific points, taking into account learned point and normal features as well as geometric priors. Second, we develop a feature refinement module to fuse point and normal features, capitalizing on the strengths of point features in describing geometric details and normal features in representing geometric structures, such as sharp edges and corners. This combination overcomes the limitations of each feature type and better recovers geometric information. Extensive evaluations, comparisons, and ablation studies demonstrate that the proposed method outperforms state-of-the-art approaches in both point cloud denoising and normal filtering.
C1 [Liu, Zheng; Zhao, Yaowu; Liu, Yuanyuan] China Univ Geosci Wuhan, Sch Comp Sci, Wuhan 430079, Peoples R China.
   [Zhan, Sijing] China Univ Geosci Wuhan, Natl Engn Res Ctr Geog Informat Syst, Wuhan 430079, Peoples R China.
   [Liu, Yuanyuan] China Univ Geosci Wuhan, Hubei Key Lab Intelligent Geoinformat Proc, Wuhan 430079, Peoples R China.
   [Chen, Renjie] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
   [He, Ying] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 China University of Geosciences; China University of Geosciences; China
   University of Geosciences; Chinese Academy of Sciences; University of
   Science & Technology of China, CAS; Nanyang Technological University
RP Chen, RJ (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
EM liu.zheng.jojo@gmail.com; 912626756@qq.com; 1585730049@qq.com;
   liuyy@cug.edu.cn; renjiec@ustc.edu.cn; yhe@ntu.edu.sg
RI He, Ying/A-3708-2011; Chen, Renjie/AFU-3325-2022; chen,
   renjie/I-5995-2016
OI Liu, Zheng/0000-0001-6713-6680; chen, renjie/0000-0001-8395-4392
FU National Key R#x0026;D Program of China [2022YFB3904100]; NSF of China
   [62072422, 62076227]; NSF of Anhui Province [2008085MF195];
   Collaborative Innovation Center for Natural Resources Planning and
   Marine Technology of Guangzhou [2023B04J0301]; Key-Area Research and
   Development Program of Guangdong Province [2020B0101130009]; Ministry of
   Education, Singapore [MOE-T2EP20220-000, RT19/22]
FX No Statement Available
NR 59
TC 11
Z9 11
U1 19
U2 33
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5419
EP 5436
DI 10.1109/TVCG.2023.3292464
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400013
PM 37405886
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhang, PZ
   Yang, LX
   Xie, XH
   Lai, JH
AF Zhang, Pengze
   Yang, Lingxiao
   Xie, Xiaohua
   Lai, Jianhuang
TI Pose Guided Person Image Generation Via Dual-Task Correlation and
   Affinity Learning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Correlation; Semantics; Image synthesis; Transformers;
   Three-dimensional displays; Training; Generative adversarial networks;
   neural rendering; person image generation; pose transfer; view synthesis
AB Pose Guided Person Image Generation (PGPIG) is the task of transforming a person's image from the source pose to a target pose. Existing PGPIG methods often tend to learn an end-to-end transformation between the source image and the target image, but do not seriously consider two issues: 1) the PGPIG is an ill-posed problem, and 2) the texture mapping requires effective supervision. In order to alleviate these two challenges, we propose a novel method by incorporating Dual-task Pose Transformer Network and Texture Affinity learning mechanism (DPTN-TA). To assist the ill-posed source-to-target task learning, DPTN-TA introduces an auxiliary task, i.e., source-to-source task, by a Siamese structure and further explores the dual-task correlation. Specifically, the correlation is built by the proposed Pose Transformer Module (PTM), which can adaptively capture the fine-grained mapping between sources and targets and can promote the source texture transmission to enhance the details of the generated images. Moreover, we propose a novel texture affinity loss to better supervise the learning of texture mapping. In this way, the network is able to learn complex spatial transformations effectively. Extensive experiments show that our DPTN-TA can produce perceptually realistic person images under significant pose changes. Furthermore, our DPTN-TA is not limited to processing human bodies but can be flexibly extended to view synthesis of other objects, i.e., faces and chairs, outperforming the state-of-the-arts in terms of both LPIPS and FID. Our code is available at: https://github.com/PangzeCheung/Dual-task-Pose-Transformer-Network.
C1 [Zhang, Pengze; Xie, Xiaohua] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Zhang, Pengze; Yang, Lingxiao; Xie, Xiaohua; Lai, Jianhuang] Sun Yat Sen Univ, Guangzhou 510006, Peoples R China.
   [Zhang, Pengze; Yang, Lingxiao; Xie, Xiaohua; Lai, Jianhuang] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
C3 Sun Yat Sen University; Sun Yat Sen University; Sun Yat Sen University
RP Xie, XH (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.; Xie, XH (corresponding author), Sun Yat Sen Univ, Guangzhou 510006, Peoples R China.
EM zhangpz3@mail2.sysu.edu.cn; yanglx9@mail.sysu.edu.cn;
   xiexiaoh6@mail.sysu.edu.cn; stsljh@mail.sysu.edu.cn
RI yang, yong/GYQ-7408-2022; Zhang, Pengze/JAX-8304-2023; Xie,
   Xiaohua/AAW-8365-2020
OI Xie, Xiaohua/0000-0002-0310-4679
FU National Natural Science Foundation of China [U22A2095, 62072482]
FX No Statement Available
NR 74
TC 2
Z9 2
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5111
EP 5128
DI 10.1109/TVCG.2023.3286394
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400097
PM 37318966
DA 2025-03-07
ER

PT J
AU Zhou, WY
   Yuan, L
   Chen, SY
   Gao, L
   Hu, SM
AF Zhou, Wen-Yang
   Yuan, Lu
   Chen, Shu-Yu
   Gao, Lin
   Hu, Shi-Min
TI LC-NeRF: Local Controllable Face Generation in Neural Radiance Field
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Geometry; Faces; Three-dimensional displays; Generators; Semantics;
   Codes; Controllability; 3D face generation; neural radiance fields;
   semantic manipulation
AB 3D face generation has achieved high visual quality and 3D consistency thanks to the development of neural radiance fields (NeRF). However, these methods model the whole face as a neural radiance field, which limits the controllability of the local regions. In other words, previous methods struggle to independently control local regions, such as the mouth, nose, and hair. To improve local controllability in NeRF-based face generation, we propose LC-NeRF, which is composed of a Local Region Generators Module (LRGM) and a Spatial-Aware Fusion Module (SAFM), allowing for geometry and texture control of local facial regions. The LRGM models different facial regions as independent neural radiance fields and the SAFM is responsible for merging multiple independent neural radiance fields into a complete representation. Finally, LC-NeRF enables the modification of the latent code associated with each individual generator, thereby allowing precise control over the corresponding local region. Qualitative and quantitative evaluations show that our method provides better local controllability than state-of-the-art 3D-aware face generation methods. A perception study reveals that our method outperforms existing state-of-the-art methods in terms of image quality, face consistency, and editing effects. Furthermore, our method exhibits favorable performance in downstream tasks, including real image editing and text-driven facial image editing.
C1 [Zhou, Wen-Yang; Hu, Shi-Min] Tsinghua Univ, BNRist, Beijing 100084, Peoples R China.
   [Yuan, Lu] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
   [Chen, Shu-Yu; Gao, Lin] Chinese Acad Sci, Inst Comp Technol, Beijing 100045, Peoples R China.
C3 Tsinghua University; Stanford University; Chinese Academy of Sciences;
   Institute of Computing Technology, CAS
RP Hu, SM (corresponding author), Tsinghua Univ, BNRist, Beijing 100084, Peoples R China.
EM zhouwy19@mails.tsinghua.edu.cn; luyuan@stanford.edu;
   chenshuyu@ict.ac.cn; gaolin@ict.ac.cn; shimin@tsinghua.edu.cn
RI Hu, Shi-Min/AAW-1952-2020; Wenyang, Zhou/GSD-3239-2022
OI Hu, Shi-Min/0000-0001-7507-6542; Yuan, Lu/0009-0004-6399-4337
FU National Key R&D Program of China [2021ZD0112902]; Natural Science
   Foundation of China [62220106003]; Research Grant of Beijing Higher
   Institution Engineering Research Center; Tsinghua-Tencent Joint
   Laboratory for Internet Innovation Technology
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2021ZD0112902, in part by the Natural Science Foundation of
   China under Grant 62220106003, in part by the Research Grant of Beijing
   Higher Institution Engineering Research Center and Tsinghua-Tencent
   Joint Laboratory for Internet Innovation Technology.
NR 31
TC 2
Z9 2
U1 2
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5437
EP 5448
DI 10.1109/TVCG.2023.3293653
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400031
PM 37459257
DA 2025-03-07
ER

PT J
AU Zhuang, JF
   Zeng, P
   Zhuang, W
   Guo, XY
   Liu, PZ
AF Zhuang, Jiafu
   Zeng, Pan
   Zhuang, Wei
   Guo, Xiaoyu
   Liu, Peizhong
TI Supervertex Sampling Network: A Geodesic Differential SLIC Approach for
   3D Mesh
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Clustering algorithms; Shape; Task analysis;
   Manifolds; Deep learning; Training; 3D mesh segmentation; 3D shape
   learning; deep clustering; graph neural network
ID SEGMENTATION
AB The analysis of 3D meshes with deep learning has become prevalent in computer graphics. As an essential structure, hierarchical representation is critical for mesh pooling in multiscale analysis. Existing clustering-based mesh hierarchy construction methods involve nonlinear discretization optimization operations, making them nondifferential and challenging to embed in other trainable networks for learning. Inspired by deep superpixel learning methods in image processing, we extend them from 2D images to 3D meshes by proposing a novel differentiable chart-based segmentation method named geodesic differential supervertex (GDSV). The key to the GDSV method is to ensure that the geodesic position updates are differentiable while satisfying the constraint that the renewed supervertices lie on the manifold surface. To this end, in addition to using the differential SLIC clustering algorithm to update the nonpositional features of the supervertices, a reparameterization trick, the Gumbel-Softmax trick, is employed to renew the geodesic positions of the supervertices. Therefore, the geodesic position update problem is converted into a linear matrix multiplication issue. The GDSV method can be an independent module for chart-based segmentation tasks. Meanwhile, it can be combined with the front-end feature learning network and the back-end task-specific network as a plug-in-plug-out module for training; and be applied to tasks such as shape classification, part segmentation, and 3D scene understanding. Experimental results show the excellent performance of our proposed algorithm on a range of datasets.
C1 [Zhuang, Jiafu] Quanzhou Normal Univ, Sch Phys & Informat Engn, Quanzhou 362000, Fujian, Peoples R China.
   [Zeng, Pan] Huaqiao Univ, Sch Med, Quanzhou 362021, Fujian, Peoples R China.
   [Zhuang, Wei] Univ Manchester, Dept Social Stat, Manchester M13 9PL, England.
   [Guo, Xiaoyu] Quanzhou Normal Univ, Coll Oceanol & Food Sci, Quanzhou 362000, Fujian, Peoples R China.
   [Liu, Peizhong] Huaqiao Univ, Coll Engn, Quanzhou 362021, Fujian, Peoples R China.
   [Liu, Peizhong] Quanzhou Med Coll, Sch Med, Quanzhou 362021, Fujian, Peoples R China.
C3 Quanzhou Normal University; Huaqiao University; University of
   Manchester; Quanzhou Normal University; Huaqiao University
RP Liu, PZ (corresponding author), Huaqiao Univ, Coll Engn, Quanzhou 362021, Fujian, Peoples R China.; Liu, PZ (corresponding author), Quanzhou Med Coll, Sch Med, Quanzhou 362021, Fujian, Peoples R China.
EM jfzhuang001@qq.com; 954181775@qq.com;
   wei.zhuang-2@postgrad.manchester.ac.uk; guoxiaoyu@qztc.edu.cn;
   pzliu@hqu.edu.cn
RI zeng, pan/HHS-0833-2022
OI Liu, Peizhong/0000-0001-8785-0195; , Xiaoyu/0000-0001-7712-4149; Zhuang,
   Wei/0000-0002-6992-4039; zhuang, jiafu/0000-0001-6072-2679
FU Education and Scientific Research Project for Middle-Aged and Young
   Teachers of Fujian Province [JAT210302, JAT210314]; Innovation and
   Entrepreneurship Project for College Students [202110399024]; Fujian
   Provincial Science and Technology Major Project [2020HZ02014]; Fujian
   Province Science and Technology Program of China [2021H0053]; Quanzhou
   Science and Technology Major Project [2021GZ1]; Quanzhou City Science
   and Technology Program of China [2020C019R, 2021C031R]
FX No Statement Available
NR 70
TC 0
Z9 0
U1 5
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5553
EP 5565
DI 10.1109/TVCG.2023.3294845
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400067
PM 37440384
DA 2025-03-07
ER

PT J
AU Fukuoka, M
   Nakamura, F
   Verhulst, A
   Inami, M
   Kitazaki, M
   Sugimoto, M
AF Fukuoka, Masaaki
   Nakamura, Fumihiko
   Verhulst, Adrien
   Inami, Masahiko
   Kitazaki, Michiteru
   Sugimoto, Maki
TI Sensory Attenuation With a Virtual Robotic Arm Controlled Using Facial
   Movements
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Manipulators; Robot sensing systems; Rubber; Cognition; Virtual
   environments; Task analysis; Standards; Sensory attenuation; human
   augmentation; robotic arm; virtual reality
ID SELF-INITIATED SOUNDS; BODY OWNERSHIP; INTERNAL-MODEL; AUDITORY ERPS;
   AGENCY; INTEGRATION; PERCEPTION; AWARENESS; HEARING; SPEECH
AB When humans generate stimuli voluntarily, they perceive the stimuli more weakly than those produced by others, which is called sensory attenuation (SA). SA has been investigated in various body parts, but it is unclear whether an extended body induces SA. This study investigated the SA of audio stimuli generated by an extended body. SA was assessed using a sound comparison task in a virtual environment. We prepared robotic arms as extended bodies, and the robotic arms were controlled by facial movements. To evaluate the SA of robotic arms, we conducted two experiments. Experiment 1 investigated the SA of the robotic arms under four conditions. The results showed that robotic arms manipulated by voluntary actions attenuated audio stimuli. Experiment 2 investigated the SA of the robotic arm and innate body under five conditions. The results indicated that the innate body and robotic arm induced SA, while there were differences in the sense of agency between the innate body and robotic arm. Analysis of the results indicated three findings regarding the SA of the extended body. First, controlling the robotic arm with voluntary actions in a virtual environment attenuates the audio stimuli. Second, there were differences in the sense of agency related to SA between extended and innate bodies. Third, the SA of the robotic arm was correlated with the sense of body ownership.
C1 [Fukuoka, Masaaki; Nakamura, Fumihiko; Sugimoto, Maki] Keio Univ, Fac Sci & Technol, Tokyo 1088345, Japan.
   [Verhulst, Adrien] Keio Univ, Sony Comp Sci Labs, Tokyo 1088345, Japan.
   [Inami, Masahiko] Univ Tokyo, Dept Adv Interdisciplinary Studies, Tokyo 1138654, Japan.
   [Kitazaki, Michiteru] Toyohashi Univ Technol, Dept Comp Sci & Engn, Toyohashi 4418580, Japan.
C3 Keio University; Keio University; University of Tokyo; Toyohashi
   University of Technology
RP Fukuoka, M (corresponding author), Keio Univ, Fac Sci & Technol, Tokyo 1088345, Japan.
EM mskifukuoka@imlab.ics.keio.ac.jp; f.nakamura@imlab.ics.keio.ac.jp;
   adrien.verhulst@imlab.ics.keio.ac.jp; inami@star.rcast.u-tokyo.ac.jp;
   mich@tut.jp; sugimoto@imlab.ics.keio.ac.jp
RI Kitazaki, Mitchiteru/AAI-6554-2020
OI Fukuoka, Masaaki/0000-0001-7892-4623; Kitazaki,
   Michiteru/0000-0003-0966-4842; Nakamura, Fumihiko/0000-0001-6285-3963;
   INAMI, MASAHIKO/0000-0002-8652-0730
FU JST ERATO [JP-MJER1701]
FX This project was supported by JST ERATO under Grant JP-MJER1701.
NR 75
TC 0
Z9 0
U1 2
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4023
EP 4038
DI 10.1109/TVCG.2023.3246092
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700042
PM 37027567
DA 2025-03-07
ER

PT J
AU Hoang, D
   Bhatia, H
   Lindstrom, P
   Pascucci, V
AF Hoang, Duong
   Bhatia, Harsh
   Lindstrom, Peter
   Pascucci, Valerio
TI Progressive Tree-Based Compression of Large-Scale Particle Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Encoding; Decoding; Image reconstruction; Data models; Computational
   modeling; Compressors; Task analysis; Coarse approximation; compression
   (coding); data compaction and compression; hierarchical;
   multiresolution; particle datasets; progressive decompression; tree
   traversal; visualization
ID POINT CLOUDS; MOLECULAR-DYNAMICS; SELF-SIMILARITY; FRAMEWORK; EFFICIENT;
   SURFACES; SCHEME; SYSTEM
AB Scientific simulations and observations using particles have been creating large datasets that require effective and efficient data reduction to store, transfer, and analyze. However, current approaches either compress only small data well while being inefficient for large data, or handle large data but with insufficient compression. Toward effective and scalable compression/decompression of particle positions, we introduce new kinds of particle hierarchies and corresponding traversal orders that quickly reduce reconstruction error while being fast and low in memory footprint. Our solution to compression of large-scale particle data is a flexible block-based hierarchy that supports progressive, random-access, and error-driven decoding, where error estimation heuristics can be supplied by the user. For low-level node encoding, we introduce new schemes that effectively compress both uniform and densely structured particle distributions. Our proposed methods thus target all three phases of a tree-based particle compression pipeline, namely tree construction, tree traversal, and node encoding. The improved efficacy and flexibility of these methods over existing compressors are demonstrated through extensive experimentation, using a wide range of scientific particle datasets.
C1 [Hoang, Duong; Pascucci, Valerio] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
   [Bhatia, Harsh; Lindstrom, Peter] Lawrence Livermore Natl Lab, Ctr Appl Sci Comp, Livermore, CA 94550 USA.
C3 Utah System of Higher Education; University of Utah; United States
   Department of Energy (DOE); Lawrence Livermore National Laboratory
RP Hoang, D (corresponding author), Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
EM duong@sci.utah.edu; hbhatia@llnl.gov; pl@llnl.gov; pascucci@sci.utah.edu
RI pascucci, Valerio/GXF-0616-2022
OI Hoang, Duong/0000-0003-4707-7198; pascucci, valerio/0000-0002-8877-2042;
   Lindstrom, Peter/0000-0003-3817-4199
FU U.S. Department of Energy (DOE) [DE-FE0031880]; Exascale Computing
   Project [17-SC-20-SC]; National Nuclear Security Administration; NSF OAC
   award [1842042, 1941085]; NSF CMMI award [1629660]; DOE
   [DE-AC52-07NA27344]; LLNL-LDRD Program [17-SI-004]
FX This work was supported in part by the U.S. Department of Energy
   (DOE)under Grant DE-FE0031880 and in part by the Exascale Computing
   Project under Grant 17-SC-20-SC, a collaborative effort of the DOE
   Office of Science and the National Nuclear Security Administration. This
   work was supported in part by the NSF OAC award under Grant 1842042, in
   part by the NSF OAC award under Grant 1941085, and in part by the NSF
   CMMI award under Grant 1629660. This work was also performed under the
   auspices of the DOE by Lawrence Livermore National Laboratory under
   Grant DE-AC52-07NA27344 and supported in part by the LLNL-LDRD Program
   under Project under Grant 17-SI-004.
NR 96
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4321
EP 4338
DI 10.1109/TVCG.2023.3260628
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700094
PM 37027261
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Hu, JB
   Wang, SF
   He, Y
   Luo, ZX
   Lei, N
   Liu, LG
AF Hu, Jiangbei
   Wang, Shengfa
   He, Ying
   Luo, Zhongxuan
   Lei, Na
   Liu, Ligang
TI A Parametric Design Method for Engraving Patterns on Thin Shells
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Optimization; Shape; Topology; Design methodology; Computational
   modeling; Solid modeling; Three-dimensional displays; Parametric design;
   pattern engraving; structural optimization; thin shells
ID FINITE-ELEMENT-ANALYSIS; TOPOLOGY OPTIMIZATION; SURFACES
AB Designing thin-shell structures that are diverse, lightweight, and physically viable is a challenging task for traditional heuristic methods. To address this challenge, we present a novel parametric design framework for engraving regular, irregular, and customized patterns on thin-shell structures. Our method optimizes pattern parameters such as size and orientation, to ensure structural stiffness while minimizing material consumption. Our method is unique in that it works directly with shapes and patterns represented by functions, and can engrave patterns through simple function operations. By eliminating the need for remeshing in traditional FEM methods, our method is more computationally efficient in optimizing mechanical properties and can significantly increase the diversity of shell structure design. Quantitative evaluation confirms the convergence of the proposed method. We conduct experiments on regular, irregular, and customized patterns and present 3D printed results to demonstrate the effectiveness of our approach.
C1 [Hu, Jiangbei; Wang, Shengfa; Lei, Na] Dalian Univ Technol, DUT RU Int Sch Informat & Software Engn, Dalian 116024, Peoples R China.
   [Hu, Jiangbei; Lei, Na] Dalian Univ Technol, Key Lab Ubiquitous Network & Serv Software Liaonin, Dalian 116024, Peoples R China.
   [He, Ying] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Luo, Zhongxuan] Dalian Univ Technol, Sch Software Technol, Dalian 116024, Peoples R China.
   [Luo, Zhongxuan] Guilin Univ Elect Technol, Inst Artificial Intelligence, Guilin 541004, Guangxi, Peoples R China.
   [Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei, Anhui, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology;
   Nanyang Technological University; Dalian University of Technology;
   Guilin University of Electronic Technology; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS
RP Wang, SF (corresponding author), Dalian Univ Technol, DUT RU Int Sch Informat & Software Engn, Dalian 116024, Peoples R China.; He, Y (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM jiangbei.hu@ntu.edu.sg; sfwang@dlut.edu.cn; yhe@ntu.edu.sg;
   zxluo@dlut.edu.cn; nalei@dlut.edu.cn; lgliu@ustc.edu.cn
RI Hu, Jiangbei/KXJ-1687-2024; He, Ying/A-3708-2011; Liu,
   Ligang/IZQ-5817-2023; liu, na/HKF-7392-2023
OI Hu, Jiangbei/0000-0002-6774-6267; Lei, Na/0000-0003-3361-0756
FU National Key R&D Program of China [2020YFB1709402, 2021YFA1003003];
   Academic Research Fund of the Ministry of Education of Singapore
   [MOE-T2EP20220-0005, RG20/20]
FX This work was supported by the National Key R&D Program of China under
   Grants 2020YFB1709402 and 2021YFA1003003 and in part by the Academic
   Research Fund under Grant MOE-T2EP20220-0005 & RG20/20 of the Ministry
   of Education of Singapore.
NR 58
TC 1
Z9 1
U1 6
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3719
EP 3730
DI 10.1109/TVCG.2023.3240503
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700058
PM 37022859
DA 2025-03-07
ER

PT J
AU Lin, YN
   Li, HT
   Wu, AY
   Wang, Y
   Qu, HM
AF Lin, Yanna
   Li, Haotian
   Wu, Aoyu
   Wang, Yong
   Qu, Huamin
TI DMiner: Dashboard Design Mining and Recommendation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Layout; Encoding; Feature extraction; Data mining;
   Visualization; Software development management; Dashboards; design
   mining; multiple-view visualization; visualization recommendation
ID MULTIPLE VIEWS
AB Dashboards, which comprise multiple views on a single display, help analyze and communicate multiple perspectives of data simultaneously. However, creating effective and elegant dashboards is challenging since it requires careful and logical arrangement and coordination of multiple visualizations. To solve the problem, we propose a data-driven approach for mining design rules from dashboards and automating dashboard organization. Specifically, we focus on two prominent aspects of the organization: arrangement, which describes the position, size, and layout of each view in the display space; and coordination, which indicates the interaction between pairwise views. We build a new dataset containing 854 dashboards crawled online, and develop feature engineering methods for describing the single views and view-wise relationships in terms of data, encoding, layout, and interactions. Further, we identify design rules among those features and develop a recommender for dashboard design. We demonstrate the usefulness of DMiner through an expert study and a user study. The expert study shows that our extracted design rules are reasonable and conform to the design practice of experts. Moreover, a comparative user study shows that our recommender could help automate dashboard organization and reach human-level performance. In summary, our work offers a promising starting point for design mining visualizations to build recommenders.
C1 [Lin, Yanna; Li, Haotian; Wu, Aoyu; Qu, Huamin] Hong Kong Univ Sci & Technol, Kowloon, Hong Kong, Peoples R China.
   [Wang, Yong] Singapore Management Univ, Singapore 178902, Singapore.
C3 Hong Kong University of Science & Technology; Singapore Management
   University
RP Lin, YN (corresponding author), Hong Kong Univ Sci & Technol, Kowloon, Hong Kong, Peoples R China.
EM ylindg@connect.ust.hk; haotian.li@connect.ust.hk; awuac@connect.ust.hk;
   yongwang@smu.edu.sg; huamin@cse.ust.hk
RI wu, au/KLC-5346-2024; Wang, Yong/HKF-3903-2023; Li,
   Haotian/LXV-6051-2024
OI Lin, Yanna/0000-0003-3730-0827; Wu, Aoyu/0000-0001-9187-9265; Wang,
   Yong/0000-0002-0092-0793
FU HK RGC GRF [16210722]; Singapore Ministry of Education (MOE) Academic
   Research Fund (AcRF) Tier 2 [T2EP20222-0049]
FX This work was partially supported by HK RGC GRF under Grant 16210722,
   and in part by the Singapore Ministry of Education (MOE) Academic
   Research Fund (AcRF) Tier 2 under Grant T2EP20222-0049.
NR 55
TC 4
Z9 4
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4108
EP 4121
DI 10.1109/TVCG.2023.3251344
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700016
PM 37028006
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Shao, ZK
   Sun, SR
   Zhao, YH
   Wang, SY
   Wei, ZY
   Gui, T
   Turkay, C
   Chen, SM
AF Shao, Zekai
   Sun, Shuran
   Zhao, Yuheng
   Wang, Siyuan
   Wei, Zhongyu
   Gui, Tao
   Turkay, Cagatay
   Chen, Siming
TI Visual Explanation for Open-Domain Question Answering With BERT
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Analytical models; Transformers; Task analysis; Data models; Visual
   analytics; Bit error rate; Semantics; Explainable machine learning;
   open-domain question answering; visual analytics
ID ANALYTICS
AB Open-domain question answering (OpenQA) is an essential but challenging task in natural language processing that aims to answer questions in natural language formats on the basis of large-scale unstructured passages. Recent research has taken the performance of benchmark datasets to new heights, especially when these datasets are combined with techniques for machine reading comprehension based on Transformer models. However, as identified through our ongoing collaboration with domain experts and our review of literature, three key challenges limit their further improvement: (i) complex data with multiple long texts, (ii) complex model architecture with multiple modules, and (iii) semantically complex decision process. In this paper, we present VEQA, a visual analytics system that helps experts understand the decision reasons of OpenQA and provides insights into model improvement. The system summarizes the data flow within and between modules in the OpenQA model as the decision process takes place at the summary, instance and candidate levels. Specifically, it guides users through a summary visualization of dataset and module response to explore individual instances with a ranking visualization that incorporates context. Furthermore, VEQA supports fine-grained exploration of the decision flow within a single module through a comparative tree visualization. We demonstrate the effectiveness of VEQA in promoting interpretability and providing insights into model enhancement through a case study and expert evaluation.
C1 [Shao, Zekai; Sun, Shuran; Zhao, Yuheng; Wang, Siyuan; Wei, Zhongyu; Gui, Tao; Chen, Siming] Fudan Univ, Shanghai 200437, Peoples R China.
   [Turkay, Cagatay] Univ Warwick, Coventry CV4 7AL, England.
C3 Fudan University; University of Warwick
RP Chen, SM (corresponding author), Fudan Univ, Shanghai 200437, Peoples R China.
EM zkshao19@fudan.edu.cn; srsun20@fudan.edu.cn; yuhengzhao@fudan.edu.cn;
   wangsy18@fudan.edu.cn; zywei@fudan.edu.cn; tgui@fudan.edu.cn;
   cagatay.turkay@warwick.ac.uk; simingchen3@gmail.com
RI Gui, Tao/LWI-6783-2024; Turkay, Cagatay/AAA-3810-2020; Chen,
   Siming/AAK-1874-2020
OI Sun, Shuran/0000-0003-2297-5602; Wang, Siyuan/0000-0001-7357-2913;
   Turkay, Cagatay/0000-0001-6788-251X; Shao, Zekai/0000-0003-2014-5293;
   Wei, Zhongyu/0000-0003-3789-8507; Zhao, Yuheng/0000-0003-1573-8772
FU National Natural Science Foundation of China [62202105]; Shanghai
   Municipal Science and Technology Major [2018SHZDZX01, 2021SHZDZX0103];
   General Program [21ZR1403300]; Sailing Program [21YF1402900]
FX This work was supported in part by National Natural Science Foundation
   of China under Grant 62202105, in part by Shanghai Municipal Science and
   Technology Major Project under Grants 2018SHZDZX01, and 2021SHZDZX0103,
   in part by General Program under Grant 21ZR1403300,in part by Sailing
   Program under Grant 21YF1402900, and in part by ZJLab.
NR 94
TC 1
Z9 1
U1 1
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3779
EP 3797
DI 10.1109/TVCG.2023.3243676
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700093
PM 37027746
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Chen, CJ
   Chen, JS
   Yang, WK
   Wang, HZ
   Knittel, J
   Zhao, XB
   Koch, S
   Ertl, T
   Liu, SX
AF Chen, Changjian
   Chen, Jiashu
   Yang, Weikai
   Wang, Haoze
   Knittel, Johannes
   Zhao, Xibin
   Koch, Steffen
   Ertl, Thomas
   Liu, Shixia
TI Enhancing Single-Frame Supervision for Better Temporal Action
   Localization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Location awareness; Annotations; Videos; Visualization; Noise
   measurement; Data visualization; Uncertainty; Single-frame supervision;
   storyline visualization; temporal action localization
ID BODY-MASS INDEX; VIDEO; ALGORITHM; PATTERNS
AB Temporal action localization aims to identify the boundaries and categories of actions in videos, such as scoring a goal in a football match. Single-frame supervision has emerged as a labor-efficient way to train action localizers as it requires only one annotated frame per action. However, it often suffers from poor performance due to the lack of precise boundary annotations. To address this issue, we propose a visual analysis method that aligns similar actions and then propagates a few user-provided annotations (e.g., boundaries, category labels) to similar actions via the generated alignments. Our method models the alignment between actions as a heaviest path problem and the annotation propagation as a quadratic optimization problem. As the automatically generated alignments may not accurately match the associated actions and could produce inaccurate localization results, we develop a storyline visualization to explain the localization results of actions and their alignments. This visualization facilitates users in correcting wrong localization results and misalignments. The corrections are then used to improve the localization results of other actions. The effectiveness of our method in improving localization performance is demonstrated through quantitative evaluation and a case study.
C1 [Chen, Changjian] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410012, Hunan, Peoples R China.
   [Chen, Jiashu; Yang, Weikai; Wang, Haoze; Zhao, Xibin; Liu, Shixia] Tsinghua Univ, Sch Software, BNRist, Beijing 100190, Peoples R China.
   [Knittel, Johannes; Koch, Steffen; Ertl, Thomas] Univ Stuttgart, D-70174 Stuttgart, Germany.
C3 Hunan University; Tsinghua University; University of Stuttgart
RP Liu, SX (corresponding author), Tsinghua Univ, Sch Software, BNRist, Beijing 100190, Peoples R China.
EM changjianchen@hnu.edu.cn; johannes.knittel@vis.uni-stuttgart.de;
   zxb@tsinghua.edu.cn; steffen.koch@vis.uni-stuttgart.de;
   Thomas.Ertl@vis.uni-stuttgart.de; shixia@tsinghua.edu.cn
RI Chen, Changjian/KBA-9462-2024; Liu, Shi-Xia/C-5574-2016; Haoze,
   Wang/T-8717-2019; Yin, Zhaoxia/HRD-7425-2023
OI Ertl, Thomas/0000-0003-4019-2505
FU National Natural Science Foundation of China
FX No Statement Available
NR 69
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2024
VL 30
IS 6
BP 2903
EP 2915
DI 10.1109/TVCG.2024.3388521
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WC8Z6
UT WOS:001252775500008
PM 38619947
OA Green Submitted
DA 2025-03-07
ER

PT J
AU He, H
   Xu, XH
   Li, SM
   Wang, F
   Schroeder, I
   Aldrich, EM
   Murrell, SD
   Xue, LX
   Gu, YY
AF He, Hao
   Xu, Xinhao
   Li, Shangman
   Wang, Fang
   Schroeder, Isaac
   Aldrich, Eric M.
   Murrell, Scottie D.
   Xue, Lanxin
   Gu, Yuanyuan
TI Learning Middle-Latitude Cyclone Formation up in the Air: Student
   Learning Experience, Outcomes, and Perceptions in a CAVE-Enabled
   Meteorology Class
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Meteorology; Three-dimensional displays; Education; Federated learning;
   Virtual reality; Glass; Resists; CAVE; meteorology; learning expereince;
   learning outcomes; student perceptions
ID IMMERSIVE VIRTUAL-REALITY; ENVIRONMENT; DESIGN
AB Cave Automatic Virtual Environment (CAVE) is a virtual reality (VR) environment that has not been fully studied due to its high cost and complexity in system integration. Previous CAVE-related studies mainly focused on comparing its effectiveness with other learning media, such as textbooks, desktop VR, or head-mounted display (HMD) VR. In this study, through the utilization of CAVE in a meteorology class, we concentrated on CAVE itself, measured how CAVE impacted learners' learning outcomes before and after using CAVE in an actual ongoing undergraduate-level class, and investigated how learners perceived their learning experiences. Quantitative data were collected to examine the students' knowledge acquisition and learning experience. We also triangulated the quantitative results with qualitative data from the interviews regarding learners' perceptions of the CAVE-enabled class and their knowledge mastery. The results indicated that their learning outcomes increased through learning with CAVE and that their perceptions of immersion, presence, and engagement significantly correlated with each other. The interview results showed a great fondness of and satisfaction with the learning experience, group collaboration, and effectiveness of the CAVE-enabled class from the learners. We also learned that the learners' learning experiences in CAVE could be further improved if we provided them with more learner-environment interaction, offered them a better sense of immersion, and reduced cybersickness. Implications of these findings are discussed.
C1 [He, Hao] Emporia State Univ, Emporia, KS 66801 USA.
   [Xu, Xinhao; Li, Shangman; Wang, Fang; Schroeder, Isaac; Aldrich, Eric M.; Murrell, Scottie D.; Xue, Lanxin; Gu, Yuanyuan] Univ Missouri Columbia, Columbia, MO USA.
C3 University of Missouri System; University of Missouri Columbia
RP He, H (corresponding author), Emporia State Univ, Emporia, KS 66801 USA.
EM hhe1@emporia.edu; xuxinhao@missouri.edu; sli@missouri.edu;
   wangfan@missouri.edu; isaacschrdr@gmail.com; AldrichE@mail.missouri.edu;
   sdm6f8@mail.missouri.edu; lxbzy@mail.missouri.edu;
   yggcc@mail.missouri.edu
RI XUE, LANXIN/JAO-1460-2023; He, Hao/GQB-1369-2022; Xu, Xinhao/L-2992-2019
OI He, Hao/0000-0002-5385-8022; Murrell, Scott/0009-0007-8727-8468; Wang,
   Fang/0000-0003-3669-3948; Xu, Xinhao/0000-0002-4981-4641
FU NSF
FX No Statement Available
NR 83
TC 0
Z9 0
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2807
EP 2817
DI 10.1109/TVCG.2024.3372072
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400036
PM 38437089
DA 2025-03-07
ER

PT J
AU Ye, SQ
   Chen, DD
   Han, SF
   Liao, J
AF Ye, Shuquan
   Chen, Dongdong
   Han, Songfang
   Liao, Jing
TI 3D Question Answering
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Point cloud; scene understanding
ID LANGUAGE; VISION
AB Visual question answering (VQA) has experienced tremendous progress in recent years. However, most efforts have only focused on 2D image question-answering tasks. In this article, we extend VQA to its 3D counterpart, 3D question answering (3DQA), which can facilitate a machine's perception of 3D real-world scenarios. Unlike 2D image VQA, 3DQA takes the color point cloud as input and requires both appearance and 3D geometrical comprehension to answer the 3D-related questions. To this end, we propose a novel transformer-based 3DQA framework "3DQA-TR", which consists of two encoders to exploit the appearance and geometry information, respectively. Finally, the multi-modal information about the appearance, geometry, and linguistic question can attend to each other via a 3D-linguistic Bert to predict the target answers. To verify the effectiveness of our proposed 3DQA framework, we further develop the first 3DQA dataset "ScanQA", which builds on the ScanNet dataset and contains over 10 K question-answer pairs for 806 scenes. To the best of our knowledge, ScanQA is the first large-scale dataset with natural-language questions and free-form answers in 3D environments that is fully human-annotated. We also use several visualizations and experiments to investigate the astonishing diversity of the collected questions and the significant differences between this task from 2D VQA and 3D captioning. Extensive experiments on this dataset demonstrate the obvious superiority of our proposed 3DQA framework over state-of-the-art VQA frameworks and the effectiveness of our major designs. Our code and dataset will be made publicly available to facilitate research in this direction. The code and data are available at http://shuquanye.com/3DQA_website/.
C1 [Ye, Shuquan; Liao, Jing] City Univ Hong Kong, Kowloon Tong, Hong Kong, Peoples R China.
   [Chen, Dongdong] Microsoft Cloud AI, Redmond, WA 98052 USA.
   [Han, Songfang] Univ Calif San Diego, La Jolla, CA 92093 USA.
C3 City University of Hong Kong; University of California System;
   University of California San Diego
RP Liao, J (corresponding author), City Univ Hong Kong, Kowloon Tong, Hong Kong, Peoples R China.
EM shuquanye2-c@my.cityu.edu.hk; cddlyf@gmail.com; hansongfang@gmail.com;
   jingliao@cityu.edu.hk
RI Chen, Dongdong/AAR-4481-2020; Han, Songfang/AAA-5781-2020
OI Ye, Shuquan/0000-0001-5121-8040; Chen, Dongdong/0000-0002-4642-4373;
   LIAO, Jing/0000-0001-7014-5377; Han, Songfang/0000-0002-6432-8764
FU Hong Kong Research Grants Council
FX No Statement Available
NR 84
TC 2
Z9 2
U1 3
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2024
VL 30
IS 3
BP 1772
EP 1786
DI 10.1109/TVCG.2022.3225327
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IN0A9
UT WOS:001166876500008
PM 36446015
DA 2025-03-07
ER

PT J
AU Gaba, A
   Kaufman, Z
   Cheung, J
   Shvakel, M
   Hall, KW
   Brun, Y
   Bearfield, CX
AF Gaba, Aimen
   Kaufman, Zhanna
   Cheung, Jason
   Shvakel, Marie
   Hall, Kyle Wm.
   Brun, Yuriy
   Bearfield, Cindy Xiong
TI My Model is Unfair, Do People Even Care? Visual Design Affects Trust and
   Perceived Bias in Machine Learning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Computational modeling; Analytical models; Data models;
   Bars; Investment; Games; machine learning; fairness; bias; trust; visual
   design; gender; human-subjects studies
ID PERCEPTION
AB Machine learning technology has become ubiquitous, but, unfortunately, often exhibits bias. As a consequence, disparate stakeholders need to interact with and make informed decisions about using machine learning models in everyday systems. Visualization technology can support stakeholders in understanding and evaluating trade-offs between, for example, accuracy and fairness of models. This paper aims to empirically answer "Can visualization design choices affect a stakeholder's perception of model bias, trust in a model, and willingness to adopt a model?" Through a series of controlled, crowd-sourced experiments with more than 1,500 participants, we identify a set of strategies people follow in deciding which models to trust. Our results show that men and women prioritize fairness and performance differently and that visual design choices significantly affect that prioritization. For example, women trust fairer models more often than men do, participants value fairness more when it is explained using text than as a bar chart, and being explicitly told a model is biased has a bigger impact than showing past biased performance. We test the generalizability of our results by comparing the effect of multiple textual and visual design choices and offer potential explanations of the cognitive mechanisms behind the difference in fairness perception and trust. Our research guides design considerations to support future work developing visualization systems for machine learning.
C1 [Gaba, Aimen; Kaufman, Zhanna; Cheung, Jason; Shvakel, Marie; Brun, Yuriy; Bearfield, Cindy Xiong] Univ Massachusetts, Amherst, MA 01003 USA.
   [Hall, Kyle Wm.] TD Bank, Global Compliance, Cherry Hill, NJ USA.
C3 University of Massachusetts System; University of Massachusetts Amherst
RP Gaba, A (corresponding author), Univ Massachusetts, Amherst, MA 01003 USA.
EM agaba@umass.edu; zhannakaufma@umass.edu; jasoncheung@umass.edu;
   mshvakel@umass.edu; kyle.hall@td.com; brun@umass.edu; yaxiong@umass.edu
OI Brun, Yuriy/0000-0003-3027-7986; Xiong Bearfield,
   Cindy/0000-0002-1451-4083
FU National Science Foundation
FX No Statement Available
NR 92
TC 1
Z9 1
U1 6
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 327
EP 337
DI 10.1109/TVCG.2023.3327192
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500057
PM 37878441
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kale, A
   Guo, ZY
   Qiao, XL
   Heer, J
   Hullman, J
AF Kale, Alex
   Guo, Ziyang
   Qiao, Xiao Li
   Heer, Jeffrey
   Hullman, Jessica
TI EVM: Incorporating Model Checking into Exploratory Visual Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; model checks; exploratory analysis
ID INFERENCE; SELECTION; VISUALIZATION; PLOTS
AB Visual analytics (VA) tools support data exploration by helping analysts quickly and iteratively generate views of data which reveal interesting patterns. However, these tools seldom enable explicit checks of the resulting interpretations of data-e.g., whether patterns can be accounted for by a model that implies a particular structure in the relationships between variables. We present EVM, a data exploration tool that enables users to express and check provisional interpretations of data in the form of statistical models. EVM integrates support for visualization-based model checks by rendering distributions of model predictions alongside user-generated views of data. In a user study with data scientists practicing in the private and public sector, we evaluate how model checks influence analysts' thinking during data exploration. Our analysis characterizes how participants use model checks to scrutinize expectations about data generating process and surfaces further opportunities to scaffold model exploration in VA tools.
C1 [Kale, Alex] Univ Chicago, Chicago, IL 60637 USA.
   [Guo, Ziyang; Qiao, Xiao Li; Hullman, Jessica] Northwestern Univ, Evanston, IL USA.
   [Heer, Jeffrey] Univ Washington, Seattle, WA USA.
C3 University of Chicago; Northwestern University; University of
   Washington; University of Washington Seattle
RP Kale, A (corresponding author), Univ Chicago, Chicago, IL 60637 USA.
EM kalea@uchicago.edu; ziyangguo2027@u.northwestern.edu; emqiao@gmail.com;
   jheer@uw.edu; jhullman@northwestern.edu
RI Hullman, Jessica/P-7130-2018
OI Heer, Jeffrey/0000-0002-6175-1655; Guo, Ziyang/0009-0004-4200-6774;
   Hullman, Jessica/0000-0001-6826-3550
FU NSF
FX No Statement Available
NR 70
TC 3
Z9 3
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 208
EP 218
DI 10.1109/TVCG.2023.3326516
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500107
PM 37871070
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Klenert, N
   Lepper, V
   Baum, D
AF Klenert, Nicolas
   Lepper, Verena
   Baum, Daniel
TI A Local Iterative Approach for the Extraction of 2D Manifolds from
   Strongly Curved and Folded Thin-Layer Structures
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Ridge surface; crease surface; 2D manifold extraction; fast marching;
   virtual unfolding; historical documents
ID VISUALIZATION; CREASES
AB Ridge surfaces represent important features for the analysis of 3-dimensional (3D) datasets in diverse applications and are often derived from varying underlying data including flow fields, geological fault data, and point data, but they can also be present in the original scalar images acquired using a plethora of imaging techniques. Our work is motivated by the analysis of image data acquired using micro-computed tomography (mu CT) of ancient, rolled and folded thin-layer structures such as papyrus, parchment, and paper as well as silver and lead sheets. From these documents we know that they are 2-dimensional (2D) in nature. Hence, we are particularly interested in reconstructing 2D manifolds that approximate the document's structure. The image data from which we want to reconstruct the 2D manifolds are often very noisy and represent folded, densely-layered structures with many artifacts, such as ruptures or layer splitting and merging. Previous ridge-surface extraction methods fail to extract the desired 2D manifold for such challenging data. We have therefore developed a novel method to extract 2D manifolds. The proposed method uses a local fast marching scheme in combination with a separation of the region covered by fast marching into two sub-regions. The 2D manifold of interest is then extracted as the surface separating the two sub-regions. The local scheme can be applied for both automatic propagation as well as interactive analysis. We demonstrate the applicability and robustness of our method on both artificial data as well as real-world data including folded silver and papyrus sheets.
C1 [Klenert, Nicolas; Baum, Daniel] Zuse Inst Berlin, Berlin, Germany.
   [Lepper, Verena] Agypt Museum & Papyrussammlung, Berlin, Germany.
C3 Zuse Institute Berlin
RP Klenert, N (corresponding author), Zuse Inst Berlin, Berlin, Germany.
EM klenert@zib.de; v.lepper@smb.spk-berlin.de; baum@zib.de
OI Klenert, Nicolas/0009-0006-4443-8620; Baum, Daniel/0000-0003-1550-7245
FU German Research Foundation (DFG)
FX No Statement Available
NR 42
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 1260
EP 1270
DI 10.1109/TVCG.2023.3327403
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500103
PM 37930919
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Oliver, P
   Zhang, E
   Zhang, Y
AF Oliver, Peter
   Zhang, Eugene
   Zhang, Yue
TI Scalable Hypergraph Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Hypergraph visualization; scalable visualization; polygon layout;
   hypergraph embedding; primal-dual visualization
ID SPARSIFICATION; ALGORITHM; SET; LAYOUT
AB Hypergraph visualization has many applications in network data analysis. Recently, a polygon-based representation for hypergraphs has been proposed with demonstrated benefits. However, the polygon-based layout often suffers from excessive self-intersections when the input dataset is relatively large. In this paper, we propose a framework in which the hypergraph is iteratively simplified through a set of atomic operations. Then, the layout of the simplest hypergraph is optimized and used as the foundation for a reverse process that brings the simplest hypergraph back to the original one, but with an improved layout. At the core of our approach is the set of atomic simplification operations and an operation priority measure to guide the simplification process. In addition, we introduce necessary definitions and conditions for hypergraph planarity within the polygon representation. We extend our approach to handle simultaneous simplification and layout optimization for both the hypergraph and its dual. We demonstrate the utility of our approach with datasets from a number of real-world applications.
C1 [Oliver, Peter; Zhang, Eugene; Zhang, Yue] Oregon State Univ, Sch Elect Engn & Comp Sci, Corvalis, OR 97331 USA.
C3 Oregon State University
RP Zhang, E (corresponding author), Oregon State Univ, Sch Elect Engn & Comp Sci, Corvalis, OR 97331 USA.
EM oliverpe@oregonstate.edu; zhange@eecs.oregonstate.edu;
   zhangyue@oregonstate.edu
OI Zhang, Eugene/0000-0003-4752-3119; Zhang, Yue/0000-0002-8467-2781;
   Oliver, Peter/0009-0002-5090-6057
NR 57
TC 4
Z9 5
U1 5
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 595
EP 605
DI 10.1109/TVCG.2023.3326599
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500047
PM 37871049
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Cao, CX
   An, ZL
   Ren, Z
   Manocha, D
   Zhou, K
AF Cao, Chunxiao
   An, Zili
   Ren, Zhong
   Manocha, Dinesh
   Zhou, Kun
TI A Psychoacoustic Quality Criterion for Path-Traced Sound Propagation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Signal to noise ratio; Psychoacoustic models; Solid modeling;
   Mathematical models; Band-pass filters; Spectral analysis; Resonant
   frequency; Psychoacoustics; sound simulation; path tracing; virtual
   reality
ID AUDITORY FILTER SHAPES; LOUDNESS; COMPLEX; BANDS
AB In developing virtual acoustic environments, it is important to understand the relationship between the computation cost and the perceptual significance of the resultant numerical error. In this article, we propose a quality criterion that evaluates the error significance of path-tracing-based sound propagation simulators. We present an analytical formula that estimates the error signal power spectrum. With this spectrum estimation, we can use a modified Zwicker's loudness model to calculate the relative loudness of the error signal masked by the ideal output. Our experimental results show that the proposed criterion can explain the human perception of simulation error in a variety of cases.
C1 [Cao, Chunxiao; An, Zili; Ren, Zhong; Zhou, Kun] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [Manocha, Dinesh] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
C3 Zhejiang University; University System of Maryland; University of
   Maryland College Park
RP Ren, Z (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM ccx4graphics@gmail.com; 22021151@zju.edu.cn; renzhong@cad.zju.edu.cn;
   dmanocha@umd.edu; kunzhou@acm.org
RI Cao, Chunxiao/GRY-1846-2022; zhou, kun/KRP-1631-2024
OI Manocha, Dinesh/0000-0001-7047-9801; Cao, Chunxiao/0000-0002-6292-0975
FU NSFC [61732016, 61772458]
FX The work of Chunxiao Cao, Zili An, Zhong Ren, and Kun Zhou was supported
   by the NSFC under Grants 61732016 and 61772458.
NR 39
TC 0
Z9 0
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5422
EP 5433
DI 10.1109/TVCG.2022.3213514
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300042
PM 36219658
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Cárdenas, JL
   Ogayar, CJ
   Feito, FR
   Jurado, JM
AF Cardenas, Jose L.
   Ogayar, Carlos J.
   Feito, Francisco R.
   Jurado, Juan M.
TI Modeling of the 3D Tree Skeleton Using Real-World Data: A Survey
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Vegetation; Three-dimensional displays; Solid modeling; Data models;
   Skeleton; Image reconstruction; Geometry; Tree modeling; 3D
   reconstruction; real-world data processing; computational geometry;
   realistic rendering
ID CROWN DELINEATION; RECONSTRUCTION; TERRESTRIAL; EFFICIENT; SYSTEMS;
   IMAGES; LIDAR
AB Tree modeling has been extensively studied in computer graphics. Recent advances in the development of high-resolution sensors and data processing techniques are extremely useful for collecting 3D datasets of real-world trees and generating increasingly plausible branching structures. The wide availability of versatile acquisition platforms allows us to capture multi-view images and scanned data that can be used for guided 3D tree modeling. In this paper, we carry out a comprehensive review of the state-of-the-art methods for the 3D modeling of botanical tree geometry by taking input data from real scenarios. A wide range of studies has been proposed following different approaches. The most relevant contributions are summarized and classified into three categories: (1) procedural reconstruction, (2) geometry-based extraction, and (3) image-based modeling. In addition, we describe other approaches focused on the reconstruction process by adding additional features to achieve a realistic appearance of the tree models. Thus, we provide an overview of the most effective procedures to assist researchers in the photorealistic modeling of trees in geometry and appearance. The article concludes with remarks and trends for promising research opportunities in 3D tree modeling using real-world data.
C1 [Cardenas, Jose L.; Ogayar, Carlos J.; Feito, Francisco R.] Univ Jaen, Dept Comp Sci, Jaen 23071, Spain.
   [Jurado, Juan M.] Univ Granada, Dept Software Engn, Granada 18014, Spain.
C3 Universidad de Jaen; University of Granada
RP Jurado, JM (corresponding author), Univ Granada, Dept Software Engn, Granada 18014, Spain.
EM jcdonoso@ujaen.es; cogayar@ujaen.es; ffeito@ujaen.es; jjurado@ugr.es
RI Feito, Francisco/M-1672-2014; Ogayar Anguita, Carlos Javier/K-2166-2017
OI Feito, Francisco/0000-0001-8230-6529; Ogayar Anguita, Carlos
   Javier/0000-0003-0958-990X; Cardenas-Donoso, Jose
   Luis/0000-0002-0315-4094; Jurado, Juan M./0000-0002-8009-9033
FU MCIN/AEI; Junta de Andaluci'a (Spain); European Union's ERDF funds
   [PID2021-126339OB-I00, 1381202-GEU, PYC20-RE-005-UJA]; Spanish Ministry
   of Science, Innovation and Universities [FPU17/01902]
FX This work was supported in part by MCIN/AEI/10.13039/501100011033, Junta
   de Andaluci ' a (Spain), and European Union's ERDF funds under Projects
   PID2021-126339OB-I00, 1381202-GEU, and PYC20-RE-005-UJA, and in part by
   the Spanish Ministry of Science, Innovation and Universities through the
   Doctoral Grant to the first author under Grant FPU17/01902.
NR 130
TC 4
Z9 4
U1 6
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 4920
EP 4935
DI 10.1109/TVCG.2022.3193018
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300008
PM 35862319
DA 2025-03-07
ER

PT J
AU Hergl, C
   Witt, C
   Nsonga, B
   Menzel, A
   Scheuermann, G
AF Hergl, Chiara
   Witt, Carina
   Nsonga, Baldwin
   Menzel, Andreas
   Scheuermann, Gerik
TI Electromechanical Coupling in Electroactive Polymers a Visual Analysis
   of a Third-Order Tensor Field
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Tensors; Couplings; Visualization; Strain; Behavioral sciences; Shape;
   Plastics; Tensor visualization; third-order tensor; deviatoric
   decomposition; electro-active polymer
ID GLYPHS
AB Electroactive polymers are frequently used in engineering applications due to their ability to change their shape and properties under the influence of an electric field. This process also works vice versa, such that mechanical deformation of the material induces an electric field in the EAP device. This specific behavior makes such materials highly attractive for the construction of actuators and sensors in various application areas. The electromechanical behaviour of electroactive polymers can be described by a third-order coupling tensor, which represents the sensitivity of mechanical stresses concerning the electric field, i.e., it establishes a relation between a second-order and a first-order tensor field. Due to this coupling tensor's complexity and the lack of meaningful visualization methods for third-order tensors in general, an interpretation of the tensor is rather difficult. Thus, the central engineering research question that this contribution deals with is a deeper understanding of electromechanical coupling by analyzing the third-order coupling tensor with the help of specific visualization methods. Starting with a deviatoric decomposition of the tensor, the multipoles of each deviator are visualized, which allows a first insight into this highly complex third-order tensor. In the present contribution, four examples, including electromechanical coupling, are simulated within a finite element framework and subsequently analyzed using the tensor visualization method.
C1 [Hergl, Chiara; Nsonga, Baldwin; Scheuermann, Gerik] Univ Leipzig, Inst Comp Sci, D-04109 Leipzig, Germany.
   [Witt, Carina; Menzel, Andreas] TU Dortmund, Inst Mech, D-44227 Dortmund, Germany.
   [Menzel, Andreas] Lund Univ, Div Solid Mech, S-22100 Lund, Sweden.
C3 Leipzig University; Dortmund University of Technology; Lund University
RP Hergl, C (corresponding author), Univ Leipzig, Inst Comp Sci, D-04109 Leipzig, Germany.
EM hergl@informatik.uni-leipzig.de; carina.witt@tu-dortmund.de;
   nsonga@informatik.uni-leipzig.de; andreas.menzel@udo.edu;
   scheuermann@informatik.uni-leipzig.de
RI Menzel, Andreas/C-4769-2008
OI Scheuermann, Gerik/0000-0001-5200-8870; Nsonga,
   Baldwin/0000-0002-0651-952X; Hergl, Chiara/0000-0002-4016-9113; Witt,
   Carina/0000-0001-6085-7418; Menzel, Andreas/0000-0002-7819-9254
NR 46
TC 0
Z9 0
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5357
EP 5371
DI 10.1109/TVCG.2022.3209328
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300037
PM 36170402
OA hybrid
DA 2025-03-07
ER

PT J
AU van Onzenoodt, C
   Vázquez, PP
   Ropinski, T
AF van Onzenoodt, Christian
   Vazquez, Pere-Pau
   Ropinski, Timo
TI Out of the Plane: Flower versus Star Glyphs to Support High-Dimensional
   Exploration in Two-Dimensional Embeddings
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Stars; Visualization; Encoding; Data visualization;
   Dimensionality reduction; Image color analysis; Glyph visualization;
   high-dimensional data visualization; two-dimensional embeddings
ID SHAPE CHARACTERISTICS; COLOR; PERCEPTION
AB Exploring high-dimensional data is a common task in many scientific disciplines. To address this task, two-dimensional embeddings, such as tSNE and UMAP, are widely used. While these determine the 2D position of data items, effectively encoding the first two dimensions, suitable visual encodings can be employed to communicate higher-dimensional features. To investigate such encodings, we have evaluated two commonly used glyph types, namely flower glyphs and star glyphs. To evaluate their capabilities for communicating higher-dimensional features in two-dimensional embeddings, we ran a large set of crowd-sourced user studies using real-world data obtained from data.gov. During these studies, participants completed a broad set of relevant tasks derived from related research. This article describes the evaluated glyph designs, details our tasks, and the quantitative study setup before discussing the results. Finally, we will present insights and provide guidance on the choice of glyph encodings when exploring high-dimensional data.
C1 [van Onzenoodt, Christian; Ropinski, Timo] Ulm Univ, Visual Comp Grp, D-89081 Ulm, Germany.
   [Vazquez, Pere-Pau] UPC Barcelona, ViRVIG Grp, Barcelona 08034, Spain.
C3 Ulm University
RP van Onzenoodt, C (corresponding author), Ulm Univ, Visual Comp Grp, D-89081 Ulm, Germany.
EM christian.van-onzenoodt@uni-ulm.de; pere.pau.vazquez@upc.edu;
   timo.ropinski@uni-ulm.de
RI Vázquez, Pere-Pau/HTP-9691-2023
OI van Onzenoodt, Christian/0000-0002-5951-6795; Ropinski,
   Timo/0000-0002-7857-5512
NR 41
TC 2
Z9 2
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5468
EP 5482
DI 10.1109/TVCG.2022.3216919
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300045
PM 36288226
OA hybrid, Green Published
DA 2025-03-07
ER

PT J
AU Lu, ZC
   Chen, XM
   Chung, VYY
   Cai, WD
   Shen, YR
AF Lu, Zhicheng
   Chen, Xiaoming
   Chung, Vera Yuk Ying
   Cai, Weidong
   Shen, Yiran
TI EV-LFV: Synthesizing Light Field Event Streams from an Event Camera and
   Multiple RGB Cameras
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 17th International Conference on High Speed Machining (HSM)
CY OCT 25-28, 2023
CL Nanjing, PEOPLES R CHINA
DE Machine Learning; Computer Vision; Light Field Image Processing;
   Event-based Vision
ID RECONSTRUCTION
AB Light field videos captured in RGB frames (RGB-LFV) can provide users with a 6 degree-of-freedom immersive video experience by capturing dense multi-subview video. Despite its potential benefits, the processing of dense multi-subview video is extremely resource-intensive, which currently limits the frame rate of RGB-LFV (i.e., lower than 30 fps) and results in blurred frames when capturing fast motion. To address this issue, we propose leveraging event cameras, which provide high temporal resolution for capturing fast motion. However, the cost of current event camera models makes it prohibitive to use multiple event cameras for RGB-LFV platforms. Therefore, we propose EV-LFV, an event synthesis framework that generates full multi-subview event-based RGB-LFV with only one event camera and multiple traditional RGB cameras. EV-LFV utilizes spatial-angular convolution, ConvLSTM, and Transformer to model RGB-LFV's angular features, temporal features, and long-range dependency, respectively, to effectively synthesize event streams for RGB-LFV. To train EV-LFV, we construct the first event-to-LFV dataset consisting of 200 RGB-LFV sequences with ground-truth event streams. Experimental results demonstrate that EV-LFV outperforms state-of-the-art event synthesis methods for generating event-based RGB-LFV, effectively alleviating motion blur in the reconstructed RGB-LFV.
C1 [Lu, Zhicheng; Chen, Xiaoming] Beijing Technol & Business Univ, Sch Comp Sci & Engn, Beijing, Peoples R China.
   [Lu, Zhicheng; Chen, Xiaoming; Chung, Vera Yuk Ying; Cai, Weidong] Univ Sydney, Sch Comp Sci, Sydney, Australia.
   [Shen, Yiran] Shandong Univ, Sch Software, Jinan, Peoples R China.
C3 Beijing Technology & Business University; University of Sydney; Shandong
   University
RP Chen, XM (corresponding author), Beijing Technol & Business Univ, Sch Comp Sci & Engn, Beijing, Peoples R China.; Chen, XM (corresponding author), Univ Sydney, Sch Comp Sci, Sydney, Australia.; Shen, YR (corresponding author), Shandong Univ, Sch Software, Jinan, Peoples R China.
EM zhlu2106@uni.sydney.edu.au; xiaoming.chen@btbu.edu.cn;
   vera.chung@sydney.edu.au; tom.cai@sydney.edu.au; yiran.shen@sdu.edu.cn
RI Cai, Tingwei/AAJ-8822-2020
OI Cai, Weidong/0000-0003-3706-8896; Shen, Yiran/0000-0003-1385-1480; Lu,
   Zhicheng/0000-0002-2995-0996; Chen, Xiaoming/0000-0002-7503-3021
FU Beijing Natural Science Foundation [4222003]; National Natural Science
   Foundation of China [62177001]; Research Foundation for Advanced Talents
   of Beijing Technology and Business University [19008022321]
FX This work was supported in part by Beijing Natural Science Foundation
   under Grant 4222003, National Natural Science Foundation of China under
   Grant 62177001, and Research Foundation for Advanced Talents of Beijing
   Technology and Business University under Grant 19008022321.
NR 42
TC 1
Z9 1
U1 5
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2023
VL 29
IS 11
BP 4546
EP 4555
DI 10.1109/TVCG.2023.3320271
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA X6ZW5
UT WOS:001099919100020
PM 37788211
DA 2025-03-07
ER

PT J
AU Zhao, GH
   Orlosky, J
   Feiner, S
   Ratsamee, P
   Uranishi, Y
AF Zhao, Guanghan
   Orlosky, Jason
   Feiner, Steven
   Ratsamee, Photchara
   Uranishi, Yuki
TI Mitigation of VR Sickness During Locomotion With a Motion-Based Dynamic
   Vision Modulator
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Optical flow; Modulation; Angular velocity; Image color
   analysis; Teleportation; Legged locomotion; VR sickness; contrast
   manipulation; vision modulation; shading and rendering
ID SIMULATOR SICKNESS; SPEED PERCEPTION; SELF-MOTION; MOVEMENT; DEPTH;
   NAVIGATION; PARALLAX; VELOCITY; VIEW
AB In virtual reality, VR sickness resulting from continuous locomotion via controllers or joysticks is still a significant problem. In this article, we present a set of algorithms to mitigate VR sickness that dynamically modulate the user's field of view by modifying the contrast of the periphery based on movement, color, and depth. In contrast with previous work, this vision modulator is a shader that is triggered by specific motions known to cause VR sickness, such as acceleration, strafing, and linear velocity. Moreover, the algorithm is governed by delta velocity, delta angle, and average color of the view. We ran two experiments with different washout periods to investigate the effectiveness of dynamic modulation on the symptoms of VR sickness, in which we compared this approach against a baseline and pitch-black field-of-view restrictors. Our first experiment made use of a just-noticeable-sickness design, which can be useful for building experiments with a short washout period.
C1 [Zhao, Guanghan; Orlosky, Jason] Osaka Univ, Suita, Osaka 5650871, Japan.
   [Ratsamee, Photchara; Uranishi, Yuki] Osaka Univ, Cybermedia Ctr, Suita, Osaka 5650871, Japan.
   [Feiner, Steven] Columbia Univ, Comp Sci, New York, NY 10027 USA.
C3 Osaka University; Osaka University; Columbia University
RP Zhao, GH (corresponding author), Osaka Univ, Suita, Osaka 5650871, Japan.
EM zhao.guanghan@lab.ime.cmc.osaka-u.ac.jp; jasonorlosky@gmail.com;
   feiner@cs.columbia.edu; photchara@ime.cmc.osaka-u.ac.jp;
   uranishi@ime.cmc.osaka-u.ac.jp
RI Ratsamee, Photchara/ABI-5958-2020
OI Feiner, Steven/0000-0001-9978-7090; Orlosky, Jason/0000-0002-0538-6630;
   Ratsamee, Photchara/0000-0002-3081-2232; Zhao,
   Guanghan/0000-0002-1216-9685
FU ONRG [N62909-18-1-2036]; Grants-in-Aid for Scientific Research
   [23K21688] Funding Source: KAKEN
FX This work was supported by ONRG under Grant N62909-18-1-2036.
NR 67
TC 3
Z9 4
U1 8
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2023
VL 29
IS 10
BP 4089
EP 4103
DI 10.1109/TVCG.2022.3181262
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8ZW3
UT WOS:001060356200005
PM 35687624
DA 2025-03-07
ER

PT J
AU Domova, V
   Vrotsou, K
AF Domova, Veronika
   Vrotsou, Katerina
TI A Model for Types and Levels of Automation in Visual Analytics: A
   Survey, a Taxonomy, and Examples
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual analytics; levels of automation; taxonomy; framework;
   event-sequence analytics
ID FUNCTION ALLOCATION; VISUALIZATION; TRUST; EXPLORATION; DESIGN;
   TRANSFORMATIONS; UNCERTAINTY; PERFORMANCE; DIRECTIONS; ALGORITHM
AB The continuous growth in availability and access to data presents a major challenge to the human analyst. As the manual analysis of large and complex datasets is nowadays practically impossible, the need for assisting tools that can automate the analysis process while keeping the human analyst in the loop is imperative. A large and growing body of literature recognizes the crucial role of automation in Visual Analytics and suggests that automation is among the most important constituents for effective Visual Analytics systems. Today, however, there is no appropriate taxonomy nor terminology for assessing the extent of automation in a Visual Analytics system. In this article, we aim to address this gap by introducing a model of levels of automation tailored for the Visual Analytics domain. The consistent terminology of the proposed taxonomy could provide a ground for users/readers/reviewers to describe and compare automation in Visual Analytics systems. Our taxonomy is grounded on a combination of several existing and well-established taxonomies of levels of automation in the human-machine interaction domain and relevant models within the visual analytics field. To exemplify the proposed taxonomy, we selected a set of existing systems from the event-sequence analytics domain and mapped the automation of their visual analytics process stages against the automation levels in our taxonomy.
C1 [Domova, Veronika] Stanford Univ, Human Comp Interact, Stanford, CA 94305 USA.
   [Vrotsou, Katerina] Linkoping Univ, Informat Visualizat, S-58183 Linkoping, Sweden.
C3 Stanford University; Linkoping University
RP Domova, V (corresponding author), Stanford Univ, Human Comp Interact, Stanford, CA 94305 USA.
EM vdomova@stanford.edu; katerina.vrotsou@liu.se
OI Domova, Veronika/0000-0002-1647-9402; Vrotsou,
   Katerina/0000-0003-4761-8601
FU Wallenberg AI, Autonomous Systems and Software Program
FX This work was supported by Wallenberg AI, Autonomous Systems and
   Software Program.
NR 141
TC 4
Z9 5
U1 0
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2023
VL 29
IS 8
BP 3550
EP 3568
DI 10.1109/TVCG.2022.3163765
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L3CU5
UT WOS:001022080200008
PM 35358047
DA 2025-03-07
ER

PT J
AU Narciso, D
   Melo, M
   Rodrigues, S
   Cunha, JP
   Vasconcelos-Raposo, J
   Bessa, M
AF Narciso, David
   Melo, Miguel
   Rodrigues, Susana
   Cunha, Joao Paulo
   Vasconcelos-Raposo, Jose
   Bessa, Maximino
TI Using Heart Rate Variability for Comparing the Effectiveness of Virtual
   vs Real Training Environments for Firefighters
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Training; Particle measurements; Atmospheric measurements; Heart rate
   variability; Stress; Physiology; Task analysis; Computer graphics;
   virtual reality; professional training; biofeedback
ID ENHANCES REALISTIC RESPONSE; PRESENCE QUESTIONNAIRE; STRESS ASSESSMENT;
   FATIGUE; SYSTEM
AB The use of Virtual Reality (VR) technology to train professionals has increased over the years due to its advantages over traditional training. This paper presents a study comparing the effectiveness of a Virtual Environment (VE) and a Real Environment (RE) designed to train firefighters. To measure the effectiveness of the environments, a new method based on participants' Heart Rate Variability (HRV) was used. This method was complemented with self-reports, in the form of questionnaires, of fatigue, stress, sense of presence, and cybersickness. An additional questionnaire was used to measure and compare knowledge transfer enabled by the environments. The results from HRV analysis indicated that participants were under physiological stress in both environments, albeit with less intensity on the VE. Regarding reported fatigue and stress, the results showed that none of the environments increased such variables. The results of knowledge transfer showed that the VE obtained a significant increase while the RE obtained a positive but non-significant increase (median values, VE: before - 4 after - 7, p = .003; RE: before - 4 after - 5, p = .375). Lastly, the results of presence and cybersickness suggested that participants experienced high overall presence and no cybersickness. Considering all results, the authors conclude that the VE provided effective training but that its effectiveness was lower than that of the RE.
C1 [Narciso, David; Vasconcelos-Raposo, Jose; Bessa, Maximino] Univ Tras os Montes & Alto Douro UTAD, P-5000801 Vila Real, Portugal.
   [Narciso, David; Melo, Miguel; Rodrigues, Susana; Cunha, Joao Paulo; Vasconcelos-Raposo, Jose; Bessa, Maximino] Inst Syst & Comp Engn Technol & Sci INESC TEC, P-4200465 Porto, Portugal.
   [Cunha, Joao Paulo] Univ Porto FEUP, Fac Engn, P-4200465 Porto, Portugal.
C3 University of Tras-os-Montes & Alto Douro; INESC TEC; Universidade do
   Porto
RP Narciso, D (corresponding author), Univ Tras os Montes & Alto Douro UTAD, P-5000801 Vila Real, Portugal.
EM davidnarciso@utad.pt; mcmelo@inesctec.pt;
   susana.c.rodrigues@inesctec.pt; jpcunha@fe.up.pt; jvraposo@utad.pt;
   maxbessa@utad.pt
RI Melo, Miguel/AAN-1855-2020; VASCONCELOS-RAPOSO, JOSE/JMB-6306-2023;
   Bessa, Maximino/B-4729-2012; Cunha, Joao Paulo/F-9039-2010; Branco
   VASCONCELOS-RAPOSO, JOSE Jacinto/G-3743-2010
OI Cunha, Joao Paulo/0000-0003-4131-9045; Bessa,
   Maximino/0000-0002-3002-704X; Rodrigues, Susana/0000-0001-6546-340X;
   Melo, Miguel/0000-0003-4050-3473; Branco VASCONCELOS-RAPOSO, JOSE
   Jacinto/0000-0002-3456-9727
FU ERDF - European Regional Development Fund through the Operational
   Program for Competitiveness and Internationaliza-tion - COMPETE 2020
   Program; National Funds through the Portuguese Funding Agency, FCT -
   Fundacao para a Ciencia e a Tecnologia, [POCI-01-0145-FEDER-028618];
   PERFECT - Perceptual Equivalence in Virtual Reality For authEntiC
   Training [UIDB/50014/2020, SFRH/BD/147334/2019]
FX This work was supported in part by the ERDF - European Regional
   Development Fund through the Operational Program for Competitiveness and
   Internationaliza-tion - COMPETE 2020 Program, and in part by National
   Funds through the Portuguese Funding Agency, FCT - Fundacao para a
   Ciencia e a Tecnologia, Project under Grant POCI-01-0145-FEDER-028618
   entitled PERFECT - Perceptual Equivalence in Virtual Reality For
   authEntiC Training, Project under Grant UIDB/50014/2020, and PhD under
   Grant SFRH/BD/147334/2019. All the works were conducted at INESC TEC's
   MASSIVE VR Laboratory.
NR 59
TC 6
Z9 6
U1 2
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2023
VL 29
IS 7
BP 3238
EP 3250
DI 10.1109/TVCG.2022.3156734
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA H4XO7
UT WOS:000996011900008
PM 35254983
DA 2025-03-07
ER

PT J
AU Bhargava, A
   Venkatakrishnan, R
   Venkatakrishnan, R
   Lucaites, K
   Solini, H
   Robb, AC
   Pagano, CC
   Babu, SV
AF Bhargava, Ayush
   Venkatakrishnan, Rohith
   Venkatakrishnan, Roshan
   Lucaites, Kathryn
   Solini, Hannah
   Robb, Andrew C. C.
   Pagano, Christopher C. C.
   Babu, Sabarish V. V.
TI Can I Squeeze Through? Effects of Self-Avatars and Calibration in a
   Person-Plus-Virtual-Object System on Perceived Lateral Passability in VR
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Affordances; Apertures; Calibration; Visualization; Training; Task
   analysis; Propioception; Affordance; Passability; Self-Avatar; Virtual
   Reality
ID DISTANCE PERCEPTION; AFFORDANCES; ENVIRONMENTS
AB With the popularity of Virtual Reality (VR) on the rise, creators from a variety of fields are building increasingly complex experiences that allow users to express themselves more naturally. Self-avatars and object interaction in virtual worlds are at the heart of these experiences. However, these give rise to several perception based challenges that have been the focus of research in recent years. One area that garners most interest is understanding the effects of self-avatars and object interaction on action capabilities or affordances in VR. Affordances have been shown to be influenced by the anthropometric and anthropomorphic properties of the self-avatar embodied. However, self-avatars cannot fully represent real world interaction and fail to provide information about the dynamic properties of surfaces in the environment. For example, pressing against a board to feel its rigidity. This lack of accurate dynamic information can be further amplified when interacting with virtual handheld objects as the weight and inertial feedback associated with them is often mismatched. To investigate this phenomenon, we looked at how the absence of dynamic surface properties affect lateral passability judgments when carrying virtual handheld objects in the presence or absence of gender matched body-scaled self-avatars. Results suggest that participants can calibrate to the missing dynamic information in the presence of self-avatars to make lateral passability judgments, but rely on their internal body schema of a compressed physical body depth in the absence of self-avatars.
C1 [Bhargava, Ayush; Venkatakrishnan, Rohith; Venkatakrishnan, Roshan; Robb, Andrew C. C.; Babu, Sabarish V. V.] Clemson Univ, Sch Comp, Clemson, SC 29634 USA.
   [Lucaites, Kathryn; Solini, Hannah; Pagano, Christopher C. C.] Clemson Univ, Dept Psychol, Clemson, SC USA.
C3 Clemson University; Clemson University
RP Bhargava, A (corresponding author), Clemson Univ, Sch Comp, Clemson, SC 29634 USA.
EM ayush.bhargava92@gmail.com; rohithv@g.clemson.edu;
   rvenkat@g.clemson.edu; arobb@clemson.edu; cpagano@clemson.edu;
   sbabu@clemson.edu
RI Venkatakrishnan, Roshan/JDC-3508-2023; Bhargava, Ayush/AAJ-2387-2021;
   Venkatakrishnan, Rohith/JCE-8736-2023
OI Venkatakrishnan, Roshan/0000-0002-6538-627X; Babu,
   Sabarish/0000-0002-8348-0534; Pagano, Christopher/0000-0002-0110-2055;
   Bhargava, Ayush/0000-0001-8957-1317; Venkatakrishnan,
   Rohith/0000-0002-8484-3915; Robb, Andrew/0000-0002-0398-5576
FU Clemson University Dissertation Completion Grant; CECAS TIGER seed grant
   programs
FX This work was partly supported by the Clemson University Dissertation
   Completion Grant and CECAS TIGER seed grant programs. We would also like
   to thank all our participants for their time.
NR 61
TC 4
Z9 6
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2348
EP 2357
DI 10.1109/TVCG.2023.3247067
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D2NZ6
UT WOS:000967154500001
PM 37027739
DA 2025-03-07
ER

PT J
AU Liu, GX
   Iuricich, F
   Fellegara, R
   De Floriani, L
AF Liu, Guoxi
   Iuricich, Federico
   Fellegara, Riccardo
   De Floriani, Leila
TI TopoCluster: A Localized Data Structure for Topology-Based Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Terms-Data visualization; data structures; topological data analysis;
   simplicial meshes; tetrahedral meshes
ID DISCRETE MORSE COMPLEXES; COMPACT REPRESENTATION
AB Unstructured data are collections of points with irregular topology, often represented through simplicial meshes, such as triangle and tetrahedral meshes. Whenever possible such representations are avoided in visualization since they are computationally demanding if compared with regular grids. In this work, we aim at simplifying the encoding and processing of simplicial meshes. The article proposes TopoCluster, a new localized data structure for tetrahedral meshes. TopoCluster provides efficient computation of the connectivity of the mesh elements with a low memory footprint. The key idea of TopoCluster is to subdivide the simplicial mesh into clusters. Then, the connectivity information is computed locally for each cluster and discarded when it is no longer needed. We define two instances of TopoCluster. The first instance prioritizes time efficiency and provides only a modest savings in memory, while the second instance drastically reduces memory consumption up to an order of magnitude with respect to comparable data structures. Thanks to the simple interface provided by TopoCluster, we have been able to integrate both data structures into the existing Topological Toolkit (TTK) framework. As a result, users can run any plugin of TTK using TopoCluster without changing a single line of code.
C1 [Liu, Guoxi; Iuricich, Federico] Clemson Univ, Sch Comp, Clemson, SC 29631 USA.
   [Fellegara, Riccardo] German Aerosp Ctr DLR, Inst Software Technol, Braunschweig, Germany.
   [De Floriani, Leila] Univ Maryland, College Pk, MD 20742 USA.
C3 Clemson University; Helmholtz Association; German Aerospace Centre
   (DLR); University System of Maryland; University of Maryland College
   Park
RP Liu, GX (corresponding author), Clemson Univ, Sch Comp, Clemson, SC 29631 USA.
EM guoxil@clemson.edu; fiurici@clemson.edu; riccardo.fellegara@dlr.de;
   deflo@umd.edu
RI Liu, Guoxi/J-4618-2014; Fellegara, Riccardo/AEN-0183-2022
OI Fellegara, Riccardo/0000-0002-8758-2802; DE FLORIANI,
   Leila/0000-0002-1361-2888; Liu, Guoxi/0000-0002-8164-7185; Iuricich,
   Federico/0000-0003-1782-9715
FU U.S. National Science Foundation [IIS-1910766]; German Aerospace Center
   (DLR) [DLR-SC-2467209]
FX This work was supported in part by the U.S. National Science Foundation
   under Grant IIS-1910766 and in part by the auspices of the German
   Aerospace Center (DLR) under Grant DLR-SC-2467209.
NR 33
TC 1
Z9 1
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2023
VL 29
IS 2
BP 1506
EP 1517
DI 10.1109/TVCG.2021.3121229
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7M2HO
UT WOS:000906475100017
PM 34673490
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Lyi, S
   Gehlenborg, N
AF Lyi, Sehi
   Gehlenborg, Nils
TI Multi-View Design Patterns and Responsive Visualization for Genomics
   Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Responsive visualization; multi-view visualization; genomics;
   visualization grammar
ID BROWSER; DISPLAY; SPACE
AB A series of recent studies has focused on designing cross-resolution and cross-device visualizations, i.e., responsive visualization, a concept adopted from responsive web design. However, these studies mainly focused on visualizations with a single view to a small number of views, and there are still unresolved questions about how to design responsive multi-view visualizations. In this paper, we present a reusable and generalizable framework for designing responsive multi-view visualizations focused on genomics data. To gain a better understanding of existing design challenges, we review web-based genomics visualization tools in the wild. By characterizing tools based on a taxonomy of responsive designs, we find that responsiveness is rarely supported in existing tools. To distill insights from the survey results in a systematic way, we classify typical view composition patterns, such as "vertically long," "horizontally wide," "circular," and "cross-shaped" compositions. We then identify their usability issues in different resolutions that stem from the composition patterns, as well as discussing approaches to address the issues and to make genomics visualizations responsive. By extending the Gosling visualization grammar to support responsive constructs, we show how these approaches can be supported. A valuable follow-up study would be taking different input modalities into account, such as mouse and touch interactions, which was not considered in our study.
C1 [Lyi, Sehi; Gehlenborg, Nils] Harvard Med Sch, Boston, MA 02115 USA.
C3 Harvard University; Harvard Medical School
RP Lyi, S (corresponding author), Harvard Med Sch, Boston, MA 02115 USA.
EM sehi_lyi@hms.harvard.edu; nils@hms.harvard.edu
RI LYi, Sehi/IXD-9981-2023
OI L'Yi, Sehi/0000-0001-7720-2848; Gehlenborg, Nils/0000-0003-0327-8297
FU National Institutes of Health [U01CA200059, U24CA237617, UM1HG011536,
   R01HG011773]
FX This work was supported by the National Institutes of Health
   (U01CA200059, U24CA237617, UM1HG011536, R01HG011773). We are grateful to
   the reviewers and to Qianwen Wang for their valuable feedback.
NR 69
TC 5
Z9 6
U1 1
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2023
VL 29
IS 1
BP 559
EP 569
DI 10.1109/TVCG.2022.3209398
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7F6YZ
UT WOS:000901991800008
PM 36166553
OA Green Accepted, Green Submitted, hybrid
DA 2025-03-07
ER

PT J
AU Cakmak, E
   Jäckle, D
   Schreck, T
   Keim, DA
   Fuchs, J
AF Cakmak, Eren
   Jaeckle, Dominik
   Schreck, Tobias
   Keim, Daniel A.
   Fuchs, Johannes
TI Multiscale Visualization: A Structured Literature Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Navigation; Visualization; Taxonomy; Encoding;
   Molecular biology; Libraries; Multiscale visualization; multiscale
   navigation; multiscale exploration; literature analysis; taxonomy;
   survey
ID VISUAL ANALYSIS; TIME-SERIES; EXPLORATION; SCALE; SUMMARIES; DISPLAYS;
   SPACE
AB Multiscale visualizations are typically used to analyze multiscale processes and data in various application domains, such as the visual exploration of hierarchical genome structures in molecular biology. However, creating such multiscale visualizations remains challenging due to the plethora of existing work and the expression ambiguity in visualization research. Up to today, there has been little work to compare and categorize multiscale visualizations to understand their design practices. In this article, we present a structured literature analysis to provide an overview of common design practices in multiscale visualization research. We systematically reviewed and categorized 122 published journal or conference articles between 1995 and 2020. We organized the reviewed articles in a taxonomy that reveals common design factors. Researchers and practitioners can use our taxonomy to explore existing work to create new multiscale navigation and visualization techniques. Based on the reviewed articles, we examine research trends and highlight open research challenges.
C1 [Cakmak, Eren; Keim, Daniel A.; Fuchs, Johannes] Univ Konstanz, Dept Comp & Informat Sci, D-78547 Constance, Germany.
   [Schreck, Tobias] Graz Univ Technol, Inst Comp Graph & Knowledge Visualisat, A-8010 Graz, Austria.
C3 University of Konstanz; Graz University of Technology
RP Cakmak, E (corresponding author), Univ Konstanz, Dept Comp & Informat Sci, D-78547 Constance, Germany.
EM eren.cakmak@uni-konstanz.de; dominikjaeckle@gmail.com;
   tobias.schreck@cgv.tugraz.at; keim@uni-konstanz.de;
   johannes.fuchs@uni-konstanz.de
RI Keim, Daniel/X-7749-2019
OI Schreck, Tobias/0000-0003-0778-8665; Fuchs, Johannes/0000-0001-5474-4214
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under
   Germany's Excellence Strategy [EXC 2117 - 422037984]
FX This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German
   Research Foundation) under Germany's Excellence Strategy - EXC 2117 -
   422037984. MaterialDesign icons are licensed under theApache License
   2.0.
NR 82
TC 6
Z9 6
U1 4
U2 26
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4918
EP 4929
DI 10.1109/TVCG.2021.3109387
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400068
PM 34478370
OA Green Published
DA 2025-03-07
ER

PT J
AU Martin-Gomez, A
   Weiss, J
   Keller, A
   Eck, U
   Roth, D
   Navab, N
AF Martin-Gomez, Alejandro
   Weiss, Jakob
   Keller, Andreas
   Eck, Ulrich
   Roth, Daniel
   Navab, Nassir
TI The Impact of Focus and Context Visualization Techniques on Depth
   Perception in Optical See-Through Head-Mounted Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Estimation; Task analysis; Color; Augmented reality;
   Rendering (computer graphics); Head-mounted displays; Augmented reality;
   perception; depth estimation; visualization techniques; human computer
   interaction; design and evaluation methods; user studies
ID AUGMENTED REALITY; JUDGMENTS; ISSUES
AB Estimating the depth of virtual content has proven to be a challenging task in Augmented Reality (AR) applications. Existing studies have shown that the visual system makes use of multiple depth cues to infer the distance of objects, occlusion being one of the most important ones. The ability to generate appropriate occlusions becomes particularly important for AR applications that require the visualization of augmented objects placed below a real surface. Examples of these applications are medical scenarios in which the visualization of anatomical information needs to be observed within the patient's body. In this regard, existing works have proposed several focus and context (F+C) approaches to aid users in visualizing this content using Video See-Through (VST) Head-Mounted Displays (HMDs). However, the implementation of these approaches in Optical See-Through (OST) HMDs remains an open question due to the additive characteristics of the display technology. In this article, we, for the first time, design and conduct a user study that compares depth estimation between VST and OST HMDs using existing in-situ visualization methods. Our results show that these visualizations cannot be directly transferred to OST displays without increasing error in depth perception tasks. To tackle this gap, we perform a structured decomposition of the visual properties of AR F+C methods to find best-performing combinations. We propose the use of chromatic shadows and hatching approaches transferred from computer graphics. In a second study, we perform a factorized analysis of these combinations, showing that varying the shading type and using colored shadows can lead to better depth estimation when using OST HMDs.
C1 [Martin-Gomez, Alejandro; Weiss, Jakob; Keller, Andreas; Eck, Ulrich; Navab, Nassir] Tech Univ Munich, Dept Informat, Chair Comp Aided Med Procedures & Augmented Real, D-80333 Munich, Germany.
   [Martin-Gomez, Alejandro; Navab, Nassir] Johns Hopkins Univ, Whiting Sch Engn, Lab Comp Aided Med Procedures, Baltimore, MD 21218 USA.
   [Roth, Daniel] FAU Erlangen Nurnberg, D-91054 Erlangen, Germany.
C3 Technical University of Munich; Johns Hopkins University; University of
   Erlangen Nuremberg
RP Martin-Gomez, A; Weiss, J (corresponding author), Tech Univ Munich, Dept Informat, Chair Comp Aided Med Procedures & Augmented Real, D-80333 Munich, Germany.
EM alejandro.martin@tum.de; jakob.weiss@tum.de; andi.keller@tum.de;
   ulrich.eck@tum.de; daniel.roth@tum.de; nassir.navab@tum.de
RI Keller, Andreas/ABB-6412-2021; Gil-Ley, Alejandro/J-5851-2012; Roth,
   Daniel/AFK-2613-2022
OI Martin-Gomez, Alejandro/0000-0001-9341-3477; Eck,
   Ulrich/0000-0002-5322-4724
FU Bayerische Forschungsstiftung [DOK-178-17]; Deutsche
   Forschungsgemeinschaft [NA-620/33-2]
FX This work was supported in part by the Bayerische Forschungsstiftung
   under Grant DOK-178-17 and in part by the Deutsche
   Forschungsgemeinschaft under Grant NA-620/33-2. The authors would also
   like to thank Marc Lazarovici and the Institut fur Notfallmedizin for
   their valuable support in conducting their user study on their premises.
   Alejandro Martin-Gomez and Jakob Weiss are contributed equally to this
   work as corresponding authors.
NR 58
TC 11
Z9 11
U1 1
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4156
EP 4171
DI 10.1109/TVCG.2021.3079849
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400015
PM 33979287
OA hybrid
DA 2025-03-07
ER

PT J
AU Zhao, DY
   Li, YJ
   Chaudhuri, S
   Langlois, T
   Barbic, J
AF Zhao, Danyong
   Li, Yijing
   Chaudhuri, Siddhartha
   Langlois, Timothy
   Barbic, Jernej
TI ERGOBOSS: Ergonomic Optimization of Body-Supporting Surfaces
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Ergonomics; CAD; optimization; shape design; FEM; contact; 3D printing
ID SITTING COMFORT; DISCOMFORT; MODELS; CONTACT; COMPLEX; DESIGN
AB Humans routinely sit or lean against supporting surfaces and it is important to shape these surfaces to be comfortable and ergonomic. We give a method to design the geometric shape of rigid supporting surfaces to maximize the ergonomics of physically based contact between the surface and a deformable human. We model the soft deformable human using a layer of FEM deformable tissue surrounding a rigid core, with measured realistic elastic material properties, and large-deformation nonlinear analysis. We define a novel cost function to measure the ergonomics of contact between the human and the supporting surface. We give a stable and computationally efficient contact model that is differentiable with respect to the supporting surface shape. This makes it possible to optimize our ergonomic cost function using gradient-based optimizers. Our optimizer produces supporting surfaces superior to prior work on ergonomic shape design. Our examples include furniture, apparel and tools. We also validate our results by scanning a real human subject's foot and optimizing a shoe sole shape to maximize foot contact ergonomics. We 3D-print the optimized shoe sole, measure contact pressure using pressure sensors, and demonstrate that the real unoptimized and optimized pressure distributions qualitatively match those predicted by our simulation.
C1 [Zhao, Danyong; Li, Yijing; Barbic, Jernej] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
   [Chaudhuri, Siddhartha] Adobe Syst Inc, Adobe Res, San Jose, CA 95110 USA.
   [Langlois, Timothy] Adobe Res, San Jose, CA 95110 USA.
C3 University of Southern California; Adobe Systems Inc.; Adobe Systems
   Inc.
RP Barbic, J (corresponding author), Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.
EM danyongz@usc.edu; yijingl@usc.edu; sidch@adobe.com; tlangloi@adobe.com;
   jnb@usc.edu
RI ZHAO, DANYONG/HNP-0495-2023
OI ZHAO, DANYONG/0000-0003-0714-5715
NR 61
TC 2
Z9 2
U1 1
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4032
EP 4047
DI 10.1109/TVCG.2021.3112127
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400006
PM 34520357
DA 2025-03-07
ER

PT J
AU Shi, XH
   Wang, LL
   Wu, J
   Fan, RZ
   Hao, AM
AF Shi, Xuehuai
   Wang, Lili
   Wu, Jian
   Fan, Runze
   Hao, Aimin
TI Foveated Stochastic Lightcuts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 21st IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 17-22, 2022
CL Singapore, SINGAPORE
SP IEEE
DE Virtual Reality; Foveated Rendering; Lightcuts; Many-lights Rendering
AB Foveated rendering provides an idea for accelerating rendering algorithms without sacrificing the perceived rendering quality in virtual reality applications. In this paper, we propose a foveated stochastic lightcuts method to render high-quality many-lights illumination effects in high perception-sensitive regions. First, we introduce a spatiotemporal-luminance based lightcuts generation method to generate lightcuts with different accuracy for different visual perception-sensitive regions. Then we propose a multi-resolution light samples selection method to select the light sample for each node in the lightcuts more efficiently. Our method supports full-dynamic scenes containing over 250k dynamic light sources and dynamic diffuse/specular/glossy objects. It provides frame rates up to 110fps for high-quality many-lights illumination effects in high perception-sensitive regions of the HVS in VR HMDs. Compared with the state-of-the-art stochastic lightcuts method using the same rendering time, our method achieves smaller mean squared errors in the fovea and periphery. We also conduct user studies to prove that the perceived quality of our method has a high visual similarity with the results of the ground truth rendered by using the stochastic lightcuts with 2048 light samples per pixel.
C1 [Wang, Lili] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Wang, Lili] Peng Cheng Lab, Shengzhen, Peoples R China.
   [Wang, Lili] Beihang Univ, Beijing Adv Innovat Ctr Biomed Engn, Beijing, Peoples R China.
   [Shi, Xuehuai; Wu, Jian; Fan, Runze; Hao, Aimin] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
C3 Beihang University; Beihang University; Beihang University
RP Wang, LL (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.; Wang, LL (corresponding author), Peng Cheng Lab, Shengzhen, Peoples R China.; Wang, LL (corresponding author), Beihang Univ, Beijing Adv Innovat Ctr Biomed Engn, Beijing, Peoples R China.
EM shixuehuaireal@buaa.edu.cn; wanglily@buaa.edu.cn; lanayawj@buaa.edu.cn;
   BY2106131@buaa.edu.cn; ham@buaa.edu.cn
RI jiang, jun/GWC-9329-2022; Zhao, Mingyu/HHS-0141-2022; Shi,
   Xuehuai/ISB-5757-2023; wang, lili/HJP-8047-2023
FU National Key RD plan [2019YFC1521102]; National Natural Science
   Foundation of China [61932003, 61772051]
FX This work is supported by National Key R&D plan 2019YFC1521102, by the
   National Natural Science Foundation of China through Projects 61932003
   and 61772051.
NR 43
TC 2
Z9 2
U1 6
U2 24
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2022
VL 28
IS 11
BP 3684
EP 3693
DI 10.1109/TVCG.2022.3203089
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 5M3ZS
UT WOS:000871038200013
PM 36049004
DA 2025-03-07
ER

PT J
AU Weerasinghe, M
   Quigley, A
   Pucihar, KC
   Toniolo, A
   Miguel, A
   Kljun, M
AF Weerasinghe, Maheshya
   Quigley, Aaron
   Pucihar, Klen Copic
   Toniolo, Alice
   Miguel, Angela
   Kljun, Matjaz
TI Arigato: Effects of Adaptive Guidance on Engagement and Performance in
   Augmented Reality Learning Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 21st IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 17-22, 2022
CL Singapore, SINGAPORE
SP IEEE
DE Experiential learning; instructional guidance; adaptive learning
   systems; augmented reality; engagement; language Learning
ID INSTRUCTION; EXPERIENCE; DRIVERS; SCHOOL
AB Experiential learning (ExL) is the process of learning through experience or more specifically "learning through reflection on doing". In this paper, we propose a simulation of these experiences, in Augmented Reality (AR), addressing the problem of language learning. Such systems provide an excellent setting to support "adaptive guidance", in a digital form, within a real environment. Adaptive guidance allows the instructions and learning content to be customised for the individual learner, thus creating a unique learning experience. We developed an adaptive guidance AR system for language learning, we call Arigato (Augmented Reality Instructional Guidance & Tailored Omniverse), which offers immediate assistance, resources specific to the learner's needs, manipulation of these resources, and relevant feedback. Considering guidance, we employ this prototype to investigate the effect of the amount of guidance (fixed vs. adaptive-amount) and the type of guidance (fixed vs. adaptive-associations) on the engagement and consequently the learning outcomes of language learning in an AR environment. The results for the amount of guidance show that compared to the adaptive-amount, the fixed-amount of guidance group scored better in the immediate and delayed (after 7 days) recall tests. However, this group also invested a significantly higher mental effort to complete the task. The results for the type of guidance show that the adaptive-associations group outperforms the fixed-associations group in the immediate, delayed (after 7 days) recall tests, and learning efficiency. The adaptive-associations group also showed significantly lower mental effort and spent less time to complete the task.
C1 [Weerasinghe, Maheshya; Pucihar, Klen Copic; Kljun, Matjaz] Univ Primorska, Koper, Slovenia.
   [Weerasinghe, Maheshya; Toniolo, Alice; Miguel, Angela] Univ St Andrews, St Andrews, Fife, Scotland.
   [Quigley, Aaron] Univ New South Wales, Sydney, NSW, Australia.
C3 University of Primorska; University of St Andrews; University of New
   South Wales Sydney
RP Weerasinghe, M (corresponding author), Univ Primorska, Koper, Slovenia.; Weerasinghe, M (corresponding author), Univ St Andrews, St Andrews, Fife, Scotland.
EM amw31@st-andrews.ac.uk; a.quigley@unsw.edu.au; klen.copic@famnit.upr.si;
   a.toniolo@st-andrews.ac.uk; arm14@st-andrews.ac.uk;
   matjaz.kljun@famnit.upr.si
RI Kljun, Matjaž/G-6415-2015; Toniolo, Alice/ADE-9513-2022; Quigley,
   Aaron/JHS-5032-2023
OI Copic Pucihar, Klen/0000-0002-7784-1356; Quigley,
   Aaron/0000-0002-5274-6889; Weerasinghe, Dr. Maheshya/0000-0003-2691-601X
FU European Commission [739574]; Republic of Slovenia; Republic of Slovenia
   (European Union of the European Regional Development Fund); Slovenian
   research agency ARRS [B1-DE/20-21-002, P1-0383, J1-9186, J1-1715,
   J5-1796, J1-1692]
FX This research was supported by European Commission through the InnoRenew
   CoE project (Grant Agreement 739574) under the Horizon2020
   Widespread-Teaming program and the Republic of Slovenia (investment
   funding of the Republic of Slovenia and the European Union of the
   European Regional Development Fund). We also acknowledge support from
   the Slovenian research agency ARRS (program no. B1-DE/20-21-002,
   P1-0383, J1-9186, J1-1715, J5-1796, and J1-1692).
NR 107
TC 6
Z9 6
U1 7
U2 30
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2022
VL 28
IS 11
BP 3737
EP 3747
DI 10.1109/TVCG.2022.3203088
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 5M3ZS
UT WOS:000871038200018
PM 36048999
OA Green Submitted, Green Accepted
DA 2025-03-07
ER

PT J
AU Martin, D
   Serrano, A
   Bergman, AW
   Wetzstein, G
   Masia, B
AF Martin, Daniel
   Serrano, Ana
   Bergman, Alexander W.
   Wetzstein, Gordon
   Masia, Belen
TI ScanGAN360: A Generative Model of Realistic Scanpaths for 360° Images
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 12-16, 2022
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, ChristchurchNZ, Virbela, Univ Canterbury, Immers Learning Res Network, Qualcomm, HIT Lab NZ, Appl Immers Gaming Initiat
DE Scanpath generation; 360 degrees images; virtual reality; generative
   adversarial models; saliency; human behavior
ID VISUAL-ATTENTION; SALIENCY MAPS; EYE-MOVEMENTS; PREDICTION; GAZE
AB Understanding and modeling the dynamics of human gaze behavior in 360 degrees environments is crucial for creating, improving, and developing emerging virtual reality applications. However, recruiting human observers and acquiring enough data to analyze their behavior when exploring virtual environments requires complex hardware and software setups, and can be time-consuming. Being able to generate virtual observers can help overcome this limitation, and thus stands as an open problem in this medium. Particularly, generative adversarial approaches could alleviate this challenge by generating a large number of scanpaths that reproduce human behavior when observing new scenes, essentially mimicking virtual observers. However, existing methods for scanpath generation do not adequately predict realistic scanpaths for 360 degrees images. We present ScanGAN360, a new generative adversarial approach to address this problem. We propose a novel loss function based on dynamic time warping and tailor our network to the specifics of 360 degrees images. The quality of our generated scanpaths outperforms competing approaches by a large margin, and is almost on par with the human baseline. ScanGAN360 allows fast simulation of large numbers of virtual observers, whose behavior mimics real users, enabling a better understanding of gaze behavior, facilitating experimentation, and aiding novel applications in virtual reality and beyond.
C1 [Martin, Daniel; Serrano, Ana; Masia, Belen] Univ Zaragoza, I3A, Zaragoza, Spain.
   [Bergman, Alexander W.; Wetzstein, Gordon] Stanford Univ, Stanford, CA 94305 USA.
C3 University of Zaragoza; Stanford University
RP Martin, D (corresponding author), Univ Zaragoza, I3A, Zaragoza, Spain.
EM danims@unizar.es
RI Martin, Daniel/KLZ-9356-2024; Serrano Pacheu, Ana Belen/ABC-3358-2021
OI Serrano Pacheu, Ana Belen/0000-0002-7796-3177; Martin,
   Daniel/0000-0002-0073-6398
FU European Research Council (ERC) under the EU [682080]; Leonardo Grant
   for Researchers and Cultural Creators, BBVA Foundation; NSF [1839974];
   Gobierno de Aragon; European Research Council (ERC) [682080] Funding
   Source: European Research Council (ERC)
FX We thank Diego Gutierrez for the revision of the manuscript. This work
   has received funding from the European Research Council (ERC) under the
   EU's Horizon 2020 research and innovation programme (project CHAMELEON,
   Grant no. 682080). This project was also supported by a 2020 Leonardo
   Grant for Researchers and Cultural Creators, BBVA Foundation (the BBVA
   Foundation accepts no responsibility for the opinions, statements and
   contents included in the project and/or the results thereof, which are
   entirely the responsibility of the authors). This project was in part
   supported by NSF award 1839974. Additionally, Daniel Martin was
   supported by a Gobierno de Aragon (2020-2024) predoctoral grant.
NR 69
TC 28
Z9 27
U1 4
U2 28
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY 1
PY 2022
VL 28
IS 5
BP 2003
EP 2013
DI 10.1109/TVCG.2022.3150502
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 1R1AK
UT WOS:000803110400019
PM 35167469
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Hinterreiter, A
   Ruch, P
   Stitz, H
   Ennemoser, M
   Bernard, J
   Strobelt, H
   Streit, M
AF Hinterreiter, Andreas
   Ruch, Peter
   Stitz, Holger
   Ennemoser, Martin
   Bernard, Jurgen
   Strobelt, Hendrik
   Streit, Marc
TI ConfusionFlow: A Model-Agnostic Visualization for Temporal Analysis of
   Classifier Confusion
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Analytical models; Data models; Training; Tools;
   Adaptation models; Data visualization; Classification; performance
   analysis; time series visualization; machine learning; information
   visualization; quality assessment
ID VISUAL ANALYTICS
AB Classifiers are among the most widely used supervised machine learning algorithms. Many classification models exist, and choosing the right one for a given task is difficult. During model selection and debugging, data scientists need to assess classifiers' performances, evaluate their learning behavior over time, and compare different models. Typically, this analysis is based on single-number performance measures such as accuracy. A more detailed evaluation of classifiers is possible by inspecting class errors. The confusion matrix is an established way for visualizing these class errors, but it was not designed with temporal or comparative analysis in mind. More generally, established performance analysis systems do not allow a combined temporal and comparative analysis of class-level information. To address this issue, we propose ConfusionFlow, an interactive, comparative visualization tool that combines the benefits of class confusion matrices with the visualization of performance characteristics over time. ConfusionFlow is model-agnostic and can be used to compare performances for different model types, model architectures, and/or training and test datasets. We demonstrate the usefulness of ConfusionFlow in a case study on instance selection strategies in active learning. We further assess the scalability of ConfusionFlow and present a use case in the context of neural network pruning.
C1 [Hinterreiter, Andreas; Stitz, Holger] Johannes Kepler Univ Linz, Inst Comp Graph, A-4040 Linz, Austria.
   [Hinterreiter, Andreas] Imperial Coll London, Biomed Image Anal Grp, London SW7 2AZ, England.
   [Ruch, Peter] Johannes Kepler Univ Linz, Inst Machine Learning, A-4040 Linz, Austria.
   [Streit, Marc] Johannes Kepler Univ Linz, Visual Data Sci, A-4040 Linz, Austria.
   [Stitz, Holger] Datavisyn GmbH, A-4040 Linz, Austria.
   [Ennemoser, Martin] Salesbeat GmbH, A-4060 Leonding, Austria.
   [Strobelt, Hendrik] IBM Res, Cambridge, MA 02142 USA.
   [Bernard, Jurgen] Univ British Columbia, Vancouver, BC V6T 1Z4, Canada.
C3 Johannes Kepler University Linz; Imperial College London; Johannes
   Kepler University Linz; Johannes Kepler University Linz; International
   Business Machines (IBM); IBM USA; University of British Columbia
RP Hinterreiter, A (corresponding author), Johannes Kepler Univ Linz, Inst Comp Graph, A-4040 Linz, Austria.
EM andreas.hinterreiter@jku.at; peter.ruch@jku.at; holger.stitz@jku.at;
   m.ennemoser@salesbeat.io; jubernar@cs.ubc.ca; hendrik.strobelt@ibm.com;
   marc.streit@jku.at
RI Bernard, Jürgen/AAK-5732-2021
OI Strobelt, Hendrik/0000-0002-8995-1683; Stitz,
   Holger/0000-0002-4742-2636; Streit, Marc/0000-0001-9186-2092
FU State of Upper Austria; Austrian Federal Ministry of Education, Science
   and Research via the LIT -Linz Institute of Technology
   [LIT2019-7-SEE-117]; State of Upper Austria (HumanInterpretable Machine
   Learning); Austrian Research Promotion Agency [FFG 851460]; Austrian
   Science Fund (FWF) [P27975-NBL]; Austrian Science Fund (FWF) [P27975]
   Funding Source: Austrian Science Fund (FWF)
FX This work was supported in part by the State of Upper Austria and the
   Austrian Federal Ministry of Education, Science and Research via the LIT
   -Linz Institute of Technology (LIT2019-7-SEE-117), by the State of Upper
   Austria (HumanInterpretable Machine Learning), by the Austrian Research
   Promotion Agency (FFG 851460), and by the Austrian Science Fund (FWF
   P27975-NBL). Andreas Hinterreiter and Peter Ruch contributed equally to
   this work.
NR 63
TC 24
Z9 27
U1 1
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2022
VL 28
IS 2
BP 1222
EP 1236
DI 10.1109/TVCG.2020.3012063
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XY1KL
UT WOS:000736740300002
PM 32746284
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Woodin, G
   Winter, B
   Padilla, L
AF Woodin, Greg
   Winter, Bodo
   Padilla, Lace
TI Conceptual Metaphor and Graphical Convention Influence the
   Interpretation of Line Graphs
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Conceptual metaphor theory; more is up; mental number line; cognition;
   linguistics; emotional valence; line graph; axis reversal; handedness;
   empirical evaluation
ID R PACKAGE; TIME; REPRESENTATIONS; ASSOCIATION; LANGUAGE; VALENCE; SPACE;
   EYE
AB Many metaphors in language reflect conceptual metaphors that structure thought. In line with metaphorical expressions such as 'high number', experiments show that people associate larger numbers with upward space. Consistent with this metaphor, high numbers are conventionally depicted in high positions on the y-axis of line graphs. People also associate good and bad (emotional valence) with upward and downward locations, in line with metaphorical expressions such as 'uplifting' and 'down in the dumps'. Graphs depicting good quantities (e.g., vacation days) are consistent with graphical convention and the valence metaphor, because 'more' of the good quantity is represented by higher y-axis positions. In contrast, graphs depicting bad quantities (e.g., murders) are consistent with graphical convention, but not the valence metaphor, because more of the bad quantity is represented by higher (rather than lower) y-axis positions. We conducted two experiments (N = 300 per experiment) where participants answered questions about line graphs depicting good and bad quantities. For some graphs, we inverted the conventional axis ordering of numbers. Line graphs that aligned (versus misaligned) with valence metaphors (up = good) were easier to interpret, but this beneficial effect did not outweigh the adverse effect of inverting the axis numbering. Line graphs depicting good (versus bad) quantities were easier to interpret, as were graphs that depicted quantity using the x-axis (versus y-axis). Our results suggest that conceptual metaphors matter for the interpretation of line graphs. However, designers of line graphs are warned against subverting graphical convention to align with conceptual metaphors.
C1 [Woodin, Greg; Winter, Bodo] Univ Birmingham, Dept English Language & Linguist, Birmingham B15 2SQ, W Midlands, England.
   [Woodin, Greg] Univ Calif Merced, Appl Cognit & Educ Lab, Spatial Percept, Merced, CA 95343 USA.
C3 University of Birmingham; University of California System; University of
   California Merced
RP Woodin, G (corresponding author), Univ Birmingham, Dept English Language & Linguist, Birmingham B15 2SQ, W Midlands, England.; Woodin, G (corresponding author), Univ Calif Merced, Appl Cognit & Educ Lab, Spatial Percept, Merced, CA 95343 USA.
EM gawoodin@gmail.com; bodo@bodowinter.com
RI Winter, Bodo/K-6975-2018
OI Woodin, Greg/0000-0001-7992-4991
FU Economic and Social Research Council [ES/P000711/1]; UKRI Future Leaders
   Fellowship [MR/T040505/1]; ESRC [2067243] Funding Source: UKRI; FLF
   [MR/T040505/1] Funding Source: UKRI
FX The work of Greg Woodin was supported by the Economic and Social
   Research Council under Grant ES/P000711/1. The work of Bodo Winter was
   supported by the UKRI Future Leaders Fellowship under Grant
   MR/T040505/1.
NR 80
TC 7
Z9 7
U1 4
U2 22
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2022
VL 28
IS 2
BP 1209
EP 1221
DI 10.1109/TVCG.2021.3088343
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA YC3YL
UT WOS:000739630200001
PM 34110996
DA 2025-03-07
ER

PT J
AU Zhang, JY
   Chen, KY
   Zheng, JM
AF Zhang, Juyong
   Chen, Keyu
   Zheng, Jianmin
TI Facial Expression Retargeting From Human to Avatar Made Easy
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Avatars; Three-dimensional displays; Strain; Animation; Shape; Solid
   modeling; Machine learning; Facial expression retargeting; variational
   autoencoder; deformation transfer; cross domain translation; triplet
ID MODEL
AB Facial expression retargeting from humans to virtual characters is a useful technique in computer graphics and animation. Traditional methods use markers or blendshapes to construct a mapping between the human and avatar faces. However, these approaches require a tedious 3D modeling process, and the performance relies on the modelers' experience. In this article, we propose a brand-new solution to this cross-domain expression transfer problem via nonlinear expression embedding and expression domain translation. We first build low-dimensional latent spaces for the human and avatar facial expressions with variational autoencoder. Then we construct correspondences between the two latent spaces guided by geometric and perceptual constraints. Specifically, we design geometric correspondences to reflect geometric matching and utilize a triplet data structure to express users' perceptual preference of avatar expressions. A user-friendly method is proposed to automatically generate triplets for a system allowing users to easily and efficiently annotate the correspondences. Using both geometric and perceptual correspondences, we trained a network for expression domain translation from human to avatar. Extensive experimental results and user studies demonstrate that even nonprofessional users can apply our method to generate high-quality facial expression retargeting results with less time and effort.
C1 [Zhang, Juyong; Chen, Keyu] Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Anhui, Peoples R China.
   [Zheng, Jianmin] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Nanyang Technological University
RP Zhang, JY (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Anhui, Peoples R China.
EM juyong@ustc.edu.cn; cky95@mail.ustc.edu.cn; asjmzheng@ntu.edu.sg
RI Zheng, Jianmin/A-3717-2011; Chen, KeYu/KOD-2789-2024
OI Zheng, Jianmin/0000-0002-5062-6226; Chen, Keyu/0000-0002-0440-5852
FU National Natural Science Foundation of China [61672481]; Youth
   Innovation Promotion Association CAS [2018495]; Zhejiang Lab
   [2019NB0AB03]; NTU Data Science and Artificial Intelligence Research
   Center (DSAIR) [04INS000518C130]; Ministry of Education, Singapore,
   under its MoE Tier-2 Grant [MoE 2017-T2-1-076]
FX This research was supported in part by the National Natural Science
   Foundation of China (No. 61672481), Youth Innovation Promotion
   Association CAS (No. 2018495), Zhejiang Lab (NO. 2019NB0AB03), NTU Data
   Science and Artificial Intelligence Research Center (DSAIR) (No.
   04INS000518C130), and the Ministry of Education, Singapore, under its
   MoE Tier-2 Grant (MoE 2017-T2-1-076).
NR 52
TC 25
Z9 27
U1 2
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2022
VL 28
IS 2
BP 1274
EP 1287
DI 10.1109/TVCG.2020.3013876
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XY1KL
UT WOS:000736740300006
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kristiansen, YS
   Garrison, L
   Bruckner, S
AF Kristiansen, Yngve S.
   Garrison, Laura
   Bruckner, Stefan
TI Semantic Snapping for Guided Multi-View Visualization Design
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Semantics; Task analysis; Guidelines; Visualization;
   Tools; Image color analysis; Tabular data; guidelines; mixed initiative
   human-machine analysis; coordinated and multiple views
ID INFORMATION; PATTERNS; QUERY
AB Visual information displays are typically composed of multiple visualizations that are used to facilitate an understanding of the underlying data. A common example are dashboards, which are frequently used in domains such as finance, process monitoring and business intelligence. However, users may not be aware of existing guidelines and lack expert design knowledge when composing such multi-view visualizations. In this paper, we present semantic snapping, an approach to help non-expert users design effective multi-view visualizations from sets of pre-existing views. When a particular view is placed on a canvas, it is "aligned" with the remaining views-not with respect to its geometric layout, but based on aspects of the visual encoding itself, such as how data dimensions are mapped to channels. Our method uses an on-the-fly procedure to detect and suggest resolutions for conflicting, misleading, or ambiguous designs, as well as to provide suggestions for alternative presentations. With this approach, users can be guided to avoid common pitfalls encountered when composing visualizations. Our provided examples and case studies demonstrate the usefulness and validity of our approach.
C1 [Kristiansen, Yngve S.; Garrison, Laura; Bruckner, Stefan] Univ Bergen, Dept Informat, Bergen, Norway.
C3 University of Bergen
RP Kristiansen, YS (corresponding author), Univ Bergen, Dept Informat, Bergen, Norway.
EM ykr088@uib.no; laura.garrison@uib.no; stefan.bruckner@uib.no
FU MetaVis project - Research Council of Norway [250133]; VIDI project -
   Trond Mohn Foundation in Bergen, Norway [813558]
FX The research presented in this paper was supported by the MetaVis
   project (#250133) funded by the Research Council of Norway as well as
   the VIDI project (#813558) funded by the the Trond Mohn Foundation in
   Bergen, Norway.
NR 50
TC 4
Z9 5
U1 1
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 43
EP 53
DI 10.1109/TVCG.2021.3114860
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000021
PM 34591769
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Li, CH
   Baciu, G
   Wang, YZ
   Chen, JJ
   Wang, CB
AF Li, Chenhui
   Baciu, George
   Wang, Yunzhe
   Chen, Junjie
   Wang, Changbo
TI DDLVis: Real-time Visual Query of Spatiotemporal Data Distribution via
   Density Dictionary Learning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Spatiotemporal phenomena; Data visualization; Real-time
   systems; Machine learning; Estimation; Encoding; Visual query;
   information visualization; spatiotemporal data; data compression;
   interaction; density map
ID K-SVD; VISUALIZATION; EXPLORATION; DISPLAY; MAPS
AB Visual query of spatiotemporal data is becoming an increasingly important function in visual analytics applications. Various works have been presented for querying large spatiotemporal data in real time. However, the real-time query of spatiotemporal data distribution is still an open challenge. As spatiotemporal data become larger, methods of aggregation, storage and querying become critical. We propose a new visual query system that creates a low-memory storage component and provides real-time visual interactions of spatiotemporal data. We first present a peak-based kernel density estimation method to produce the data distribution for the spatiotemporal data. Then a novel density dictionary learning approach is proposed to compress temporal density maps and accelerate the query calculation. Moreover, various intuitive query interactions are presented to interactively gain patterns. The experimental results obtained on three datasets demonstrate that the presented system offers an effective query for visual analytics of spatiotemporal data.
C1 [Li, Chenhui; Chen, Junjie; Wang, Changbo] East China Normal Univ, Sch Comp Sci & Technol, Shanghai, Peoples R China.
   [Baciu, George] Hong Kong Polytech Univ, Hong Kong, Peoples R China.
   [Wang, Yunzhe] Suzhou Univ Sci & Technol, Suzhou, Peoples R China.
C3 East China Normal University; Hong Kong Polytechnic University; Suzhou
   University of Science & Technology
RP Wang, CB (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai, Peoples R China.
EM chli@cs.ecnu.edu.cn; cbwang@cs.ecnu.edu.cn
RI Baciu, George/AAU-7143-2021; Li, Chenhui/AAR-3682-2020
FU NSFC [61802128, 62072183]
FX The authors wish to acknowledge the support from NSFC under Grants (No.
   61802128 and 62072183).
NR 59
TC 8
Z9 11
U1 1
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 1062
EP 1072
DI 10.1109/TVCG.2021.3114762
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000102
PM 34587020
DA 2025-03-07
ER

PT J
AU Sohns, JT
   Schmitt, M
   Jirasek, F
   Hasse, H
   Leitte, H
AF Sohns, Jan-Tobias
   Schmitt, Michaela
   Jirasek, Fabian
   Hasse, Hans
   Leitte, Heike
TI Attribute-based Explanation of Non-Linear Embeddings of High-Dimensional
   Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Task analysis; Data analysis;
   Topology; Image color analysis; Dimensionality reduction; Dimensionality
   reduction; embedding; augmented projections; point set contours;
   explainable artificial intelligence
ID PREDICTION; REDUCTION; POINTS; SET
AB Embeddings of high-dimensional data are widely used to explore data, to verify analysis results, and to communicate information. Their explanation, in particular with respect to the input attributes, is often difficult. With linear projects like PCA the axes can still be annotated meaningfully. With non-linear projections this is no longer possible and alternative strategies such as attribute-based color coding are required. In this paper, we review existing augmentation techniques and discuss their limitations. We present the Non-Linear Embeddings Surveyor (NoLiES) that combines a novel augmentation strategy for projected data (rangesets) with interactive analysis in a small multiples setting. Rangesets use a set-based visualization approach for binned attribute values that enable the user to quickly observe structure and detect outliers. We detail the link between algebraic topology and rangesets and demonstrate the utility of NoLiES in case studies with various challenges (complex attribute value distribution, many attributes, many data points) and a real-world application to understand latent features of matrix completion in thermodynamics.
C1 [Sohns, Jan-Tobias; Schmitt, Michaela; Leitte, Heike] TU Kaiserslautern, Visual Informat Anal Grp, Kaiserslautern, Germany.
   [Jirasek, Fabian; Hasse, Hans] TU Kaiserslautern, Lab Engn Thermodynam LTD, Kaiserslautern, Germany.
C3 University of Kaiserslautern; University of Kaiserslautern
RP Hasse, H (corresponding author), TU Kaiserslautern, Lab Engn Thermodynam LTD, Kaiserslautern, Germany.
EM hans.hasse@mv.uni-kl.de; leitte@cs.uni-kl.de
RI Jirasek, Fabian/ABC-2604-2021
OI Leitte, Heike/0000-0002-7112-2190
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
   [252408385 -IRTG 2057]
FX This research was funded by the Deutsche Forschungsgemeinschaft (DFG,
   German Research Foundation) -252408385 -IRTG 2057.
NR 62
TC 9
Z9 9
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 540
EP 550
DI 10.1109/TVCG.2021.3114870
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000062
PM 34587086
DA 2025-03-07
ER

PT J
AU Tang, JX
   Zhou, YH
   Tang, T
   Weng, D
   Xie, BY
   Yu, LY
   Zhang, HQ
   Wu, YC
AF Tang, Junxiu
   Zhou, Yuhua
   Tang, Tan
   Weng, Di
   Xie, Boyang
   Yu, Lingyun
   Zhang, Huaqiang
   Wu, Yingcai
TI A Visualization Approach for Monitoring Order Processing in E-Commerce
   Warehouse
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Monitoring; Real-time systems; Schedules; Delays;
   Warehousing; Visual analytics; Streaming data; time-series data;
   e-commerce warehouse; order processing
ID ANOMALY DETECTION; VISUAL ANALYTICS; PICKING; ALGORITHMS; DESIGN;
   SYSTEM; MODEL
AB The efficiency of warehouses is vital to e-commerce. Fast order processing at the warehouses ensures timely deliveries and improves customer satisfaction. However, monitoring, analyzing, and manipulating order processing in the warehouses in real time are challenging for traditional methods due to the sheer volume of incoming orders, the fuzzy definition of delayed order patterns, and the complex decision-making of order handling priorities. In this paper, we adopt a data-driven approach and propose OrderMonitor, a visual analytics system that assists warehouse managers in analyzing and improving order processing efficiency in real time based on streaming warehouse event data. Specifically, the order processing pipeline is visualized with a novel pipeline design based on the sedimentation metaphor to facilitate real-time order monitoring and suggest potentially abnormal orders. We also design a novel visualization that depicts order timelines based on the Gantt charts and Marey's graphs. Such a visualization helps the managers gain insights into the performance of order processing and find major blockers for delayed orders. Furthermore, an evaluating view is provided to assist users in inspecting order details and assigning priorities to improve the processing performance. The effectiveness of OrderMonitor is evaluated with two case studies on a real-world warehouse dataset.
C1 [Tang, Junxiu; Zhou, Yuhua; Tang, Tan; Weng, Di; Xie, Boyang; Wu, Yingcai] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.
   [Yu, Lingyun] Xian Jiaotong Liverpool Univ, Dept Comp, Suzhou, Peoples R China.
   [Zhang, Huaqiang] Alibaba Grp, Hangzhou, Peoples R China.
C3 Zhejiang University; Xi'an Jiaotong-Liverpool University; Alibaba Group
RP Tang, JX (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.
EM tangjunxiu@zju.edu.cn; zhouyuhua@zju.edu.cn; tangtan@zju.edu.cn;
   dweng@zju.edu.cn; xboyang@zju.edu.cn; lingyun.yu@xjdu.edu.cn;
   huaqiang.zhq@cainiao.com; ycwu@zju.edu.cn
RI Weng, Di/ABG-7408-2020; Tang, Tan/JJD-3333-2023; Zhou,
   Yuhua/LMQ-3076-2024; Xie, Boyang/R-5819-2018
OI Weng, Di/0000-0003-2712-7274; Tang, Junxiu/0000-0003-3594-926X
FU NSFC [62072400]; Zhejiang Provincial Natural Science Foundation
   [LR18F020001]; Alibaba-Zhejiang University Joint Institute of Frontier
   Technologies; Collaborative Innovation Center of Artificial Intelligence
   by MOE; Zhejiang Provincial Government (ZJU)
FX This work was supported by NSFC (62072400) and Zhejiang Provincial
   Natural Science Foundation (LR18F020001). This work was also supported
   by Alibaba-Zhejiang University Joint Institute of Frontier Technologies
   and the Collaborative Innovation Center of Artificial Intelligence by
   MOE and Zhejiang Provincial Government (ZJU).
NR 67
TC 12
Z9 13
U1 5
U2 42
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 857
EP 867
DI 10.1109/TVCG.2021.3114878
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000088
PM 34596553
DA 2025-03-07
ER

PT J
AU Venkat, A
   Gyulassy, A
   Kosiba, G
   Maiti, A
   Reinstein, H
   Gee, R
   Bremer, PT
   Pascucci, V
AF Venkat, Aniketh
   Gyulassy, Attila
   Kosiba, Graham
   Maiti, Amitesh
   Reinstein, Henry
   Gee, Richard
   Bremer, Peer-Timo
   Pascucci, Valerio
TI Towards replacing physical testing of granular materials with a
   Topology-based Model
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Mathematical models; Surface treatment; Powders; Computational
   modeling; Area measurement; Particle measurements; Physical and
   Environmental Sciences; Computational Topology-based Techniques; Data
   Abstractions and Types; Scalar Field Data; Pore Network Model;
   Morse-Smale Complex
ID PORE-NETWORK; EVOLUTION; FLOW
AB In the study of packed granular materials, the performance of a sample (e.g., the detonation of a high-energy explosive) often correlates to measurements of a fluid flowing through it. The "effective surface area," the surface area accessible to the airflow, is typically measured using a permeametry apparatus that relates the flow conductance to the permeable surface area via the Carman-Kozeny equation. This equation allows calculating the flow rate of a fluid flowing through the granules packed in the sample for a given pressure drop. However, Carman-Kozeny makes inherent assumptions about tunnel shapes and flow paths that may not accurately hold in situations where the particles possess a wide distribution in shapes, sizes, and aspect ratios, as is true with many powdered systems of technological and commercial interest. To address this challenge, we replicate these measurements virtually on micro-CT images of the powdered material, introducing a new Pore Network Model based on the skeleton of the Morse-Smale complex. Pores are identified as basins of the complex, their incidence encodes adjacency, and the conductivity of the capillary between them is computed from the cross-section at their interface. We build and solve a resistive network to compute an approximate laminar fluid flow through the pore structure. We provide two means of estimating flow-permeable surface area: (i) by direct computation of conductivity, and (ii) by identifying dead-ends in the flow coupled with isosurface extraction and the application of the Carman-Kozeny equation, with the aim of establishing consistency over a range of particle shapes, sizes, porosity levels, and void distribution patterns.
C1 [Venkat, Aniketh; Gyulassy, Attila; Pascucci, Valerio] Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.
   [Kosiba, Graham; Maiti, Amitesh; Reinstein, Henry; Gee, Richard; Bremer, Peer-Timo] Lawrence Livermore Natl Lab, Lawrence, KS USA.
C3 Utah System of Higher Education; University of Utah; United States
   Department of Energy (DOE); Lawrence Livermore National Laboratory
RP Venkat, A (corresponding author), Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.
EM aniketh.venkat@sci.utah.edu; bremer5@llnl.gov
RI pascucci, Valerio/GXF-0616-2022; Maiti, Amitesh/HTP-2787-2023
OI Gee, Richard/0000-0001-6668-903X; Maiti, Amitesh/0000-0003-0831-0700;
   pascucci, valerio/0000-0002-8877-2042
FU Department of Energy [DE-FE0031880]; Exascale Computing Project
   [l7-SC-20-SC]; NSF OAC award [1941085, 1842042]; NSF CMMI award
   [1629660]; u.S. Department of Energy by Lawrence Livermore National
   Laboratory [DE-AC52-07NA27344]
FX This research is supported in part by the Department of Energy under
   Award Number(s) DE-FE0031880 and the Exascale Computing Project
   (l7-SC-20-SC), a collaborative effort of the u.S. Department of Energy
   Office of Science and the National Nuclear Security Administration, and
   in part by NSF OAC award 1842042, NSF OAC award 1941085, and NSF CMMI
   award 1629660. The work at LLNL was performed under the auspices of the
   u.S. Department of Energy by Lawrence Livermore National Laboratory
   under Contract DE-AC52-07NA27344.
NR 46
TC 3
Z9 3
U1 0
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 76
EP 85
DI 10.1109/TVCG.2021.3114819
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000024
PM 34882553
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhang, DP
   Adar, E
   Hullman, J
AF Zhang, Dongping
   Adar, Eytan
   Hullman, Jessica
TI Visualizing Uncertainty in Probabilistic Graphs with Network
   Hypothetical Outcome Plots (NetHOPs)
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Probabilistic logic; Visualization; Uncertainty; Layout; Task analysis;
   Stability analysis; Encoding; Network; Uncertainty; Application
ID P-ASTERISK MODELS; COMMUNITY STRUCTURE; LAYOUT; FREQUENCY; INFORMATION;
   RELIABILITY
AB Probabilistic graphs are challenging to visualize using the traditional node-link diagram. Encoding edge probability using visual variables like width or fuzziness makes it difficult for users of static network visualizations to estimate network statistics like densities, isolates, path lengths, or clustering under uncertainty. We introduce Network Hypothetical Outcome Plots (NetHOPs), a visualization technique that animates a sequence of network realizations sampled from a network distribution defined by probabilistic edges. NetHOPs employ an aggregation and anchoring algorithm used in dynamic and longitudinal graph drawing to parameterize layout stability for uncertainty estimation. We present a community matching algorithm to enable visualizing the uncertainty of cluster membership and community occurrence. We describe the results of a study in which 51 network experts used NetHOPs to complete a set of common visual analysis tasks and reported how they perceived network structures and properties subject to uncertainty. Participants' estimates fell, on average, within 11% of the ground truth statistics, suggesting NetHOPs can be a reasonable approach for enabling network analysts to reason about multiple properties under uncertainty. Participants appeared to articulate the distribution of network statistics slightly more accurately when they could manipulate the layout anchoring and the animation speed. Based on these findings, we synthesize design recommendations for developing and using animated visualizations for probabilistic networks.
C1 [Zhang, Dongping; Hullman, Jessica] Northwestern Univ, Evanston, IL 60208 USA.
   [Adar, Eytan] Univ Michigan, Ann Arbor, MI 48109 USA.
C3 Northwestern University; University of Michigan System; University of
   Michigan
RP Zhang, DP (corresponding author), Northwestern Univ, Evanston, IL 60208 USA.
EM dzhang@u.northwestern.edu; eadar@umich.edu; jhullman@northwestern.edu
RI Zhang, Dongping/AAD-9164-2019; Hullman, Jessica/P-7130-2018
OI Zhang, Dongping/0000-0001-9825-1411
FU NSF [IIS-1815760, IIS-1907941]; Microsoft
FX This work was supported by NSF IIS-1907941, NSF IIS-1815760, and
   Microsoft.
NR 93
TC 5
Z9 7
U1 1
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 443
EP 453
DI 10.1109/TVCG.2021.3114679
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA XU0IB
UT WOS:000733959000054
PM 34587012
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Li, ZM
   Menon, H
   Maljovec, D
   Livnat, Y
   Liu, SS
   Mohror, K
   Bremer, PT
   Pascucci, V
AF Li, Zhimin
   Menon, Harshitha
   Maljovec, Dan
   Livnat, Yarden
   Liu, Shusen
   Mohror, Kathryn
   Bremer, Peer-Timo
   Pascucci, Valerio
TI SpotSDC: Revealing the Silent Data Corruption Propagation in
   High-Performance Computing Systems
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Transient analysis; Resilience; Tools; Analytical
   models; Hardware; Computer crashes; Fault injection sampling; error
   propagation; information visualization; silent data corruption
ID TREE VISUALIZATION; ERROR
AB The trend of rapid technology scaling is expected to make the hardware of high-performance computing (HPC) systems more susceptible to computational errors due to random bit flips. Some bit flips may cause a program to crash or have a minimal effect on the output, but others may lead to silent data corruption (SDC), i.e., undetected yet significant output errors. Classical fault injection analysis methods employ uniform sampling of random bit flips during program execution to derive a statistical resiliency profile. However, summarizing such fault injection result with sufficient detail is difficult, and understanding the behavior of the fault-corrupted program is still a challenge. In this article, we introduce SpotSDC, a visualization system to facilitate the analysis of a program's resilience to SDC. SpotSDC provides multiple perspectives at various levels of detail of the impact on the output relative to where in the source code the flipped bit occurs, which bit is flipped, and when during the execution it happens. SpotSDC also enables users to study the code protection and provide new insights to understand the behavior of a fault-injected program. Based on lessons learned, we demonstrate how what we found can improve the fault injection campaign method.
C1 [Li, Zhimin; Maljovec, Dan; Livnat, Yarden; Pascucci, Valerio] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
   [Menon, Harshitha; Liu, Shusen; Mohror, Kathryn; Bremer, Peer-Timo] Lawrence Livermore Natl Lab, Livermore, CA 94550 USA.
C3 Utah System of Higher Education; University of Utah; United States
   Department of Energy (DOE); Lawrence Livermore National Laboratory
RP Li, ZM (corresponding author), Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
EM zhimin@sci.utah.edu; harshitha@llnl.gov; maljovec@sci.utah.edu;
   yarden@sci.utah.edu; liu42@llnl.gov; mohror1@llnl.gov; bremer5@llnl.gov;
   pascucci@sci.utah.edu
RI pascucci, Valerio/GXF-0616-2022
OI li, zhimin/0000-0003-4324-741X; pascucci, valerio/0000-0002-8877-2042;
   Bremer, Peer-Timo/0000-0003-4107-3831
FU U.S. Department of Energy by Lawrence Livermore National Laboratory
   [DE-AC52-07NA27344 (LLNL-CONF-764021)]
FX This work was performed under the auspices of the U.S. Department of
   Energy by Lawrence Livermore National Laboratory under Contract
   DE-AC52-07NA27344 (LLNL-CONF-764021).
NR 57
TC 7
Z9 7
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2021
VL 27
IS 10
BP 3938
EP 3952
DI 10.1109/TVCG.2020.2994954
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL8JB
UT WOS:000692890200008
PM 32746251
OA Green Submitted, Bronze
DA 2025-03-07
ER

PT J
AU Yu, GX
   Hu, YT
   Dai, JW
AF Yu, Guoxing
   Hu, Yongtao
   Dai, Jingwen
TI TopoTag: A Robust and Scalable Topological Fiducial Marker System
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Robustness; Pose estimation; Shape; Encoding; Jitter; Image color
   analysis; Cameras; Fiducial marker; monocular pose estimation;
   topological information; marker design; ID decoding
ID GENERATION; TAG
AB Fiducial markers have been playing an important role in augmented reality (AR), robot navigation, and general applications where the relative pose between a camera and an object is required. Here we introduce TopoTag, a robust and scalable topological fiducial marker system, which supports reliable and accurate pose estimation from a single image. TopoTag uses topological and geometrical information in marker detection to achieve higher robustness. Topological information is extensively used for 2D marker detection, and further corresponding geometrical information for ID decoding. Robust 3D pose estimation is achieved by taking advantage of all TopoTag vertices. Without sacrificing bits for higher recall and precision like previous systems, TopoTag can use full bits for ID encoding. TopoTag supports tens of thousands unique IDs and easily extends to millions of unique tags resulting in massive scalability. We collected a large test dataset including in total 169,713 images for evaluation, involving in-plane and out-of-plane rotation, image blur, different distances, and various backgrounds, etc. Experiments on the dataset and real indoor and outdoor scene tests with a rolling shutter camera both show that TopoTag significantly outperforms previous fiducial marker systems in terms of various metrics, including detection accuracy, vertex jitter, pose jitter and accuracy, etc. In addition, TopoTag supports occlusion as long as the main tag topological structure is maintained and allows for flexible shape design where users can customize internal and external marker shapes. Code for our marker design/generation, marker detection, and dataset are available at http://herohuyongtao.github.io/research/publications/topo-tag/.
C1 [Yu, Guoxing; Hu, Yongtao; Dai, Jingwen] Guangdong Virtual Real Technol Co Ltd aka Ximmers, Shenzhen 518000, Guangdong, Peoples R China.
RP Hu, YT (corresponding author), Guangdong Virtual Real Technol Co Ltd aka Ximmers, Shenzhen 518000, Guangdong, Peoples R China.
EM calvin.yu@ximmerse.com; ythu@ximmerse.com; dai@ximmerse.com
RI Dai, Jingwen/AAJ-8767-2020; Hu, Yongtao/AHH-2482-2022
OI Hu, Yongtao/0000-0002-3768-6590
NR 51
TC 38
Z9 42
U1 1
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2021
VL 27
IS 9
BP 3769
EP 3780
DI 10.1109/TVCG.2020.2988466
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TS3CP
UT WOS:000679532000013
PM 32324556
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Nardini, P
   Chen, M
   Samsel, F
   Bujack, R
   Böttinger, M
   Scheuermann, G
AF Nardini, Pascal
   Chen, Min
   Samsel, Francesca
   Bujack, Roxana
   Boettinger, Michael
   Scheuermann, Gerik
TI The Making of Continuous Colormaps
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image color analysis; Data visualization; Tools; Semantics; Guidelines;
   Standards; Software systems; CCC-Tool; charting continuous colormaps;
   colormap specification; perceptual uniformity; colormap analysis
ID DIFFERENCE FORMULA; VISUALIZATION; UNIVARIATE; SEQUENCES; PALETTES
AB Continuous colormaps are integral parts of many visualization techniques, such as heat-maps, surface plots, and flow visualization. Despite that the critiques of rainbow colormaps have been around and well-acknowledged for three decades, rainbow colormaps are still widely used today. One reason behind the resilience of rainbow colormaps is the lack of tools for users to create a continuous colormap that encodes semantics specific to the application concerned. In this paper, we present a web-based software system, CCC-Tool (short for Charting Continuous Colormaps) under the URL https://ccctool.com, for creating, editing, and analyzing such application-specific colormaps. We introduce the notion of "colormap specification (CMS)" that maintains the essential semantics required for defining a color mapping scheme. We provide users with a set of advanced utilities for constructing CMS's with various levels of complexity, examining their quality attributes using different plots, and exporting them to external application software. We present two case studies, demonstrating that the CCC-Tool can help domain scientists as well as visualization experts in designing semantically-rich colormaps.
C1 [Nardini, Pascal] Univ Leipzig, Inst Comp Sci, D-04109 Leipzig, Germany.
   [Chen, Min] Univ Oxford, Dept Engn Sci, Oxford OX1 2JD, England.
   [Samsel, Francesca] Univ Texas Austin, Ctr Agile Technol, Austin, TX 78712 USA.
   [Bujack, Roxana] Los Alamos Natl Lab, Data Sci Scale Team, Los Alamos, NM 87545 USA.
   [Boettinger, Michael] German Climate Comp Ctr DKRZ, D-20146 Hamburg, Germany.
   [Scheuermann, Gerik] Univ Leipzig, Inst Comp Sci, D-04109 Leipzig, Germany.
C3 Leipzig University; University of Oxford; University of Texas System;
   University of Texas Austin; United States Department of Energy (DOE);
   Los Alamos National Laboratory; Leipzig University
RP Nardini, P (corresponding author), Univ Leipzig, Inst Comp Sci, D-04109 Leipzig, Germany.
EM nardini@informatik.unileipzig.de; min.chen@oerc.ox.ac.uk;
   fsamsel@tacc.utexas.edu; bujack@lanl.gov; boettinger@dkrz.de;
   scheuermann@informatik.uni-leipzig.de
OI Bottinger, Michael/0000-0001-9704-8235; Bujack,
   Roxana/0000-0002-5479-3726; Chen, Min/0000-0001-5320-5729; Scheuermann,
   Gerik/0000-0001-5200-8870; Samsel, Francesca/0000-0002-8596-6159
NR 90
TC 20
Z9 20
U1 2
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2021
VL 27
IS 6
BP 3048
EP 3063
DI 10.1109/TVCG.2019.2961674
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SA9KC
UT WOS:000649620700020
PM 31870986
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Xu, ZW
   Foi, A
AF Xu, Zhongwei
   Foi, Alessandro
TI Anisotropic Denoising of 3D Point Clouds by Aggregation of Multiple
   Surface-Adaptive Estimates
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Noise reduction; Noise measurement;
   Estimation; Adaptation models; Approximation algorithms; Shape; 3D point
   cloud; denoising; anisotropic; surface-adaptive filtering
ID ROBUST NORMAL ESTIMATION
AB 3D point clouds commonly contain positional errors which can be regarded as noise. We propose a point cloud denoising algorithm based on aggregation of multiple anisotropic estimates computed on local coordinate systems. These local estimates are adaptive to the shape of the surface underlying the point cloud, leveraging an extension of the Local Polynomial Approximation (LPA) - Intersection of Confidence Intervals (ICI) technique to 3D point clouds. The adaptivity due to LPA-ICI is further strengthened by the dense aggregation with data-driven weights. Experimental results demonstrate state-of-the-art restoration quality of both sharp features and smooth areas.
C1 [Xu, Zhongwei; Foi, Alessandro] Noiseless Imaging Oy Ltd, Tampere 33720, Finland.
   [Foi, Alessandro] Tampere Univ, Signal Proc, Tampere 33720, Finland.
C3 Tampere University
RP Xu, ZW (corresponding author), Noiseless Imaging Oy Ltd, Tampere 33720, Finland.
EM xu@noiselessimaging.com; alessandro.foi@tuni.fi
RI Foi, Alessandro/HKV-7567-2023; Foi, Alessandro/D-6010-2012
OI Xu, Zhongwei/0000-0002-2953-5602; Foi, Alessandro/0000-0001-8228-3187
FU European Union's Seventh Framework Programme (FP7-PEOPLE-2013-ITN)
   [607290 SpaRTaN]; Academy of Finland [310779]
FX The work was funded by the European Union's Seventh Framework Programme
   (FP7-PEOPLE-2013-ITN) under Grant Agreement No. 607290 SpaRTaN, and from
   the Academy of Finland under Project No. 310779.
NR 51
TC 19
Z9 20
U1 9
U2 59
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2021
VL 27
IS 6
BP 2851
EP 2868
DI 10.1109/TVCG.2019.2959761
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SA9KC
UT WOS:000649620700007
PM 31841412
DA 2025-03-07
ER

PT J
AU Espadoto, M
   Martins, RM
   Kerren, A
   Hirata, NST
   Telea, AC
AF Espadoto, Mateus
   Martins, Rafael M.
   Kerren, Andreas
   Hirata, Nina S. T.
   Telea, Alexandru C.
TI Toward a Quantitative Survey of Dimension Reduction Techniques
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Benchmark testing; Data visualization; Machine learning; Scalability;
   Extraterrestrial measurements; Taxonomy; Dimensionality reduction;
   quality metrics; benchmarking; quantitative analysis; design space
ID DATA VISUALIZATION; VISUAL ANALYSIS
AB Dimensionality reduction methods, also known as projections, are frequently used in multidimensional data exploration in machine learning, data science, and information visualization. Tens of such techniques have been proposed, aiming to address a wide set of requirements, such as ability to show the high-dimensional data structure, distance or neighborhood preservation, computational scalability, stability to data noise and/or outliers, and practical ease of use. However, it is far from clear for practitioners how to choose the best technique for a given use context. We present a survey of a wide body of projection techniques that helps answering this question. For this, we characterize the input data space, projection techniques, and the quality of projections, by several quantitative metrics. We sample these three spaces according to these metrics, aiming at good coverage with bounded effort. We describe our measurements and outline observed dependencies of the measured variables. Based on these results, we draw several conclusions that help comparing projection techniques, explain their results for different types of data, and ultimately help practitioners when choosing a projection for a given context. Our methodology, datasets, projection implementations, metrics, visualizations, and results are publicly open, so interested stakeholders can examine and/or extend this benchmark.
C1 [Espadoto, Mateus; Hirata, Nina S. T.] Univ Sao Paulo, Inst Math & Stat, BR-05508090 Sao Paulo, Brazil.
   [Martins, Rafael M.; Kerren, Andreas] Linnaeus Univ, Dept Comp Sci & Media Technol, S-35195 Vaxjo, Sweden.
   [Telea, Alexandru C.] Univ Utrecht, Dept Informat & Comp Sci, NL-3584 CS Utrecht, Netherlands.
C3 Universidade de Sao Paulo; Linnaeus University; Utrecht University
RP Espadoto, M (corresponding author), Univ Sao Paulo, Inst Math & Stat, BR-05508090 Sao Paulo, Brazil.
EM mespadot@ime.usp.br; rafael.martins@lnu.se; andreas.kerren@lnu.se;
   nina@ime.usp.br; a.c.telea@uu.nl
RI Kerren, Andreas/AAV-9187-2020; Martins, Rafael/H-9192-2019; Hirata,
   Nina/C-1491-2012
OI Hirata, Nina/0000-0001-9722-5764; Espadoto, Mateus/0000-0002-1922-4309
FU FAPESP [2017/25835-9]; CAPES
FX We are grateful for the help provided by prof. Luis Gustavo Nonato in
   discussing early versions of our paper. This study was financed in part
   by FAPESP (2017/25835-9) and CAPES.
NR 119
TC 166
Z9 179
U1 4
U2 61
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2021
VL 27
IS 3
BP 2153
EP 2173
DI 10.1109/TVCG.2019.2944182
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QA9EF
UT WOS:000613744500020
PM 31567092
HC Y
HP N
DA 2025-03-07
ER

PT J
AU Lyu, WT
   Ding, P
   Zhang, YL
   Chen, AP
   Wu, MY
   Yin, S
   Yu, JY
AF Lyu, Wentao
   Ding, Peng
   Zhang, Yingliang
   Chen, Anpei
   Wu, Minye
   Yin, Shu
   Yu, Jingyi
TI Refocusable Gigapixel Panoramas for Immersive VR Experiences
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Gigapixel panoramas; dynamic refocusing; memory management; I/O
   scheduling; virtual reality
ID DISPLAYS
AB There have been significant advances in capturing gigapixel panoramas (GPP). However, solutions for viewing GPPs on head-mounted displays (HMDs) are lagging: an immersive experience requires ultra-fast rendering while directly loading a GPP onto the GPU is infeasible due to limited texture memory capacity. In this paper, we present a novel out-of-core rendering technique that supports not only classic panning, tilting, and zooming but also dynamic refocusing for viewing a GPP on HMD. Inspired by the network package transmission mechanisms in distributed visualization, our approach employs hierarchical image tiling and on-demand data updates across the main and the GPU memory. We further present a multi-resolution rendering scheme and a refocused light field rendering technique based on RGBD GPPs with minimal memory overhead. Comprehensive experiments demonstrate that our technique is highly efficient and reliable, able to achieve ultra-high frame rates (>50 fps) even on low-end GPUs. With an embedded gaze tracker, our technique enables immersive panorama viewing experiences with unprecedented resolutions, field-of-view, and focus variations while maintaining smooth spatial, angular, and focal transitions.
C1 [Lyu, Wentao; Ding, Peng; Zhang, Yingliang; Chen, Anpei; Wu, Minye; Yu, Jingyi] ShanghaiTech Univ, VRVC Lab, Pudong 201210, Peoples R China.
   [Yin, Shu] ShanghaiTech Univ, LION Lab, Pudong 201210, Peoples R China.
C3 ShanghaiTech University; ShanghaiTech University
RP Zhang, YL (corresponding author), ShanghaiTech Univ, VRVC Lab, Pudong 201210, Peoples R China.
EM lvwt@shanghaitech.edu.cn; dingpeng@shanghaitech.edu.cn;
   zhangyl@shanghaitech.edu.cn; chenap@shanghaitech.edu.cn;
   wumy@shanghaitech.edu.cn; yinshu@shanghaitech.edu.cn;
   yujingyi@shanghaitech.edu.cn
RI Wu, Minye/AAT-6694-2021; Yin, Shu/AGO-5485-2022
OI Wu, Minye/0000-0002-8163-9513; Ding, Peng/0000-0001-8353-0821; Zhang,
   Yingliang/0000-0002-0594-7549
FU STCSM [17XD1402900, 17JC1403800, 17511108201, 17511105805,
   2015F0203-000-06]; SHMEC [2019-01-07-00-01-E00003]; SHEITC
   [2018-RGZN-01011]
FX This work is partially supported by the programs of STCSM (17XD1402900,
   17JC1403800, 17511108201, 17511105805, 2015F0203-000-06), of SHMEC
   (2019-01-07-00-01-E00003), and of SHEITC (2018-RGZN-01011).
NR 46
TC 0
Z9 0
U1 0
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2021
VL 27
IS 3
BP 2028
EP 2040
DI 10.1109/TVCG.2019.2940444
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QA9EF
UT WOS:000613744500012
PM 31514140
DA 2025-03-07
ER

PT J
AU Streeb, D
   El-Assady, M
   Keim, DA
   Chen, M
AF Streeb, Dirk
   El-Assady, Mennatallah
   Keim, Daniel A.
   Chen, Min
TI Why Visualize? Untangling a Large Network of Arguments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Task analysis; Pipelines; Biology;
   Bars; Manuals; Visualization; theory; argument network; cognition;
   design
AB Visualization has been deemed a useful technique by researchers and practitioners, alike, leaving a trail of arguments behind that reason why visualization works. In addition, examples of misleading usages of visualizations in information communication have occasionally been pointed out. Thus, to contribute to the fundamental understanding of our discipline, we require a comprehensive collection of arguments on "why visualize?" (or "why not?"), untangling the rationale behind positive and negative viewpoints. In this paper, we report a theoretical study to understand the underlying reasons of various arguments; their relationships (e.g., built-on, and conflict); and their respective dependencies on tasks, users, and data. We curated an argumentative network based on a collection of arguments from various fields, including information visualization, cognitive science, psychology, statistics, philosophy, and others. Our work proposes several categorizations for the arguments, and makes their relations explicit. We contribute the first comprehensive and systematic theoretical study of the arguments on visualization. Thereby, we provide a roadmap towards building a foundation for visualization theory and empirical research as well as for practical application in the critique and design of visualizations. In addition, we provide our argumentation network and argument collection online at https://whyvis.dbvis.de, supported by an interactive visualization.
C1 [Streeb, Dirk; El-Assady, Mennatallah; Keim, Daniel A.] Univ Konstanz, Grp Data Anal & Visualizat, D-78464 Constance, Germany.
   [Chen, Min] Univ Oxford, Dept Engn Sci, Oxford OX1 2JD, England.
C3 University of Konstanz; University of Oxford
RP Streeb, D (corresponding author), Univ Konstanz, Grp Data Anal & Visualizat, D-78464 Constance, Germany.
EM streeb@dbvis.inf.uni-konstanz.de; elassady@dbvis.inf.uni-konstanz.de;
   keim@dbvis.inf.uni-konstanz.de; min.chen@oerc.ox.ac.uk
RI Keim, Daniel/X-7749-2019
OI Chen, Min/0000-0001-5320-5729; El-Assady,
   Mennatallah/0000-0001-8526-2613
FU Graduate School of Decision Sciences at the University of Konstanz
FX The authors are grateful to Georges G. Grinstein, University of
   Massachusetts Amherst, for his comments and discussions. Part of this
   work was funded by the Graduate School of Decision Sciences at the
   University of Konstanz.
NR 85
TC 12
Z9 13
U1 0
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2021
VL 27
IS 3
BP 2220
EP 2236
DI 10.1109/TVCG.2019.2940026
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA QA9EF
UT WOS:000613744500024
PM 31514139
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Battle, L
   Scheidegger, C
AF Battle, Leilani
   Scheidegger, Carlos
TI A Structured Review of Data Management Technology for Interactive
   Visualization and Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Optimization; Encoding; Visual databases;
   Visualization; Task analysis
ID EXPLORATION; QUERY; CUBE; VEGA
AB In the last two decades, interactive visualization and analysis have become a central tool in data-driven decision making. Concurrently to the contributions in data visualization, research in data management has produced technology that directly benefits interactive analysis. Here, we contribute a systematic review of 30 years of work in this adjacent field, and highlight techniques and principles we believe to be underappreciated in visualization work. We structure our review along two axes. First, we use task taxonomies from the visualization literature to structure the space of interactions in usual systems. Second, we created a categorization of data management work that strikes a balance between specificity and generality. Concretely, we contribute a characterization of 131 research papers along these two axes. We find that five notions in data management venues fit interactive visualization systems well: materialized views, approximate query processing, user modeling and query prediction, muiti-query optimization, lineage techniques, and indexing techniques. In addition, we find a preponderance of work in materialized views and approximate query processing, most targeting a limited subset of the interaction tasks in the taxonomy we used. This suggests natural avenues of future research both in visualization and data management. Our categorization both changes how we visualization researchers design and build our systems, and highlights where future work is necessary.
C1 [Battle, Leilani] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
   [Scheidegger, Carlos] Univ Arizona, HDC Lab, Tucson, AZ 85721 USA.
C3 University System of Maryland; University of Maryland College Park;
   University of Arizona
RP Battle, L (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
EM leilani@cs.umd.edu; cscheid@email.arizona.edu
FU NSF [IIS-1815238, IIS-1850115]
FX This work was partially supported by the NSF awards IIS-1815238 and
   IIS-1850115.
NR 191
TC 12
Z9 14
U1 3
U2 34
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1128
EP 1138
DI 10.1109/TVCG.2020.3028891
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100096
PM 33031039
DA 2025-03-07
ER

PT J
AU Fischer, MT
   Arya, D
   Streeb, D
   Seebacher, D
   Keim, DA
   Worring, M
AF Fischer, Maximilian T.
   Arya, Devanshu
   Streeb, Dirk
   Seebacher, Daniel
   Keim, Daniel A.
   Worring, Marcel
TI Visual Analytics for Temporal Hypergraph Model Exploration
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Computational modeling; Machine learning; Predictive
   models; Analytical models; Semantics; Data models; Hypergraph;
   communication analysis; geometric deep learning; semantic zoom; matrix
   ordering; visual analytics
ID PROTEIN INTERACTIONS; TASK TAXONOMY; NETWORK; MATRICES
AB Many processes, from gene interaction in biology to computer networks to social media, can be modeled more precisely as temporal hypergraphs than by regular graphs. This is because hypergraphs generalize graphs by extending edges to connect any number of vertices, allowing complex relationships to be described more accurately and predict their behavior over time. However, the interactive exploration and seamless refinement of such hypergraph-based prediction models still pose a major challenge. We contribute Hyper-Matrix, a novel visual analytics technique that addresses this challenge through a tight coupling between machine-learning and interactive visualizations. In particular, the technique incorporates a geometric deep learning model as a blueprint for problem-specific models while integrating visualizations for graph-based and category-based data with a novel combination of interactions for an effective user-driven exploration of hypergraph models. To eliminate demanding context switches and ensure scalability, our matrix-based visualization provides drill-down capabilities across multiple levels of semantic zoom, from an overview of model predictions down to the content. We facilitate a focused analysis of relevant connections and groups based on interactive user-steering for filtering and search tasks, a dynamically modifiable partition hierarchy, various matrix reordering techniques, and interactive model feedback. We evaluate our technique in a case study and through formative evaluation with law enforcement experts using real-world internet forum communication data. The results show that our approach surpasses existing solutions in terms of scalability and applicability, enables the incorporation of domain knowledge, and allows for fast search-space traversal. With the proposed technique, we pave the way for the visual analytics of temporal hypergraphs in a wide variety of domains.
C1 [Fischer, Maximilian T.; Streeb, Dirk; Seebacher, Daniel; Keim, Daniel A.] Univ Konstanz, Constance, Germany.
   [Arya, Devanshu; Worring, Marcel] Univ Amsterdam, Amsterdam, Netherlands.
C3 University of Konstanz; University of Amsterdam
RP Fischer, MT (corresponding author), Univ Konstanz, Constance, Germany.
EM max.fischer@uva.nl; d.arya@uva.nl; dirk.streeb@uva.nl;
   daniel.seebacher@uva.nl; keim@uva.nl; m.worring@uva.nl
RI Worring, Marcel/JRW-7059-2023; Fischer, Maximilian/AAP-8820-2021; Keim,
   Daniel/X-7749-2019
OI Fischer, Maximilian/0000-0001-8076-1376; Seebacher,
   Daniel/0000-0003-0097-5855
FU European Union [700381]
FX This project has received funding from the European Union's Horizon 2020
   research and innovation programme under grant agreement No. 700381. This
   material reflects only the authors' views, and the Commission is not
   liable for any use that may be made of the information contained
   therein.
NR 46
TC 17
Z9 18
U1 0
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 550
EP 560
DI 10.1109/TVCG.2020.3030408
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA WF5FO
UT WOS:000706330100042
PM 33048721
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Friess, F
   Braun, M
   Bruder, V
   Frey, S
   Reina, G
   Ertl, T
AF Friess, Florian
   Braun, Matthias
   Bruder, Valentin
   Frey, Steffen
   Reina, Guido
   Ertl, Thomas
TI Foveated Encoding for Large High-Resolution Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Bandwidth; Encoding; Streaming media; Image coding;
   Hardware; Computational modeling; Large high-resolution displays;
   Fovetaed Encoding; Remote Visualisation
ID FRAMEWORK
AB Collaborative exploration of scientific data sets across large high-resolution displays requires both high visual detail as well as low-latency transfer of image data (oftentimes inducing the need to trade one for the other). In this work, we present a system that dynamically adapts the encoding quality in such systems in a way that reduces the required bandwidth without impacting the details perceived by one or more observers. Humans perceive sharp, colourful details, in the small foveal region around the centre of the field of view, while information in the periphery is perceived blurred and colourless. We account for this by tracking the gaze of observers, and respectively adapting the quality parameter of each macroblock used by the H.264 encoder, considering the so-called visual acuity fall-off. This allows to substantially reduce the required bandwidth with barely noticeable changes in visual quality, which is crucial for collaborative analysis across display walls at different locations. We demonstrate the reduced overall required bandwidth and the high quality inside the foveated regions using particle rendering and parallel coordinates.
C1 [Friess, Florian; Braun, Matthias; Bruder, Valentin; Reina, Guido; Ertl, Thomas] Univ Stuttgart, Stuttgart, Germany.
   [Frey, Steffen] Univ Groningen, Groningen, Netherlands.
C3 University of Stuttgart; University of Groningen
RP Friess, F (corresponding author), Univ Stuttgart, Stuttgart, Germany.
EM florian.friess@visus.uni-stuttgart.de;
   matthias.braun@visus.uni-stuttgart.de;
   valentin.bruder@visus.uni-stuttgart.de; s.d.frey@rug.nl;
   guido.reina@visus.uni-stuttgart.de; thomas.ertl@vis.uni-stuttgart.de
OI Frey, Steffen/0000-0002-1872-6905; Braun, Matthias/0000-0001-8591-3690;
   Friess, Florian/0000-0001-8817-0853
FU Deutsche Forschungsgemeinschaft (DFG) [SFB-TRR 161, 251654672]; Software
   Sustainability for the Open Source Particle Visualization Framework
   MegaMol [391302154]
FX ThisworkwaspartiallyfundedbyDeutscheForschungsgemeinschaft(DFG)
   aspartofSFB-TRR161(ProjectID251654672)
   aswellaspartofSoftwareSustainabilityfortheOpenSourceParticleVisualizatio
   nFrameworkMegaMol(ProjectID391302154).
NR 35
TC 6
Z9 7
U1 1
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1850
EP 1859
DI 10.1109/TVCG.2020.3030445
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100160
PM 33052862
OA Green Published
DA 2025-03-07
ER

PT J
AU Karduni, A
   Markant, D
   Wesslen, R
   Dou, WW
AF Karduni, Alireza
   Markant, Douglas
   Wesslen, Ryan
   Dou, Wenwen
TI A Bayesian cognition approach for belief updating of correlation
   judgement through uncertainty visualizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Terms Information visualization; Bayesian modeling; uncertainty
   visualizations; correlations; belief elicitation
ID RANKING VISUALIZATIONS; BIAS; PERCEPTION; FRAMEWORK; MODELS
AB Understanding correlation judgement is important to designing effective visualizations of bivariate data. Prior work on correlation perception has not considered how factors including prior beliefs and uncertainty representation impact such judgements. The present work focuses on the impact of uncertainty communication when judging bivariate visualizations. Specifically, we model how users update their beliefs about variable relationships after seeing a scatterplot with and without uncertainty representation. To model and evaluate the belief updating, we present three studies. Study 1 focuses on a proposed "Line + Cone" visual elicitation method for capturing users' beliefs in an accurate and intuitive fashion. The findings reveal that our proposed method of belief solicitation reduces complexity and accurately captures the users' uncertainty about a range of bivariate relationships. Study 2 leverages the "Line + Cone" elicitation method to measure belief updating on the relationship between different sets of variables when seeing correlation visualization with and without uncertainty representation. We compare changes in users beliefs to the predictions of Bayesian cognitive models which provide normative benchmarks for how users should update their prior beliefs about a relationship in light of observed data. The findings from Study 2 revealed that one of the visualization conditions with uncertainty communication led to users being slightly more confident about their judgement compared to visualization without uncertainty information. Study 3 builds on findings from Study 2 and explores differences in belief update when the bivariate visualization is congruent or incongruent with users' prior belief. Our results highlight the effects of incorporating uncertainty representation, and the potential of measuring belief updating on correlation judgement with Bayesian cognitive models.
C1 [Karduni, Alireza; Markant, Douglas; Wesslen, Ryan; Dou, Wenwen] Univ N Carolina, Charlotte, NC 28223 USA.
C3 University of North Carolina; University of North Carolina Charlotte
RP Karduni, A (corresponding author), Univ N Carolina, Charlotte, NC 28223 USA.
EM akarduni@uncc.edu; dmarkant@uncc.edu; rwesslen@uncc.edu; wdou1@uncc.edu
OI Karduni, Alireza/0000-0001-9719-7513
NR 59
TC 14
Z9 14
U1 1
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 978
EP 988
DI 10.1109/TVCG.2020.3029412
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100082
PM 33031041
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Sabando, MV
   Ulbrich, P
   Selzer, M
   Byska, J
   Mican, J
   Ponzoni, I
   Soto, AJ
   Ganuza, ML
   Kozlíková, B
AF Sabando, Maria Virginia
   Ulbrich, Pavol
   Selzer, Matias
   Byska, Jan
   Mican, Jan
   Ponzoni, Ignacio
   Soto, Axel J.
   Ganuza, Maria Lujan
   Kozlikova, Barbora
TI ChemVA: Interactive Visual Analysis of Chemical Compound Similarity in
   Virtual Screening
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Tools; Compounds; Visualization; Two dimensional displays; Drugs;
   Three-dimensional displays; Chemicals; Virtual screening; visual
   analysis; dimensionality reduction; coordinated views; cheminformatics
ID DIMENSIONALITY REDUCTION; VISUALIZATION; SCATTERPLOT
AB In the modern drug discovery process, medicinal chemists deal with the complexity of analysis of large ensembles of candidate molecules. Computational tools, such as dimensionality reduction (DR) and classification, are commonly used to efficiently process the multidimensional space of features. These underlying calculations often hinder interpretability of results and prevent experts from assessing the impact of individual molecular features on the resulting representations. To provide a solution for scrutinizing such complex data, we introduce ChemVA, an interactive application for the visual exploration of large molecular ensembles and their features. Our tool consists of multiple coordinated views: Hexagonal view, Detail view, 3D view, Table view, and a newly proposed Difference view designed for the comparison of DR projections. These views display DR projections combined with biological activity, selected molecular features, and confidence scores for each of these projections. This conjunction of views allows the user to drill down through the dataset and to efficiently select candidate compounds. Our approach was evaluated on two case studies of finding structurally similar ligands with similar binding affinity to a target protein, as well as on an external qualitative evaluation. The results suggest that our system allows effective visual inspection and comparison of different high-dimensional molecular representations. Furthermore, ChemVA assists in the identification of candidate compounds while providing information on the certainty behind different molecular representations.
C1 [Sabando, Maria Virginia; Ponzoni, Ignacio; Soto, Axel J.; Ganuza, Maria Lujan] UNS, CONICET, Inst Comp Sci & Engn, Bahia Blanca, Buenos Aires, Argentina.
   [Sabando, Maria Virginia; Ponzoni, Ignacio; Soto, Axel J.] Univ Nacl Sur, Dept Comp Sci & Engn, Bahia Blanca, Buenos Aires, Argentina.
   [Ulbrich, Pavol; Byska, Jan; Kozlikova, Barbora] Masaryk Univ, Fac Informat, Visitlab, Brno, Czech Republic.
   [Selzer, Matias; Ganuza, Maria Lujan] Univ Nacl, Dept Comp Sci & Engn, VyGLab Res Lab UNS CICPBA, Bahia Blanca, Buenos Aires, Argentina.
   [Mican, Jan] Masaryk Univ, Dept Expt Biol & RECETOX, Loschmidt Labs, Brno, Czech Republic.
   [Mican, Jan] Masaryk Univ, Fac Med, Brno, Czech Republic.
C3 Consejo Nacional de Investigaciones Cientificas y Tecnicas (CONICET);
   National University of the South; National University of the South;
   Instituto de Investigaciones en Ingenieria Electrica (IIIE); Masaryk
   University Brno; Masaryk University Brno; Masaryk University Brno
RP Sabando, MV (corresponding author), UNS, CONICET, Inst Comp Sci & Engn, Bahia Blanca, Buenos Aires, Argentina.; Sabando, MV (corresponding author), Univ Nacl Sur, Dept Comp Sci & Engn, Bahia Blanca, Buenos Aires, Argentina.
EM virginia.sabando@cs.uns.edu.ar; paloulbrich@gmail.com;
   matias.selzer@cs.uns.edu.ar; jan.byska@gmail.com; honzamicann@gmail.com;
   ip@cs.uns.edu.ar; axel.soto@cs.uns.edu.ar; mlg@cs.uns.edu.ar;
   kozlikova@fi.muni.cz
RI Ponzoni, Ignacio/A-7149-2008; Kozlikova, Barbora/G-3890-2014; Soto,
   Axel/JHU-5618-2023; Mičan, Jan/GYJ-6476-2022
OI Ponzoni, Ignacio/0000-0002-6923-9592; Soto, Axel/0000-0002-9021-7566;
   Ulbrich, Pavol/0000-0003-1661-7905; Ganuza, Maria
   Lujan/0000-0003-4576-2124
FU DFG-GACR [GC18-18647J]; CONICET [PIP 112-20170100829]; UNS [PGI 24/N042,
   PGI 24/N048]; ANPCyT (Argentina) [PICT-2017-1246]
FX The presented work has been supported by DFG-GACR research project no.
   GC18-18647J, by CONICET research grant PIP 112-20170100829, by UNS
   research grants PGI 24/N042 and PGI 24/N048, and by ANPCyT (Argentina)
   research grant PICT-2017-1246. We thank Sergio M. Marques and Ondrej
   Vavra from the Loschmidt Laboratories, and Michael Aupetit from the
   Qatar Computing Research Institute QCRI for evaluating ChemVA and for
   their insightful comments and suggestions on the features of the tool.
NR 79
TC 7
Z9 7
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 891
EP 901
DI 10.1109/TVCG.2020.3030438
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100074
PM 33048734
OA Green Published, Green Submitted
DA 2025-03-07
ER

PT J
AU So, W
   Bogucka, EP
   Scepanovic, S
   Joglekar, S
   Zhou, K
   Quercia, D
AF So, Wonyoung
   Bogucka, Edyta P.
   Scepanovic, Sanja
   Joglekar, Sagar
   Zhou, Ke
   Quercia, Daniele
TI Humane Visual AI: Telling the Stories Behind a Medical Condition
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE complex problem communication; storytelling; AI; social media data;
   healthcare; Martini Glass structure
ID NARRATIVE VISUALIZATION; BIOPSYCHOSOCIAL MODEL; HEALTH; EMOTIONS; POWER
AB A biological understanding is key for managing medical conditions, yet psychological and social aspects matter too. The main problem is that these two aspects are hard to quantify and inherently difficult to communicate. To quantify psychological aspects, this work mined around half a million Reddit posts in the sub -communities specialised in 14 medical conditions, and it did so with a new deep -learning framework. In so doing, it was able to associate mentions of medical conditions with those of emotions. To then quantify social aspects, this work designed a probabilistic approach that mines open prescription data from the National Health Service in England to compute the prevalence of drug prescriptions, and to relate such a prevalence to census data. To finally visually communicate each medical condition's biological, psychological, and social aspects through storytelling, we designed a narrative -style layered Martini Glass visualization. In a user study involving 52 participants, after interacting with our visualization, a considerable number of them changed their mind on previously held opinions: 10% gave more importance to the psychological aspects of medical conditions, and 27% were more favourable to the use of social media data in healthcare, suggesting the importance of persuasive elements in interactive visualizations.
C1 [So, Wonyoung] MIT, Cambridge, MA 02139 USA.
   [Bogucka, Edyta P.] Tech Univ Munich, Chair Cartog, Munich, Germany.
   [Scepanovic, Sanja; Joglekar, Sagar; Zhou, Ke; Quercia, Daniele] Nokia Bell Labs, Cambridge, England.
   [Quercia, Daniele] CUSP Kings Coll London, London, England.
C3 Massachusetts Institute of Technology (MIT); Technical University of
   Munich; Nokia Corporation; Nokia United Kingdom
RP So, W (corresponding author), MIT, Cambridge, MA 02139 USA.
EM wso@mit.edu; e.p.bogucka@tum.de; sanja.scepanovic@nokia-bell-labs.com;
   sagar.joglekar@nokia-bell-labs.com; ke.zhou@nokia-bell-labs.com;
   daniele.quercia@nokia-bell-labs.com
RI Zhou, Ke/B-6745-2014; So, Wonyoung/KBB-9680-2024; Scepanovic,
   Sanja/AAB-1247-2021
OI scepanovic, sanja/0000-0002-1534-8128; Quercia,
   Daniele/0000-0001-9461-5804; So, Wonyoung/0000-0002-4867-3429
NR 64
TC 2
Z9 2
U1 5
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 678
EP 688
DI 10.1109/TVCG.2020.3030391
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA WF5FO
UT WOS:000706330100054
PM 33048711
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Weib, M
   Angerbauer, K
   Voit, A
   Schwarzl, M
   Sedlmair, M
   Mayer, S
AF Weib, Maximilian
   Angerbauer, Katrin
   Voit, Alexandra
   Schwarzl, Magdalena
   Sedlmair, Michael
   Mayer, Sven
TI Revisited: Comparison of Empirical Methods to Evaluate Visualizations
   Supporting Crafting and Assembly Purposes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Situated visualization; evaluation; comparison
ID RANKING VISUALIZATIONS; DESIGN; SCIENCE
AB Ubiquitous, situated, and physical visualizations create entirely new possibilities for tasks contextualized in the real world, such as doctors inserting needles. During the development of situated visualizations, evaluating visualizations is a core requirement. However, performing such evaluations is intrinsically hard as the real scenarios are safety-critical or expensive to test. To overcome these issues, researchers and practitioners adapt classical approaches from ubiquitous computing and use surrogate empirical methods such as Augmented Reality (AR), Virtual Reality (VR) prototypes, or merely online demonstrations. This approach's primary assumption is that meaningful insights can also be gained from different, usually cheaper and less cumbersome empirical methods. Nevertheless, recent efforts in the Human-Computer Interaction (HCI) community have found evidence against this assumption, which would impede the use of surrogate empirical methods. Currently, these insights rely on a single investigation of four interactive objects. The goal of this work is to investigate if these prior findings also hold for situated visualizations. Therefore, we first created a scenario where situated visualizations support users in do-it-yourself (DIY) tasks such as crafting and assembly. We then set up five empirical study methods to evaluate the four tasks using an online survey, as well as VR, AR, laboratory, and in-situ studies. Using this study design, we conducted a new study with 60 participants. Our results show that the situated visualizations we investigated in this study are not prone to the same dependency on the empirical method, as found in previous work. Our study provides the first evidence that analyzing situated visualizations through different empirical (surrogate) methods might lead to comparable results.
C1 [Weib, Maximilian; Angerbauer, Katrin; Schwarzl, Magdalena; Sedlmair, Michael] Univ Stuttgart, Stuttgart, Germany.
   [Voit, Alexandra] Adesso SE, Dortmund, Germany.
   [Mayer, Sven] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
C3 University of Stuttgart; Carnegie Mellon University
RP Weib, M (corresponding author), Univ Stuttgart, Stuttgart, Germany.
EM maximilianlweiss@gmail.com; Katrin.Angerbauer@visus.uni-stuttgart.de;
   alexandra.voit@adesso.de; Magdalena.Schwarzl@visus.uni-stuttgart.de;
   Michael.Sedlmair@visus.uni-stuttgart.de; info@sven-mayer.com
RI Mayer, Sven/A-5174-2019
OI Mayer, Sven/0000-0001-5462-8782
NR 67
TC 8
Z9 9
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1204
EP 1213
DI 10.1109/TVCG.2020.3030400
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA WF5FO
UT WOS:000706330100103
PM 33055033
DA 2025-03-07
ER

PT J
AU Yousef, T
   Jänicke, S
AF Yousef, Tariq
   Jaenicke, Stefan
TI A Survey of Text Alignment Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Data visualization; Visualization; Heuristic algorithms;
   Tools; Bioinformatics; Dynamic programming; Text Alignment; Text
   Visualization; Collation; Text Re-Use; Plagiarism Analysis; Translation
   Studies
ID MULTIPLE SEQUENCE ALIGNMENT
AB Text alignment is one of the fundamental techniques text-related domains like natural language processing, computational linguistics, and digital humanities. It compares two or more texts with each other aiming to find similar textual patterns, or to estimate in general how different or similar the texts are. Visualizing alignment results is an essential task, because it helps researchers getting a comprehensive overview of individual findings and the overall pattern structure. Different approaches have been developed to visualize and help making sense of these patterns depending on text size, alignment methods, and, most importantly, the underlying research tasks demanding for alignment. On the basis of those tasks, we reviewed existing text alignment visualization approaches, and discuss their advantages and drawbacks. We finally derive design implications and shed light on related future challenges.
C1 [Yousef, Tariq] Univ Leipzig, Leipzig, Germany.
   [Jaenicke, Stefan] Univ Southern Denmark, Odense, Denmark.
C3 Leipzig University; University of Southern Denmark
RP Yousef, T (corresponding author), Univ Leipzig, Leipzig, Germany.
EM tariq.yousef@uni-leipzig.de; stjaenicke@imada.sdu.dk
RI Yousef, Tariq/LYO-5308-2024
OI Yousef, Tariq/0000-0001-6136-3970
NR 98
TC 21
Z9 23
U1 2
U2 32
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1149
EP 1159
DI 10.1109/TVCG.2020.3028975
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100098
PM 33044932
OA Green Published
DA 2025-03-07
ER

PT J
AU Ma, GX
   Li, S
   Chen, CLZ
   Hao, AM
   Qin, H
AF Ma, Guangxiao
   Li, Shuai
   Chen, Chenglizhao
   Hao, Aimin
   Qin, Hong
TI Stage-wise Salient Object Detection in 360° Omnidirectional Image via
   Object-level Semantical Saliency Ranking
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 19th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY NOV 09-13, 2020
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, IEEE VGTC, ACM SIGGRAPH, VirBELA, Immers Learning Res Network, Liferay, React Real, Qualcomm
DE Virtual Reality; 360 Omnidirectional Image; Multi-stage Salient Object
   Detection; Semantical Saliency
ID MEMORY
AB The 2D image based salient object detection (SOD) has been extensively explored, while the 360 omnidirectional image based SOD has received less research attention and there exist three major bottlenecks that are limiting its performance. Firstly, the currently available training data is insufficient for the training of 360 SOD deep model. Secondly, the visual distortions in 360 omnidirectional images usually result in large feature gap between 360 images and 2D images; consequently, the widely used stage-wise training a widely-used solution to alleviate the training data shortage problem, becomes infeasible when conducing SOD in 360 omnidirectional images. Thirdly, the existing 360 SOD approach has followed a multi-task methodology that performs salient object localization and segmentation-like saliency refinement at the same time, being faced with extremely large problem domain, making the training data shortage dilemma even worse. To tackle all these issues, this paper divides the 360 SOD into a multi-stage task, the key rationale of which is to decompose the original complex problem domain into sequential easy sub problems that only demand for small-scale training data. Meanwhile, we learn how to rank the "object-level semantical saliency", aiming to locate salient viewpoints and objects accurately. Specifically, to alleviate the training data shortage problem, we have released a novel dataset named 360-SSOD, containing 1,105 360 omnidirectional images with manually annotated object-level saliency ground truth, whose semantical distribution is more balanced than that of the existing dataset. Also, we have compared the proposed method with 13 SOTA methods, and all quantitative results have demonstrated the performance superiority.
C1 [Ma, Guangxiao] Beihang Univ, Qingdao Res Inst, Beijing, Peoples R China.
   [Li, Shuai; Hao, Aimin] Beihang Univ, Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Li, Shuai; Hao, Aimin] Peng Cheng Lab, Shenzhen, Peoples R China.
   [Chen, Chenglizhao] Qingdao Univ, Qingdao, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Beihang University; Beihang University; Peng Cheng Laboratory; Qingdao
   University; State University of New York (SUNY) System; Stony Brook
   University
RP Chen, CLZ (corresponding author), Qingdao Univ, Qingdao, Peoples R China.
EM mgx@buaa.edu.cn; lishuai@buaa.edu.cn; cclz123@163.com; ham@buaa.edu.cn;
   qin@cs.stonybrook.edu
RI Zhao, Mingyu/HHS-0141-2022
FU National Key RAMP;D Program of China [2017YFF0106407]; National Natural
   Science Foundation of China [61802215, 61806106]; Natural Science
   Foundation of Shandong Province [ZR201807120086]; National Science
   Foundation of USA [IIS-1715985, IIS0949467, IIS-1047715, IIS-1049448]
FX This research was supported in part by National Key R&D Program of China
   (No. 2017YFF0106407), National Natural Science Foundation of China (No.
   61802215 and No. 61806106), Natural Science Foundation of Shandong
   Province (No. ZR201807120086) and National Science Foundation of USA
   (No. IIS-1715985, IIS0949467, IIS-1047715, and IIS-1049448).
NR 55
TC 41
Z9 42
U1 3
U2 35
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2020
VL 26
IS 12
BP 3535
EP 3545
DI 10.1109/TVCG.2020.3023636
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA OR1DW
UT WOS:000589217900016
PM 32941153
DA 2025-03-07
ER

PT J
AU Mahmood, S
   Mueller, K
AF Mahmood, Salman
   Mueller, Klaus
TI Taxonomizer: Interactive Construction of Fully Labeled Hierarchical
   Groupings from Attributes of Multivariate Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Taxonomy; Semantics; Tools; Task analysis; Aerospace electronics;
   Correlation; Labeling; High-dimensional data; data fusion and
   integration; hierarchy data; taxonomy; neural embeddings; lexical
   databases
AB Organizing multivariate data spaces by their dimensions or attributes can be a rather difficult task. Most of the work in this area focuses on the statistical aspects such as correlation clustering, dimension reduction, and the like. These methods typically produce hierarchies in which the leaf nodes are labeled by the attribute names while the inner nodes are often represented by just a statistical measure and criterion, such as a threshold. This makes them difficult to understand for mainstream users. Taxonomies in science, biology, engineering, etc. on the other hand, are easy to comprehend since they provide meaningful labels at the inner nodes as well. Labeling inner nodes of taxonomies automatically requires the identification of hypernyms. Our proposed framework, called Taxonomizer, takes a visual analytics approach to meet this challenge. It appeals to the wisdom of humans to liaise with state of the art data analytics, neural word embeddings, and lexical databases. It consists of a set of visual tools that starts out with an automatically computed hierarchy where the leaf nodes are the original data attributes, and it then allows users to sculpt high-quality taxonomies for any multivariate dataset.
C1 [Mahmood, Salman; Mueller, Klaus] SUNY Stony Brook, Dept Comp Sci, Visual Analyt & Imaging Lab, Stony Brook, NY 11794 USA.
   [Mahmood, Salman; Mueller, Klaus] SUNY Korea, Incheon 21985, South Korea.
C3 State University of New York (SUNY) System; Stony Brook University
RP Mahmood, S (corresponding author), SUNY Stony Brook, Dept Comp Sci, Visual Analyt & Imaging Lab, Stony Brook, NY 11794 USA.
EM samahmood@cs.stonybrook.edu; mueller@cs.stonybrook.edu
OI Mueller, Klaus/0000-0002-0996-8590
FU US National Science Foundation [IIS 1527200]; Ministry of Science, ICT
   and Future Planning, Korea, under the "IT Consilience Creative Program
   (ITCCP)"
FX This work was supported in part by US National Science Foundation grant
   IIS 1527200 and the Ministry of Science, ICT and Future Planning, Korea,
   under the "IT Consilience Creative Program (ITCCP)" supervised by NIPA.
   We would also like to that Prof. Jeffrey Heinz from the Stony Brook
   University Department of Linguistics for his advice on the final user
   study.
NR 52
TC 5
Z9 6
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2020
VL 26
IS 9
BP 2875
EP 2890
DI 10.1109/TVCG.2019.2895642
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MS7LW
UT WOS:000554457900011
PM 30735999
OA Bronze
DA 2025-03-07
ER

PT J
AU Chen, SM
   Li, J
   Andrienko, G
   Andrienko, N
   Wang, Y
   Nguyen, PH
   Turkay, C
AF Chen, Siming
   Li, Jie
   Andrienko, Gennady
   Andrienko, Natalia
   Wang, Yun
   Nguyen, Phong H.
   Turkay, Cagatay
TI Supporting Story Synthesis: Bridging the Gap between Visual Analytics
   and Storytelling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual analytics; Rivers; Bridges; Social network services; Tools;
   Hospitals; Story synthesis; visual analytics; social media;
   spatio-temporal data
ID VISUALIZATION; EXPLORATION; SENSEMAKING; PROVENANCE; MOVEMENT; MODEL
AB Visual analytics usually deals with complex data and uses sophisticated algorithmic, visual, and interactive techniques supporting the analysis. Findings and results of the analysis often need to be communicated to an audience that lacks visual analytics expertise. This requires analysis outcomes to be presented in simpler ways than that are typically used in visual analytics systems. However, not only analytical visualizations may be too complex for target audiences but also the information that needs to be presented. Analysis results may consist of multiple components, which may involve multiple heterogeneous facets. Hence, there exists a gap on the path from obtaining analysis findings to communicating them, within which two main challenges lie: information complexity and display complexity. We address this problem by proposing a general framework where data analysis and result presentation are linked by story synthesis, in which the analyst creates and organises story contents. Unlike previous research, where analytic findings are represented by stored display states, we treat findings as data constructs. We focus on selecting, assembling and organizing findings for further presentation rather than on tracking analysis history and enabling dual (i.e., explorative and communicative) use of data displays. In story synthesis, findings are selected, assembled, and arranged in meaningful layouts that take into account the structure of information and inherent properties of its components. We propose a workflow for applying the proposed conceptual framework in designing visual analytics systems and demonstrate the generality of the approach by applying it to two diverse domains, social media and movement analysis.
C1 [Chen, Siming; Andrienko, Gennady; Andrienko, Natalia] Fraunhofer Inst IAIS, D-53757 St Augustin, Germany.
   [Chen, Siming] Univ Bonn, D-53113 Bonn, Germany.
   [Li, Jie] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China.
   [Andrienko, Gennady; Andrienko, Natalia; Nguyen, Phong H.; Turkay, Cagatay] City Univ London, London EC1V 0HB, England.
   [Wang, Yun] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Fraunhofer Gesellschaft; Fraunhofer Germany; University of Bonn; Tianjin
   University; City St Georges, University of London; City, University of
   London; Microsoft; Microsoft China; Microsoft Research Asia
RP Li, J (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China.
EM siming.chen@iais.fraunhofer.de; jie.li@tju.edu.cn;
   gennady.andrienko@iais.fraunhofer.de;
   natalia.andrienko@iais.fraunhofer.de; wangyun@microsoft.com;
   Phong.Nguyen.3@city.ac.uk; Cagatay.Turkay.1@city.ac.uk
RI Chen, Siming/AAK-1874-2020; Nguyen, Phong H/HHN-2723-2022; Turkay,
   Cagatay/AAA-3810-2020; Andrienko, Natalia/KHV-4755-2024; Andrienko,
   Gennady/B-6486-2014; Li, Jie/X-4832-2018
OI Andrienko, Gennady/0000-0002-8574-6295; Li, Jie/0000-0001-6511-4090;
   Andrienko, Natalia/0000-0003-3313-1560; Turkay,
   Cagatay/0000-0001-6788-251X
FU Fraunhofer Cluster of Excellence on "Cognitive Internet Technologies";
   EU; DFG (German Research Foundation) [SPP 1894]; National Natural
   Science Foundation of China [61602340, 61572348]; National Key RAMP;D
   Program of China [2018YFC0809800]
FX This research was supported by Fraunhofer Cluster of Excellence on
   "Cognitive Internet Technologies", by EU in projects DiSIEM and
   SoBigData, by DFG (German Research Foundation) in priority research
   program SPP 1894 "Volunteered Geographic Information: Interpretation,
   Visualization and Social Computing". The work is partially supported by
   National Natural Science Foundation of China (61602340, 61572348), by
   National Key R&D Program of China (2018YFC0809800). The corresponding
   author is J. Li.
NR 67
TC 74
Z9 79
U1 5
U2 48
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2020
VL 26
IS 7
BP 2499
EP 2516
DI 10.1109/TVCG.2018.2889054
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA MB9QV
UT WOS:000542933100013
PM 30582542
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Strauss, RR
   Ramanujan, R
   Becker, A
   Peck, TC
AF Strauss, Ryan R.
   Ramanujan, Raghuram
   Becker, Andrew
   Peck, Tabitha C.
TI A Steering Algorithm for Redirected Walking Using Reinforcement Learning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 27th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 22-26, 2020
CL Atlanta, GA
SP IEEE, IEEE Comp Soc
DE Legged locomotion; Learning (artificial intelligence); Prediction
   algorithms; Meters; Tracking; Heuristic algorithms; Space exploration;
   Virtual Reality; Locomotion; Redirected Walking; Steering Algorithms;
   Reinforcement Learning
ID GO
AB Redirected Walking (RDW) steering algorithms have traditionally relied on human-engineered logic. However, recent advances in reinforcement learning (RL) have produced systems that surpass human performance on a variety of control tasks. This paper investigates the potential of using RL to develop a novel reactive steering algorithm for RDW. Our approach uses RL to train a deep neural network that directly prescribes the rotation, translation, and curvature gains to transform a virtual environment given a user's position and orientation in the tracked space. We compare our learned algorithm to steer-to-center using simulated and real paths. We found that our algorithm outperforms steer-to-center on simulated paths, and found no significant difference on distance traveled on real paths. We demonstrate that when modeled as a continuous control problem, RDW is a suitable domain for RL, and moving forward, our general framework provides a promising path towards an optimal RDW steering algorithm.
C1 [Strauss, Ryan R.; Ramanujan, Raghuram; Becker, Andrew; Peck, Tabitha C.] Davidson Coll, Davidson, NC 28036 USA.
C3 Davidson College
RP Strauss, RR (corresponding author), Davidson Coll, Davidson, NC 28036 USA.
EM rystrauss@davidson.edu; raramanujan@davidson.edu; tapeck@davidson.edu
RI Peck, Tabitha/AAH-2032-2021
OI Strauss, Ryan/0000-0001-5179-665X
FU Davidson Research Initiative
FX The authors wish to thank the Davidson Research Initiative for
   supporting this work.
NR 55
TC 58
Z9 61
U1 3
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2020
VL 26
IS 5
BP 1955
EP 1963
DI 10.1109/TVCG.2020.2973060
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA LA1WU
UT WOS:000523746000014
PM 32078549
DA 2025-03-07
ER

PT J
AU Borland, D
   Wang, WY
   Zhang, J
   Shrestha, J
   Gotz, D
AF Borland, David
   Wang, Wenyuan
   Zhang, Jonathan
   Shrestha, Joshua
   Gotz, David
TI Selection Bias Tracking and Detailed Subset Comparison for
   High-Dimensional Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE High-dimensional visualization; visual analytics; cohort selection;
   medical informatics; selection bias
ID VISUAL ANALYTICS; ADJUST; VISUALIZATION
AB The collection of large, complex datasets has become common across a wide variety of domains. Visual analytics tools increasingly play a key role in exploring and answering complex questions about these large datasets. However, many visualizations are not designed to concurrently visualize the large number of dimensions present in complex datasets (e.g. tens of thousands of distinct codes in an electronic health record system). This fact, combined with the ability of many visual analytics systems to enable rapid, ad-hoc specification of groups, or cohorts, of individuals based on a small subset of visualized dimensions, leads to the possibility of introducing selection when the user creates a cohort based on a specified set of dimensions, differences across many other unseen dimensions may also be introduced. These unintended side effects may result in the cohort no longer being representative of the larger population intended to be studied, which can negatively affect the validity of subsequent analyses. We present techniques for selection bias tracking and visualization that can be incorporated into high-dimensional exploratory visual analytics systems, with a focus on medical data with existing data hierarchies. These techniques include: (1) tree-based cohort provenance and visualization, including a user-specified baseline cohort that all other cohorts are compared against, and visual encoding of cohort, which indicates where selection bias may have occurred, and (2) a set of visualizations, including a novel icicle-plot based visualization, to compare in detail the per-dimension differences between the baseline and a user-specified focus cohort. These techniques are integrated into a medical temporal event sequence visual analytics tool. We present example use cases and report findings from domain expert user interviews.
C1 [Borland, David] Univ N Carolina, RENCI, Chapel Hill, NC 27515 USA.
   [Wang, Wenyuan; Gotz, David] Univ N Carolina, Sch Informat & Lib Sci, Chapel Hill, NC 27515 USA.
   [Zhang, Jonathan] Univ N Carolina, Dept Biostat, Chapel Hill, NC 27515 USA.
   [Shrestha, Joshua] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA.
C3 University of North Carolina; University of North Carolina Chapel Hill;
   University of North Carolina; University of North Carolina Chapel Hill;
   University of North Carolina; University of North Carolina Chapel Hill;
   University of North Carolina; University of North Carolina Chapel Hill
RP Borland, D (corresponding author), Univ N Carolina, RENCI, Chapel Hill, NC 27515 USA.
EM borland@renci.org; vaapad@live.unc.edu; jzhang42@live.unc.edu;
   prayash@live.unc.edu; gotz@unc.edu
OI Borland, David/0000-0002-0162-4080
FU National Science Foundation [1704018]; Div Of Information & Intelligent
   Systems; Direct For Computer & Info Scie & Enginr [1704018] Funding
   Source: National Science Foundation
FX The research reported in this article was supported in part by a grant
   from the National Science Foundation (#1704018).
NR 56
TC 14
Z9 14
U1 1
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 429
EP 439
DI 10.1109/TVCG.2019.2934209
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100040
PM 31442975
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Hu, RZ
   Sha, TK
   van Kaick, O
   Deussen, O
   Huang, H
AF Hu, Ruizhen
   Sha, Tingkai
   van Kaick, Oliver
   Deussen, Oliver
   Huang, Hui
TI Data Sampling in Multi-view and Multi-class Scatterplots via Set Cover
   Optimization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Sampling; Scatterplot; SPLOM; Exact Cover Problem
ID QUALITY METRICS; VISUALIZATION; REDUCTION
AB We present a method for data sampling in scatterplots by jointly optimizing point selection for different views or classes. Our method uses space-filling curves (Z-order curves) that partition a point set into subsets that, when covered each by one sample, provide a sampling or coreset with good approximation guarantees in relation to the original point set. For scatterplot matrices with multiple views, different views provide different space-filling curves, leading to different partitions of the given point set. For multi-class scatterplots, the focus on either per-class distribution or global distribution provides two different partitions of the given point set that need to be considered in the selection of the coreset. For both cases, we convert the coreset selection problem into an Exact Cover Problem (ECP), and demonstrate with quantitative and qualitative evaluations that an approximate solution that solves the ECP efficiently is able to provide high-quality samplings.
C1 [Hu, Ruizhen; Sha, Tingkai; Huang, Hui] Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Guangdong, Peoples R China.
   [van Kaick, Oliver] Carleton Univ, Sch Comp Sci, Ottawa, ON, Canada.
   [Deussen, Oliver] Konstanz Univ, Constance, Germany.
   [Deussen, Oliver] SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Guangdong, Peoples R China.
C3 Shenzhen University; Carleton University; University of Konstanz;
   Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS
RP Hu, RZ (corresponding author), Shenzhen Univ, Visual Comp Res Ctr, Shenzhen, Guangdong, Peoples R China.
EM ruizhen.hu@gmail.com; shatingkai@gmail.com; ovankaic@gmail.com;
   oliver.deussen@uni-konstanz.de; hhzhiyan@gmail.com
RI Deussen, Oliver/HKF-2004-2023; Huang, Hui/JGB-1049-2023
OI Huang, Hui/0000-0003-3212-0544
FU NSFC [61872250, 61602311]; GD Science and Technology Program
   [2015A030312015]; GD Leading Talents Program [00201509]; Shenzhen
   Innovation Program [JCYJ20170302153208613, KQJSCX20170727101233642];
   LHTD [20170003]; NSERC [2015-05407]; DFG [422037984]; National
   Engineering Laboratory for Big Data System Computing Technology
FX We thank the reviewers for their valuable comments. This work was
   supported in parts by NSFC (61872250, 61602311), GD Science and
   Technology Program (2015A030312015), GD Leading Talents Program
   (00201509), Shenzhen Innovation Program (JCYJ20170302153208613,
   KQJSCX20170727101233642), LHTD (20170003), NSERC (2015-05407), DFG
   (422037984), and the National Engineering Laboratory for Big Data System
   Computing Technology.
NR 43
TC 17
Z9 19
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 739
EP 748
DI 10.1109/TVCG.2019.2934799
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100068
PM 31443021
DA 2025-03-07
ER

PT J
AU Mumtaz, H
   Latif, S
   Beck, F
   Weiskopf, D
AF Mumtaz, Haris
   Latif, Shahid
   Beck, Fabian
   Weiskopf, Daniel
TI Exploranative Code Quality Documents
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Code quality; interactive documents; natural language generation;
   sparklines
ID NATURAL-LANGUAGE; GENERATION; DESIGN
AB Good code quality is a prerequisite for efficiently developing maintainable software. In this paper, we present a novel approach to generate exploranative (explanatory and exploratory) data-driven documents that report code quality in an interactive, exploratory environment. We employ a template-based natural language generation method to create textual explanations about the code quality, dependent on data from software metrics. The interactive document is enriched by different kinds of visualization, including parallel coordinates plots and scatterplots for data exploration and graphics embedded into text. We devise an interaction model that allows users to explore code quality with consistent linking between text and visualizations; through integrated explanatory text, users are taught background knowledge about code quality aspects. Our approach to interactive documents was developed in a design study process that included software engineering and visual analytics experts. Although the solution is specific to the software engineering scenario, we discuss how the concept could generalize to multivariate data and report lessons learned in a broader scope.
C1 [Mumtaz, Haris; Weiskopf, Daniel] Univ Stuttgart, VISUS, Stuttgart, Germany.
   [Latif, Shahid; Beck, Fabian] Univ Duisburg Essen, Paluno, Duisburg, Germany.
C3 University of Stuttgart; University of Duisburg Essen
RP Mumtaz, H (corresponding author), Univ Stuttgart, VISUS, Stuttgart, Germany.
EM haris.mumtaz@visus.uni-stuttgart.de; shahid.latif@paluno.uni-due.de;
   fabian.beck@paluno.uni-due.de; daniel.weiskopf@visus.uni-stuttgart.de
RI Weiskopf, Daniel/KWT-7459-2024
OI Weiskopf, Daniel/0000-0003-1174-1026; Beck, Fabian/0000-0003-4042-3043
FU Baden-Wurttemberg Stiftung
FX Fabian Beck is indebted to the Baden-Wurttemberg Stiftung for the
   financial support of this research project within the Postdoctoral
   Fellowship for Leading Early Career Researchers.
NR 55
TC 10
Z9 11
U1 2
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 1129
EP 1139
DI 10.1109/TVCG.2019.2934669
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100104
PM 31443011
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Sun, D
   Huang, RF
   Chen, YZ
   Wang, Y
   Zeng, J
   Yuan, MX
   Pong, TC
   Qu, HM
AF Sun, Dong
   Huang, Renfei
   Chen, Yuanzhe
   Wang, Yong
   Zeng, Jia
   Yuan, Mingxuan
   Pong, Ting-Chuen
   Qu, Huamin
TI PlanningVis: A Visual Analytics Approach to Production Planning in Smart
   Factories
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Production Planning; Time Series Data; Comparative Analysis; Visual
   Analytics; Smart Factory; Industry 4.0
AB Production planning in the manufacturing industry is crucial for fully utilizing factory resources (e.g., machines, raw materials and workers) and reducing costs. With the advent of industry 4.0, plenty of data recording the status of factory resources have been collected and further involved in production planning, which brings an unprecedented opportunity to understand, evaluate and adjust complex production plans through a data-driven approach. However, developing a systematic analytics approach for production planning is challenging due to the large volume of production data, the complex dependency between products, and unexpected changes in the market and the plant. Previous studies only provide summarized results and fail to show details for comparative analysis of production plans. Besides, the rapid adjustment to the plan in the case of an unanticipated incident is also not supported. In this paper, we propose PlanningVis, a visual analytics system to support the exploration and comparison of production plans with three levels of details: a plan overview presenting the overall difference between plans, a product view visualizing various properties of individual products, and a production detail view displaying the product dependency and the daily production details in related factories. By integrating an automatic planning algorithm with interactive visual explorations, PlanningVis can facilitate the efficient optimization of daily production planning as well as support a quick response to unanticipated incidents in manufacturing. Two case studies with real-world data and carefully designed interviews with domain experts demonstrate the effectiveness and usability of PlanningVis.
C1 [Sun, Dong; Huang, Renfei; Wang, Yong; Pong, Ting-Chuen; Qu, Huamin] Hong Kong Univ Sccience & Technol, Hong Kong, Peoples R China.
   [Chen, Yuanzhe; Zeng, Jia; Yuan, Mingxuan] Huawei Technol Co Ltd, Noahs Ark Lab, Shenzhen, Peoples R China.
C3 Huawei Technologies
RP Wang, Y (corresponding author), Hong Kong Univ Sccience & Technol, Hong Kong, Peoples R China.
EM dsunae@ust.hk; rhuangan@ust.hk; chenyuanzhe@huawei.com; ywangct@ust.hk;
   zeng.jia@huawei.com; yuan.mingxuan@huawei.com; tcpong@ust.hk;
   huamin@ust.hk
RI Sun, Dong/AAG-6677-2021; Wang, Yong/HKF-3903-2023
OI Wang, Yong/0000-0002-0092-0793
FU RGC GRF [16241916]; HK TRS [T44-707/16-N]
FX We would like to thank the domain experts from Huawei in China for the
   helpful discussions and the support of user studies. Besides, we would
   like to thank Zezheng Feng for video editing. Finally, we would also
   like to thank the anonymous reviewers for their constructive comments.
   This work is partially supported by grant RGC GRF 16241916 and HK TRS
   T44-707/16-N.
NR 53
TC 33
Z9 40
U1 1
U2 34
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 579
EP 589
DI 10.1109/TVCG.2019.2934275
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100054
PM 31425087
OA Green Published, Green Submitted
DA 2025-03-07
ER

PT J
AU Wang, YH
   Wang, ZY
   Liu, TT
   Correll, M
   Cheng, ZL
   Deussen, O
   Sedlmair, M
AF Wang, Yunhai
   Wang, Zeyu
   Liu, Tingting
   Correll, Michael
   Cheng, Zhanglin
   Deussen, Oliver
   Sedlmair, Michael
TI Improving the Robustness of Scagnostics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Scagnostics; scatterplots; sensitivity analysis; Robust Scagnostics
ID RANKING VISUALIZATIONS; QUALITY METRICS; SCATTERPLOT; VIEWS; GUIDANCE
AB In this paper, we examine the robustness of scagnostics through a series of theoretical and empirical studies. First, we investigate the sensitivity of scagnostics by employing perturbing operations on more than 60M synthetic and real-world scatterplots. We found that two scagnostic measures, Outlying and Clumpy, are overly sensitive to data binning. To understand how these measures align with human judgments of visual features, we conducted a study with 24 participants, which reveals that i) humans are not sensitive to small perturbations of the data that cause large changes in both measures, and ii) the perception of clumpiness heavily depends on per-cluster topologies and structures. Motivated by these results, we propose Robust Scagnostics (RScag) by combining adaptive binning with a hierarchy-based form of scagnostics. An analysis shows that RScag improves on the robustness of original scagnostics, aligns better with human judgments, and is equally fast as the traditional scagnostic measures.
C1 [Wang, Yunhai; Wang, Zeyu; Liu, Tingting] Shandong Univ, Jinan, Peoples R China.
   [Wang, Zeyu; Cheng, Zhanglin; Deussen, Oliver] SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China.
   [Correll, Michael] Tableau Res, Seattle, WA USA.
   [Deussen, Oliver] Konstanz Univ, Constance, Germany.
   [Sedlmair, Michael] Univ Stuttgart, VISUS, Stuttgart, Germany.
C3 Shandong University; Chinese Academy of Sciences; Shenzhen Institute of
   Advanced Technology, CAS; University of Konstanz; University of
   Stuttgart
RP Wang, YH (corresponding author), Shandong Univ, Jinan, Peoples R China.
EM cloudseawang@gmail.com; zywangx@gmail.com; sduhammer@gmail.com;
   mcorrell@tableau.com; zl.cheng@siat.ac.cn;
   oliver.deussen@uni-konstanz.de; michael.sedlmair@visus.uni-stuttgart.de
RI Liu, Tingting/JCE-9180-2023; Deussen, Oliver/HKF-2004-2023; Cheng,
   Zhanglin/AAP-1760-2021
OI Sedlmair, Michael/0000-0001-7048-9292; Cheng,
   Zhanglin/0000-0002-3360-2679
FU National Key Research & Development Plan of China [2016YFB1001404]; NSFC
   [61772315, 61861136012]; Science Challenge Project [TZ2016002]; Leading
   Talents of Guangdong Program [00201509]; CAS grant [GJHZ1862]; Deutsche
   Forschungsgemeinschaft (DFG, German Research Foundation) [251654672 -
   TRR 161]; DFG Center of Excellence 2117 "Centre for the advanced Study
   of Collective Behaviour" [422037984]
FX This work is supported by the grants of the National Key Research &
   Development Plan of China (2016YFB1001404), NSFC (61772315,
   61861136012), Science Challenge Project (TZ2016002), the Leading Talents
   of Guangdong Program (00201509), the CAS grant (GJHZ1862), the Deutsche
   Forschungsgemeinschaft (DFG, German Research Foundation) - Projektnummer
   251654672 - TRR 161, and the DFG Center of Excellence 2117 "Centre for
   the advanced Study of Collective Behaviour" (ID: 422037984).
NR 52
TC 12
Z9 12
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 759
EP 769
DI 10.1109/TVCG.2019.2934796
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100070
PM 31443018
DA 2025-03-07
ER

PT J
AU Min, X
   Zhang, WQ
   Sun, SQ
   Zhao, N
   Tang, SL
   Zhuang, YT
AF Min, Xin
   Zhang, Wenqiao
   Sun, Shouqian
   Zhao, Nan
   Tang, Siliang
   Zhuang, Yueting
TI VPModel: High-Fidelity Product Simulation in a Virtual-Physical
   Environment
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 18th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 14-18, 2019
CL Beijing, PEOPLES R CHINA
SP IEEE, IEEE Comp Soc, Beijing Soc Image & Graph, Beijing Inst Technol, Beihang Univ, IEEE VGTC, BUAA, Sensetime, ArcSoft, Dell, Intel, VR State Key Lab Virtual Real Technol & Syst, Luster, Seengene, Lenovo Res, Oppo, Goertek, NED AR
DE Designers; high-fidelity prototyping tool; 3D printed model; mixed
   reality
ID AUGMENTED REALITY
AB In the development of a new product, the design team must describe the expected effects of the final products to potential users and stakeholders. However, existing prototyping tools can only present a product imperfectly, due to limitations at different levels. Specifically, the physical product model, which may be the product of 3D printing, could lack a visual interface; the presentation of the product through modeling software such as Rhinoceros 3D does not provide good realistic tactile perception; or the interface platforms, such as Axure RP, used to display the interactive effects differ from those to be used in the actual operation. Thus, we present the VPModel, a high-fidelity prototyping tool, able to integrate multiple prototyping methods simultaneously. It combines a touchable 3D-printed product model (3DPM) and a corresponding visualized virtual model, and the interactive interfaces are rendered synchronously in a mixed-reality device. Through the tangible, visual, and interactive demonstration, designers and normal users can each obtain a similar experience to the experience of the finished product. Furthermore, the VPModel also enhances design practices by enabling comparisons between modular models. However, the implementation of this system is a challenging task, which subsumes several fundamental problems as sub-tasks: object detection, real-time matching, hand-gesture detection and action recognition. To achieve the expected goals of the VPModel, this system uses physical hardware (a Microsoft MR HoloLens headset, a Leap Motion Controller, and a 3D printer) and existing machine learning algorithms. To evaluate our VPModel, we report the user experience of 16 participants, evaluated using a closed-ended questionnaire survey, a quantitative analysis of task performance, and a qualitative analysis of open-ended interviews. The results show a significant improvement in realism and enjoyment using the VPModel over the two traditional camera prototype approaches. In summary, the VPModel can be used to support design strategy and to convey design concepts fully and efficiently, which indicates a potential use for the VPModel in shortening product development cycles and reducing communication costs.
C1 [Min, Xin; Zhang, Wenqiao; Sun, Shouqian; Tang, Siliang; Zhuang, Yueting] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
   [Zhao, Nan] Alibaba Cloud, IoT UED Dept, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University; Alibaba Group
RP Min, X (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
EM minx@zju.edu.cn; wenqiaozhang@zju.edu.cn; ssq@zju.edu.cn;
   zhaonan.ace@gmail.com; siliang@zju.edu.cn; yzhuang@cs.zju.edu.cn
OI Min, Xin/0009-0002-8561-7909
FU National Natural Science Foundation of China [91748127]
FX This workwas supported in part by the National Natural Science
   Foundation of China (grant#91748127).
NR 48
TC 17
Z9 19
U1 2
U2 46
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2019
VL 25
IS 11
BP 3083
EP 3093
DI 10.1109/TVCG.2019.2932276
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA JD2VM
UT WOS:000489833000006
PM 31403424
DA 2025-03-07
ER

PT J
AU Guo, HQ
   He, WB
   Seo, S
   Shen, HW
   Constantinescu, EM
   Liu, CH
   Peterka, T
AF Guo, Hanqi
   He, Wenbin
   Seo, Sangmin
   Shen, Han-Wei
   Constantinescu, Emil Mihai
   Liu, Chunhui
   Peterka, Tom
TI Extreme-Scale Stochastic Particle Tracing for Uncertain Unsteady Flow
   Visualization and Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Parallel particle tracing; uncertain flow visualization; CPU-GPU hybrid
   parallelism
ID OF-THE-ART; COHERENT STRUCTURES; VECTOR; ADVECTION; TOPOLOGY
AB We present an efficient and scalable solution to estimate uncertain transport behaviors-stochastic flow maps (SFMs)-for visualizing and analyzing uncertain unsteady flows. Computing flow maps from uncertain flow fields is extremely expensive because it requires many Monte Carlo runs to trace densely seeded particles in the flow. We reduce the computational cost by decoupling the time dependencies in SFMs so that we can process shorter sub time intervals independently and then compose them together for longer time periods. Adaptive refinement is also used to reduce the number of runs for each location. We parallelize over tasks-packets of particles in our design-to achieve high efficiency in MPI/thread hybrid programming. Such a task model also enables CPU/GPU coprocessing. We show the scalability on two supercomputers, Mira (up to 256K Blue Gene/Q cores) and Titan (up to 128K Opteron cores and 8K GPUs), that can trace billions of particles in seconds.
C1 [Guo, Hanqi; Constantinescu, Emil Mihai; Peterka, Tom] Argonne Natl Lab, Math & Comp Sci Div, Lemont, IL 60439 USA.
   [He, Wenbin; Shen, Han-Wei] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
   [Seo, Sangmin] Ground X Inc, Seoul, South Korea.
   [Liu, Chunhui] Kyoto Univ, Fac Sci, Dept Math, Kyoto 6068502, Japan.
C3 United States Department of Energy (DOE); Argonne National Laboratory;
   University System of Ohio; Ohio State University; Kyoto University
RP Guo, HQ (corresponding author), Argonne Natl Lab, Math & Comp Sci Div, Lemont, IL 60439 USA.
EM hguo@anl.gov; he.495@buckeyemail.osu.edu; seo.sangmin@gmail.com;
   shen.94@osu.edu; emconsta@mcs.anl.gov; chunhui.liu@math.kyoto-u.ac.jp;
   tpeterka@mcs.anl.gov
RI Guo, Hanqi/AAL-1929-2021; Shen, Han-wei/A-4710-2012; Guo,
   Hanqi/ADW-4234-2022
OI He, Wenbin/0000-0002-5376-5803; Constantinescu,
   Emil/0000-0002-7003-6899; Guo, Hanqi/0000-0001-7776-1834
FU JSPS KAKENHI [JP17F17730]; U.S. Department of Energy, Office of Science
   [DE-AC02-06CH11357]; U.S. Department of Energy, Office of Advanced
   Scientific Computing Research, Scientific Discovery through Advanced
   Computing (SciDAC) program; DOE Office of Science User Facility
   [DE-AC02-06CH11357]; Office of Science of the U.S. Department of Energy
   [DE-AC05-00OR22725]
FX The authors would like to thank Dr. Hong Zhang and Dr. Julie Bessac for
   useful discussions. Dr. Chunhui Liu is supported by JSPS KAKENHI Grant
   Number JP17F17730. This material is based upon work supported by the
   U.S. Department of Energy, Office of Science, under contract number
   DE-AC02-06CH11357. This work is also supported by the U.S. Department of
   Energy, Office of Advanced Scientific Computing Research, Scientific
   Discovery through Advanced Computing (SciDAC) program. This research
   used resources of the Argonne Leadership Computing Facility, which is a
   DOE Office of Science User Facility supported under Contract
   DE-AC02-06CH11357. This research also used resources of the Oak Ridge
   Leadership Computing Facility at the Oak Ridge National Laboratory,
   which is supported by the Office of Science of the U.S. Department of
   Energy under Contract No. DE-AC05-00OR22725.
NR 45
TC 9
Z9 9
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2019
VL 25
IS 9
BP 2710
EP 2724
DI 10.1109/TVCG.2018.2856772
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IN8OV
UT WOS:000478940300002
PM 30047883
OA Bronze, Green Submitted
DA 2025-03-07
ER

PT J
AU Rajamäki, J
   Hämäläinen, P
AF Rajamaki, Joose
   Hamalainen, Perttu
TI Continuous Control Monte Carlo Tree Search Informed by Multiple Experts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Monte Carlo tree search; continuous control; reinforcement learning
AB Efficient algorithms for 3D character control in continuous control setting remain an open problem in spite of the remarkable recent advances in the field. We present a sampling-based model-predictive controller that comes in the form of a Monte Carlo tree search (MCTS). The tree search utilizes information from multiple sources including two machine learning models. This allows rapid development of complex skills such as 3D humanoid locomotion with less than a million simulation steps, in less than a minute of computing on a modest personal computer. We demonstrate locomotion of 3D characters with varying topologies under disturbances such as heavy projectile hits and abruptly changing target direction. In this paper we also present a new way to combine information from the various sources such that minimal amount of information is lost. We furthermore extend the neural network, involved in the algorithm, to represent stochastic policies. Our approach yields a robust control algorithm that is easy to use. While learning, the algorithm runs in near real-time, and after learning the sampling budget can be reduced for real-time operation.
C1 [Rajamaki, Joose; Hamalainen, Perttu] Aalto Univ, Dept Comp Sci, Helsinki 00076, Finland.
C3 Aalto University
RP Rajamäki, J (corresponding author), Aalto Univ, Dept Comp Sci, Helsinki 00076, Finland.
EM joose.rajamaki@aalto.fi; perttu.hamalainen@aalto.fi
OI Hamalainen, Perttu/0000-0001-7764-3459
FU Academy of Finland [299358, 305737]; Tekniikan edistamissaatio; Academy
   of Finland (AKA) [305737] Funding Source: Academy of Finland (AKA)
FX The authors would like to thank the reviewers for their comments for
   improving this manuscript. This work was supported by the Academy of
   Finland (grants 299358 and 305737). Joose Rajamaki additionally thanks
   Tekniikan edistamissaatio for their support.
NR 53
TC 7
Z9 8
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2019
VL 25
IS 8
BP 2540
EP 2553
DI 10.1109/TVCG.2018.2849386
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IG2AT
UT WOS:000473597800003
PM 29994613
OA hybrid
DA 2025-03-07
ER

PT J
AU Wu, K
   Yuksel, C
AF Wu, Kui
   Yuksel, Cem
TI Real-Time Cloth Rendering with Fiber-Level Detail
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cloth rendering; procedural geometry; real-time rendering
AB Modeling cloth with fiber-level geometry can produce highly realistic details. However, rendering fiber-level cloth models not only has a high memory cost but it also has a high computation cost even for offline rendering applications. In this paper we present a real-time fiber-level cloth rendering method for current GPUs. Our method procedurally generates fiber-level geometric details on-the-fly using yarn-level control points for minimizing the data transfer to the GPU. We also reduce the rasterization operations by collectively representing the fibers near the center of each ply that form the yarn structure. Moreover, we employ a level-of-detail strategy to minimize or completely eliminate the generation of fiber-level geometry that would have little or no impact on the final rendered image. Furthermore, we introduce a simple self-shadow computation method that allows lighting with self-shadows using relatively low-resolution shadow maps. We also provide a simple distance-based ambient occlusion approximation as well as an ambient illumination precomputation approach, both of which account for fiber-level self-occlusion of yarn. Finally, we discuss how to use a physical-based shading model with our fiber-level cloth rendering method and how to handle cloth animations with temporal coherency. We demonstrate the effectiveness of our approach by comparing our simplified fiber geometry to procedurally generated references and display knitwear containing more than a hundred million individual fiber curves at real-time frame rates with shadows and ambient occlusion.
C1 [Wu, Kui; Yuksel, Cem] Univ Utah, Salt Lake City, UT 84112 USA.
C3 Utah System of Higher Education; University of Utah
RP Yuksel, C (corresponding author), Univ Utah, Salt Lake City, UT 84112 USA.
EM kwu@cs.utah.edu; cem@cemyuksel.com
RI Wu, Kui/LRC-0682-2024
OI Yuksel, Cem/0000-0002-0122-4159
FU US National Science Foundation [1538593]; Div Of Civil, Mechanical, &
   Manufact Inn; Directorate For Engineering [1538593] Funding Source:
   National Science Foundation
FX We thank Charles Hansen, Yang Song, and Konstantin Shkurko for valuable
   discussions and suggestions. This work was supported in part by US
   National Science Foundation grant #1538593.
NR 29
TC 18
Z9 19
U1 1
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2019
VL 25
IS 2
BP 1297
EP 1308
DI 10.1109/TVCG.2017.2731949
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG5ZY
UT WOS:000455062000005
PM 28749354
OA Bronze
DA 2025-03-07
ER

PT J
AU Beyer, J
   Mohammed, H
   Agus, M
   Al-Awami, AK
   Pfister, H
   Hadwiger, M
AF Beyer, Johanna
   Mohammed, Haneen
   Agus, Marco
   Al-Awami, Ali K.
   Pfister, Hanspeter
   Hadwiger, Markus
TI Culling for Extreme-Scale Segmentation Volumes: A Hybrid Deterministic
   and Probabilistic Approach
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Hierarchical Culling; Segmented Volume Data; Bloom Filter; Volume
   Rendering; Spatial Queries
ID VISUALIZATION; RECONSTRUCTION
AB With the rapid increase in raw volume data sizes, such as terabyte-sized microscopy volumes, the corresponding segmentation label volumes have become extremely large as well. We focus on integer label data, whose efficient representation in memory, as well as fast random data access, pose an even greater challenge than the raw image data. Often, it is crucial to be able to rapidly identify which segments are located where, whether for empty space skipping for fast rendering, or for spatial proximity queries. We refer to this process as culling. In order to enable efficient culling of millions of labeled segments, we present a novel hybrid approach that combines deterministic and probabilistic representations of label data in a data-adaptive hierarchical data structure that we call the label list tree. In each node, we adaptively encode label data using either a probabilistic constant-time access representation for fast conservative culling, or a deterministic logarithmic-time access representation for exact queries. We choose the best data structures for representing the labels of each spatial region while building the label list tree. At run time, we further employ a novel query-adaptive culling strategy. While filtering a query down the tree, we prune it successively, and in each node adaptively select the representation that is best suited for evaluating the pruned query, depending on its size. We show an analysis of the efficiency of our approach with several large data sets from connectomics, including a brain scan with more than 13 million labeled segments, and compare our method to conventional culling approaches. Our approach achieves significant reductions in storage size as well as faster query times.
C1 [Beyer, Johanna; Mohammed, Haneen; Pfister, Hanspeter] Harvard Univ, Cambridge, MA 02138 USA.
   [Mohammed, Haneen; Agus, Marco; Hadwiger, Markus] KAUST, Thuwal, Saudi Arabia.
   [Al-Awami, Ali K.] KAUST, Dhahran, Saudi Arabia.
   [Al-Awami, Ali K.] Saudi Aramco, Dhahran, Saudi Arabia.
C3 Harvard University; King Abdullah University of Science & Technology;
   King Abdullah University of Science & Technology
RP Beyer, J (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.
EM jbeyer@seas.harvard.edu; haneen.mohammed@kaust.edu.sa;
   marco.agus@kaust.edu.sa; awami.ali@gmail.com; pfister@seas.harvard.edu;
   markus.hadwiger@kaust.edu.sa
RI Agus, Marco/AAM-5898-2020
OI Agus, Marco/0000-0003-2752-3525; Mohammed, Haneen/0000-0002-4535-1926;
   Al-Awami, Ali/0000-0002-8725-1958; Hadwiger, Markus/0000-0003-1239-4871;
   Pfister, Hanspeter/0000-0002-3620-2582
FU King Abdullah University of Science and Technology (KAUST); KAUST Office
   of Sponsored Research (OSR) award [OSR-2015-CCF-2533-01]; Direct For
   Computer & Info Scie & Enginr; Div Of Information & Intelligent Systems
   [1607800] Funding Source: National Science Foundation
FX We thank John Keyser for the 'KESM Mouse Brain' data set [34]. This work
   is partially supported by King Abdullah University of Science and
   Technology (KAUST) and the KAUST Office of Sponsored Research (OSR)
   award OSR-2015-CCF-2533-01.
NR 42
TC 8
Z9 9
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 1132
EP 1141
DI 10.1109/TVCG.2018.2864847
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000108
PM 30136947
DA 2025-03-07
ER

PT J
AU Falk, M
   Ynnerman, A
   Treanor, D
   Lundström, C
AF Falk, Martin
   Ynnerman, Anders
   Treanor, Darren
   Lundstrom, Claes
TI Interactive Visualization of 3D Histopathology in Native Resolution
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Histology; Pathology; Volume Rendering; Expert Evaluation
ID OF-THE-ART; VOLUME
AB We present a visualization application that enables effective interactive visual analysis of large-scale 3D histopathology, that is, high-resolution 3D microscopy data of human tissue. Clinical work flows and research based on pathology have, until now, largely been dominated by 2D imaging. As we will show in the paper, studying volumetric histology data will open up novel and useful opportunities for both research and clinical practice. Our starting point is the current lack of appropriate visualization tools in histopathology, which has been a limiting factor in the uptake of digital pathology. Visualization of 3D histology data does pose difficult challenges in several aspects. The full-color datasets are dense and large in scale, on the order of 100,000 x 100,000x 100 voxels. This entails serious demands on both rendering performance and user experience design. Despite this, our developed application supports interactive study of 3D histology datasets at native resolution. Our application is based on tailoring and tuning of existing methods, system integration work, as well as a careful study of domain specific demands emanating from a close participatory design process with domain experts as team members. Results from a user evaluation employing the tool demonstrate a strong agreement among the 14 participating pathologists that 3D histopathology will be a valuable and enabling tool for their work.
C1 [Falk, Martin; Ynnerman, Anders] Linkoping Univ, Dept Sci & Technol, Linkoping, Sweden.
   [Treanor, Darren] Leeds Teaching Hosp NHS Trust, Leeds, W Yorkshire, England.
   [Treanor, Darren; Lundstrom, Claes] Linkoping Univ, Ctr Med Image Sci & Visualizat CMIV, Linkoping, Sweden.
   [Lundstrom, Claes] Sectra AB, Linkoping, Sweden.
C3 Linkoping University; University of Leeds; Linkoping University; Sectra
   AB
RP Falk, M (corresponding author), Linkoping Univ, Dept Sci & Technol, Linkoping, Sweden.
EM martin.falk@liu.se; anders.ynnerman@liu.se; darrentreanor@nhs.net;
   claes.lundstrom@liu.se
OI Falk, Martin/0000-0003-1511-5006
FU Excellence Center at Linkoping and Lund in Information Technology
   (ELLIIT); Swedish e-Science Research Centre (SeRC); DigiPat project by
   VINNOVA grant [2014-04257]; Vinnova [2014-04257] Funding Source: Vinnova
FX The authors would like to thank Derek Magee for his input and fruitful
   discussions as well as the anonymous reviewers for their comments. This
   work was supported through grants from the Excellence Center at
   Linkoping and Lund in Information Technology (ELLIIT), the Swedish
   e-Science Research Centre (SeRC), and the DigiPat project by VINNOVA
   grant 2014-04257.
NR 33
TC 17
Z9 19
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 1008
EP 1017
DI 10.1109/TVCG.2018.2864816
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000096
PM 30130214
OA Green Published, Green Accepted
DA 2025-03-07
ER

PT J
AU Krekhov, A
   Krüger, J
AF Krekhov, Andrey
   Kruger, Jens
TI Deadeye: A Novel Preattentive Visualization Technique Based on Dichoptic
   Presentation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Popout; preattentive vision; comparative visualization; dichoptic
   presentation
ID VISUAL-SEARCH; ATTENTION; FORM; BINOCULARITY; PERCEPTION; FLICKER;
   SERIAL; DEPTH
AB Preattentive visual features such as hue or flickering can effectively draw attention to an object of interest for instance, an important feature in a scientific visualization. These features appear to pop out and can be recognized by our visual system. independently from the number of distractors. Most cues do not take advantage of the fact that most humans have two eyes. In cases where binocular vision is applied, it is almost exclusively used to convey depth by exposing stereo pairs. We present Deadeye. a novel preattentive visualization technique based on presenting different stimuli to each eye. The target object is rendered for one eye only and is instantly detected by our visual system. In contrast to existing cues, Deadeye does not modify any visual properties of the target and, thus, is particularly suited for visualization applications. Our evaluation confirms that Deadeye is indeed perceived preattentively. We also explore a conjunction search based on our technique and show that, in contrast to 3D depth. the task cannot be processed in parallel.
C1 [Krekhov, Andrey; Kruger, Jens] Univ Duisburg Essen, Ctr Visual Data Anal & Comp Graph COVIDAG, Duisburg, Germany.
   [Kruger, Jens] Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.
C3 University of Duisburg Essen; Utah System of Higher Education;
   University of Utah
RP Krekhov, A (corresponding author), Univ Duisburg Essen, Ctr Visual Data Anal & Comp Graph COVIDAG, Duisburg, Germany.
EM andrey.krekhov@uni-due.de; jens.krueger@uni-due.de
NR 59
TC 7
Z9 11
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 936
EP 945
DI 10.1109/TVCG.2018.2864498
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA HD6IJ
UT WOS:000452640000089
PM 30130196
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Wall, E
   Agnihotri, M
   Matzen, L
   Divis, K
   Haass, M
   Endert, A
   Stasko, J
AF Wall, Emily
   Agnihotri, Meeshu
   Matzen, Laura
   Divis, Kristin
   Haass, Michael
   Endert, Alex
   Stasko, John
TI A Heuristic Approach to Value-Driven Evaluation of Visualizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization evaluation; heuristics; value of visualization
ID INFORMATION VISUALIZATION; INSIGHT; ENVIRONMENTS
AB Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however. providing no actionable method of assessing a visualization's value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of the course in which they were developed.
C1 [Wall, Emily; Agnihotri, Meeshu; Endert, Alex; Stasko, John] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Matzen, Laura; Divis, Kristin; Haass, Michael] Sandia Natl Labs, POB 5800, Albuquerque, NM 87185 USA.
C3 University System of Georgia; Georgia Institute of Technology; United
   States Department of Energy (DOE); Sandia National Laboratories
RP Wall, E (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM emilywall@gatech.edu; magnihotri6@gatech.edu; lematze@sandia.gov;
   kmdivis@sandia.gov; mjhaass@sandia.gov; endert@gatech.edu;
   stasko@gatech.edu
OI Wall, Emily/0000-0003-4568-0698
FU Laboratory Directed Research and Development program at Sandia National
   Laboratories; U.S. Department of Energys National Nuclear Security
   Administration [DE-NA0003525]
FX This work was partially supported by the Laboratory Directed Research
   and Development program at Sandia National Laboratories. Sandia is a
   multimission laboratory managed and operated by National Technology &
   Engineering Solutions of Sandia, LLC, a wholly owned subsidiary of
   Honeywell International Inc., for the U.S. Department of Energys
   National Nuclear Security Administration under contract DE-NA0003525.
NR 35
TC 54
Z9 57
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 491
EP 500
DI 10.1109/TVCG.2018.2865146
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000047
PM 30188826
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Rautenhaus, M
   Böttinger, M
   Siemen, S
   Hoffman, R
   Kirby, RM
   Mirzargar, M
   Röber, N
   Westermann, R
AF Rautenhaus, Marc
   Bottinger, Michael
   Siemen, Stephan
   Hoffman, Robert
   Kirby, Robert M.
   Mirzargar, Mahsa
   Rober, Niklas
   Westermann, Rudiger
TI Visualization in Meteorology-A Survey of Techniques and Tools for Data
   Analysis Tasks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; meteorology; atmospheric science; weather forecasting;
   climatology; spatiotemporal data; survey
ID NUMERICAL WEATHER PREDICTION; HIGH-RESOLUTION SIMULATIONS; VARYING DATA
   VISUALIZATION; UNCERTAIN SCALAR FIELDS; INTERACTIVE DATA ACCESS; EXTREME
   FORECAST INDEX; VISUAL ANALYSIS; CLIMATE DATA; DATA SETS; FLOW
   VISUALIZATION
AB This article surveys the history and current state of the art of visualization in meteorology, focusing on visualization techniques and tools used for meteorological data analysis. We examine characteristics of meteorological data and analysis tasks, describe the development of computer graphics methods for visualization in meteorology from the 1960s to today, and visit the state of the art of visualization techniques and tools in operational weather forecasting and atmospheric research. We approach the topic from both the visualization and the meteorological side, showing visualization techniques commonly used in meteorological practice, and surveying recent studies in visualization research aimed at meteorological applications. Our overview covers visualization techniques from the fields of display design, 3D visualization, flow dynamics, feature-based visualization, comparative visualization and data fusion, uncertainty and ensemble visualization, interactive visual analysis, efficient rendering, and scalability and reproducibility. We discuss demands and challenges for visualization research targeting meteorological data analysis, highlighting aspects in demonstration of benefit, interactive visual analysis, seamless visualization, ensemble visualization, 3D visualization, and technical issues.
C1 [Rautenhaus, Marc; Westermann, Rudiger] Tech Univ Munich, Comp Graph & Visualizat Grp, D-85748 Garching, Germany.
   [Bottinger, Michael; Rober, Niklas] Deutsch Klimarechenzentrum GmbH, D-20146 Hamburg, Germany.
   [Siemen, Stephan] European Ctr Medium Range Weather Forecasts, Reading RG2 9AX, Berks, England.
   [Hoffman, Robert] Inst Human & Machine Cognit, Pensacola, FL 32502 USA.
   [Kirby, Robert M.] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
   [Mirzargar, Mahsa] Univ Miami, Dept Comp Sci, Miami, FL 33146 USA.
C3 Technical University of Munich; European Centre for Medium-Range Weather
   Forecasts (ECMWF); Florida Institute for Human & Machine Cognition
   (IHMC); Utah System of Higher Education; University of Utah; University
   of Miami
RP Rautenhaus, M (corresponding author), Tech Univ Munich, Comp Graph & Visualizat Grp, D-85748 Garching, Germany.
EM marc.rautenhaus@tum.de; boettinger@dkrz.de; stephan.siemen@ecmwf.int;
   rhoffman@ihmc.us; kirby@cs.utah.edu; mirzargar@cs.miami.edu;
   roeber@dkrz.de; westermann@tum.de
OI Kirby, Robert/0000-0001-5712-4141; Bottinger,
   Michael/0000-0001-9704-8235; Rautenhaus, Marc/0000-0002-2715-2165
FU European Union under the ERC [291372]; ERC Proof-of-Concept Grant
   "Vis4Weather"; Transregional Collaborative Research Center SFB/TRR 165 "
   Waves - German Research Foundation (DFG); US National Science Foundation
   (NSF) [IIS-1212806]; European Research Council (ERC) [291372] Funding
   Source: European Research Council (ERC)
FX This work was partly supported by the European Union under the ERC
   Advanced Grant 291372 "SaferVis" and the ERC Proof-of-Concept Grant
   "Vis4Weather", and by the Transregional Collaborative Research Center
   SFB/TRR 165 " Waves to Weather" funded by the German Research Foundation
   (DFG). The fifth and sixth authors acknowledge the support of US
   National Science Foundation (NSF) grant IIS-1212806.
NR 368
TC 79
Z9 88
U1 11
U2 151
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2018
VL 24
IS 12
BP 3268
EP 3296
DI 10.1109/TVCG.2017.2779501
PG 29
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GZ0TW
UT WOS:000449079000021
PM 29990196
OA Bronze
DA 2025-03-07
ER

PT J
AU Chen, GW
   Ma, C
   Fan, ZC
   Cui, XW
   Liao, HE
AF Chen, Guowen
   Ma, Cong
   Fan, Zhencheng
   Cui, Xiwen
   Liao, Hongen
TI Real-Time Lens Based Rendering Algorithm for Super-Multiview Integral
   Photography without Image Resampling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Lens based rendering; integral photography; super-multiview display; GPU
ID ALIGNED OLED MICRODISPLAYS; 3-DIMENSIONAL DISPLAY; 3-D DISPLAY; SYSTEM
AB We propose a computer generated integral photography (CGIP) method that employs a lens based rendering (LBR) algorithm for super-multiview displays to achieve higher frame rates and better image quality without pixel resampling or view interpolation. The algorithm can utilize both fixed and programmable graphics pipelines to accelerate CGIP rendering and inter-perspective antialiasing. Two hardware prototypes were fabricated with two high-resolution liquid crystal displays and micro-lens arrays (MLA). Qualitative and quantitative experiments were performed to evaluate the feasibility of the proposed algorithm. To the best of our knowledge, the proposed LBR method outperforms state-of-the-art CGIP algorithms relative to rendering speed and image quality with our super-multiview hardware configurations. A demonstration experiment was also conducted to reveal the interactivity of a super-multiview display utilizing the proposed algorithm.
C1 [Chen, Guowen; Ma, Cong; Fan, Zhencheng; Cui, Xiwen; Liao, Hongen] Tsinghua Univ, Sch Med, Dept Biomed Engn, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Liao, HE (corresponding author), Tsinghua Univ, Sch Med, Dept Biomed Engn, Beijing 100084, Peoples R China.
EM cgw13@mails.tsinghua.edu.cn; mc15@mails.tsinghua.edu.cn;
   fanzc13@mails.tsinghua.edu.cn; cuixw16@mails.tsinghua.edu.cn;
   liao@tsinghua.edu.cn
RI Liao, Hongen/C-3097-2009
OI Liao, Hongen/0000-0003-3847-9347; Ma, Cong/0000-0003-2107-5186
FU National Natural Science Foundation of China [81427803]; Beijing
   Municipal Science & Technology Commission [Z151100003915079]; National
   Key Technology R&D Program of China [2015BAI01B03]; Natural Science
   Foundation of Beijing Municipality [7172122]; Soochow-Tsinghua
   Innovation Project [2016SZ0206]
FX This work was supported in part by the National Natural Science
   Foundation of China (81427803), Beijing Municipal Science & Technology
   Commission (Z151100003915079), National Key Technology R&D Program of
   China (2015BAI01B03), Natural Science Foundation of Beijing Municipality
   (7172122), and Soochow-Tsinghua Innovation Project (2016SZ0206).
NR 41
TC 17
Z9 17
U1 0
U2 24
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2018
VL 24
IS 9
BP 2600
EP 2609
DI 10.1109/TVCG.2017.2756634
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GP3XC
UT WOS:000440787200011
PM 28961116
DA 2025-03-07
ER

PT J
AU Harrison, DG
   Efford, ND
   Fisher, QJ
   Ruddle, RA
AF Harrison, Dave Graham
   Efford, Nick D.
   Fisher, Quentin J.
   Ruddle, Roy Alan
TI PETMiner-A Visual Analysis Tool for Petrophysical Properties of Core
   Sample Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization systems and software; information visualization; design
   study
ID DIMENSIONALITY REDUCTION; VISUALIZATION; EXPLORATION; SEGMENTATION;
   DESIGN; TREES
AB The aim of the PETMiner software is to reduce the time and monetary cost of analysing petrophysical data that is obtained from reservoir sample cores. Analysis of these data requires tacit knowledge to fill 'gaps' so that predictions can be made for incomplete data. Through discussions with 30 industry and academic specialists, we identified three analysis use cases that exemplified the limitations of current petrophysics analysis tools. We used those use cases to develop nine core requirements for PETMiner, which is innovative because of its ability to display detailed images of the samples as data points, directly plot multiple sample properties and derived measures for comparison, and substantially reduce interaction cost. An 11-month evaluation demonstrated benefits across all three use cases by allowing a consultant to: (1) generate more accurate reservoir flow models, (2) discover a previously unknown relationship between one easy-to-measure property and another that is costly, and (3) make a 100-fold reduction in the time required to produce plots for a report.
C1 [Harrison, Dave Graham; Efford, Nick D.] Univ Leeds, Dept Comp, Leeds LS2 9JT, W Yorkshire, England.
   [Fisher, Quentin J.] Univ Leeds, Dept Petr Geoengn, Leeds LS2 9JT, W Yorkshire, England.
   [Ruddle, Roy Alan] Univ Leeds, Sch Comp, Leeds LS2 9JT, W Yorkshire, England.
C3 University of Leeds; University of Leeds; University of Leeds
RP Harrison, DG (corresponding author), Univ Leeds, Dept Comp, Leeds LS2 9JT, W Yorkshire, England.
EM D.G.Harrison@leeds.ac.uk; N.D.Efford@leeds.ac.uk;
   Q.J.Fisher@leeds.ac.uk; R.A.Ruddle@leeds.ac.uk
OI Fisher, Quentin/0000-0002-2881-7018; Harrison, Dave/0000-0002-8976-5186
FU British Gas; BP; EBN; GDF Suez; Shell; Wintershall
FX We wish to thank our sponsors for supporting the PET-Miner project:
   British Gas, BP, EBN, GDF Suez, Shell and Wintershall. Aurelian provided
   core samples, but did not participate in the PETMiner project.
NR 35
TC 2
Z9 2
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2018
VL 24
IS 5
BP 1728
EP 1741
DI 10.1109/TVCG.2017.2682865
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE7EW
UT WOS:000431397100004
PM 28320668
OA Green Accepted
DA 2025-03-07
ER

PT J
AU John, NW
   Pop, SR
   Day, TW
   Ritsos, PD
   Headleand, CJ
AF John, Nigel W.
   Pop, Serban R.
   Day, Thomas W.
   Ritsos, Panagiotis D.
   Headleand, Christopher J.
TI The Implementation and Validation of a Virtual Environment for Training
   Powered Wheelchair Manoeuvres
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual reality; virtual environment; simulation; wheelchair navigation
ID REALITY; SIMULATOR; PERFORMANCE; NAVIGATION
AB Navigating a powered wheelchair and avoiding collisions is often a daunting task for new wheelchair users. It takes time and practice to gain the coordination needed to become a competent driver and this can be even more of a challenge for someone with a disability. We present a cost effective virtual reality (VR) application that takes advantage of consumer level VR hardware. The system can be easily deployed in an assessment centre or for home use, and does not depend on a specialized high-end virtual environment such as a Powerwall or CAVE. This paper reviews previous work that has used virtual environments technology for training tasks, particularly wheelchair simulation. We then describe the implementation of our own system and the first validation study carried out using thirty three able bodied volunteers. The study results indicate that at a significance level of 5 percent then there is an improvement in driving skills from the use of our VR system. We thus have the potential to develop the competency of a wheelchair user whilst avoiding the risks inherent to training in the real world. However, the occurrence of cybersickness is a particular problem in this application that will need to be addressed.
C1 [John, Nigel W.; Pop, Serban R.; Day, Thomas W.] Univ Chester, Dept Comp Sci, Chester CH1 4BJ, Cheshire, England.
   [Ritsos, Panagiotis D.] Bangor Univ, Sch Comp Sci, Bangor LL57 2DG, Gwynedd, Wales.
   [Headleand, Christopher J.] Univ Lincoln, Sch Comp Sci, Lincoln LN6 7TS, England.
C3 University of Chester; Bangor University; University of Lincoln
RP John, NW (corresponding author), Univ Chester, Dept Comp Sci, Chester CH1 4BJ, Cheshire, England.
EM nigel.john@chester.ac.uk; spop@chester.ac.uk; t.day@chester.ac.uk;
   p.ritsos@bangor.ac.uk; cHeadleand@lincoln.ac.uk
RI Ritsos, Panagiotis/AAE-8990-2022; John, Nigel/ABC-8011-2020; John,
   Nigel/H-2876-2013
OI Ritsos, Panagiotis/0000-0001-9308-3885; John, Nigel/0000-0001-5153-182X;
   Day, Thomas/0000-0002-9153-4862
NR 46
TC 58
Z9 61
U1 1
U2 24
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2018
VL 24
IS 5
BP 1867
EP 1878
DI 10.1109/TVCG.2017.2700273
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE7EW
UT WOS:000431397100014
PM 28475060
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Erat, O
   Isop, WA
   Kalkofen, D
   Schmalstieg, D
AF Erat, Okan
   Isop, Werner Alexander
   Kalkofen, Denis
   Schmalstieg, Dieter
TI Drone-Augmented Human Vision: Exocentric Control for Drones Exploring
   Hidden Areas
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 25th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 18-22, 2018
CL Reutlingen, GERMANY
SP IEEE
DE X-ray; mixed reality; hololens; drone; pick-and-place
ID INTERFACE; REALITY; MODELS
AB Drones allow exploring dangerous or impassable areas safely from a distant point of view. However, flight control from an egocentric view in narrow or constrained environments can be challenging. Arguably, an exocentric view would afford a better overview and, thus, more intuitive flight control of the drone. Unfortunately, such an exocentric view is unavailable when exploring indoor environments. This paper investigates the potential of drone-augmented human vision, i. e., of exploring the environment and controlling the drone indirectly from an exocentric viewpoint. If used with a see-through display, this approach can simulate X-ray vision to provide a natural view into an otherwise occluded environment. The user's view is synthesized from a three-dimensional reconstruction of the indoor environment using image-based rendering. This user interface is designed to reduce the cognitive load of the drone's flight control. The user can concentrate on the exploration of the inaccessible space, while flight control is largely delegated to the drone's autopilot system. We assess our system with a first experiment showing how drone-augmented human vision supports spatial understanding and improves natural interaction with the drone.
C1 [Erat, Okan; Isop, Werner Alexander; Kalkofen, Denis; Schmalstieg, Dieter] Graz Univ Technol, Inst Comp Graph & Vis, Graz, Austria.
C3 Graz University of Technology
RP Erat, O (corresponding author), Graz Univ Technol, Inst Comp Graph & Vis, Graz, Austria.
EM okan.erat@icg.tugraz.at; isop@icg.tugraz.at; kalkofen@icg.tugraz.at;
   dieter@icg.tugraz.at
OI Kalkofen, Denis/0000-0002-0359-206X
FU Austrian Science Fund (FWF) [I 1681]; Austrian Research Promotion Agency
   (FFG) [859208]
FX The authors wish to thank Sebastian Reicher. This work was supported by
   the Austrian Science Fund (FWF, Project I 1681) and Austrian Research
   Promotion Agency (FFG, Project 859208).
NR 34
TC 65
Z9 68
U1 1
U2 39
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2018
VL 24
IS 4
BP 1437
EP 1446
DI 10.1109/TVCG.2018.2794058
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FZ6BS
UT WOS:000427682500006
PM 29543162
DA 2025-03-07
ER

PT J
AU Huang, YJ
   Lin, WC
   Yeh, IC
   Lee, TY
AF Huang, Yi-Jheng
   Lin, Wen-Chieh
   Yeh, I-Cheng
   Lee, Tong-Yee
TI Geometric and Textural Blending for 3D Model Stylization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computer graphics; modeling
ID DECOMPOSITION
AB Stylizing a 3D model with characteristic shapes or appearances is common in product design, particularly in the design of 3D model merchandise, such as souvenirs, toys, furniture, and stylized items. A model stylization approach is proposed in this study. The approach combines base and style models while preserving user-specified shape features of the base model and the attractive features of the style model with limited assistance from a user. The two models are first combined at the topological level. A tree-growing technique is utilized to search for all possible combinations of the two models. Second, the models are combined at textural and geometric levels by employing a morphing technique. Results show that the proposed approach generates various appealing models and allows users to control the diversity of the output models and adjust the blending degree between the base and style models. The results of this work are also experimentally compared with those of a recent work through a user study. The comparison indicates that our results are more appealing, feature-preserving, and reasonable than those of the compared previous study. The proposed system allows product designers to easily explore design possibilities and assists novice users in creating their own stylized models.
C1 [Huang, Yi-Jheng; Lin, Wen-Chieh] Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 30010, Taiwan.
   [Yeh, I-Cheng] Yuan Ze Univ, Dept Comp Sci & Engn, Taoyuan 32003, Taiwan.
   [Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
C3 National Yang Ming Chiao Tung University; Yuan Ze University; National
   Cheng Kung University
RP Huang, YJ (corresponding author), Natl Chiao Tung Univ, Dept Comp Sci, Hsinchu 30010, Taiwan.
EM jennyhuang914@gmail.com; wclin@cs.nctu.edu.tw; ichenyeh@gmail.com;
   tonylee@mail.ncku.edu.tw
OI Yeh, I-Cheng/0000-0003-1045-843X
FU Ministry of Science and Technology of Taiwan
   [MOST-102-2221-E-009-082-MY3, MOST-104-2221-E-006-044-MY3,
   MOST-104-2628-E-009-001-MY3, MOST-105-2221-E-009-095-MY3,
   MOST-105-2218-E-009-013-, MOST-105-2218-E-155-005]
FX We thank the reviewers for their insightful comments, Noah Duncan for
   providing the zoomorphic design results, and Yi-Hsuan Huang and Pin-Xuan
   Huang for assisting the user studies. This work was supported in part by
   the Ministry of Science and Technology of Taiwan under grant no.
   MOST-102-2221-E-009-082-MY3, MOST-104-2221-E-006-044-MY3,
   MOST-104-2628-E-009-001-MY3, MOST-105-2221-E-009-095-MY3,
   MOST-105-2218-E-009-013-, and MOST-105-2218-E-155-005.
NR 38
TC 7
Z9 7
U1 2
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2018
VL 24
IS 2
BP 1114
EP 1126
DI 10.1109/TVCG.2017.2657751
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR8BW
UT WOS:000419299900008
PM 28129179
DA 2025-03-07
ER

PT J
AU Punpongsanon, P
   Guy, E
   Iwai, D
   Sato, K
   Boubekeur, T
AF Punpongsanon, Parinya
   Guy, Emilie
   Iwai, Daisuke
   Sato, Kosuke
   Boubekeur, Tamy
TI Extended LazyNav: Virtual 3D Ground Navigation for Large Displays and
   Head-Mounted Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE 3D user interface; spatial interaction; virtual reality; navigation
ID INTERFACE
AB This paper presents the extended work on LazyNav, a head-free, eyes-free and hands-free mid-air ground navigation control model presented at the IEEE 3D User Interfaces (3DUI) 2015, in particular with a new application to the head-mounted display (HMD). Our mid-air interaction metaphor makes use of only a single pair of the remaining tracked body elements to tailor the navigation. Therefore, the user can navigate in the scene while still being able to performother interactions with her hands and head, e.g., carrying a bag, grasping a cup of coffee, or observing the content by moving her eyes and locally rotating her head. Wedesign several body motions for navigation by considering the use of non-critical body parts and develop assumptions about ground navigation techniques. Through the user studies, we investigate the motions that are easy to discover, easy to control, socially acceptable, accurate and not tiring. Finally, we evaluate the desired ground navigation features with a prototype application in both a large display (LD) and a HMD navigation scenarios. We highlight several recommendations for designing a particular mid-air ground navigation technique for a LD and a HMD.
C1 [Punpongsanon, Parinya; Iwai, Daisuke; Sato, Kosuke] Osaka Univ, Grad Sch Engn Sci, Toyonaka, Osaka 5608531, Japan.
   [Guy, Emilie; Boubekeur, Tamy] Univ Paris Saclay, CNRS, LTCI, Telecom ParisTech, F-75013 Paris, France.
C3 Osaka University; Universite Paris Saclay; IMT - Institut Mines-Telecom;
   Institut Polytechnique de Paris; Telecom Paris; Centre National de la
   Recherche Scientifique (CNRS)
RP Punpongsanon, P (corresponding author), Osaka Univ, Grad Sch Engn Sci, Toyonaka, Osaka 5608531, Japan.
EM parinya@sens.sys.es.osaka-u.ac.jp; emilie.guy@telecom-paristech.fr;
   daisuke.iwai@sys.es.osaka-u.ac.jp; sato@sys.es.osaka-u.ac.jp;
   tamy.boubekeur@telecom-paristech.fr
RI Iwai, Daisuke/R-8174-2019; PUNPONGSANON, PARINYA/B-4884-2013
OI Punpongsanon, Parinya/0000-0003-2720-7768; Iwai,
   Daisuke/0000-0002-3493-5635
FU JSPS KAKENHI [25540083, 16J00049]; European Commission [FP7-323567];
   Grants-in-Aid for Scientific Research [16J00049, 25540083] Funding
   Source: KAKEN
FX This work has been supported by the JSPS KAKENHI grant number 25540083,
   and 16J00049, and the European Commission under contracts FP7-323567
   (HARVEST4D).
NR 20
TC 13
Z9 14
U1 1
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2017
VL 23
IS 8
BP 1952
EP 1963
DI 10.1109/TVCG.2016.2586071
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EZ8LR
UT WOS:000404977900006
PM 27362981
DA 2025-03-07
ER

PT J
AU Ament, M
   Zirr, T
   Dachsbacher, C
AF Ament, Marco
   Zirr, Tobias
   Dachsbacher, Carsten
TI Extinction-Optimized Volume Illumination
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Direct volume rendering; volume illumination; extinction optimization
ID LIGHTING-DESIGN; MODELS
AB We present a novel method to optimize the attenuation of light for the single scattering model in direct volume rendering. A common problem of single scattering is the high dynamic range between lit and shadowed regions due to the exponential attenuation of light along a ray. Moreover, light is often attenuated too strong between a sample point and the camera, hampering the visibility of important features. Our algorithm employs an importance function to selectively illuminate important structures and make them visible from the camera. With the importance function, more light can be transmitted to the features of interest, while contextual structures cast shadows which provide visual cues for perception of depth. At the same time, more scattered light is transmitted from the sample point to the camera to improve the primary visibility of important features. We formulate a minimization problem that automatically determines the extinction along a view or shadow ray to obtain a good balance between sufficient transmittance and attenuation. In contrast to previous approaches, we do not require a computationally expensive solution of a global optimization, but instead provide a closed-form solution for each sampled extinction value along a view or shadow ray and thus achieve interactive performance.
C1 [Ament, Marco; Zirr, Tobias] Karlsruhe Inst Technol, Karlsruhe, Germany.
   [Dachsbacher, Carsten] Karlsruhe Inst Technol, Comp Sci, Karlsruhe, Germany.
C3 Helmholtz Association; Karlsruhe Institute of Technology; Helmholtz
   Association; Karlsruhe Institute of Technology
RP Ament, M (corresponding author), Karlsruhe Inst Technol, Karlsruhe, Germany.
EM ament@kit.edu; tobais.zirr@kit.edu; dachsbacher@kit.edu
NR 62
TC 6
Z9 6
U1 1
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2017
VL 23
IS 7
BP 1767
EP 1781
DI 10.1109/TVCG.2016.2569080
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EW7OV
UT WOS:000402705000004
PM 27214903
DA 2025-03-07
ER

PT J
AU Dunn, D
   Tippets, C
   Torell, K
   Kellnhofer, P
   Aksit, K
   Didyk, P
   Myszkowski, K
   Luebke, D
   Fuchs, H
AF Dunn, David
   Tippets, Cary
   Torell, Kent
   Kellnhofer, Petr
   Aksit, Kaan
   Didyk, Piotr
   Myszkowski, Karol
   Luebke, David
   Fuchs, Henry
TI Wide Field Of View Varifocal Near-Eye Display Using See-Through
   Deformable Membrane Mirrors
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 19th IEEE Virtual Reality Conference (VR)
CY MAR 18-22, 2017
CL Los Angeles, CA
SP IEEE, IEEE Comp Soc, IEEE Comp Soc, Visualizat & Graph Tech Comm
DE Terms Augmented reality; displays; focus accommodation; perception; user
   study
ID DEPTH-OF-FOCUS; VERGENCE; DYNAMICS
AB Accommodative depth cues, a wide field of view, and ever-higher resolutions all present major hardware design challenges for near-eye displays. Optimizing a design to overcome one of these challenges typically leads to a trade-off in the others. We tackle this problem by introducing an all-in-one solution a new wide field of view, gaze-tracked near-eye display for augmented reality applications. The key component of our solution is the use of a single see-through, varifocal deformable membrane mirror for each eye reflecting a display. They are controlled by airtight cavities and change the effective focal power to present a virtual image at a target depth plane which is determined by the gaze tracker. The benefits of using the membranes include wide field of view (100 diagonal) and fast depth switching (from 20 cm to infinity within 300 ms). Our subjective experiment verifies the prototype and demonstrates its potential benefits for near-eye see-through displays.
C1 [Dunn, David; Tippets, Cary; Torell, Kent; Fuchs, Henry] UNC, Chapel Hill, NC USA.
   [Kellnhofer, Petr; Myszkowski, Karol] MPI Informat, Saarbrucken, Germany.
   [Aksit, Kaan; Luebke, David] NVIDIA Res, Preston, Lancs, England.
   [Didyk, Piotr] Univ Saarland, MMCI, D-66123 Saarbrucken, Germany.
C3 University of North Carolina; University of North Carolina Chapel Hill;
   Max Planck Society; Saarland University
RP Dunn, D (corresponding author), UNC, Chapel Hill, NC USA.
EM dunn@unc.edu; tippetsc@ad.unc.edu; torell@live.unc.edu;
   pkellnho@mpi-inf.mpg.de; kaksit@nvidia.com; pdidyk@mmci.uni-saarland.de;
   karol@mpi-inf.mpg.de; dluebke@nvidia.com; fuchs@cs.unc.edu
RI Aksit, Kaan/AAY-6704-2020
OI AKSIT, KAAN/0000-0002-5934-5500; Didyk, Piotr/0000-0003-0768-8939
NR 43
TC 131
Z9 150
U1 0
U2 31
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2017
VL 23
IS 4
BP 1275
EP 1284
DI 10.1109/TVCG.2017.2657058
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EO0QU
UT WOS:000396403800003
PM 28129167
DA 2025-03-07
ER

PT J
AU Narita, G
   Watanabe, Y
   Ishikawa, M
AF Narita, Gaku
   Watanabe, Yoshihiro
   Ishikawa, Masatoshi
TI Dynamic Projection Mapping onto Deforming Non-Rigid Surface Using
   Deformable Dot Cluster Marker
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Non-rigid surface tracking; fiducial marker; projection mapping; spatial
   augmented reality; high-speed vision
ID AUGMENTED REALITY; TRACKING; LATENCY
AB Dynamic projection mapping for moving objects has attracted much attention in recent years. However, conventional approaches have faced some issues, such as the target objects being limited to rigid objects, and the limited moving speed of the targets. In this paper, we focus on dynamic projection mapping onto rapidly deforming non-rigid surfaces with a speed sufficiently high that a human does not perceive any misalignment between the target object and the projected images. In order to achieve such projection mapping, we need a high-speed technique for tracking non-rigid surfaces, which is still a challenging problem in the field of computer vision. We propose the Deformable Dot Cluster Marker (DDCM), a novel fiducial marker for high-speed tracking of non-rigid surfaces using a high-frame-rate camera. The DDCM has three performance advantages. First, it can be detected even when it is strongly deformed. Second, it realizes robust tracking even in the presence of external and self occlusions. Third, it allows millisecond-order computational speed. Using DDCM and a high-speed projector, we realized dynamic projection mapping onto a deformed sheet of paper and a T-shirt with a speed sufficiently high that the projected images appeared to be printed on the objects.
C1 [Narita, Gaku; Watanabe, Yoshihiro; Ishikawa, Masatoshi] Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo 1138654, Japan.
C3 University of Tokyo
RP Narita, G (corresponding author), Univ Tokyo, Grad Sch Informat Sci & Technol, Tokyo 1138654, Japan.
EM gaku_narita@ipc.i.u-tokyo.ac.jp; yoshihiro_watanabe@ipc.i.u-tokyo.ac.jp;
   masatoshi_ishikawa@ipc.i.u-tokyo.ac.jp
OI Ishikawa, Masatoshi/0000-0002-6096-830X
NR 51
TC 101
Z9 106
U1 0
U2 28
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2017
VL 23
IS 3
BP 1235
EP 1248
DI 10.1109/TVCG.2016.2592910
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM8CR
UT WOS:000395539300009
PM 27448365
DA 2025-03-07
ER

PT J
AU Ceneda, D
   Gschwandtner, T
   May, T
   Miksch, S
   Schulz, HJ
   Streit, M
   Tominski, C
AF Ceneda, Davide
   Gschwandtner, Theresia
   May, Thorsten
   Miksch, Silvia
   Schulz, Hans-Jorg
   Streit, Marc
   Tominski, Christian
TI Characterizing Guidance in Visual Analytics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Visual analytics; guidance model; assistance; user support
ID DESIGN; NAVIGATION
AB Visual analytics (VA) is typically applied in scenarios where complex data has to be analyzed. Unfortunately, there is a natural correlation between the complexity of the data and the complexity of the tools to study them. An adverse effect of complicated tools is that analytical goals are more difficult to reach. Therefore, it makes sense to consider methods that guide or assist users in the visual analysis process. Several such methods already exist in the literature, yet we are lacking a general model that facilitates in-depth reasoning about guidance. We establish such a model by extending van Wijk's model of visualization with the fundamental components of guidance. Guidance is defined as a process that gradually narrows the gap that hinders effective continuation of the data analysis. We describe diverse inputs based on which guidance can be generated and discuss different degrees of guidance and means to incorporate guidance into VA tools. We use existing guidance approaches from the literature to illustrate the various aspects of our model. As a conclusion, we identify research challenges and suggest directions for future studies. With our work we take a necessary step to pave the way to a systematic development of guidance techniques that effectively support users in the context of VA.
C1 [Ceneda, Davide] Theresia Gschwandtner, Vienna, Austria.
   [Miksch, Silvia] Vienna Univ Technol, Vienna, Austria.
   [May, Thorsten] Fraunhofer IGD, Darmstadt, Germany.
   [Streit, Marc] Johannes Kepler Univ Linz, Linz, Austria.
   [Schulz, Hans-Jorg; Tominski, Christian] Univ Rostock, Rostock, Germany.
C3 Technische Universitat Wien; Johannes Kepler University Linz; University
   of Rostock
RP Ceneda, D (corresponding author), Theresia Gschwandtner, Vienna, Austria.
EM davide.ceneda@tuwien.ac.at; theresia.gschwandtner@tuwien.ac.at;
   thorsten.may@igd.fraunhofer.de; silvia.miksch@tuwien.ac.at;
   hjschulz@informatik.uni-rostock.de; streit@jku.at;
   ct@informatik.uni-rostock.de
RI Ceneda, Davide/HTT-2753-2023; Tominski, Christian/H-6388-2019; Schulz,
   Hans-Jorg/G-1788-2013
OI Streit, Marc/0000-0001-9186-2092; Gschwandtner,
   Theresia/0000-0002-9555-3374; Ceneda, Davide/0000-0003-1198-567X;
   Schulz, Hans-Jorg/0000-0001-9974-535X; Miksch,
   Silvia/0000-0003-4427-5703
FU Centre for Visual Analytics Science and Technology CVAST; Austrian
   Federal Ministry of Science, Research, and Economy [822746]; State of
   Upper Austria (FFG) [851460]
FX We thank the participants of the Rostock Workshop on Emerging Topics in
   Visualization and Computer Graphics 2013 (WET-VCG) for valuable initial
   discussions on guidance. The presented research drew inspiration from
   the Dagstuhl Seminar 13352 on Interaction with Information for Visual
   Reasoning. This work was supported by the Centre for Visual Analytics
   Science and Technology CVAST, funded by the Austrian Federal Ministry of
   Science, Research, and Economy in the exceptional Laura Bassi Centres of
   Excellence initiative (#822746). Further support has been received from
   the State of Upper Austria under grant number (FFG #851460). Finally, we
   thank the anonymous reviewers for their helpful comments and valuable
   feedback.
NR 53
TC 155
Z9 165
U1 1
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 111
EP 120
DI 10.1109/TVCG.2016.2598468
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600014
PM 27514054
DA 2025-03-07
ER

PT J
AU von Landesberger, T
   Basgier, D
   Becker, M
AF von Landesberger, Tatiana
   Basgier, Dennis
   Becker, Meike
TI Comparative Local Quality Assessment of 3D Medical Image Segmentations
   with Focus on Statistical Shape Model-Based Algorithms
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual analytics; 3D medical image segmentation quality; comparison;
   clustering; statistical shape models
ID VISUAL ANALYTICS; BOX
AB The quality of automatic 3D medical segmentation algorithms needs to be assessed on test datasets comprising several 3D images (i.e., instances of an organ). The experts need to compare the segmentation quality across the dataset in order to detect systematic segmentation problems. However, such comparative evaluation is not supported well by current methods. We present a novel system for assessing and comparing segmentation quality in a dataset with multiple 3D images. The data is analyzed and visualized in several views. We detect and show regions with systematic segmentation quality characteristics. For this purpose, we extended a hierarchical clustering algorithm with a connectivity criterion. We combine quality values across the dataset for determining regions with characteristic segmentation quality across instances. Using our system, the experts can also identify 3D segmentations with extraordinary quality characteristics. While we focus on algorithms based on statistical shape models, our approach can also be applied to cases, where landmark correspondences among instances can be established. We applied our approach to three real datasets: liver, cochlea and facial nerve. The segmentation experts were able to identify organ regions with systematic segmentation characteristics as well as to detect outlier instances.
C1 [von Landesberger, Tatiana] Tech Univ Darmstadt, Visual Search & Anal Grp, Interact Graph Syst Grp, Darmstadt, Germany.
   [Basgier, Dennis] Tech Univ Darmstadt, Visual Comp, Darmstadt, Germany.
   [Becker, Meike] Tech Univ Darmstadt, Interact Graph Syst Grp, Darmstadt, Germany.
C3 Technical University of Darmstadt; Technical University of Darmstadt;
   Technical University of Darmstadt
RP von Landesberger, T (corresponding author), Tech Univ Darmstadt, Visual Search & Anal Grp, Interact Graph Syst Grp, Darmstadt, Germany.
EM tatiana.von-landesberger@gris.tu-darmstadt.de;
   dennis.basgier@gris.tu-darmstadt.de; meike.becker@gris.tu-darmstadt.de
OI von Landesberger, Tatiana/0000-0002-5279-1444
FU German Research Foundation (DFG)
FX Work on this research has been funded by the German Research Foundation
   (DFG). The authors are thankful to Prof. Georgios Sakas and Prof. Arjan
   Kuijper for his helpfull comments and suggestions. They thank Dr.
   Matthias Kirschner for providing us with datasets and useful feedback to
   our system. The authors are grateful to the users of our system for
   helpful feedback and comments.
NR 50
TC 12
Z9 14
U1 1
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2016
VL 22
IS 12
BP 2537
EP 2549
DI 10.1109/TVCG.2015.2501813
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EB4RJ
UT WOS:000387360500004
PM 26595923
DA 2025-03-07
ER

PT J
AU Lee, S
   Hu, XD
   Hua, H
AF Lee, Sangyoon
   Hu, Xinda
   Hua, Hong
TI Effects of Optical Combiner and IPD Change for Convergence on Near-Field
   Depth Perception in an Optical See-Through HMD
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Near-field depth perception; mixed/augmented reality; optical
   see-through head-mounted display
ID HEAD-MOUNTED DISPLAY; CALIBRATION; LOCALIZATION; DESIGN; VIEW
AB Many error sources have been explored in regards to the depth perception problem in augmented reality environments using optical see-through head-mounted displays (OST-HMDs). Nonetheless, two error sources are commonly neglected: the ray-shift phenomenon and the change in interpupillary distance (IPD). The first source of error arises from the difference in refraction for virtual and see-through optical paths caused by an optical combiner, which is required of OST-HMDs. The second occurs from the change in the viewer's IPD due to eye convergence. In this paper, we analyze the effects of these two error sources on near-field depth perception and propose methods to compensate for these two types of errors. Furthermore, we investigate their effectiveness through an experiment comparing the conditions with and without our error compensation methods applied. In our experiment, participants estimated the egocentric depth of a virtual and a physical object located at seven different near-field distances (40 similar to 200 cm) using a perceptual matching task. Although the experimental results showed different patterns depending on the target distance, the results demonstrated that the near-field depth perception error can be effectively reduced to a very small level (at most 1 percent error) by compensating for the two mentioned error sources.
C1 [Lee, Sangyoon; Hu, Xinda; Hua, Hong] Univ Arizona, Coll Opt Sci, Tucson, AZ 85721 USA.
C3 University of Arizona
RP Lee, S; Hu, XD; Hua, H (corresponding author), Univ Arizona, Coll Opt Sci, Tucson, AZ 85721 USA.
EM sylee@optics.arizona.edu; xhu@optics.arizona.edu;
   hhua@optics.arizona.edu
RI Lee, Sangyoon/D-2884-2011; Hu, Xinda/G-7203-2016
OI Hua, Hong/0000-0002-7255-610X; Hu, Xinda/0000-0002-7030-8305
FU National Science Foundation (NSF) [0915035, 1115489]; Direct For
   Computer & Info Scie & Enginr; Div Of Information & Intelligent Systems
   [0915035, 1115489] Funding Source: National Science Foundation
FX This work was supported by the National Science Foundation (NSF) Grant
   awards 0915035 and 1115489. We thank Mr. Jason Kuhn and Mr. Leonard D.
   Brown for helping to calculate the ray-shift amounts and proofreading
   the manuscript, respectively.
NR 33
TC 16
Z9 22
U1 0
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2016
VL 22
IS 5
BP 1540
EP 1554
DI 10.1109/TVCG.2015.2440272
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DH7ZH
UT WOS:000373012300005
PM 27045910
DA 2025-03-07
ER

PT J
AU Byrne, L
   Angus, D
   Wiles, J
AF Byrne, Lydia
   Angus, Daniel
   Wiles, Janet
TI Acquired Codes of Meaning in Data Visualization and Infographics: Beyond
   Perceptual Primitives
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Visual Design; Taxonomies; Illustrative Visualization; Design
   Methodologies
ID INFORMATION VISUALIZATION
AB While information visualization frameworks and heuristics have traditionally been reluctant to include acquired codes of meaning, designers are making use of them in a wide variety of ways. Acquired codes leverage a user's experience to understand the meaning of a visualization. They range from figurative visualizations which rely on the reader's recognition of shapes, to conventional arrangements of graphic elements which represent particular subjects. In this study, we used content analysis to codify acquired meaning in visualization. We applied the content analysis to a set of infographics and data visualizations which are exemplars of innovative and effective design. 88% of the infographics and 71% of data visualizations in the sample contain at least one use of figurative visualization. Conventions on the arrangement of graphics are also widespread in the sample. In particular, a comparison of representations of time and other quantitative data showed that conventions can be specific to a subject. These results suggest that there is a need for information visualization research to expand its scope beyond perceptual channels, to include social and culturally constructed meaning. Our paper demonstrates a viable method for identifying figurative techniques and graphic conventions and integrating them into heuristics for visualization design.
C1 [Byrne, Lydia; Angus, Daniel; Wiles, Janet] Univ Queensland, Brisbane, Qld 4072, Australia.
C3 University of Queensland
RP Byrne, L (corresponding author), Univ Queensland, Brisbane, Qld 4072, Australia.
EM l.byrne2@uq.edu.au; d.angus@uq.edu.au; j.wiles@uq.edu.au
RI Angus, Daniel/C-8163-2009; wiles, j/KBA-9055-2024; Wiles,
   Janet/C-1989-2008
OI Angus, Daniel/0000-0002-1412-5096; Wiles, Janet/0000-0002-4051-4116
FU Australian Defence Science Technology Organisation; Australian
   Postgraduate Award; Australian Research Council
FX The authors would like to thank the reviewers for their contribution.
   The authors are members of the Centre of Excellence for the Dynamics of
   Language. This research was supported by the Australian Defence Science
   Technology Organisation and an Australian Postgraduate Award to Lydia
   Byrne, and Australian Research Council grants to Janet Wiles.
NR 61
TC 22
Z9 26
U1 0
U2 32
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 509
EP 518
DI 10.1109/TVCG.2015.2467321
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400056
PM 26529716
DA 2025-03-07
ER

PT J
AU Hao, LH
   Healey, CG
   Bass, SA
AF Hao, Lihua
   Healey, Christopher G.
   Bass, Steffen A.
TI Effective Visualization of Temporal Ensembles
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Ensemble visualization
ID VISUAL ANALYSIS
AB An ensemble is a collection of related datasets, called members, built from a series of runs of a simulation or an experiment. Ensembles are large, temporal, multidimensional, and multivariate, making them difficult to analyze. Another important challenge is visualizing ensembles that vary both in space and time. Initial visualization techniques displayed ensembles with a small number of members, or presented an overview of an entire ensemble, but without potentially important details. Recently, researchers have suggested combining these two directions, allowing users to choose subsets of members to visualization. This manual selection process places the burden on the user to identify which members to explore. We first introduce a static ensemble visualization system that automatically helps users locate interesting subsets of members to visualize. We next extend the system to support analysis and visualization of temporal ensembles. We employ 3D shape comparison, cluster tree visualization, and glyph based visualization to represent different levels of detail within an ensemble. This strategy is used to provide two approaches for temporal ensemble analysis: (1) segment based ensemble analysis, to capture important shape transition time-steps, clusters groups of similar members, and identify common shape changes over time across multiple members; and (2) time-step based ensemble analysis, which assumes ensemble members are aligned in time by combining similar shapes at common time-steps. Both approaches enable users to interactively visualize and analyze a temporal ensemble from different perspectives at different levels of detail. We demonstrate our techniques on an ensemble studying matter transition from hadronic gas to quark-gluon plasma during gold-on-gold particle collisions.
C1 [Hao, Lihua; Healey, Christopher G.] NC State Univ, Raleigh, NC 27695 USA.
   [Bass, Steffen A.] Duke Univ, Durham, NC 27706 USA.
C3 North Carolina State University; Duke University
RP Hao, LH (corresponding author), NC State Univ, Raleigh, NC 27695 USA.
EM lhao2@ncsu.edu; healey@ncsu.edu; bass@phy.duke.edu
RI Healey, Christopher/ABH-9682-2020; Bass, Steffen/AAB-9800-2020
NR 28
TC 31
Z9 38
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 787
EP 796
DI 10.1109/TVCG.2015.2468093
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400084
PM 26529728
DA 2025-03-07
ER

PT J
AU Marino, J
   Kaufman, A
AF Marino, Joseph
   Kaufman, Arie
TI Planar Visualization of Treelike Structures
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Geometry-based techniques; view-dependent visualization; medical
   visualization; planar embedding
ID VIRTUAL ENDOSCOPY; CURVE; MAPS; EXTRACTION; PATH; WALL
AB We present a novel method to create planar visualizations of treelike structures (e.g., blood vessels and airway trees) where the shape of the object is well preserved, allowing for easy recognition by users familiar with the structures. Based on the extracted skeleton within the treelike object, a radial planar embedding is first obtained such that there are no self-intersections of the skeleton which would have resulted in occlusions in the final view. An optimization procedure which adjusts the angular positions of the skeleton nodes is then used to reconstruct the shape as closely as possible to the original, according to a specified view plane, which thus preserves the global geometric context of the object. Using this shape recovered embedded skeleton, the object surface is then flattened to the plane without occlusions using harmonic mapping. The boundary of the mesh is adjusted during the flattening step to account for regions where the mesh is stretched over concavities. This parameterized surface can then be used either as a map for guidance during endoluminal navigation or directly for interrogation and decision making. Depth cues are provided with a grayscale border to aid in shape understanding. Examples are presented using bronchial trees, cranial and lower limb blood vessels, and upper aorta datasets, and the results are evaluated quantitatively and with a user study.
C1 [Marino, Joseph; Kaufman, Arie] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; Stony Brook University
RP Marino, J (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM jmarino@cs.stonybrook.edu; ari@cs.stonybrook.edu
FU NSF [CNS-0959979, IIP-1069147, CNS-1302246]; Division Of Computer and
   Network Systems; Direct For Computer & Info Scie & Enginr [1302246]
   Funding Source: National Science Foundation
FX This work was supported in part by NSF grants CNS-0959979, IIP-1069147,
   and CNS-1302246. Bronchial CT scans courtesy of the EXACT'09 study,
   lower limb vessels courtesy of OsiriX, upper aorta courtesy of the
   Visible Korean Human Project, and cranial vessels courtesy of VolVis. We
   wish to thank Dr. Matthew Barish from Stony Brook University Hospital
   for his feedback on our work.
NR 41
TC 14
Z9 20
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 906
EP 915
DI 10.1109/TVCG.2015.2467413
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400096
PM 26529735
DA 2025-03-07
ER

PT J
AU Bradley, D
   Strain, G
   Jay, C
   Stewart, AJ
AF Bradley, Duncan
   Strain, Gabriel
   Jay, Caroline
   Stewart, Andrew J.
TI Magnitude Judgements are Influenced by the Relative Positions of Data
   Points Within Axis Limits
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Bars; Computer science; Strain; Sensitivity;
   Encoding; Visualization; Magnitude; axis manipulation; cognition; bias;
   framing effects
ID GRAPH LITERACY; PROBABILITY
AB When visualising data, chart designers have the freedom to choose the upper and lower limits of numerical axes. Axis limits can determine the physical characteristics of plotted values, such as the physical position of data points in dot plots. In two experiments (total N=300), we demonstrate that axis limits affect viewers' interpretations of the magnitudes of plotted values. Participants did not simply associate values presented at higher vertical positions with greater magnitudes. Instead, participants considered the relative positions of data points within the axis limits. Data points were considered to represent larger values when they were closer to the end of the axis associated with greater values, even when they were presented at the bottom of a chart. This provides further evidence of framing effects in the display of data, and offers insight into the cognitive mechanisms involved in assessing magnitude in data visualisations.
C1 [Bradley, Duncan] Univ Manchester, Div Psychol Commun & Human Neurosci, Manchester M13 9PL, England.
   [Strain, Gabriel; Jay, Caroline; Stewart, Andrew J.] Univ Manchester, Dept Comp Sci, Manchester M13 9PL, England.
C3 University of Manchester; University of Manchester
RP Bradley, D (corresponding author), Univ Manchester, Div Psychol Commun & Human Neurosci, Manchester M13 9PL, England.
EM duncan.bradley@manchester.ac.uk; gabriel.strain@manchester.ac.uk;
   caroline.jay@manchester.ac.uk; andrew.stewart@manchester.ac.uk
RI Stewart, Andrew/D-4758-2009
OI Strain, Gabriel/0000-0002-4769-9221; Stewart,
   Andrew/0000-0002-9795-4104; Jay, Caroline/0000-0002-6080-1382; Bradley,
   Duncan/0000-0001-7328-8779
FU Economic and Social Research Council [ES/P000665/1]
FX This work was supported by Economic and Social Research Council under
   Grant ES/P000665/1.
NR 34
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2025
VL 31
IS 2
BP 1414
EP 1421
DI 10.1109/TVCG.2024.3364069
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA R6W2Y
UT WOS:001392823200021
PM 38329854
DA 2025-03-07
ER

PT J
AU Monica, R
   Rizzini, DL
   Aleotti, J
AF Monica, Riccardo
   Rizzini, Dario Lodi
   Aleotti, Jacopo
TI Adaptive Complementary Filter for Hybrid Inside-Out Outside-In HMD
   Tracking With Smooth Transitions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Tracking; Resists; Kalman filters; Sensor fusion; Legged locomotion;
   User experience; Information filters; Head mounted displays; room-scale
   virtual reality; sensor fusion; tracking technologies
ID EXTENDED KALMAN FILTER; VISION
AB Head-mounted displays (HMDs) in room-scale virtual reality are usually tracked using inside-out visual SLAM algorithms. Alternatively, to track the motion of the HMD with respect to a fixed real-world reference frame, an outside-in instrumentation like a motion capture system can be adopted. However, outside-in tracking systems may temporarily lose tracking as they suffer by occlusion and blind spots. A possible solution is to adopt a hybrid approach where the inside-out tracker of the HMD is augmented with an outside-in sensing system. On the other hand, when the tracking signal of the outside-in system is recovered after a loss of tracking the transition from inside-out tracking to hybrid tracking may generate a discontinuity, i.e a sudden change of the virtual viewpoint, that can be uncomfortable for the user. Therefore, hybrid tracking solutions for HMDs require advanced sensor fusion algorithms to obtain a smooth transition. This work proposes a method for hybrid tracking of a HMD with smooth transitions based on an adaptive complementary filter. The proposed approach can be configured with several parameters that determine a trade-off between user experience and tracking error. A user study was carried out in a room-scale virtual reality environment, where users carried out two different tasks while multiple signal tracking losses of the outside-in sensor system occurred. The results show that the proposed approach improves user experience compared to a standard Extended Kalman Filter, and that tracking error is lower compared to a state-of-the-art complementary filter when configured for the same quality of user experience.
C1 [Monica, Riccardo; Rizzini, Dario Lodi; Aleotti, Jacopo] Univ Parma, Dept Engn & Architecture, Parco Area Sci, I-43124 Parma, Italy.
C3 University of Parma
RP Monica, R (corresponding author), Univ Parma, Dept Engn & Architecture, Parco Area Sci, I-43124 Parma, Italy.
EM riccardo.monica@unipr.it; dario.lodirizzini@unipr.it;
   jacopo.aleotti@unipr.it
RI ; Lodi Rizzini, Dario/K-8557-2015
OI Monica, Riccardo/0000-0002-1262-6348; Lodi Rizzini,
   Dario/0000-0002-3904-0638
FU University of Parma; MUR-Italian Ministry ofUniversities and Research
   [D.M. 737/2021]
FX This work was supported by University of Parma through the action "Bando
   diAteneo 2021 per la ricerca", which was co-funded by MUR-Italian
   Ministry ofUniversities and Research - D.M. 737/2021 - PNR - PNRR -
   NextGenerationEU.
NR 43
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2025
VL 31
IS 2
BP 1598
EP 1612
DI 10.1109/TVCG.2024.3464738
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA R6W2Y
UT WOS:001392823200009
PM 39298310
OA hybrid
DA 2025-03-07
ER

PT J
AU Wu, D
   Li, Z
   Ansari, MHD
   Ha, XT
   Ourak, M
   Dankelman, J
   Menciassi, A
   De Momi, E
   Poorten, EV
AF Wu, Di
   Li, Zhen
   Ansari, Mohammad Hasan Dad
   Ha, Xuan Thao
   Ourak, Mouloud
   Dankelman, Jenny
   Menciassi, Arianna
   De Momi, Elena
   Poorten, Emmanuel Vander
TI Comparative Analysis of Interactive Modalities for Intuitive
   Endovascular Interventions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Catheters; Three-dimensional displays; Visualization; Robots;
   Navigation; Resists; Input devices; Augmented reality; catheter
   navigation; endovascular intervention; robotic catheter; user study
ID PERCUTANEOUS CORONARY INTERVENTION; CATHETER; VISUALIZATION; ROBOT
AB Endovascular intervention is a minimally invasive method for treating cardiovascular diseases. Although fluoroscopy, known for real-time catheter visualization, is commonly used, it exposes patients and physicians to ionizing radiation and lacks depth perception due to its 2D nature. To address these limitations, a study was conducted using teleoperation and 3D visualization techniques. This in-vitro study involved the use of a robotic catheter system and aimed to evaluate user performance through both subjective and objective measures. The focus was on determining the most effective modes of interaction. Three interactive modes for guiding robotic catheters were compared in the study: 1) Mode GM, using a gamepad for control and a standard 2D monitor for visual feedback; 2) Mode GH, with a gamepad for control and HoloLens providing 3D visualization; and 3) Mode HH, where HoloLens serves as both control input and visualization device. Mode GH outperformed other modalities in subjective metrics, except for mental demand. It exhibited a median tracking error of 4.72 mm, a median targeting error of 1.01 mm, a median duration of 82.34 s, and a median natural logarithm of dimensionless squared jerk of 40.38 in the in-vitro study. Mode GH showed 8.5%, 4.7%, 6.5%, and 3.9% improvements over Mode GM and 1.5%, 33.6%, 34.9%, and 8.1% over Mode HH for tracking error, targeting error, duration, and dimensionless squared jerk, respectively. To sum up, the user study emphasizes the potential benefits of employing HoloLens for enhanced 3D visualization in catheterization. The user study also illustrates the advantages of using a gamepad for catheter teleoperation, including user-friendliness and passive haptic feedback, compared to HoloLens. To further gauge the potential of using a more traditional joystick as a control input device, an additional study utilizing the Haption Virtuose robot was conducted. It reveals the potential for achieving smoother trajectories, with a 38.9% reduction in total path length compared to a gamepad, potentially due to its larger range of motion and single-handed control.
C1 [Wu, Di; Ansari, Mohammad Hasan Dad; Ha, Xuan Thao; Ourak, Mouloud; Poorten, Emmanuel Vander] Katholieke Univ Leuven, Dept Mech Engn, B-3000 Leuven, Belgium.
   [Wu, Di; Dankelman, Jenny] Delft Univ Technol, Fac 3mE, NL-2628 CD Delft, Netherlands.
   [Li, Zhen; De Momi, Elena] Politecn Milan, Dept Elect Informat & Bioengn, I-20133 Milan, Italy.
   [Ansari, Mohammad Hasan Dad; Menciassi, Arianna] Scuola Super Sant Anna, BioRobot Inst, I-56127 Pisa, Italy.
   [Ansari, Mohammad Hasan Dad; Ha, Xuan Thao; Menciassi, Arianna] Scuola Super Sant Anna, Dept Excellence Robot & AI, I-56127 Pisa, Italy.
C3 KU Leuven; Delft University of Technology; Polytechnic University of
   Milan; Scuola Superiore Sant'Anna; Scuola Superiore Sant'Anna
RP Li, Z (corresponding author), Politecn Milan, Dept Elect Informat & Bioengn, I-20133 Milan, Italy.
EM di.wu@kuleuven.be; zhen.li@polimi.it; hasan.mohammad@santannapisa.it;
   xuanthao.ha@kuleuven.be; mouloud.ourak@kuleuven.be;
   j.dankelman@tudelft.nl; arianna.menciassi@santannapisa.it;
   elena.demomi@polimi.it; emmanuel.vanderpoorten@kuleuven.be
RI Wu, Di/AAB-6141-2022; Li, Zhen/JDD-5941-2023; Mohammad, Hasan Dad
   Ansari/JMP-9302-2023; Vander Poorten, Emmanuel/O-7273-2016; Dankelman,
   Jenny/H-9100-2017; DE MOMI, ELENA/D-7375-2016
OI Li, Zhen/0000-0001-6959-7608; Ha, Xuan Thao/0000-0001-9184-6164; Ansari,
   Mohammad Hasan Dad/0000-0002-8450-1931; Vander Poorten,
   Emmanuel/0000-0003-3764-9551; Dankelman, Jenny/0000-0003-3951-2129; DE
   MOMI, ELENA/0000-0002-8819-2734; Ourak, mouloud/0000-0003-4885-8965; WU,
   Di/0000-0002-7585-8912
FU ATLAS project; European Union [813782, 101017140]; ARTERY project;
   Postdoctoral Mandates (PDM), an internal fund of KU Leuven [3E230593];
   CURE project internal KU Leuven fund [3E210658]
FX This work was supported in part by the ATLAS project. This work was
   supported in part by the European Union's Horizon 2020 research and
   innovation programme under the Marie Sklodowska-Curie under Grant
   813782. This work was also supported in part by the European Union's
   Horizon 2020 research and innovation programme under Grant 101017140, in
   part by the ARTERY project. Moreover, this research was funded in part
   by the Postdoctoral Mandates (PDM), an internal fund of KU Leuven under
   Grant 3E230593, and in part by the CURE project, another internal KU
   Leuven fund, with under grant 3E210658.
NR 46
TC 2
Z9 2
U1 1
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2025
VL 31
IS 2
BP 1371
EP 1388
DI 10.1109/TVCG.2024.3362628
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA R6W2Y
UT WOS:001392823200020
PM 38319759
OA Bronze
DA 2025-03-07
ER

PT J
AU Larionov, E
   Longva, A
   Ascher, UM
   Bender, J
   Pai, DK
AF Larionov, Egor
   Longva, Andreas
   Ascher, Uri M.
   Bender, Jan
   Pai, Dinesh K.
TI Implicit Frictional Dynamics With Soft Constraints
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Friction; Mathematical models; Accuracy; Force; Computational modeling;
   Solid modeling; Optimization; Dry friction; contact; elasticity;
   deformable object dynamics
ID DRY FRICTION; DESIGN
AB Dynamics simulation with frictional contacts is important for a wide range of applications, from cloth simulation to object manipulation. Recent methods using smoothed lagged friction forces have enabled robust and differentiable simulation of elastodynamics with friction. However, the resulting frictional behavior can be inaccurate and may not converge to analytic solutions. Here we evaluate the accuracy of lagged friction models in comparison with implicit frictional contact systems. We show that major inaccuracies near the stick-slip threshold in such systems are caused by lagging of friction forces rather than by smoothing the Coulomb friction curve. Furthermore, we demonstrate how systems involving implicit or lagged friction can be correctly used with higher-order time integration and highlight limitations in earlier attempts. We demonstrate how to exploit forward-mode automatic differentiation to simplify and, in some cases, improve the performance of the inexact Newton method. Finally, we show that other complex phenomena can also be simulated effectively while maintaining smoothness of the entire system. We extend our method to exhibit stick-slip frictional behavior and preserve volume on compressible and nearly-incompressible media using soft constraints.
C1 [Larionov, Egor; Ascher, Uri M.; Pai, Dinesh K.] Univ British Columbia, Comp Sci, Vancouver, BC V6T 1Z4, Canada.
   [Longva, Andreas; Bender, Jan] Rhein Westfal TH Aachen, Comp Sci, D-52056 Aachen, Germany.
   [Pai, Dinesh K.] Vital Mech Res, Vancouver, BC V6T 1Z4, Canada.
C3 University of British Columbia; RWTH Aachen University
RP Larionov, E (corresponding author), Univ British Columbia, Comp Sci, Vancouver, BC V6T 1Z4, Canada.
EM egor.larionov@gmail.com; longva@cs.rwth-aachen.de; ascher@cs.ubc.ca;
   bender@cs.rwth-aachen.de; pai@cs.ubc.ca
OI Longva, Andreas/0000-0002-6665-8302; Pai, Dinesh K/0000-0002-5115-7230;
   Larionov, Egor/0000-0002-9900-3150
FU Natural Sciences and Engineering Research Council Discovery Grant;
   Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) [BE
   5132/5-1]
FX This work was supported in part by the Natural Sciences and Engineering
   Research Council Discovery Grant , and in part by the Deutsche
   Forschungsgemeinschaft (DFG, German Research Foundation) under Grant BE
   5132/5-1.
NR 48
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2024
VL 30
IS 12
BP 7776
EP 7787
DI 10.1109/TVCG.2024.3437417
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K8D1L
UT WOS:001346124800008
PM 39093677
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhai, HJ
   Huang, G
   Hu, QR
   Li, GL
   Bao, HJ
   Zhang, GF
AF Zhai, Hongjia
   Huang, Gan
   Hu, Qirui
   Li, Guanglin
   Bao, Hujun
   Zhang, Guofeng
TI NIS-SLAM: Neural Implicit Semantic RGB-D SLAM for 3D Consistent Scene
   Understanding
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Implicit representation; Neural dense SLAM; Semantic segmentation;
   Semantic segmentation; Scene understanding; Scene understanding;
   Semantic segmentation; Scene understanding
AB In recent years, the paradigm of neural implicit representations has gained substantial attention in the field of Simultaneous Localization and Mapping (SLAM). However, a notable gap exists in the existing approaches when it comes to scene understanding. In this paper, we introduce NIS-SLAM, an efficient neural implicit semantic RGB-D SLAM system, that leverages a pre-trained 2D segmentation network to learn consistent semantic representations. Specifically, for high-fidelity surface reconstruction and spatial consistent scene understanding, we combine high-frequency multi-resolution tetrahedron-based features and low-frequency positional encoding as the implicit scene representations. Besides, to address the inconsistency of 2D segmentation results from multiple views, we propose a fusion strategy that integrates the semantic probabilities from previous non-keyframes into keyframes to achieve consistent semantic learning. Furthermore, we implement a confidence-based pixel sampling and progressive optimization weight function for robust camera tracking. Extensive experimental results on various datasets show the better or more competitive performance of our system when compared to other existing neural dense implicit RGB-D SLAM approaches. Finally, we also show that our approach can be used in augmented reality applications.
C1 [Zhai, Hongjia; Huang, Gan; Hu, Qirui; Li, Guanglin; Bao, Hujun; Zhang, Guofeng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
C3 Zhejiang University
RP Zhang, GF (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
EM zhjsmileeveryday@gmail.com; huanggan@zju.edu.cn; 3200105912@zju.edu.cn;
   liguanglin@zju.edu.cn; baohujun@zju.edu.cn; zhangguofeng@zju.edu.cn
RI Zhang, Ge/K-9118-2019
OI Huang, Gan/0000-0001-8515-2721; Zhang, Guofeng/0000-0001-5661-8430; Bao,
   Hujun/0000-0002-2662-0334
FU NSF of China [61932003]
FX This work was partially supported by NSF of China (No.61932003).
NR 78
TC 0
Z9 0
U1 23
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7129
EP 7139
DI 10.1109/TVCG.2024.3456166
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300020
PM 39255118
DA 2025-03-07
ER

PT J
AU Huang, K
   Zhang, FL
   Zhao, JH
   Li, YH
   Dodgson, N
AF Huang, Kun
   Zhang, Fang-Lue
   Zhao, Junhong
   Li, Yiheng
   Dodgson, Neil
TI 360<SUP>°</SUP> Stereo Image Composition With Depth Adaption
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Cameras; Stereo image processing; Videos;
   Estimation; Geometry; Image synthesis; Image composition; image
   synthesis; stereoscopic panoramic image; virtual reality
AB 360(degrees) images and videos have become an economic and popular way to provide VR experiences using real-world content. However, the manipulation of the stereo panoramic content remains less explored. In this paper, we focus on the 360(degrees) image composition problem, and develop a solution that can take an object from a stereo image pair and insert it at a given 3D position in a target stereo panorama, with well-preserved geometry information. Our method uses recovered 3D point clouds to guide the composited image generation. More specifically, we observe that using only a one-off operation to insert objects into equirectangular images will never produce satisfactory depth perception and generate ghost artifacts when users are watching the result from different view directions. Therefore, we propose a novel view-dependent projection method that segments the object in 3D spherical space with the stereo camera pair facing in that direction. A deep depth densification network is proposed to generate depth guidance for the stereo image generation of each view segment according to the desired position and pose of the inserted object. We finally merge the synthesized view segments and blend the objects into the target stereo 360(degrees) scene. A user study demonstrates that our method can provide good depth perception and removes ghost artifacts. The view-dependent solution is a potential paradigm for other content manipulation methods for 360(degrees) images and videos.
C1 [Huang, Kun; Zhang, Fang-Lue; Li, Yiheng; Dodgson, Neil] Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington 6012, New Zealand.
   [Zhao, Junhong] Victoria Univ Wellington, CMIC, Wellington 6012, New Zealand.
C3 Victoria University Wellington; Victoria University Wellington
RP Zhang, FL (corresponding author), Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington 6012, New Zealand.
EM kun.huang@vuw.ac.nz; fanglue.zhang@vuw.ac.nz;
   junhong.jennifer@gmail.com; yiheng.li@vuw.ac.nz; neil.dodgson@vuw.ac.nz
OI Dodgson, Neil/0000-0001-7649-8528
FU Royal Society of New Zealand [MFP-20-VUW-180]
FX This work was supported by Marsden Fund Council managed by Royal Society
   of New Zealand under Grant MFP-20-VUW-180.
NR 62
TC 0
Z9 0
U1 4
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6177
EP 6191
DI 10.1109/TVCG.2023.3327943
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000036
PM 37889815
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Tian, H
   Zhu, CY
   Shi, YF
   Xu, K
AF Tian, Hui
   Zhu, Chenyang
   Shi, Yifei
   Xu, Kai
TI SuperUDF: Self-Supervised UDF Estimation for Surface Reconstruction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Surface reconstruction; Point cloud compression; Estimation;
   Three-dimensional displays; Shape; Geometry; Feature extraction; UDF;
   point cloud reconstruction; implicit surface
AB Learning-based surface reconstruction based on unsigned distance functions (UDF) has many advantages such as handling open surfaces. We propose SuperUDF, a self-supervised UDF learning which exploits a learned geometry prior for efficient training and a novel regularization for robustness to sparse sampling. The core idea of SuperUDF draws inspiration from the classical surface approximation operator of locally optimal projection (LOP). The key insight is that if the UDF is estimated correctly, the 3D points should be locally projected onto the underlying surface following the gradient of the UDF. Based on that, a number of inductive biases on UDF geometry and a pre-learned geometry prior are devised to learn UDF estimation efficiently. A novel regularization loss is proposed to make SuperUDF robust to sparse sampling. Furthermore, we also contribute a learning-based mesh extraction from the estimated UDFs. Extensive evaluations demonstrate that SuperUDF outperforms the state of the arts on several public datasets in terms of both quality and efficiency. Code will be released after accteptance.
C1 [Tian, Hui; Zhu, Chenyang; Shi, Yifei; Xu, Kai] Natl Univ Def Technol, Changsha 410073, Hunan, Peoples R China.
C3 National University of Defense Technology - China
RP Zhu, CY; Xu, K (corresponding author), Natl Univ Def Technol, Changsha 410073, Hunan, Peoples R China.
EM tianhui13@nudt.edu.cn; zhuchenyang07@nudt.edu.cn; yifei.j.shi@gmail.com;
   kevin.kai.xu@gmail.com
FU National Key Research and Development Program of China [2018AAA0102200];
   National Natural Science Foundation of China [62002375, 62002376,
   62132021, 62325211, 62372457, 62002379]; National Science Foundation of
   Hunan Province of China [2021JJ40696, 2021RC3071, 2022RC1104,
   2023JJ20051]; NUDT Research [ZK22-52]; Science and Technology Innovation
   Program of Hunan Province [2023RC3011]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2018AAA0102200, in part by the
   National Natural Science Foundation of China under Grants 62002375,
   62002376, 62132021, 62325211, 62372457, and 62002379, in part by the
   National Science Foundation of Hunan Province of China under Grants
   2021JJ40696, 2021RC3071, 2022RC1104, and 2023JJ20051, in part by NUDT
   Research under Grant ZK22-52, and in part by the Science and Technology
   Innovation Program of Hunan Province under Grant 2023RC3011.
NR 32
TC 1
Z9 1
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 5965
EP 5975
DI 10.1109/TVCG.2023.3318085
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000001
PM 37738188
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Dennig, FL
   Miller, M
   Keim, DA
   El-Assady, M
AF Dennig, Frederik L.
   Miller, Matthias
   Keim, Daniel A.
   El-Assady, Mennatallah
TI FS/DS: A Theoretical Framework for the Dual Analysis of Feature Space
   and Data Space
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Analytical models; Data models; Visual analytics;
   Task analysis; Adaptation models; Taxonomy; dual analysis; feature
   space; data space; feature exploration; mixed data; high-dimensional
   data
ID VISUAL ANALYTICS; QUALITY METRICS; EXPLORATION; VISUALIZATION;
   GENERATION; TYPOLOGY; GUIDANCE; MODEL
AB With the surge of data-driven analysis techniques, there is a rising demand for enhancing the exploration of large high-dimensional data by enabling interactions for the joint analysis of features (i.e., dimensions). Such a dual analysis of the feature space and data space is characterized by three components, 1) a view visualizing feature summaries, 2) a view that visualizes the data records, and 3) a bidirectional linking of both plots triggered by human interaction in one of both visualizations, e.g., Linking & Brushing. Dual analysis approaches span many domains, e.g., medicine, crime analysis, and biology. The proposed solutions encapsulate various techniques, such as feature selection or statistical analysis. However, each approach establishes a new definition of dual analysis. To address this gap, we systematically reviewed published dual analysis methods to investigate and formalize the key elements, such as the techniques used to visualize the feature space and data space, as well as the interaction between both spaces. From the information elicited during our review, we propose a unified theoretical framework for dual analysis, encompassing all existing approaches extending the field. We apply our proposed formalization describing the interactions between each component and relate them to the addressed tasks. Additionally, we categorize the existing approaches using our framework and derive future research directions to advance dual analysis by including state-of-the-art visual analysis techniques to improve data exploration.
C1 [Dennig, Frederik L.] Univ Konstanz, Data Anal & Visualizat Res Grp, D-78457 Constance, Germany.
   [Miller, Matthias] Univ Konstanz, AI Ctr, D-78457 Constance, Germany.
C3 University of Konstanz; University of Konstanz
RP Dennig, FL (corresponding author), Univ Konstanz, Data Anal & Visualizat Res Grp, D-78457 Constance, Germany.
EM frederik.dennig@uni-konstanz.de; matthias.miller@uni-konstanz.de;
   keim@uni-konstanz.de; melassady@ai.ethz.ch
RI Dennig, Frederik/HTL-3123-2023
OI El-Assady, Mennatallah/0000-0001-8526-2613; Dennig, Frederik
   L./0000-0003-1116-8450
FU Deutsche Forschungsgemeinschaft(DFG, German Research Foundation)
   [251654672 - TRR 161]; Federal Ministry of Education and the Research of
   Germany (BMBF) through PEGASUS under the Program "Forschung fur
   diezivile Sicherheit"; ETH AI Center
FX This work was supported in part by the Deutsche
   Forschungsgemeinschaft(DFG, German Research Foundation) under Grant
   251654672 - TRR 161 (Project A03), in part by the Federal Ministry of
   Education and the Research of Germany (BMBF) through PEGASUS under the
   Program "Forschung fur diezivile Sicherheit 20182023" and its
   announcement "Zivile Sicherheit - Schutzvor organisierter Kriminalitat
   II," and in part by ETH AI Center.
NR 84
TC 0
Z9 0
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5165
EP 5182
DI 10.1109/TVCG.2023.3288356
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400025
PM 37342951
DA 2025-03-07
ER

PT J
AU Wu, MG
   Sun, YJ
   Jiang, SJ
AF Wu, Mingguang
   Sun, Yanjie
   Jiang, Shangjing
TI Adaptive Color Transfer From Images to Terrain Visualizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Elevation colors; color transfer; terrain visualization
ID SEMANTIC DISCRIMINABILITY; DEPTH; MODEL
AB Terrain mapping is not only dedicated to communicating how high or steep a landscape is but can also help to indicate how we feel about a place. However, crafting effective and expressive elevation colors is challenging for both nonexperts and experts. In this article, we present a two-step image-to-terrain color transfer method that can transfer color from arbitrary images to diverse terrain models. First, we present a new image color organization method that organizes discrete, irregular image colors into a continuous, regular color grid that facilitates a series of color operations, such as local and global searching, categorical color selection and sequential color interpolation. Second, we quantify a series of cartographic concerns about elevation color crafting, such as the "lower, higher" principle, color conventions, and aerial perspectives. We also define color similarity between images and terrain visualizations with aesthetic quality. We then mathematically formulate image-to-terrain color transfer as a dual-objective optimization problem and offer a heuristic searching method to solve the problem. Finally, we compare elevation colors from our method with a standard color scheme and a representative color scale generation tool based on four test terrains. The evaluations show that the elevation colors from the proposed method are most effective and that our results are visually favorable. We also showcase that our method can transfer emotion from images to terrain visualizations.
C1 [Wu, Mingguang; Sun, Yanjie; Jiang, Shangjing] Nanjing Normal Univ, Coll Geog Sci, Nanjing 210023, Jiangsu, Peoples R China.
C3 Nanjing Normal University
RP Sun, YJ (corresponding author), Nanjing Normal Univ, Coll Geog Sci, Nanjing 210023, Jiangsu, Peoples R China.
EM wmg@njnu.edu.cn; yanjiesun@njnu.edu.cn; 211301018@njnu.edu.cn
RI Jiang, Shangjing/HGF-2021-2022
OI Sun, Yanjie/0000-0002-7099-5133; Jiang, Shangjing/0000-0002-6920-3901;
   Wu, Mingguang/0000-0001-6086-4503
FU National Natural Science Foundation of China [41971417, 41930104];
   Postgraduate Research &Practice Innovation Program of Jiangsu Province
   [KYCX22_1575, KYCX22_1559]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 41971417 and 41930104 and in part by
   the Postgraduate Research &Practice Innovation Program of Jiangsu
   Province under Grants KYCX22_1575 and KYCX22_1559
NR 58
TC 2
Z9 2
U1 14
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5538
EP 5552
DI 10.1109/TVCG.2023.3295122
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400035
PM 37440387
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Xu, WP
   Zhang, P
   Yu, ML
   Yang, L
   Wang, WM
   Liu, LG
AF Xu, Wenpeng
   Zhang, Peng
   Yu, Menglin
   Yang, Li
   Wang, Weiming
   Liu, Ligang
TI Topology Optimization Via Spatially-Varying TPMS
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Microstructure; Optimization; Topology; Lattices; Three-dimensional
   printing; Shape; Three-dimensional displays; Additive manufacturing;
   microstructure; topology optimization; triply periodic minimal surface
ID POROUS SCAFFOLD DESIGN; LEVEL SET METHOD; LATTICE STRUCTURES; FEATURE
   EVOLUTION; CODE WRITTEN; HOMOGENIZATION; MICROSTRUCTURES; SPLINES;
   MODELS; INFILL
AB Structural design with multi-family triply periodic minimal surfaces (TPMS) is a meaningful work that can combine the advantages of different types of TPMS. However, very few methods consider the influence of the blending of different TPMS on structural performance, and the manufacturability of final structure. Therefore, this work proposes a method to design manufacturable microstructures with topology optimization (TO) based on spatially-varying TPMS. In our method, different types of TPMS are simultaneously considered in the optimization to maximize the performance of designed microstructure. The geometric and mechanical properties of the unit cells generated with TPMS, that is minimal surface lattice cell (MSLC), are analyzed to obtain the performance of different types of TPMS. In the designed microstructure, MSLCs of different types are smoothly blended with an interpolation method. To analyze the influence of deformed MSLCs on the performance of the final structure, the blending blocks are introduced to describe the connection cases between different types of MSLCs. The mechanical properties of deformed MSLCs are analyzed and applied in TO process to reduce the influence of deformed MSLCs on the performance of final structure. The infill resolution of MSLC within a given design domain is determined according to the minimal printable wall thickness of MSLC and structural stiffness. Both numerical and physical experimental results demonstrate the effectiveness of the proposed method.
C1 [Xu, Wenpeng; Zhang, Peng] Henan Polytech Univ, Sch Comp Sci & Technol, Jiaozuo 454099, Henan, Peoples R China.
   [Yu, Menglin] Henan Polytech Univ, Sch Mech & Power Engn, Jiaozuo 454099, Henan, Peoples R China.
   [Yang, Li] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Peoples R China.
   [Wang, Weiming] Dalian Univ Technol, Sch Math Sci, Key Lab Computat Math & Data Intelligence Liaoning, Dalian 116024, Peoples R China.
   [Wang, Weiming] Univ Manchester, Dept Mech Aerosp & Civil Engn, Manchester M13 9PL, England.
   [Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
C3 Henan Polytechnic University; Henan Polytechnic University; Dalian
   University of Technology; Dalian University of Technology; University of
   Manchester; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Wang, WM (corresponding author), Dalian Univ Technol, Sch Math Sci, Key Lab Computat Math & Data Intelligence Liaoning, Dalian 116024, Peoples R China.; Wang, WM (corresponding author), Univ Manchester, Dept Mech Aerosp & Civil Engn, Manchester M13 9PL, England.
EM wpxu08@gmail.com; hpuzp98@qq.com; 2510392773@qq.com; 690660794@qq.com;
   wwmdlut@gmail.com; lgliu@ustc.edu.cn
RI Zhang, Peng/AGZ-8970-2022; Liu, Ligang/IZQ-5817-2023; Xu,
   Wenpeng/JQW-3191-2023; Wang, Weiming/H-4944-2017
OI Zhang, Peng/0009-0005-1468-7346; Yu, Menglin/0009-0008-4633-9866; Xu,
   Wenpeng/0000-0002-0852-4904; Wang, Weiming/0000-0001-6289-0094
FU National Natural Science Foundation of China [62172073, 62025207];
   Natural Science Foundation of Liaoning Province [2021-MS-110]; Key
   Scientific Research Projects of Colleges and Universities of Henan
   Province [21A520017]; National key research and development program
   [2022YFB3303000]; Fundamental Research Fund [DUT22QN212]
FX This work was supported in part by the Natural Science Foundation of
   China under Grants 62172073, and 62025207, in part by the Natural
   Science Foundation of Liaoning Province under Grant 2021-MS-110, in part
   by the Key Scientific Research Projects of Colleges and Universities of
   Henan Province under Grant 21A520017, in part by the National key
   research and development program under Grant 2022YFB3303000, and in part
   by Fundamental Research Fund under Grant DUT22QN212.
NR 71
TC 8
Z9 8
U1 23
U2 57
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4570
EP 4587
DI 10.1109/TVCG.2023.3268068
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400036
PM 37074903
DA 2025-03-07
ER

PT J
AU Zhang, J
   Zhou, KN
   Luximon, Y
   Lee, TY
   Li, P
AF Zhang, Jie
   Zhou, Kangneng
   Luximon, Yan
   Lee, Tong-Yee
   Li, Ping
TI MeshWGAN: Mesh-to-Mesh Wasserstein GAN With Multi-Task Gradient Penalty
   for 3D Facial Geometric Age Transformation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Age transformation; 3D face geometry; MeshWGAN; mesh generative
   adversarial networks; multi-task gradient penalty
ID FACE; IMAGE
AB As the metaverse develops rapidly, 3D facial age transformation is attracting increasing attention, which may bring many potential benefits to a wide variety of users, e.g., 3D aging figures creation, 3D facial data augmentation and editing. Compared with 2D methods, 3D face aging is an underexplored problem. To fill this gap, we propose a new mesh-to-mesh Wasserstein generative adversarial network (MeshWGAN) with a multi-task gradient penalty to model a continuous bi-directional 3D facial geometric aging process. To the best of our knowledge, this is the first architecture to achieve 3D facial geometric age transformation via real 3D scans. As previous image-to-image translation methods cannot be directly applied to the 3D facial mesh, which is totally different from 2D images, we built a mesh encoder, decoder, and multi-task discriminator to facilitate mesh-to-mesh transformations. To mitigate the lack of 3D datasets containing children's faces, we collected scans from 765 subjects aged 5-17 in combination with existing 3D face databases, which provided a large training dataset. Experiments have shown that our architecture can predict 3D facial aging geometries with better identity preservation and age closeness compared to 3D trivial baselines. We also demonstrated the advantages of our approach via various 3D face-related graphics applications.
C1 [Zhang, Jie; Li, Ping] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
   [Zhang, Jie; Luximon, Yan; Li, Ping] Hong Kong Polytech Univ, Sch Design, Hong Kong, Peoples R China.
   [Zhou, Kangneng] Univ Sci & Technol Beijing, Sch Comp & Commun Engn, Beijing 100083, Peoples R China.
   [Luximon, Yan] Lab Artificial Intelligence Design, Hong Kong, Peoples R China.
   [Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
C3 Hong Kong Polytechnic University; Hong Kong Polytechnic University;
   University of Science & Technology Beijing; National Cheng Kung
   University
RP Li, P (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.; Luximon, Y; Li, P (corresponding author), Hong Kong Polytech Univ, Sch Design, Hong Kong, Peoples R China.
EM peterzhang1130@163.com; elliszkn@163.com; yan.luximon@polyu.edu.hk;
   tonylee@ncku.edu.tw; p.li@polyu.edu.hk
RI Li, Ping/AAO-2019-2020; Luximon, Yan/A-7946-2010
OI Zhou, Kangneng/0000-0001-9102-9527; Zhang, Jie/0000-0001-8219-5590;
   Luximon, Yan/0000-0003-2843-847X; Li, Ping/0000-0002-1503-0240
FU Research Grants Council of Hong Kong [PolyU 15603419]; National Science
   and Technology Council, Taiwan [110-2221-E-006-135-MY3]; Hong Kong
   Polytechnic University [P0042740, P0030419, P0043906, P0044520]
FX This work was supported in part by the Research Grants Council of Hong
   Kong under Grant PolyU 15603419, in part by the National Science and
   Technology Council under Grant 110-2221-E-006-135-MY3, Taiwan, and in
   part by The Hong Kong Polytechnic University under Grants P0042740,
   P0030419, P0043906, and P0044520.
NR 56
TC 3
Z9 3
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4927
EP 4940
DI 10.1109/TVCG.2023.3284500
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400023
PM 37307186
DA 2025-03-07
ER

PT J
AU Goudé, I
   Bruckert, A
   Olivier, AH
   Pettré, J
   Cozot, R
   Bouatouch, K
   Christie, M
   Hoyet, L
AF Goude, Ific
   Bruckert, Alexandre
   Olivier, Anne-Helene
   Pettre, Julien
   Cozot, Remi
   Bouatouch, Kadi
   Christie, Marc
   Hoyet, Ludovic
TI Real-Time Multi-Map Saliency-Driven Gaze Behavior for Non-Conversational
   Characters
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Behavioral sciences; Animation; Visualization; Solid modeling; Real-time
   systems; Biological system modeling; Head; dataset; eye-tracking data;
   gaze behavior; neural networks; simulation
ID EYE-MOVEMENTS; ATTENTION; MODEL
AB Gaze behavior of virtual characters in video games and virtual reality experiences is a key factor of realism and immersion. Indeed, gaze plays many roles when interacting with the environment; not only does it indicate what characters are looking at, but it also plays an important role in verbal and non-verbal behaviors and in making virtual characters alive. Automated computing of gaze behaviors is however a challenging problem, and to date none of the existing methods are capable of producing close-to-real results in an interactive context. We therefore propose a novel method that leverages recent advances in several distinct areas related to visual saliency, attention mechanisms, saccadic behavior modelling, and head-gaze animation techniques. Our approach articulates these advances to converge on a multi-map saliency-driven model which offers real-time realistic gaze behaviors for non-conversational characters, together with additional user-control over customizable features to compose a wide variety of results. We first evaluate the benefits of our approach through an objective evaluation that confronts our gaze simulation with ground truth data using an eye-tracking dataset specifically acquired for this purpose. We then rely on subjective evaluation to measure the level of realism of gaze animations generated by our method, in comparison with gaze animations captured from real actors. Our results show that our method generates gaze behaviors that cannot be distinguished from captured gaze animations. Overall, we believe that these results will open the way for more natural and intuitive design of realistic and coherent gaze animations for real-time applications.
C1 [Goude, Ific; Olivier, Anne-Helene; Pettre, Julien; Bouatouch, Kadi; Christie, Marc; Hoyet, Ludovic] Univ Rennes, Inria, CNRS, IRISA, F-35000 Rennes, France.
   [Bruckert, Alexandre] Nantes Univ, CNRS, IRCCyN, UMR 6004,Eecole Cent Nantes,LS2N, F-44000 Nantes, France.
   [Cozot, Remi] Littoral Opal Coast Univ, F-59140 Dunkerque, France.
C3 Inria; Universite de Rennes; Centre National de la Recherche
   Scientifique (CNRS); Centre National de la Recherche Scientifique
   (CNRS); Nantes Universite; Ecole Centrale de Nantes
RP Bruckert, A (corresponding author), Nantes Univ, CNRS, IRCCyN, UMR 6004,Eecole Cent Nantes,LS2N, F-44000 Nantes, France.
EM ific.goude@irisa.fr; alexandre.bruckert@univ-nantes.fr;
   anne-helene.olivier@univ-rennes2.fr; julien.pettre@inria.fr;
   remi.cozot@univ-littoral.fr; kadi.bouatouch@irisa.fr;
   marc.christie@irisa.fr; ludovic.hoyet@inria.fr
RI Pettre, Julien/KZT-8249-2024; Olivier, Anne-Hélène/AAH-7378-2020;
   Bruckert, Alexandre/AAW-6393-2021; Hoyet, Ludovic/IWU-9100-2023
OI Pettre, Julien/0000-0003-1812-1436; Bruckert,
   Alexandre/0000-0003-2623-4975; Olivier, Anne-Helene/0000-0002-2833-020X;
   Hoyet, Ludovic/0000-0002-7373-6049
NR 56
TC 6
Z9 6
U1 4
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3871
EP 3883
DI 10.1109/TVCG.2023.3244679
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700044
PM 37022858
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Meidiana, A
   Hong, SH
   Eades, P
   Keim, D
AF Meidiana, Amyra
   Hong, Seok-Hee
   Eades, Peter
   Keim, Daniel
TI Automorphism Faithfulness Metrics for Symmetric Graph Drawings
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Measurement; Graph drawing; Orbits; Vehicle dynamics; Time measurement;
   Stress measurement; Stress; Automorphism; faithfulness metrics; graph
   drawing; symmetry
ID PLANAR GRAPHS
AB In this article, we present new quality metrics for symmetric graph drawing based on group theory. Roughly speaking, the new metrics are faithfulness metrics, i.e., they measure how faithfully a drawing of a graph displays the ground truth (i.e., geometric automorphisms) of the graph as symmetries. More specifically, we introduce two types of automorphism faithfulness metrics for displaying: (1) a single geometric automorphism as a symmetry (axial or rotational), and (2) a group of geometric automorphisms (cyclic or dihedral). We present algorithms to compute the automorphism faithfulness metrics in O(n log n) time. Moreover, we also present efficient algorithms to detect exact symmetries in a graph drawing. We then validate our automorphism faithfulness metrics using deformation experiments. Finally, we use the metrics to evaluate existing graph drawing algorithms to compare how faithfully they display geometric automorphisms of a graph as symmetries.
C1 [Meidiana, Amyra; Hong, Seok-Hee; Eades, Peter] Univ Sydney, Camperdown, NSW 2006, Australia.
   [Keim, Daniel] Univ Konstanz, D-78464 Constance, Germany.
C3 University of Sydney; University of Konstanz
RP Meidiana, A (corresponding author), Univ Sydney, Camperdown, NSW 2006, Australia.
EM amyra.meidiana@sydney.edu.au; seokhee.hong@sydney.edu.au;
   peter.eades@sydney.edu.au; keim@uni-konstanz.de
OI Hong, Seok-Hee/0000-0003-1698-3868
FU ARC (Australian Research Council) DP (Discovery Project) [DP190103301]
FX This work was supported by an ARC (Australian Research Council) DP
   (Discovery Project) under grant (# DP190103301).
NR 37
TC 0
Z9 0
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3241
EP 3255
DI 10.1109/TVCG.2022.3229354
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700079
PM 37015686
DA 2025-03-07
ER

PT J
AU Xiong, CY
   Lee-Robbins, E
   Zhang, ICY
   Gaba, A
   Franconeri, S
AF Xiong, Cindy
   Lee-Robbins, Elsie
   Zhang, Icy
   Gaba, Aimen
   Franconeri, Steven
TI Reasoning Affordances With Tables and Bar Charts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Cognition; Urban areas; Bars; Skin; Affordances;
   Weapons; tabular displays; empirical evaluation; reasoning
ID CONFIRMATION BIAS; INFORMATION; SCIENCE; VISUALIZATIONS; PERCEPTION;
   FREQUENCY; KNOWLEDGE; PEOPLE; FORMAT; IMPACT
AB A viewer's existing beliefs can prevent accurate reasoning with data visualizations. In particular, confirmation bias can cause people to overweigh information that confirms their beliefs, and dismiss information that disconfirms them. We tested whether confirmation bias exists when people reason with visualized data and whether certain visualization designs can elicit less biased reasoning strategies. We asked crowdworkers to solve reasoning problems that had the potential to evoke both poor reasoning strategies and confirmation bias. We created two scenarios, one in which we primed people with a belief before asking them to make a decision, and another in which people held pre-existing beliefs. The data was presented as either a table, a bar table, or a bar chart. To correctly solve the problem, participants should use a complex reasoning strategy to compare two ratios, each between two pairs of values. But participants could also be tempted to use simpler, superficial heuristics, shortcuts, or biased strategies to reason about the problem. Presenting the data in a table format helped participants reason with the correct ratio strategy while showing the data as a bar table or a bar chart led participants towards incorrect heuristics. Confirmation bias was not significantly present when beliefs were primed, but it was present when beliefs were pre-existing. Additionally, the table presentation format was more likely to afford the ratio reasoning strategy, and the use of ratio strategy was more likely to lead to the correct answer. These findings suggest that data presentation formats can affect affordances for reasoning.
C1 [Xiong, Cindy] UMass Amherst, Coll Informat & Comp Sci, Amherst, MA 01002 USA.
   [Lee-Robbins, Elsie] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Zhang, Icy] UCLA, Los Angeles, CA 90095 USA.
   [Gaba, Aimen] UMass Amherst, Amherst, MA 01003 USA.
   [Franconeri, Steven] Northwestern Univ, Evanston, IL 60208 USA.
C3 University of Massachusetts System; University of Massachusetts Amherst;
   University of Michigan System; University of Michigan; University of
   California System; University of California Los Angeles; University of
   Massachusetts System; University of Massachusetts Amherst; Northwestern
   University
RP Xiong, CY (corresponding author), UMass Amherst, Coll Informat & Comp Sci, Amherst, MA 01002 USA.
EM cindy.xiong@cs.umass.edu; elsielee@umich.edu; yunyi9847@g.ucla.edu;
   agaba@umass.edu; franconeri@northwestern.edu
RI Lee-Robbins, Elsie/IUP-2028-2023
OI Franconeri, Steven/0000-0001-5244-9764; Xiong Bearfield,
   Cindy/0000-0002-1451-4083; Lee-Robbins, Elsie/0000-0002-4080-6506
FU NSF [CHS-1901485]
FX This work was supported by NSF under Grant CHS-1901485.
NR 91
TC 2
Z9 3
U1 5
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3487
EP 3502
DI 10.1109/TVCG.2022.3232959
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700025
PM 37015636
DA 2025-03-07
ER

PT J
AU Nam, JW
   Isenberg, T
   Keefe, DF
AF Nam, Jung Who
   Isenberg, Tobias
   Keefe, Daniel F.
TI V-Mail: 3D-Enabled Correspondence About Spatial Data on (Almost) All
   Your Devices
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Three-dimensional displays; Annotations; Spatial
   databases; Collaboration; Software; Task analysis; Communication;
   human-computer interaction; immersive analytics; storytelling;
   visualization of scientific 3D data
ID VISUALIZATION; STORIES
AB We present V-Mail, a framework of cross-platform applications, interactive techniques, and communication protocols for improved multi-person correspondence about spatial 3D datasets. Inspired by the daily use of e-mail, V-Mail seeks to enable a similar style of rapid, multi-person communication accessible on any device; however, it aims to do this in the new context of spatial 3D communication, where limited access to 3D graphics hardware typically prevents such communication. The approach integrates visual data storytelling with data exploration, spatial annotations, and animated transitions. V-Mail "data stories" are exported in a standard video file format to establish a common baseline level of access on (almost) any device. The V-Mail framework also includes a series of complementary client applications and plugins that enable different degrees of story co-authoring and data exploration, adjusted automatically to match the capabilities of various devices. A lightweight, phone-based V-Mail app makes it possible to annotate data by adding captions to the video. These spatial annotations are then immediately accessible to team members running high-end 3D graphics visualization systems that also include a V-Mail client, implemented as a plugin. Results and evaluation from applying V-Mail to assist communication within an interdisciplinary science team studying Antarctic ice sheets confirm the utility of the asynchronous, cross-platform collaborative framework while also highlighting some current limitations and opportunities for future work.
C1 [Nam, Jung Who; Keefe, Daniel F.] Univ Minnesota, Minneapolis, MN 55455 USA.
   [Isenberg, Tobias] Univ Paris Saclay, CNRS, Inria, LISN, F-91190 Gif Sur Yvette, France.
C3 University of Minnesota System; University of Minnesota Twin Cities;
   Inria; Centre National de la Recherche Scientifique (CNRS); Universite
   Paris Saclay
RP Nam, JW (corresponding author), Univ Minnesota, Minneapolis, MN 55455 USA.
EM namxx054@umn.edu; tobias.isenberg@inria.fr; dfk@umn.edu
RI ; Isenberg, Tobias/A-7575-2008
OI Keefe, Daniel/0000-0002-7039-2340; Isenberg, Tobias/0000-0001-7953-8644;
   Nam, Jung Who/0000-0003-0295-6906
FU National Science Foundation
FX No Statement Available
NR 43
TC 0
Z9 0
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2024
VL 30
IS 4
BP 1853
EP 1867
DI 10.1109/TVCG.2022.3229017
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JN9X1
UT WOS:001173975500010
PM 37015540
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Leon, GM
   Isenberg, P
   Breiter, A
AF Leon, Gabriela Molina
   Isenberg, Petra
   Breiter, Andreas
TI Eliciting Multimodal and Collaborative Interactions for Data Exploration
   on Large Vertical Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Collaborative work; elicitation study; large vertical
   displays; multimodal interaction; spatio-temporal data
ID INFORMATION VISUALIZATION; AUGMENTED REALITY; TOUCH; WALL; MANIPULATION;
   SPEECH; MOBILE; PEN
AB We examined user preferences to combine multiple interaction modalities for collaborative interaction with data shown on large vertical displays. Large vertical displays facilitate visual data exploration and allow the use of diverse interaction modalities by multiple users at different distances from the screen. Yet, how to offer multiple interaction modalities is a non-trivial problem. We conducted an elicitation study with 20 participants that generated 1015 interaction proposals combining touch, speech, pen, and mid-air gestures. Given the opportunity to interact using these four modalities, participants preferred speech interaction in 10 of 15 low-level tasks and direct manipulation for straightforward tasks such as showing a tooltip or selecting. In contrast to previous work, participants most favored unimodal and personal interactions. We identified what we call collaborative synonyms among their interaction proposals and found that pairs of users collaborated either unimodally and simultaneously or multimodally and sequentially. We provide insights into how end-users associate visual exploration tasks with certain modalities and how they collaborate at different interaction distances using specific interaction modalities.(1)
C1 [Leon, Gabriela Molina; Breiter, Andreas] Univ Bremen, D-28359 Bremen, Germany.
   [Isenberg, Petra] Univ Paris Saclay, CNRS, Inria, LISN, F-91405 Orsay, France.
C3 University of Bremen; Microsoft; Inria; Universite Paris Saclay; Centre
   National de la Recherche Scientifique (CNRS)
RP Leon, GM (corresponding author), Univ Bremen, D-28359 Bremen, Germany.
EM molina@uni-bremen.de; petra.isenberg@inria.fr; abreiter@uni-bremen.de
RI Breiter, Andreas/P-4859-2016
OI Molina Leon, Gabriela C./0000-0002-9223-2022; Isenberg,
   Petra/0000-0002-2948-6417
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
FX No Statement Available
NR 71
TC 0
Z9 0
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2024
VL 30
IS 2
BP 1624
EP 1637
DI 10.1109/TVCG.2023.3323150
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EC6D7
UT WOS:001136746300002
PM 37930918
OA hybrid
DA 2025-03-07
ER

PT J
AU Xu, X
   Wang, L
   Pérard-Gayot, A
   Membarth, R
   Li, CY
   Yang, CL
   Slusallek, P
AF Xu, Xiang
   Wang, Lu
   Perard-Gayot, Arsene
   Membarth, Richard
   Li, Cuiyu
   Yang, Chenglei
   Slusallek, Philipp
TI Temporal Coherence-Based Distributed Ray Tracing of Massive Scenes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Rendering (computer graphics); Ray tracing; Portals; Heuristic
   algorithms; Dynamic scheduling; Task analysis; Distributed databases;
   Computer graphics; distributed graphics; ray tracing
ID CACHE
AB Distributed ray tracing algorithms are widely used when rendering massive scenes, where data utilization and load balancing are the keys to improving performance. One essential observation is that rays are temporally coherent, which indicates that temporal information can be used to improve computational efficiency. In this paper, we use temporal coherence to optimize the performance of distributed ray tracing. First, we propose a temporal coherence-based scheduling algorithm to guide the task/data assignment and scheduling. Then, we propose a virtual portal structure to predict the radiance of rays based on the previous frame, and send the rays with low radiance to a precomputed simplified model for further tracing, which can dramatically reduce the traversal complexity and the overhead of network data transmission. The approach was validated on scenes of sizes up to 355 GB. Our algorithm can achieve a speedup of up to 81% compared to previous algorithms, with a very small mean squared error.
C1 [Xu, Xiang] Shandong Univ Finance & Econ, Shandong Key Lab Blockchain Finance, Jinan 250101, Shandong, Peoples R China.
   [Wang, Lu; Yang, Chenglei] Shandong Univ, Sch Software, Jinan 250101, Shandong, Peoples R China.
   [Perard-Gayot, Arsene] Weta Digital, Wellington 6243, New Zealand.
   [Membarth, Richard] TH Ingolstadt THI, Res Inst AImot Bavaria, D-85049 Ingolstadt, Germany.
   [Membarth, Richard; Slusallek, Philipp] Saarland Informat Campus, German Res Ctr Artificial Intelligence DFKI, D-66123 Saarbrucken, Saarland, Germany.
   [Li, Cuiyu] Adv Comp East China Subctr, Suzhou 215300, Jiangsu, Peoples R China.
C3 Shandong University of Finance & Economics; Shandong University; German
   Research Center for Artificial Intelligence (DFKI)
RP Wang, L (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Shandong, Peoples R China.
EM xuxiang8420@outlook.com; luwang_hcivr@sdu.edu.cn;
   aperardgayot@wetafx.co.nz; Richard.Membarth@thi.de; lxyystn@126.com;
   chl_yang@sdu.edu.cn; philipp.slusallek@dfki.de
RI 徐, 翔/KOC-9548-2024
OI xu, xiang/0000-0002-9570-0784; Slusallek, Philipp/0000-0002-2189-2429
FU National Key R#x0026;D Program of China
FX No Statement Available
NR 37
TC 0
Z9 1
U1 8
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2024
VL 30
IS 2
BP 1489
EP 1501
DI 10.1109/TVCG.2022.3219982
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EC6D7
UT WOS:001136746300010
PM 36342995
DA 2025-03-07
ER

PT J
AU Li, RZ
   Cui, WW
   Song, TQ
   Xie, X
   Ding, R
   Wang, Y
   Zhang, HD
   Zhou, H
   Wu, YC
AF Li, Renzhong
   Cui, Weiwei
   Song, Tianqi
   Xie, Xiao
   Ding, Rui
   Wang, Yun
   Zhang, Haidong
   Zhou, Hong
   Wu, Yingcai
TI Causality-Based Visual Analysis of Questionnaire Responses
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Causal analysis; Questionnaire; Design study
ID ASSOCIATION RULES; EXPLORATION
AB As the final stage of questionnaire analysis, causal reasoning is the key to turning responses into valuable insights and actionable items for decision-makers. During the questionnaire analysis, classical statistical methods (e.g., Differences-in-Differences) have been widely exploited to evaluate causality between questions. However, due to the huge search space and complex causal structure in data, causal reasoning is still extremely challenging and time-consuming, and often conducted in a trial-and-error manner. On the other hand, existing visual methods of causal reasoning face the challenge of bringing scalability and expert knowledge together and can hardly be used in the questionnaire scenario. In this work, we present a systematic solution to help analysts effectively and efficiently explore questionnaire data and derive causality. Based on the association mining algorithm, we dig question combinations with potential inner causality and help analysts interactively explore the causal sub-graph of each question combination. Furthermore, leveraging the requirements collected from the experts, we built a visualization tool and conducted a comparative study with the state-of-the-art system to show the usability and efficiency of our system.
C1 [Li, Renzhong; Wu, Yingcai] Zhejiang Univ, State Key Lab CAD & CG, Zhejiang, Peoples R China.
   [Cui, Weiwei; Ding, Rui; Wang, Yun; Zhang, Haidong] Microsoft Res Asia, Beijing, Peoples R China.
   [Song, Tianqi; Xie, Xiao] Zhejiang Univ, Dept Sports Sci, Hangzhou, Peoples R China.
   [Zhou, Hong] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen, Peoples R China.
C3 Zhejiang University; Microsoft Research Asia; Microsoft; Microsoft
   China; Zhejiang University; Shenzhen University
RP Xie, X (corresponding author), Zhejiang Univ, Dept Sports Sci, Hangzhou, Peoples R China.
EM renzhongli@zju.edu.cn; weiweicu@microsoft.com; holly1027@zju.edu.cn;
   xxie@zju.edu.cn; juding@microsoft.com; wangyun@microsoft.com;
   haizhang@microsoft.com; hzhou@szu.edu.cn; ycwu@zju.edu.cn
RI Li, Ren-Zhong/E-6659-2012
OI Li, Renzhong/0000-0002-6577-036X
FU National Key R&D Program of China
FX No Statement Available
NR 48
TC 0
Z9 0
U1 3
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 638
EP 648
DI 10.1109/TVCG.2023.3327376
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500018
PM 37903040
DA 2025-03-07
ER

PT J
AU Newburger, E
   Elmqvist, N
AF Newburger, Eric
   Elmqvist, Niklas
TI Visualization According to Statisticians: An Interview Study on the Role
   of Visualization for Inferential Statistics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Interviews; Visualization; Industries; Encoding;
   Cognitive science; Codes; Inferential statistics; qualitative interview
   study; thematic coding; statistical visualization
ID INFORMATION VISUALIZATION; VISUAL REPRESENTATIONS
AB Statisticians are not only one of the earliest professional adopters of data visualization, but also some of its most prolific users. Understanding how these professionals utilize visual representations in their analytic process may shed light on best practices for visual sensemaking. We present results from an interview study involving 18 professional statisticians (19.7 years average in the profession) on three aspects: (1) their use of visualization in their daily analytic work; (2) their mental models of inferential statistical processes; and (3) their design recommendations for how to best represent statistical inferences. Interview sessions consisted of discussing inferential statistics, eliciting participant sketches of suitable visual designs, and finally, a design intervention with our proposed visual designs. We analyzed interview transcripts using thematic analysis and open coding, deriving thematic codes on statistical mindset, analytic process, and analytic toolkit. The key findings for each aspect are as follows: (1) statisticians make extensive use of visualization during all phases of their work (and not just when reporting results); (2) their mental models of inferential methods tend to be mostly visually based; and (3) many statisticians abhor dichotomous thinking. The latter suggests that a multi-faceted visual display of inferential statistics that includes a visual indicator of analytically important effect sizes may help to balance the attributed epistemic power of traditional statistical testing with an awareness of the uncertainty of sensemaking.
C1 [Newburger, Eric] US Naval Acad, Annapolis, MD 21402 USA.
   [Elmqvist, Niklas] Aarhus Univ, Aarhus, Denmark.
C3 United States Department of Defense; United States Navy; United States
   Naval Academy; Aarhus University
RP Newburger, E (corresponding author), US Naval Acad, Annapolis, MD 21402 USA.
EM enewburg@terpmail.umd.edu; elm@cs.au.dk
OI Newburger, Eric/0000-0001-8777-0363; Elmqvist,
   Niklas/0000-0001-5805-5301
NR 32
TC 0
Z9 0
U1 1
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 230
EP 239
DI 10.1109/TVCG.2023.3326521
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500098
PM 37871077
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhao, LX
   Isenberg, T
   Xie, FQ
   Liang, HN
   Yu, LY
AF Zhao, Lixiang
   Isenberg, Tobias
   Xie, Fuqi
   Liang, Hai-Ning
   Yu, Lingyun
TI MeTACAST: Target- and Context-Aware Spatial Selection in VR
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Spatial selection; immersive analytics; virtual reality (VR);
   target-aware and context-aware interaction for visualization
ID GALAXIES; OBJECTS
AB We propose three novel spatial data selection techniques for particle data in VR visualization environments. They are designed to be target- and context-aware and be suitable for a wide range of data features and complex scenarios. Each technique is designed to be adjusted to particular selection intents: the selection of consecutive dense regions, the selection of filament-like structures, and the selection of clusters-with all of them facilitating post-selection threshold adjustment. These techniques allow users to precisely select those regions of space for further exploration-with simple and approximate 3D pointing, brushing, or drawing input-using flexible point- or path-based input and without being limited by 3D occlusions, non-homogeneous feature density, or complex data shapes. These new techniques are evaluated in a controlled experiment and compared with the Baseline method, a region-based 3D painting selection. Our results indicate that our techniques are effective in handling a wide range of scenarios and allow users to select data based on their comprehension of crucial features. Furthermore, we analyze the attributes, requirements, and strategies of our spatial selection methods and compare them with existing state-of-the-art selection methods to handle diverse data features and situations. Based on this analysis we provide guidelines for choosing the most suitable 3D spatial selection techniques based on the interaction environment, the given data characteristics, or the need for interactive post-selection threshold adjustment.
C1 [Zhao, Lixiang; Xie, Fuqi; Liang, Hai-Ning; Yu, Lingyun] Xian Jiaotong Liverpool Univ, Xian, Peoples R China.
   [Isenberg, Tobias] Univ Paris Saclay, INRIA, LISN, CNRS, Paris, France.
C3 Xi'an Jiaotong-Liverpool University; Centre National de la Recherche
   Scientifique (CNRS); Inria; Universite Paris Saclay
RP Yu, LY (corresponding author), Xian Jiaotong Liverpool Univ, Xian, Peoples R China.
EM lixiang.zhao17@student.xjtlu.edu.cn; tobias.isenberg@inria.fr;
   fuqi.xie18@student.xjtlu.edu.cn; haining.liang@xjtlu.edu.cn;
   lingyun.yu@xjtlu.edu.cn
RI ; Isenberg, Tobias/A-7575-2008
OI Xie, Fuqi/0009-0008-4728-9346; Liang, Hai-Ning/0000-0003-3600-8955;
   Isenberg, Tobias/0000-0001-7953-8644; Zhao, Lixiang/0000-0001-6181-1673
FU NSFC
FX No Statement Available
NR 65
TC 2
Z9 2
U1 3
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 480
EP 494
DI 10.1109/TVCG.2023.3326517
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500026
PM 37871080
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kang, H
   Kang, T
   Wallraven, C
AF Kang, Hyeokmook
   Kang, Taeho
   Wallraven, Christian
TI Putting Vision and Touch into Conflict: Results From a Multimodal Mixed
   Reality Setup
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Visualization; Haptic interfaces; Resists; Task analysis;
   Real-time systems; Mixed reality; Mixed reality; perception; vision;
   haptics; dominance; hand tracking; multisensory perception
ID LEAP MOTION CONTROLLER; SHAPE INFORMATION; PERCEPTION; CATEGORIZATION;
   INTEGRATION; DOMINANCE; ACCURACY
AB What happens if we put vision and touch into conflict? Which modality "wins"? Although several previous studies have addressed this topic, they have solely focused on integration of vision and touch for low-level object properties (such as curvature, slant, or depth). In the present study, we introduce a multimodal mixed-reality setup based on real-time hand-tracking, which was used to display real-world, haptic exploration of objects in a virtual environment through a head-mounted-display (HMD). With this setup we studied multimodal conflict situations of objects varying along higher-level, parametrically-controlled global shape properties. Participants explored these objects in both unimodal and multimodal settings with the latter including congruent and incongruent conditions and differing instructions for weighting the input modalities. Results demonstrated a surprisingly clear touch dominance throughout all experiments, which in addition was only marginally influenceable through instructions to bias their modality weighting. We also present an initial analysis of the hand-tracking patterns that illustrates the potential for our setup to investigate exploration behavior in more detail.
C1 [Kang, Hyeokmook; Kang, Taeho] Korea Univ, Dept Brain & Cognit Engn, Seoul 02841, South Korea.
   [Wallraven, Christian] Korea Univ, Dept Brain & Cognit Engn, Seoul 02841, South Korea.
   [Wallraven, Christian] Korea Univ, Dept Artificial Intelligence, Seoul 02841, South Korea.
C3 Korea University; Korea University; Korea University
RP Wallraven, C (corresponding author), Korea Univ, Dept Brain & Cognit Engn, Seoul 02841, South Korea.; Wallraven, C (corresponding author), Korea Univ, Dept Artificial Intelligence, Seoul 02841, South Korea.
EM khm812@korea.ac.kr; tae1324@korea.ac.kr; wallraven@korea.ac.kr
OI Wallraven, Christian/0000-0002-2604-9115; , Taeho/0000-0003-4483-0447
FU National Research Foundation of Korea under Grants BK21 FOUR
   [NRF-2017M3C7A1041824, NRF-2019R1A2C2007612]; Institute of Information &
   Communications Technology Planning & Evaluation (IITP); Korea Government
   [2017-0-00451, 2019-0-00079, 2021-0-02068]
FX This work was supported in part by the National Research Foundation of
   Korea under Grants BK21 FOUR, NRF-2017M3C7A1041824, and
   NRF-2019R1A2C2007612, in part by the three Institute of Information &
   Communications Technology Planning & Evaluation (IITP), in part by Korea
   Government (2017-0-00451, Development of BCI based Brain and Cognitive
   Computing Technology for Recognizing User's Intentions using Deep
   Learning; 2019-0-00079, Department of Artificial Intelligence, Korea
   University; 2021-0-02068, Artificial Intelligence Inovation Hub).
NR 52
TC 1
Z9 1
U1 2
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5224
EP 5234
DI 10.1109/TVCG.2022.3207241
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300028
PM 36112552
DA 2025-03-07
ER

PT J
AU Zhang, CY
   Yang, L
   Chen, N
   Vining, N
   Sheffer, A
   Lau, FCM
   Wang, GP
   Wang, WP
AF Zhang, Congyi
   Yang, Lei
   Chen, Nenglun
   Vining, Nicholas
   Sheffer, Alla
   Lau, Francis C. M.
   Wang, Guoping
   Wang, Wenping
TI CreatureShop: Interactive 3D Character Modeling and Texturing From a
   Single Color Drawing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Solid modeling; Shape; Surface texture;
   Geometry; Electronic mail; Annotations; Character modeling; character
   texturing; interactive techniques
ID RECONSTRUCTION; ANIMALS
AB Creating 3D shapes from 2D drawings is an important problem with applications in content creation for computer animation and virtual reality. We introduce a new sketch-based system, CreatureShop, that enables amateurs to create high-quality textured 3D character models from 2D drawings with ease and efficiency. CreatureShop takes an input bitmap drawing of a character (such as an animal or other creature), depicted from an arbitrary descriptive pose and viewpoint, and creates a 3D shape with plausible geometric details and textures from a small number of user annotations on the 2D drawing. Our key contributions are a novel oblique view modeling method, a set of systematic approaches for producing plausible textures on the invisible or occluded parts of the 3D character (as viewed from the direction of the input drawing), and a user-friendly interactive system. We validate our system and methods by creating numerous 3D characters from various drawings, and compare our results with related works to show the advantages of our method. We perform a user study to evaluate the usability of our system, which demonstrates that our system is a practical and efficient approach to create fully-textured 3D character models for novice users.
C1 [Zhang, Congyi; Yang, Lei; Chen, Nenglun; Lau, Francis C. M.] Univ Hong Kong, Hong Kong, Peoples R China.
   [Vining, Nicholas] NVIDIA, Toronto, ON, Canada.
   [Vining, Nicholas; Sheffer, Alla] Univ British Columbia, Vancouver, BC, Canada.
   [Wang, Guoping] Peking Univ, Beijing 100871, Peoples R China.
   [Wang, Wenping] Texas A&M Univ, College Stn, TX 77843 USA.
C3 University of Hong Kong; University of British Columbia; Peking
   University; Texas A&M University System; Texas A&M University College
   Station
RP Wang, GP (corresponding author), Peking Univ, Beijing 100871, Peoples R China.; Wang, WP (corresponding author), Texas A&M Univ, College Stn, TX 77843 USA.
EM cyzhang@cs.hku.hk; lyang@cs.hku.hk; nolenc@hku.hk; nvining@cs.ubc.ca;
   sheffa@cs.ubc.ca; fcmlau@cs.hku.hk; wgp@pku.edu.cn; wenping@tamu.edu
RI Zhang, Congyi/LZE-9658-2025; wang, guoping/KQU-3394-2024
OI Sheffer, Alla/0000-0001-9251-3716; Zhang, Congyi/0000-0002-4259-2863
FU Research Council of Hong Kong [GRF 17211017]; National Sciences and
   Engineering Research Council of Canada (NSERC) [RGPIN-2018-03944]
FX The work of Wenping Wang was supported in part by the Research Council
   of Hong Kong, under Grant GRF 17211017. The work of Alla Sheffer was
   supported in part by Adobe, and in part by the National Sciences and
   Engineering Research Council of Canada (NSERC) under Grant
   RGPIN-2018-03944 (Broad-Based Computational Shape Design).
NR 41
TC 0
Z9 0
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 4874
EP 4890
DI 10.1109/TVCG.2022.3197560
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300005
PM 35944000
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Yang, C
   Zhang, ZW
   Fan, ZP
   Jiang, RH
   Chen, QJ
   Song, X
   Shibasaki, R
AF Yang, Chuang
   Zhang, Zhiwen
   Fan, Zipei
   Jiang, Renhe
   Chen, Quanjun
   Song, Xuan
   Shibasaki, Ryosuke
TI EpiMob: Interactive Visual Analytics of Citywide Human Mobility
   Restrictions for Epidemic Control
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Human mobility simulation; epidemic control; visual analytics;
   interactive system; big trajectory data
ID COVID-19
AB The outbreak of coronavirus disease (COVID-19) has swept across more than 180 countries and territories since late January 2020. As a worldwide emergency response, governments have implemented various measures and policies, such as self-quarantine, travel restrictions, work from home, and regional lockdown, to control the spread of the epidemic. These countermeasures seek to restrict human mobility because COVID-19 is a highly contagious disease that is spread by human-to-human transmission. Medical experts and policymakers have expressed the urgency to effectively evaluate the outcome of human restriction policies with the aid of big data and information technology. Thus, based on big human mobility data and city POI data, an interactive visual analytics system called Epidemic Mobility (EpiMob) was designed in this study. The system interactively simulates the changes in human mobility and infection status in response to the implementation of a certain restriction policy or a combination of policies (e.g., regional lockdown, telecommuting, screening). Users can conveniently designate the spatial and temporal ranges for different mobility restriction policies. Then, the results reflecting the infection situation under different policies are dynamically displayed and can be flexibly compared and analyzed in depth. Multiple case studies consisting of interviews with domain experts were conducted in the largest metropolitan area of Japan (i.e., Greater Tokyo Area) to demonstrate that the system can provide insight into the effects of different human mobility restriction policies for epidemic control, through measurements and comparisons.
C1 [Yang, Chuang; Zhang, Zhiwen; Fan, Zipei; Jiang, Renhe; Chen, Quanjun; Song, Xuan; Shibasaki, Ryosuke] Univ Tokyo, Ctr Spatial Informat Sci, Tokyo 1138654, Japan.
   [Fan, Zipei; Jiang, Renhe; Chen, Quanjun; Song, Xuan; Shibasaki, Ryosuke] Southern Univ Sci & Technol, SUSTech UTokyo Joint Res Ctr Super Smart City, Shenzhen 518055, Guangdong, Peoples R China.
C3 University of Tokyo; Southern University of Science & Technology
RP Jiang, RH; Song, X (corresponding author), Univ Tokyo, Ctr Spatial Informat Sci, Tokyo 1138654, Japan.
EM chuang.yang@csis.u-tokyo.ac.jp; zhangzhiwen@csis.u-tokyo.ac.jp;
   fanzipei@iis.u-tokyo.ac.jp; jiangrh@csis.u-tokyo.ac.jp;
   chen1990@iis.u-tokyo.ac.jp; songxuan@csis.u-tokyo.ac.jp;
   shiba@csis.u-tokyo.ac.jp
RI 子沛, 范/AAJ-2988-2020; Yang, Chuang/HNI-7345-2023; Jiang,
   Renhe/AAU-3856-2020; Zhang, Zhiwen/ABI-4441-2022; Song,
   Xuan/L-8086-2018; Jiang, Renhe/L-8202-2018
OI Song, Xuan/0000-0003-4042-7888; YANG, Chuang/0000-0002-8504-0057; Fan,
   Zipei/0000-0002-1442-1530; Jiang, Renhe/0000-0003-2593-4638; Zhiwen,
   Zhang/0000-0001-9524-7871
FU Grants-in-Aid for Scientific Research [20K19782] Funding Source: KAKEN
NR 55
TC 15
Z9 15
U1 1
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2023
VL 29
IS 8
BP 3586
EP 3601
DI 10.1109/TVCG.2022.3165385
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L3CU5
UT WOS:001022080200010
PM 35385385
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Eckelt, K
   Hinterreiter, A
   Adelberger, P
   Walchshofer, C
   Dhanoa, V
   Humer, C
   Heckmann, M
   Steinparz, C
   Streit, M
AF Eckelt, Klaus
   Hinterreiter, Andreas
   Adelberger, Patrick
   Walchshofer, Conny
   Dhanoa, Vaishali
   Humer, Christina
   Heckmann, Moritz
   Steinparz, Christian
   Streit, Marc
TI Visual Exploration of Relationships and Structure in Low-Dimensional
   Embeddings
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Task analysis; Layout; Data visualization; Space
   exploration; Visual analytics; Trajectory; Dimensionality reduction;
   projection; visual analytics; layout enrichment; aggregation; comparison
ID PROJECTIONS; REDUCTION; SPACE
AB In this work, we propose an interactive visual approach for the exploration and formation of structural relationships in embeddings of high-dimensional data. These structural relationships, such as item sequences, associations of items with groups, and hierarchies between groups of items, are defining properties of many real-world datasets. Nevertheless, most existing methods for the visual exploration of embeddings treat these structures as second-class citizens or do not take them into account at all. In our proposed analysis workflow, users explore enriched scatterplots of the embedding, in which relationships between items and/or groups are visually highlighted. The original high-dimensional data for single items, groups of items, or differences between connected items and groups are accessible through additional summary visualizations. We carefully tailored these summary and difference visualizations to the various data types and semantic contexts. During their exploratory analysis, users can externalize their insights by setting up additional groups and relationships between items and/or groups. We demonstrate the utility and potential impact of our approach by means of two use cases and multiple examples from various domains.
C1 [Eckelt, Klaus; Hinterreiter, Andreas; Adelberger, Patrick; Walchshofer, Conny; Dhanoa, Vaishali; Humer, Christina; Heckmann, Moritz; Steinparz, Christian; Streit, Marc] Johannes Kepler Univ Linz, A-4040 Linz, Austria.
   [Dhanoa, Vaishali] Pro2Future GmbH, A-4040 Linz, Austria.
C3 Johannes Kepler University Linz
RP Eckelt, K (corresponding author), Johannes Kepler Univ Linz, A-4040 Linz, Austria.
EM klaus.eckelt@jku.at; andreas.hinterreiter@jku.at;
   patrick.adelberger@jku.at; conny.walchshofer@jku.at;
   vaishali.dhanoa@pro2future.at; christina.humer@jku.at;
   moritz.heckmann@datavisyn.io; christian.steinparz@jku.at;
   marc.streit@jku.at
OI Humer, Christina/0000-0002-0249-4062; Dhanoa,
   Vaishali/0000-0002-0493-8616; Adelberger, Patrick/0000-0003-1819-5929;
   Eckelt, Klaus/0000-0001-6832-9070; Reis, Conny/0000-0003-3942-8445;
   Streit, Marc/0000-0001-9186-2092
FU Boehringer Ingelheim Regional Center Vienna; State of Upper Austria;
   Austrian Federal Ministry of Education, Science and Research via the LIT
   - Linz Institute of Technology [LIT-2019-7-SEE-117]; State of Upper
   Austria (Human-Interpretable Machine Learning); Austrian Science Fund
   [FWF DFH 23-N]; FFG [881844]; Austrian COMET Program Competence Centers
   for Excellent Technologies under the Austrian Federal Ministry for
   Climate Action, Environment, Energy, Mobility, Innovation and
   Technology; Austrian Federal Ministry for Digital and Economic Affairs;
   Province of Upper Austria; Province of Styria
FX This work was supported in part by Boehringer Ingelheim Regional Center
   Vienna, the State of Upper Austria and the Austrian Federal Ministry of
   Education, Science and Research via the LIT - Linz Institute of
   Technology under Grant LIT-2019-7-SEE-117, in part by the State of Upper
   Austria (Human-Interpretable Machine Learning), and the Austrian Science
   Fund under Grant FWF DFH 23-N and in part by FFG, under Grant 881844:
   "Pro2Future is funded within the Austrian COMET Program Competence
   Centers for Excellent Technologies under the auspices of the Austrian
   Federal Ministry for Climate Action, Environment, Energy, Mobility,
   Innovation and Technology, the Austrian Federal Ministry for Digital and
   Economic Affairs and of the Provinces of Upper Austria and Styria. COMET
   is managed by the Austrian Research Promotion Agency FFG".
NR 70
TC 7
Z9 7
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2023
VL 29
IS 7
BP 3312
EP 3326
DI 10.1109/TVCG.2022.3156760
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA H4XO7
UT WOS:000996011900012
PM 35254984
OA Green Submitted, hybrid
DA 2025-03-07
ER

PT J
AU Mancinelli, C
   Nazzaro, G
   Pellacini, F
   Puppo, E
AF Mancinelli, Claudio
   Nazzaro, Giacomo
   Pellacini, Fabio
   Puppo, Enrico
TI b/Surf: Interactive Bezier Splines on Surface Meshes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Manifolds; Splines (mathematics); Measurement; Approximation algorithms;
   Visualization; Robustness; Geometry; Geometric meshes; spline curves;
   user interfaces; geometry processing
ID SUBDIVISION SCHEMES; CURVES; SMOOTHNESS
AB We present a practical framework to port Bezier curves to surfaces. We support the interactive drawing and editing of Bezier splines on manifold meshes with millions of triangles, by relying on just repeated manifold averages. We show that direct extensions of the de Casteljau and Bernstein evaluation algorithms to the manifold setting are fragile, and prone to discontinuities when control polygons become large. Conversely, approaches based on subdivision are robust and can be implemented efficiently. We implement manifold extensions of the recursive de Casteljau bisection, and an open-uniform Lane-Riesenfeld subdivision scheme. For both schemes, we present algorithms for curve tracing, point evaluation, and approximated point insertion. We run bulk experiments to test our algorithms for robustness and performance, and we compare them with other methods at the state of the art, always achieving correct results and superior performance. For interactive editing, we port all the basic user interface interactions found in 2D tools directly to the mesh. We also support mapping complex SVG drawings to the mesh and their interactive editing.
C1 [Mancinelli, Claudio; Puppo, Enrico] Univ Genoa, Dept Informat Bioengn Robot & Syst Engn, I-16126 Genoa, GE, Italy.
   [Nazzaro, Giacomo; Pellacini, Fabio] Sapienza Univ Rome, Dept Comp Sci, I-00185 Rome, RM, Italy.
C3 University of Genoa; Sapienza University Rome
RP Puppo, E (corresponding author), Univ Genoa, Dept Informat Bioengn Robot & Syst Engn, I-16126 Genoa, GE, Italy.
EM claudio.mancinelli@dibris.unige.it; nazzaro@di.uniroma1.it;
   pellacini@di.uniroma1.it; enrico.puppo@unige.it
RI Mancinelli, Claudio/AAU-9061-2021
OI Mancinelli, Claudio/0000-0001-7935-3500; Puppo,
   Enrico/0000-0001-9780-5283
NR 54
TC 7
Z9 7
U1 1
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2023
VL 29
IS 7
BP 3419
EP 3435
DI 10.1109/TVCG.2022.3171179
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA H4XO7
UT WOS:000996011900020
PM 35503826
DA 2025-03-07
ER

PT J
AU Zhang, SH
   Chen, CH
   Zollmann, S
AF Zhang, Song-Hai
   Chen, Chiahao
   Zollmann, Stefanie
TI One-Step Out-of-Place Resetting for Redirected Walking in VR
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Legged locomotion; Virtual environments; Aerospace electronics; User
   interfaces; Tracking; Teleportation; Reinforcement learning; Redirected
   walking; out-of-place resetting and two-arrows indicator
AB Redirected walking (RDW) allows users to explore virtual environments in limited physical spaces by imperceptibly steering them away from obstacles and space boundaries. However, even with those techniques, the risk of collision cannot always be avoided. For such situations, resetting techniques have been proposed to provide an immediate adjustment of the physical walking direction of a user. Existing resetting techniques are either applied in-place, where the user changes orientation but stays in the same position or out-of-place methods where the user is guided to move from the current position to a safe location all while freezing the movement in the virtual world. While out-of-place methods have the potential to provide more freedom to user movements after resetting, current out-of-place methods do not provide enough guidance for the users to move to optimal locations. In this work, we propose a novel out-of-place resetting strategy that guides users to optimal physical locations with the most potential for free movement and a smaller amount of resetting required for their further movements. For this purpose, we calculate a heat map of the walking area according to the average walking distance using a simulation of the currently used RDW algorithm. Based on this heat map, we identify the most suitable position for a one-step reset within a predefined searching range and use this one as the reset point. Our results show that our method increases the average moving distance within one cycle of resetting. Furthermore, our resetting method can be applied to any physical area with obstacles. That means that RDW methods that were not suitable for such environments (e.g., Steer to Center) combined with our resetting can also be extended to such complex walking areas. In addition, we present a user interface to provide a similar visual experience between these methods, using a two-arrows indicator to help users adjust their position and direction.
C1 [Zhang, Song-Hai; Chen, Chiahao] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
   [Zollmann, Stefanie] Univ Otago, Dept Comp Sci, Dunedin 9016, New Zealand.
C3 Tsinghua University; University of Otago
RP Zhang, SH (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
EM shz@tsinghua.edu.cn; accplusjh@gmail.com; stefanie.zollmann@otago.ac.nz
OI Zollmann, Stefanie/0000-0002-4690-5409
FU National Natural Science Foundation of China [62132012, 61772298];
   Research Grant of Beijing Higher Institution Engineering Research
   Center; Tsinghua-Tencent Joint Laboratory for Internet Innovation
   Technology
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62132012 and 61772298, in part by
   Research Grant of Beijing Higher Institution Engineering Research
   Center, and Tsinghua-Tencent Joint Laboratory for Internet Innovation
   Technology.
NR 32
TC 19
Z9 20
U1 2
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2023
VL 29
IS 7
BP 3327
EP 3339
DI 10.1109/TVCG.2022.3158609
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA H4XO7
UT WOS:000996011900013
PM 35275821
DA 2025-03-07
ER

PT J
AU Resck, LE
   Ponciano, JR
   Nonato, LG
   Poco, J
AF Resck, Lucas E.
   Ponciano, Jean R.
   Nonato, Luis Gustavo
   Poco, Jorge
TI LegalVis: Exploring and Inferring Precedent Citations in Legal Documents
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Law; Data visualization; Visual analytics; Task analysis; Text analysis;
   Analytical models; Natural language processing; Legal documents; visual
   analytics; Brazilian legal system; natural language processing
ID NONNEGATIVE MATRIX; VISUALIZATION; ALGORITHMS
AB To reduce the number of pending cases and conflicting rulings in the Brazilian Judiciary, the National Congress amended the Constitution, allowing the Brazilian Supreme Court (STF) to create binding precedents (BPs), i.e., a set of understandings that both Executive and lower Judiciary branches must follow. The STF's justices frequently cite the 58 existing BPs in their decisions, and it is of primary relevance that judicial experts could identify and analyze such citations. To assist in this problem, we propose LegalVis, a web-based visual analytics system designed to support the analysis of legal documents that cite or could potentially cite a BP. We model the problem of identifying potential citations (i.e., non-explicit) as a classification problem. However, a simple score is not enough to explain the results; that is why we use an interpretability machine learning method to explain the reason behind each identified citation. For a compelling visual exploration of documents and BPs, LegalVis comprises three interactive visual components: the first presents an overview of the data showing temporal patterns, the second allows filtering and grouping relevant documents by topic, and the last one shows a document's text aiming to interpret the model's output by pointing out which paragraphs are likely to mention the BP, even if not explicitly specified. We evaluated our identification model and obtained an accuracy of 96%; we also made a quantitative and qualitative analysis of the results. The usefulness and effectiveness of LegalVis were evaluated through two usage scenarios and feedback from six domain experts.
C1 [Resck, Lucas E.; Ponciano, Jean R.; Poco, Jorge] Fundacao Getulio Vargas, BR-22250900 Rio De Janeiro, Brazil.
   [Nonato, Luis Gustavo] Univ Sao Paulo, ICMC, BR-13566590 Sao Carlos, Brazil.
   [Poco, Jorge] Univ Catolica San Pablo, Arequipa 04001, Peru.
C3 Getulio Vargas Foundation; Universidade de Sao Paulo; Universidad
   Catolica San Pablo
RP Resck, LE (corresponding author), Fundacao Getulio Vargas, BR-22250900 Rio De Janeiro, Brazil.
EM lucas.domingues@fgv.edu.br; jean.ponciano@fgv.br; gnonato@icmc.usp.br;
   jorge.poco@fgv.br
RI Ponciano, Jean/AGE-0314-2022; Nonato, Luis/D-5782-2011; Poco,
   Jorge/F-3344-2016
OI Poco, Jorge/0000-0001-9096-6287; Ponciano, Jean
   Roberto/0000-0003-4629-3542; Resck, Lucas/0000-0001-9634-450X
FU CNPq-Brazil [303552/2017-4, 312483/2018-0]; Rio de Janeiro State Funding
   Agency (FAPERJ)-Brazil [E-26/201.424/2021]; Sao Paulo Research
   Foundation (FAPESP)-Brazil [2013/07375-0]; School of Applied Mathematics
   at Fundacao Getulio Vargas (FGV)
FX This work was supported in part by CNPq-Brazil under Grants
   #303552/2017-4 and #312483/2018-0, in part by Rio de Janeiro State
   Funding Agency (FAPERJ)-Brazil under Grant #E-26/201.424/2021, in part
   by Sao Paulo Research Foundation (FAPESP)-Brazil under Grant
   #2013/07375-0, and the School of Applied Mathematics at Fundacao Getulio
   Vargas (FGV).
NR 75
TC 3
Z9 4
U1 1
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2023
VL 29
IS 6
BP 3105
EP 3120
DI 10.1109/TVCG.2022.3152450
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F4DZ2
UT WOS:000981880500020
PM 35180081
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Chao, QW
   Liu, PF
   Han, Y
   Lin, YY
   Li, CE
   Miao, QG
   Jin, XG
AF Chao, Qianwen
   Liu, Pengfei
   Han, Yi
   Lin, Yingying
   Li, Chaoneng
   Miao, Qiguang
   Jin, Xiaogang
TI A Calibrated Force-Based Model for Mixed Traffic Simulation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computational modeling; Roads; Force; Solid modeling; Microscopy; Data
   models; Trajectory; Traffic simulation; simulator; detailed traffic
   flow; heterogeneous; social force
ID ANIMATION; WAVES; FLOW
AB Virtual traffic benefits a variety of applications, including video games, traffic engineering, autonomous driving, and virtual reality. To date, traffic visualization via different simulation models can reconstruct detailed traffic flows. However, each specific behavior of vehicles is always described by establishing an independent control model. Moreover, mutual interactions between vehicles and other road users are rarely modeled in existing simulators. An all-in-one simulator that considers the complex behaviors of all potential road users in a realistic urban environment is urgently needed. In this work, we propose a novel, extensible, and microscopic method to build heterogeneous traffic simulation using the force-based concept. This force-based approach can accurately replicate the sophisticated behaviors of various road users and their interactions in a simple and unified manner. We calibrate the model parameters using real-world traffic trajectory data. The effectiveness of this approach is demonstrated through many simulation experiments, as well as comparisons to real-world traffic data and popular microscopic simulators for traffic animation.
C1 [Chao, Qianwen; Li, Chaoneng; Miao, Qiguang] Xidian Univ, Dept Comp Sci, Xian 710038, Shaanxi, Peoples R China.
   [Liu, Pengfei; Han, Yi; Lin, Yingying; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
C3 Xidian University; Zhejiang University
RP Chao, QW (corresponding author), Xidian Univ, Dept Comp Sci, Xian 710038, Shaanxi, Peoples R China.; Jin, XG (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM chaoqianwen15@gmail.com; lpengfei@zju.edu.cn; hany28@mail2.sysu.edu.cn;
   missup109@163.com; xdchaonengli@163.com; qgmiao@mail.xidian.edu.cn;
   jin@cad.zju.edu.cn
RI Liu, Pengfei/A-9352-2016
OI Miao, Qiguang/0000-0002-2872-388X; Jin, Xiaogang/0000-0001-7339-2920;
   Liu, Pengfei/0000-0003-1160-1242; Han, Yi/0000-0002-9548-7979; li,
   chaoneng/0000-0002-2781-4683
FU National Natural Science Foundation of China [62036010, 61772396,
   61772392, 61902296]; Key Research and Development Program of Zhejiang
   Province [2020C03096]; National Key R&D Program of China
   [2018YFC0807500]; Xian Key Laboratory of Big Data and Intelligent Vision
   [201805053ZD4CG37]
FX The work of Xiaogang Jin was supported in part by the National Natural
   Science Foundation of China under Grant 62036010 and in part by the Key
   Research and Development Program of Zhejiang Province under Grant
   2020C03096. Qiguang Miao was supported in part by the National Key R & D
   Program of China under Grant 2018YFC0807500, in part by the National
   Natural Science Foundations of China under Grants 61772396, 61772392,and
   61902296, and in part by the Xian Key Laboratory of Big Data and
   Intelligent Vision under Grant 201805053ZD4CG37
NR 71
TC 8
Z9 8
U1 3
U2 24
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2023
VL 29
IS 3
BP 1664
EP 1677
DI 10.1109/TVCG.2021.3128286
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8N3OW
UT WOS:000925059900005
PM 34784277
DA 2025-03-07
ER

PT J
AU Bollen, B
   Tennakoon, P
   Levine, JA
AF Bollen, Brian
   Tennakoon, Pasindu
   Levine, Joshua A. A.
TI Computing a Stable Distance on Merge Trees
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Merge trees; scalar fields; distance measure; stability; edit distance;
   persistence
ID REEB GRAPHS; COMPUTATION; PERSISTENCE
AB Distances on merge trees facilitate visual comparison of collections of scalar fields. Two desirable properties for these distances to exhibit are 1) the ability to discern between scalar fields which other, less complex topological summaries cannot and 2) to still be robust to perturbations in the dataset. The combination of these two properties, known respectively as stability and discriminativity, has led to theoretical distances which are either thought to be or shown to be computationally complex and thus their implementations have been scarce. In order to design similarity measures on merge trees which are computationally feasible for more complex merge trees, many researchers have elected to loosen the restrictions on at least one of these two properties. The question still remains, however, if there are practical situations where trading these desirable properties is necessary. Here we construct a distance between merge trees which is designed to retain both discriminativity and stability. While our approach can be expensive for large merge trees, we illustrate its use in a setting where the number of nodes is small. This setting can be made more practical since we also provide a proof that persistence simplification increases the outputted distance by at most half of the simplified value. We demonstrate our distance measure on applications in shape comparison and on detection of periodicity in the von Karman vortex street.
C1 [Bollen, Brian] Univ Arizona, Dept Math, Tucson, AZ 85721 USA.
   [Tennakoon, Pasindu; Levine, Joshua A. A.] Univ Arizona, Dept Comp Sci, Tucson, AZ USA.
C3 University of Arizona; University of Arizona
RP Bollen, B (corresponding author), Univ Arizona, Dept Math, Tucson, AZ 85721 USA.
EM bbollen23@math.arizona.edu; pasindut@cs.arizona.edu; josh@cs.arizona.edu
FU U.S. Department of Energy, Office of Science, Office of Advanced
   Scientific Computing Research [DE-SC-0019039]
FX We thank Raghavendra Sridharamurthy and Vijay Natarajan for providing
   the comparison results of their algorithm [36] on experiment used in
   Sect. 7.1. We also thank our anonymous reviewers for provided their
   detailed feedback and suggestions. This work is supported in part by the
   U.S. Department of Energy, Office of Science, Office of Advanced
   Scientific Computing Research, under Award Number(s) DE-SC-0019039.
NR 45
TC 3
Z9 3
U1 7
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2023
VL 29
IS 1
BP 1168
EP 1177
DI 10.1109/TVCG.2022.3209395
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7F6YZ
UT WOS:000901991800013
PM 36197851
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Chowdhury, TI
   Quarles, J
AF Chowdhury, Tanvir Irfan
   Quarles, John
TI A Wheelchair Locomotion Interface in a VR Disability Simulation Reduces
   Implicit Bias
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual reality; disability simulation; implicit association test; IAT;
   bias; immersion; presence; learning; information recall; head-mounted
   display; HMD
ID WALKING-IN-PLACE; ASSOCIATION TEST; VIRTUAL-REALITY; ENVIRONMENTS;
   EXPERIENCE
AB This research investigates how experiencing virtual embodiment in a wheelchair affects implicit bias towards people who use wheelchairs. We also investigate how receiving information from a virtual instructor who uses a wheelchair affects implicit bias towards people who use wheelchairs. Implicit biases are actions or judgments of people towards various concepts or stereotypes (e.g., races). We hypothesized that experiencing a Disability Simulation (DS) through an avatar in a wheelchair and receiving information from an instructor with a disability will have a significant effect on participants' ability to recall disability-related information and will reduce implicit biases towards people who use wheelchairs. To investigate this hypothesis, a 2x2 between-subjects user study was conducted where participants experienced an immersive VR DS that presents information about Multiple Sclerosis (MS) with factors of instructor (i.e., instructor with a disability versus instructor without a disability) and locomotion interface (i.e., without a disability locomotion through in-place-walking, with a disability - locomotion in a wheelchair). Participants took a disability-focused Implicit Association Test two times, once before and once after experiencing the DS. They also took a test of knowledge retention about MS. The primary result is: experiencing the DS through locomotion in a wheelchair was better for both the disability-related information recall task and reducing implicit bias towards people who use wheelchairs.
C1 [Chowdhury, Tanvir Irfan] Marshall Univ, Dept Comp Sci, Huntington, WV 25755 USA.
   [Quarles, John] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX 78249 USA.
C3 Marshall University; University of Texas System; University of Texas at
   San Antonio (UTSA)
RP Chowdhury, TI (corresponding author), Marshall Univ, Dept Comp Sci, Huntington, WV 25755 USA.
EM chowdhuryt@marshall.edu; john.quarles@utsa.edu
RI Chowdhury, Tanvir Irfan/IXD-4222-2023
OI Chowdhury, Tanvir Irfan/0000-0001-9528-3784
FU National Multiple Sclerosis Society; National Science Foundation
   [IIS-1350995]
FX This work was supported jointly by National Multiple Sclerosis Society
   and National Science Foundation under Grant IIS-1350995.
NR 51
TC 7
Z9 7
U1 1
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4658
EP 4670
DI 10.1109/TVCG.2021.3099115
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400050
PM 34310308
DA 2025-03-07
ER

PT J
AU Shen, JX
   Dudley, J
   Mo, G
   Kristensson, PO
AF Shen, Junxiao
   Dudley, John
   Mo, George
   Kristensson, Per Ola
TI Gesture Spotter: A Rapid Prototyping Tool for Key Gesture Spotting in
   Virtual and Augmented Reality Applications
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 21st IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 17-22, 2022
CL Singapore, SINGAPORE
SP IEEE
DE Gesture interaction; prototyping tools; machine learning; augmented
   reality; virtual reality
AB In this paper we examine the task of key gesture spotting: accurate and timely online recognition of hand gestures. We specifically seek to address two key challenges faced by developers when integrating key gesture spotting functionality into their applications. These are: i) achieving high accuracy and zero or negative activation lag with single-time activation; and ii) avoiding the requirement for deep domain expertise in machine learning. We address the first challenge by proposing a key gesture spotting architecture consisting of a novel gesture classifier model and a novel single-time activation algorithm. This key gesture spotting architecture was evaluated on four separate hand skeleton gesture datasets, and achieved high recognition accuracy with early detection. We address the second challenge by encapsulating different data processing and augmentation strategies, as well as the proposed key gesture spotting architecture, into a graphical user interface and an application programming interface. Two user studies demonstrate that developers are able to efficiently construct custom recognizers using both the graphical user interface and the application programming interface.
C1 [Shen, Junxiao; Dudley, John; Mo, George; Kristensson, Per Ola] Univ Cambridge, Cambridge, England.
C3 University of Cambridge
RP Shen, JX (corresponding author), Univ Cambridge, Cambridge, England.
EM js2283@cam.ac.uk; jjd50@cam.ac.uk; gm621@cam.ac.uk; pok21@cam.ac.uk
OI Kristensson, Per Ola/0000-0002-7139-871X; Shen,
   Junxiao/0000-0002-1552-4689
FU EPSRC [EP/S027432/1]; EPSRC [EP/S027432/1] Funding Source: UKRI
FX John Dudley and Per Ola Kristensson were supported by EPSRC (grant
   EP/S027432/1).
NR 51
TC 8
Z9 8
U1 4
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2022
VL 28
IS 11
BP 3618
EP 3628
DI 10.1109/TVCG.2022.3203004
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 5M3ZS
UT WOS:000871038200007
PM 36048982
OA Green Published
DA 2025-03-07
ER

PT J
AU Yao, LJ
   Bezerianos, A
   Vuillemot, R
   Isenberg, P
AF Yao, Lijie
   Bezerianos, Anastasia
   Vuillemot, Romain
   Isenberg, Petra
TI Visualization in Motion: A Research Agenda and Two Evaluations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Sports; Games; Visualization; Trajectory; Animation;
   Complexity theory; Visualization; visualization in motion; perception;
   research agenda; movement; motion
ID DYNAMIC VISUAL-ACUITY; MOVING TEST OBJECTS; INFORMATION VISUALIZATION;
   ANIMATED TRANSITIONS; AUGMENTED REALITY; OCULAR PURSUIT; EYE-MOVEMENTS;
   PERCEPTION; EXPLORATION; PERFORMANCE
AB We contribute a research agenda for visualization in motion and two experiments to understand how well viewers can read data from moving visualizations. We define visualizations in motion as visual data representations that are used in contexts that exhibit relative motion between a viewer and an entire visualization. Sports analytics, video games, wearable devices, or data physicalizations are example contexts that involve different types of relative motion between a viewer and a visualization. To analyze the opportunities and challenges for designing visualization in motion, we show example scenarios and outline a first research agenda. Motivated primarily by the prevalence of and opportunities for visualizations in sports and video games we started to investigate a small aspect of our research agenda: the impact of two important characteristics of motion-speed and trajectory on a stationary viewer's ability to read data from moving donut and bar charts. We found that increasing speed and trajectory complexity did negatively affect the accuracy of reading values from the charts and that bar charts were more negatively impacted. In practice, however, this impact was small: both charts were still read fairly accurately.
C1 [Yao, Lijie; Bezerianos, Anastasia; Isenberg, Petra] Univ Paris Saclay, LISN, INRIA, CNRS, F-91190 Gif Sur Yvette, France.
   [Vuillemot, Romain] Univ Lyon, Ecole Cent Lyon, UMR5205, CNRS,LIRIS, F-69134 Lyon, France.
C3 Universite Paris Saclay; Inria; Centre National de la Recherche
   Scientifique (CNRS); Ecole Centrale de Lyon; Institut National des
   Sciences Appliquees de Lyon - INSA Lyon; Centre National de la Recherche
   Scientifique (CNRS)
RP Yao, LJ (corresponding author), Univ Paris Saclay, LISN, INRIA, CNRS, F-91190 Gif Sur Yvette, France.
EM lijie.yao@inria.fr; anastasia.bezerianos@lri.fr;
   romain.vuillemot@ec-lyon.fr; petra.isenberg@inria.fr
RI Yao, Lijie/ISS-7925-2023; Vuillemot, Romain/ABE-5719-2020
OI Vuillemot, Romain/0000-0003-1447-6926; Yao, Lijie/0000-0002-4208-5140;
   Isenberg, Petra/0000-0002-2948-6417; Bezerianos,
   Anastasia/0000-0002-7142-2548
NR 116
TC 12
Z9 12
U1 1
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2022
VL 28
IS 10
BP 3546
EP 3562
DI 10.1109/TVCG.2022.3184993
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4G5UL
UT WOS:000849261100017
PM 35727779
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zheng, CW
   Xu, F
AF Zheng, Chengwei
   Xu, Feng
TI DTexFusion: Dynamic Texture Fusion Using a Consumer RGBD Sensor
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image reconstruction; Geometry; Three-dimensional displays;
   Optimization; Dynamics; Image color analysis; Faces; Texture
   reconstruction; dynamic texture; novel view synthesis; 3D dynamic
   reconstruction
ID REGISTRATION; RECONSTRUCTION; SUPERRESOLUTION; GEOMETRY
AB In addition to 3D geometry, accurate representation of texture is important when digitizing real objects in virtual worlds. Based on a single consumer RGBD sensor, accurate texture representation for static objects can be realized by fusing multi-frame information; however, extending the process to dynamic objects, which typically have time-varying textures, is difficult. Thus, to address this problem, we propose a compact keyframe-based representation that decouples a dynamic texture into a basic static texture and a set of multiplicative changing maps. With this representation, the proposed method first aligns textures recorded from multiple keyframes with the reconstructed dynamic geometry of the object. Errors in the alignment and geometry are then compensated in an innovative iterative linear optimization framework. With the reconstructed texture, we then employ a scheme to synthesize the dynamic object from arbitrary viewpoints. By considering temporal and local pose similarities jointly, dynamic textures in all keyframes are fused to guarantee high-quality image generation. Experimental results demonstrate that the proposed method handles various dynamic objects, including faces, bodies, cloth, and toys. In addition, qualitative and quantitative comparisons demonstrate that the proposed method outperforms state-of-the-art solutions.
C1 [Zheng, Chengwei; Xu, Feng] Tsinghua Univ, Sch Software, BNRist, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Xu, F (corresponding author), Tsinghua Univ, Sch Software, BNRist, Beijing 100084, Peoples R China.
EM zhengcw18@gmail.com; xufeng2003@gmail.com
RI Zheng, Chengwei/HPC-8073-2023
OI Zheng, Chengwei/0000-0002-3657-0297
FU National Key R&D Program of China [2018YFA0704000]; NSFC [61822111,
   61727808]; Beijing Natural Science Foundation [JQ19015]
FX This work was supported by the National Key R&D Program of China under
   Grant 2018YFA0704000, the NSFC under Grants 61822111, 61727808 and
   Beijing Natural Science Foundation under Grant JQ19015.
NR 62
TC 5
Z9 5
U1 1
U2 26
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2022
VL 28
IS 10
BP 3365
EP 3375
DI 10.1109/TVCG.2021.3064846
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4G5UL
UT WOS:000849261100004
PM 33687843
DA 2025-03-07
ER

PT J
AU Anderson, CL
   Robinson, AC
AF Anderson, Cary L.
   Robinson, Anthony C.
TI Affective Congruence in Visualization Design: Influences on Reading
   Categorical Maps
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image color analysis; Data visualization; Visualization; Task analysis;
   Automobiles; Tools; Sports; Color; visualization; emotion; design;
   cartography
ID COLOR; OPTIMIZATION; PERCEPTION; SCALES
AB Recent work in data visualization has demonstrated that small, perceptually-distinct color palettes-such as those used in categorical mapping-can connote significant affective qualities. Data that are mapped or otherwise visualized are also often emotive in nature, either inherently (e.g., climate change, disease mortality rates), or by design, such as can be found in visual storytelling. However, little is known about how the affective qualities of color interact with those of data context in visualization design. This article describes the results of a crowdsourced study on the influence of affectively congruent versus incongruent color schemes on categorical map-reading response. We report both objective (pattern detection; area comparison) and subjective (affective quality; appropriateness; preference) measures of map-reader response. Our results suggest that affectively congruent colors amplify perceptions of the affective qualities of maps with emotive topics, affective incongruence may cause confusion, and that affective congruence is particularly influential in maps of positive-leaning data topics. Finally, we offer preliminary design recommendations for balancing color congruence with other design factors, and for synthesizing color and affective context in thematic map design.
C1 [Anderson, Cary L.] Univ Pittsburgh, Katz Grad Sch Business, Mkt & Business Econ Area, Pittsburgh, PA 15260 USA.
   [Robinson, Anthony C.] Penn State Univ, GeoVISTA Ctr, Dept Geog, University Pk, PA 16803 USA.
C3 Pennsylvania Commonwealth System of Higher Education (PCSHE); University
   of Pittsburgh; Pennsylvania Commonwealth System of Higher Education
   (PCSHE); Pennsylvania State University; Pennsylvania State University -
   University Park
RP Anderson, CL (corresponding author), Univ Pittsburgh, Katz Grad Sch Business, Mkt & Business Econ Area, Pittsburgh, PA 15260 USA.
EM cla77@pitt.edu; acr181@psu.edu
RI Anderson, Cary/JMC-9775-2023; Robinson, Anthony/AAZ-3051-2020
OI Robinson, Anthony/0000-0002-5249-8010; Anderson,
   Cary/0000-0003-1702-9590
NR 63
TC 12
Z9 15
U1 6
U2 43
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2022
VL 28
IS 8
BP 2867
EP 2878
DI 10.1109/TVCG.2021.3050118
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2P6BI
UT WOS:000819823600005
PM 33417558
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Li, XY
   Zhang, B
   Liao, J
   Sander, P
AF Li, Xiaoyu
   Zhang, Bo
   Liao, Jing
   Sander, Pedro, V
TI Deep Sketch-Guided Cartoon Video Inbetweening
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Animation; Interpolation; Image color analysis; Two dimensional
   displays; Estimation; Streaming media; Semantics; 2D cartoon animation;
   sketch-guided synthesis; frame interpolation
ID FLOW
AB We propose a novel framework to produce cartoon videos by fetching the color information from two input keyframes while following the animated motion guided by a user sketch. The key idea of the proposed approach is to estimate the dense cross-domain correspondence between the sketch and cartoon video frames, and employ a blending module with occlusion estimation to synthesize the middle frame guided by the sketch. After that, the input frames and the synthetic frame equipped with established correspondence are fed into an arbitrary-time frame interpolation pipeline to generate and refine additional inbetween frames. Finally, a module to preserve temporal consistency is employed. Compared to common frame interpolation methods, our approach can address frames with relatively large motion and also has the flexibility to enable users to control the generated video sequences by editing the sketch guidance. By explicitly considering the correspondence between frames and the sketch, we can achieve higher quality results than other image synthesis methods. Our results show that our system generalizes well to different movie frames, achieving better results than existing solutions.
C1 [Li, Xiaoyu] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Hong Kong, Peoples R China.
   [Zhang, Bo] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Liao, Jing] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Sander, Pedro, V] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology; Microsoft; Microsoft
   China; Microsoft Research Asia; City University of Hong Kong; Hong Kong
   University of Science & Technology
RP Sander, P (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Peoples R China.
EM xliea@connect.ust.hk; Tony.Zhang@microsoft.com; jingliao@cityu.edu.hk
RI Zhang, Bo/AAN-7181-2020
OI LIAO, Jing/0000-0001-7014-5377; Li, Xiaoyu/0000-0003-2588-1687
FU Hong Kong Research Grants Council (RGC) Early Career Scheme [CityU
   21209119]; CityU of Hong Kong under APRC [9610488]; HKUST [FSGRF1 6EG08,
   DAG06/07.EG07]
FX This work was supported in part by the Hong Kong Research Grants Council
   (RGC) Early Career Scheme under Grant CityU 21209119, the CityU of Hong
   Kong under APRC Grant 9610488, and at HKUST under Grants FSGRF1 6EG08
   and DAG06/07.EG07.
NR 60
TC 16
Z9 19
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2022
VL 28
IS 8
BP 2938
EP 2952
DI 10.1109/TVCG.2021.3049419
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2P6BI
UT WOS:000819823600010
PM 33400651
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ghosh, A
   Nashaat, M
   Miller, J
   Quader, S
AF Ghosh, Aindrila
   Nashaat, Mona
   Miller, James
   Quader, Shaikh
TI VisExPreS: A Visual Interactive Toolkit for User-Driven Evaluations of
   Embeddings
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Measurement; Visualization; Quality assessment; Data visualization;
   Stress; Distortion; Linear programming; Data and knowledge
   visualization; interactive data exploration and discovery; knowledge and
   data engineering tools and techniques
ID REPRESENTATIVE SUBSET
AB Although popularly used in big-data analytics, dimensionality reduction is a complex, black-box technique whose outcome is difficult to interpret and evaluate. In recent years, a number of quantitative and visual methods have been proposed for analyzing low-dimensional embeddings. On the one hand, quantitative methods associate numeric identifiers to qualitative characteristics of these embeddings; and, on the other hand, visual techniques allow users to interactively explore these embeddings and make decisions. However, in the former case, users do not have control over the analysis, while in the latter case. assessment decisions are entirely dependent on the user's perception and expertise. In order to bridge the gap between the two, in this article, we present VisExPreS, a visual interactive toolkit that enables a user-driven assessment of low-dimensional embeddings. VisExPreS is based on three novel techniques namely PG-LAPS, PG-GAPS, and RepSubset, that generate interpretable explanations of the preserved local and global structures in embeddings. In the first two techniques, the VisExPreS system proactively guides users during every step of the analysis. We demonstrate the utility of VisExPreS in interpreting, analyzing, and evaluating embeddings from different dimensionality reduction algorithms using multiple case studies and an extensive user study.
C1 [Ghosh, Aindrila; Nashaat, Mona; Miller, James] Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 2R3, Canada.
   [Quader, Shaikh] IBM Toronto Software Lab, Toronto, ON L6G 1C7, Canada.
C3 University of Alberta; International Business Machines (IBM); IBM Canada
RP Ghosh, A (corresponding author), Univ Alberta, Dept Elect & Comp Engn, Edmonton, AB T6G 2R3, Canada.
EM aindrila@ualberta.ca; nashaata@ualberta.ca; jimm@ualberta.ca;
   shaikhq@ca.ibm.com
RI Ghosh, Aindrila/KFA-4845-2024; Nashaat, Mona/KPY-0439-2024
OI Miller, James/0000-0001-5095-3000; Nashaat, Mona/0000-0002-7580-5757;
   Ghosh, Aindrila/0000-0003-4908-9491
NR 52
TC 3
Z9 3
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2022
VL 28
IS 7
BP 2791
EP 2807
DI 10.1109/TVCG.2020.3039106
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1P2OA
UT WOS:000801853400019
PM 33211658
DA 2025-03-07
ER

PT J
AU Yazdanpour, M
   Fan, GL
   Sheng, WH
AF Yazdanpour, Mahdi
   Fan, Guoliang
   Sheng, Weihua
TI ManhattanFusion: Online Dense Reconstruction of Indoor Scenes From Depth
   Sequences
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Image reconstruction; Solid modeling;
   Optimization; Trajectory; Computational modeling; Real-time systems;
   Manhattan frame; volumetric reconstruction; planar alignment; local pose
   optimization
AB We present a new framework for online dense 3D reconstruction of indoor scenes by using only depth sequences. This research is particularly useful in cases with a poor light condition or in a nearly featureless indoor environment. The lack of RGB information makes long-range camera pose estimation difficult in a large indoor environment. The key idea of our research is to take advantage of the geometric prior of Manhattan scenes in each stage of the reconstruction pipeline with the specific aim to reduce the cumulative registration error and overall odometry drift in a long sequence. This idea is further boosted by local Manhattan frame growing and the local-to-global strategy that leads to implicit loop closure handling for a large indoor scene. Our proposed pipeline, namely ManhattanFusion, starts with planar alignment and local pose optimization where the Manhattan constraints are imposed to create detailed local segments. These segments preserve intrinsic scene geometry by minimizing the odometry drift even under complex and long trajectories. The final model is generated by integrating all local segments into a global volumetric representation under the constraint of Manhattan frame-based registration across segments. Our algorithm outperforms others that use depth data only in terms of both the mean distance error and the absolute trajectory error, and it is also very competitive compared with RGB-D based reconstruction algorithms. Moreover, our algorithm outperforms the state-of-the-art in terms of the surface area coverage by 10-40 percent, largely due to the usefulness and effectiveness of the Manhattan assumption through the reconstruction pipeline.
C1 [Yazdanpour, Mahdi] Northern Kentucky Univ, Dept Phys Geol & Engn Technol, Highland Hts, KY 41099 USA.
   [Fan, Guoliang; Sheng, Weihua] Oklahoma State Univ, Sch Elect & Comp Engn, Stillwater, OK 74075 USA.
C3 Northern Kentucky University; Oklahoma State University System; Oklahoma
   State University - Stillwater
RP Fan, GL (corresponding author), Oklahoma State Univ, Sch Elect & Comp Engn, Stillwater, OK 74075 USA.
EM yazdanpoum1@nku.edu; guoliang.fan@okstate.edu; weihua.sheng@okstate.edu
RI Fan, Guoliang/G-2893-2011
OI Fan, Guoliang/0000-0002-8584-9040
FU US National Science Foundation (NSF) [IIS-1427345, IIS-1910993]; US
   National Institutes of Health (NIH) [R15 AG061833]; Oklahoma Center for
   the Advancement of Science and Technology (OCAST) Health Research Grant
   [HR18-069]
FX The authors would like to thank the anonymous reviewers for their
   valuable comments and suggestions that are very helpful to improve this
   paper. This work was supported in part by the US National Science
   Foundation (NSF) Grants IIS-1427345 and IIS-1910993, the US National
   Institutes of Health (NIH) Grant R15 AG061833 and the Oklahoma Center
   for the Advancement of Science and Technology (OCAST) Health Research
   Grant HR18-069.
NR 53
TC 3
Z9 3
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2022
VL 28
IS 7
BP 2668
EP 2681
DI 10.1109/TVCG.2020.3036868
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1P2OA
UT WOS:000801853400010
PM 33170778
OA Bronze, Green Accepted
DA 2025-03-07
ER

PT J
AU Lin, C
   Liu, LJ
   Li, CJ
   Kobbelt, L
   Wang, B
   Xin, SQ
   Wang, WP
AF Lin, Cheng
   Liu, Lingjie
   Li, Changjian
   Kobbelt, Leif
   Wang, Bin
   Xin, Shiqing
   Wang, Wenping
TI SEG-MAT: 3D Shape Segmentation Using Medial Axis Transform
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape analysis; shape segmentation; medial axis transform; geometry
ID POLYHEDRAL SURFACE DECOMPOSITION; MESH SEGMENTATION; CO-SEGMENTATION
AB Segmenting arbitrary 3D objects into constituent parts that are structurally meaningful is a fundamental problem encountered in a wide range of computer graphics applications. Existing methods for 3D shape segmentation suffer from complex geometry processing and heavy computation caused by using low-level features and fragmented segmentation results due to the lack of global consideration. We present an efficient method, called SEG-MAT, based on the medial axis transform (MAT) of the input shape. Specifically, with the rich geometrical and structural information encoded in the MAT, we are able to develop a simple and principled approach to effectively identify the various types of junctions between different parts of a 3D shape. Extensive evaluations and comparisons show that our method outperforms the state-of-the-art methods in terms of segmentation quality and is also one order of magnitude faster.
C1 [Lin, Cheng; Liu, Lingjie; Li, Changjian; Wang, Wenping] Univ Hong Kong, Hong Kong, Peoples R China.
   [Kobbelt, Leif] Rhein Westfal TH Aachen, Comp Sci, D-52062 Aachen, Germany.
   [Wang, Bin] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Wang, Bin] Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing 100084, Peoples R China.
   [Xin, Shiqing] Shandong Univ, Sch Comp Sci, Qingdao 266237, Peoples R China.
C3 University of Hong Kong; RWTH Aachen University; Tsinghua University;
   Shandong University
RP Lin, C (corresponding author), Univ Hong Kong, Hong Kong, Peoples R China.
EM chlin@hku.hk; liulingjie0206@gmail.com; chjili2011@gmail.com;
   kobbelt@cs.rwth-aachen.de; wangbins@tsinghua.edu.cn;
   xinshiqing@gmail.com; wenping@cs.hku.hk
RI Liu, Lingjie/LIC-0932-2024
OI Wang, Bin/0000-0002-5176-9202
FU Gottfried Wilhelm Leibniz program by DFG; National Natural Science
   Foundation of China (NSFC) [61772301, 61772016]
FX The authors would like to thank the anonymous reviewers for their
   valuable feedback and Yiling Pan for her help with data processing. This
   work is supported by the Gottfried Wilhelm Leibniz program by DFG and
   the National Natural Science Foundation of China (NSFC) under Grants No.
   61772301 and No. 61772016.
NR 54
TC 21
Z9 23
U1 0
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2022
VL 28
IS 6
BP 2430
EP 2444
DI 10.1109/TVCG.2020.3032566
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0Z1CH
UT WOS:000790817100014
PM 33079671
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Azmandian, M
   Yahata, R
   Grechkin, T
   Rosenberg, ES
AF Azmandian, Mahdi
   Yahata, Rhys
   Grechkin, Timofey
   Rosenberg, Evan Suma
TI Adaptive Redirection: A Context-Aware Redirected Walking Meta-Strategy
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Legged locomotion; Planning; Trajectory; Virtual environments; Task
   analysis; Copper; Adaptive systems; Virtual reality; redirected walking;
   locomotion; combinatorial optimization
ID MOTION COMPRESSION
AB Previous research has established redirected walking as a potential answer to exploring large virtual environments via natural locomotion within a limited physical space. However, much of the previous work has either focused on investigating human perception of redirected walking illusions or developing novel redirection techniques. In this paper, we take a broader look at the problem and formalize the concept of a complete redirected walking system. This work establishes the theoretical foundations for combining multiple redirection strategies into a unified framework known as adaptive redirection. This meta-strategy adapts based on the context, switching between a suite of strategies with a priori knowledge of their performance under the various circumstances. This paper also introduces a novel static planning strategy that optimizes gain parameters for a predetermined virtual path, known as the Combinatorially Optimized Pre-Planned Exploration Redirector (COPPER). We conducted a simulation-based experiment that demonstrates how adaptation rules can be determined empirically using machine learning, which involves partitioning the spectrum of contexts into regions according to the redirection strategy that performs best. Adaptive redirection provides a foundation for making redirected walking work in practice and can be extended to improve performance in the future as new techniques are integrated into the framework.
C1 [Azmandian, Mahdi; Yahata, Rhys; Grechkin, Timofey] Univ Southern Calif, Inst Creat Technol, Los Angeles, CA 90007 USA.
   [Rosenberg, Evan Suma] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
C3 University of Southern California; University of Minnesota System;
   University of Minnesota Twin Cities
RP Rosenberg, ES (corresponding author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.
EM suma@umn.edu
OI Suma Rosenberg, Evan/0000-0002-4826-4561
FU U.S. Army Research Laboratory [W911NF-14-D-0005]
FX The authors would like to thank Jerald Thomas for his assistance with
   the paper. This work was sponsored by the U.S. Army Research Laboratory
   under contract number W911NF-14-D-0005. Statements and opinions
   expressed do not necessarily reflect the position or the policy of the
   Government, and no official endorsement should be inferred.
NR 65
TC 7
Z9 8
U1 3
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY 1
PY 2022
VL 28
IS 5
BP 2277
EP 2287
DI 10.1109/TVCG.2022.3150500
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1R1AK
UT WOS:000803110400039
PM 35175921
DA 2025-03-07
ER

PT J
AU Kageyama, Y
   Iwai, D
   Sato, K
AF Kageyama, Yuta
   Iwai, Daisuke
   Sato, Kosuke
TI Online Projector Deblurring Using a Convolutional Neural Network
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 12-16, 2022
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, ChristchurchNZ, Virbela, Univ Canterbury, Immers Learning Res Network, Qualcomm, HIT Lab NZ, Appl Immers Gaming Initiat
DE Generators; Attenuation; Cameras; Deep learning; Calibration; Neural
   networks; Estimation; Projector deblurring; dynamic projection mapping;
   deep neural network
ID ALGORITHMS; TRACKING; ROBUST
AB Projector deblurring is an important technology for dynamic projection mapping (PM), where the distance between a projector and a projection surface changes in time. However, conventional projector deblurring techniques do not support dynamic PM because they need to project calibration patterns to estimate the amount of defocus blur each time the surface moves. We present a deep neural network that can compensate for defocus blur in dynamic PM. The primary contribution of this paper is a unique network structure that consists of an extractor and a generator. The extractor explicitly estimates a defocus blur map and a luminance attenuation map. These maps are then injected into the middle layers of the generator network that computes the compensation image. We also propose a pseudo-projection technique for synthesizing physically plausible training data, considering the geometric misregistration that potentially happens in actual PM systems. We conducted simulation and actual PM experiments and confirmed that: (1) the proposed network structure is more suitable than a simple, more general structure for projector deblurring; (2) the network trained with the proposed pseudo-projection technique can compensate projection images for defocus blur artifacts in dynamic PM; and (3) the network supports the translation speed of the surface movement within a certain range that covers normal human motions.
C1 [Kageyama, Yuta; Iwai, Daisuke; Sato, Kosuke] Osaka Univ, Grad Sch Engn Sci, Osaka, Japan.
   [Iwai, Daisuke] Japan Sci & Technol Agcy, PRESTO, Kawaguchi, Saitama, Japan.
C3 Osaka University; Japan Science & Technology Agency (JST)
RP Kageyama, Y (corresponding author), Osaka Univ, Grad Sch Engn Sci, Osaka, Japan.
EM kageyama@sens.sys.es.osaka-u.ac.jp; daisuke.iwai@sys.es.osaka-u.ac.jp;
   sato@sys.es.osaka-u.ac.jp
RI Iwai, Daisuke/R-8174-2019
OI Iwai, Daisuke/0000-0002-3493-5635
FU JSPS KAKENHI [JP20H05958]; JST,PRESTO, Japan [JPMJPRI 9J2];
   Grants-in-Aid for Scientific Research [20H05958] Funding Source: KAKEN
FX This work was supported by JSPS KAKENHI Grant Numbers JP20H05958 and
   JST,PRESTO Grant Number JPMJPRI 9J2, yJapan.
NR 52
TC 12
Z9 13
U1 0
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY 1
PY 2022
VL 28
IS 5
BP 2223
EP 2233
DI 10.1109/TVCG.2022.3150465
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 1R1AK
UT WOS:000803110400034
PM 35167455
OA hybrid
DA 2025-03-07
ER

PT J
AU Kim, H
   Lee, IK
AF Kim, Hayeon
   Lee, In-Kwon
TI Studying the Effects of Congruence of Auditory and Visual Stimuli on
   Virtual Reality Experiences
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 12-16, 2022
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, ChristchurchNZ, Virbela, Univ Canterbury, Immers Learning Res Network, Qualcomm, HIT Lab NZ, Appl Immers Gaming Initiat
DE Visualization; User experience; Solid modeling; Psychology; Multisensory
   integration; Semantics; Legged locomotion; Virtual reality; sensory
   simulation; multisensory integration; congruence; user experience
ID MULTISENSORY INTEGRATION; PERCEPTION; CORRESPONDENCES; SICKNESS;
   INFANTS; SPACE
AB Studies in virtual reality (VR) have introduced numerous multisensory simulation techniques for more immersive VR experiences. However, although they primarily focus on expanding sensory types or increasing individual sensory quality, they lack consensus in designing appropriate interactions between different sensory stimuli. This paper explores how the congruence between auditory and visual (AV) stimuli, which are the sensory stimuli typically provided by VR devices, affects the cognition and experience of VR users as a critical interaction factor in promoting multisensory integration. We defined the types of (in)congruence between AV stimuli, and then designed 12 virtual spaces with different types or degrees of congruence between AV stimuli. We then evaluated the presence, immersion, motion sickness, and cognition changes in each space. We observed the following key findings: 1) there is a limit to the degree of temporal or spatial incongruence that can be tolerated, with few negative effects on user experience until that point is exceeded; 2) users are tolerant of semantic incongruence; 3) a simulation that considers synesthetic congruence contributes to the user's sense of immersion and presence. Based on these insights, we identified the essential considerations for designing sensory simulations in VR and proposed future research directions.
C1 [Kim, Hayeon; Lee, In-Kwon] Yonsei Univ, Dept Comp Sci, Seoul, South Korea.
C3 Yonsei University
RP Kim, H (corresponding author), Yonsei Univ, Dept Comp Sci, Seoul, South Korea.
EM qoocrab@gmail.com; iklee@yonsei.ac.kr
RI Lee, In-Kwon/AGP-6124-2022; Kim, Hayeon/AAY-5003-2021
OI Kim, Hayeon/0000-0002-6529-0921
FU MSIT(Ministry of Science and ICT), Korea, under the ITRC(Information
   Technology Research Center) support program [IITP-2021-2018-0-01419];
   National Research Foundation of Korea(NRF) - Korea government(MSIT)
   [NRF2020R1A2C2014622]
FX This research was supported by the MSIT(Ministry of Science and ICT),
   Korea, under the ITRC(Information Technology Research Center) support
   program(IITP-2021-2018-0-01419) supervised by the IITP(Institute for
   Information and Communications Technology Planning and Evaluation) and
   the National Research Foundation of Korea(NRF) grant funded by the Korea
   government(MSIT). (No. NRF2020R1A2C2014622)
NR 81
TC 16
Z9 17
U1 10
U2 60
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY 1
PY 2022
VL 28
IS 5
BP 2080
EP 2090
DI 10.1109/TVCG.2022.3150514
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 1R1AK
UT WOS:000803110400023
PM 35167477
DA 2025-03-07
ER

PT J
AU Yu, K
   Eck, U
   Pankratz, F
   Lazarovici, M
   Wilhelm, D
   Navab, N
AF Yu, Kevin
   Eck, Ulrich
   Pankratz, Frieder
   Lazarovici, Marc
   Wilhelm, Dirk
   Navab, Nassir
TI Duplicated Reality for Co-located Augmented Reality Collaboration
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 12-16, 2022
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, ChristchurchNZ, Virbela, Univ Canterbury, Immers Learning Res Network, Qualcomm, HIT Lab NZ, Appl Immers Gaming Initiat
DE Collaboration; Task analysis; Three-dimensional displays; Real-time
   systems; Surgery; Annotations; Image reconstruction; User Interaction;
   Mixed Reality; 3D Reconstruction
ID RECONSTRUCTION
AB When two or more users attempt to collaborate in the same space with Augmented Reality, they often encounter conflicting intentions regarding the occupation of the same working area and self-positioning around such without mutual interference. Augmented Reality is a powerful tool for communicating ideas and intentions during a co-assisting task that requires multi-disciplinary expertise. To relax the constraint of physical co-location, we propose the concept of Duplicated Reality, where a digital copy of a 3D region of interest of the users' environment is reconstructed in real-time and visualized in-situ through an Augmented Reality user interface. This enables users to remotely annotate the region of interest while being co-located with others in Augmented Reality. We perform a user study to gain an in-depth understanding of the proposed method compared to an in-situ augmentation, including collaboration, effort, awareness, usability, and the quality of the task. The result indicates almost identical objective and subjective results, except a decrease in the consulting user's awareness of co-located users when using our method. The added benefit from duplicating the working area into a designated consulting area opens up new interaction paradigms to be further investigated for future co-located Augmented Reality collaboration systems.
C1 [Yu, Kevin; Wilhelm, Dirk] Tech Univ Munich, Univ Hosp Rechts Isar, Munich, Germany.
   [Pankratz, Frieder; Lazarovici, Marc] Ludwig Maximilians Univ Munchen, Inst Emergency Med, Munich, Germany.
   [Eck, Ulrich; Navab, Nassir] Tech Univ Munich, Chair Comp Aided Med Procedure, Munich, Germany.
C3 Technical University of Munich; University of Munich; Technical
   University of Munich
RP Yu, K (corresponding author), Tech Univ Munich, Univ Hosp Rechts Isar, Munich, Germany.
EM kevin.yu@tum.de
RI Wilhelm, Dirk/JZC-9499-2024; Lazarovici, Marc/LTC-7557-2024
OI Wilhelm, Dirk/0000-0002-2972-9802; Lazarovici, Marc/0000-0003-2694-810X
FU German Federal Ministry of Education and Research (BMBF) [16SV8092,
   16SV8090, 16SV8088]
FX This work was funded by the German Federal Ministry of Education and
   Research (BMBF), Grant No.: 16SV8092, 16SV8090, 16SV8088.
NR 78
TC 26
Z9 27
U1 0
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY 1
PY 2022
VL 28
IS 5
BP 2190
EP 2200
DI 10.1109/TVCG.2022.3150520
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 1R1AK
UT WOS:000803110400031
PM 35148264
DA 2025-03-07
ER

PT J
AU Kaul, S
   Borland, D
   Cao, N
   Gotz, D
AF Kaul, Smiti
   Borland, David
   Cao, Nan
   Gotz, David
TI Improving Visualization Interpretation Using Counterfactuals
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Social networking (online); Data
   analysis; Tools; Machine learning; Analytical models; visualization;
   counterfactuals; human-computer interaction; human-centered computing;
   empirical study
ID EXPLORATION
AB Complex, high-dimensional data is used in a wide range of domains to explore problems and make decisions. Analysis of high-dimensional data, however, is vulnerable to the hidden influence of confounding variables, especially as users apply ad hoc filtering operations to visualize only specific subsets of an entire dataset. Thus, visual data-driven analysis can mislead users and encourage mistaken assumptions about causality or the strength of relationships between features. This work introduces a novel visual approach designed to reveal the presence of confounding variables via counterfactual possibilities during visual data analysis. It is implemented in CoFact, an interactive visualization prototype that determines and visualizes counterfactual subsets to better support user exploration of feature relationships. Using publicly available datasets, we conducted a controlled user study to demonstrate the effectiveness of our approach; the results indicate that users exposed to counterfactual visualizations formed more careful judgments about feature-to-outcome relationships.
C1 [Kaul, Smiti] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA.
   [Borland, David] Univ N Carolina, RENCI, Chapel Hill, NC 27515 USA.
   [Cao, Nan] Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai, Peoples R China.
   [Gotz, David] Univ N Carolina, Sch Informat & Lib Sci, Chapel Hill, NC 27515 USA.
C3 University of North Carolina; University of North Carolina Chapel Hill;
   University of North Carolina; University of North Carolina Chapel Hill;
   Tongji University; University of North Carolina; University of North
   Carolina Chapel Hill
RP Kaul, S (corresponding author), Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA.
EM smiti@live.unc.edu; borland@renci.org; nan.cao@tongji.edu.cn;
   gotz@unc.edu
RI Cao, Nan/O-5397-2014
OI Borland, David/0000-0002-0162-4080
FU National Science Foundation [1704018]; Direct For Computer & Info Scie &
   Enginr; Div Of Information & Intelligent Systems [1704018] Funding
   Source: National Science Foundation
FX The research reported in this article was supported in part by a grant
   from the National Science Foundation (#1704018). We also thank Tabitha
   Peck for her help with data analysis.
NR 73
TC 11
Z9 11
U1 1
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 998
EP 1008
DI 10.1109/TVCG.2021.3114779
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XY1KJ
UT WOS:000736740100002
PM 34587027
OA Green Submitted, Bronze
DA 2025-03-07
ER

PT J
AU Song, H
   Dai, Z
   Xu, PP
   Ren, L
AF Song, Huan
   Dai, Zeng
   Xu, Panpan
   Ren, Liu
TI Interactive Visual Pattern Search on Graph Data via Graph Representation
   Learning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Semantics; Visual analytics; Graph neural networks;
   Computational modeling; Visual databases; Pattern matching; Graph; Graph
   Neural Network; Representation Learning; Visual Query Interface
ID EXPLORATION; ALGORITHM; VISUALIZATION; LAYOUT
AB Graphs are a ubiquitous data structure to model processes and relations in a wide range of domains. Examples include control-flow graphs in programs and semantic scene graphs in images. Identifying subgraph patterns in graphs is an important approach to understand their structural properties. We propose a visual analytics system GraphQ to support human-in-the-loop, example-based, subgraph pattern search in a database containing many individual graphs. To support fast, interactive queries, we use graph neural networks (GNNs) to encode a graph as fixed-length latent vector representation, and perform subgraph matching in the latent space. Due to the complexity of the problem, it is still difficult to obtain accurate one-to-one node correspondences in the matching results that are crucial for visualization and interpretation. We, therefore, propose a novel GNN for node-alignment called NeuroAlign, to facilitate easy validation and interpretation of the query results. GraphQ provides a visual query interface with a query editor and a multi-scale visualization of the results, as well as a user feedback mechanism for refining the results with additional constraints. We demonstrate GraphQ through two example usage scenarios: analyzing reusable subroutines in program workflows and semantic scene graph search in images. Quantitative experiments show that NeuroAlign achieves 19%-29% improvement in node-alignment accuracy compared to baseline GNN and provides up to 100x speedup compared to combinatorial algorithms. Our qualitative study with domain experts confirms the effectiveness for both usage scenarios.
C1 [Song, Huan; Dai, Zeng; Xu, Panpan; Ren, Liu] Robert Bosch Res & Technol Ctr, Cambridge, MA 02139 USA.
   [Dai, Zeng] ByteDance Inc, Beijing, Peoples R China.
   [Xu, Panpan] Amazon AWS AI, Seattle, WA USA.
C3 Bosch
RP Song, H (corresponding author), Robert Bosch Res & Technol Ctr, Cambridge, MA 02139 USA.
EM huan.song@us.bosch.com; zeng.dai@bytedance.com; xupanpan@amazon.com;
   liu.ren@us.bosch.com
NR 79
TC 6
Z9 7
U1 3
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 335
EP 345
DI 10.1109/TVCG.2021.3114857
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000045
PM 34587078
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Li, C
   Qiu, S
   Wang, CB
   Qin, H
AF Li, Chen
   Qiu, Sheng
   Wang, Changbo
   Qin, Hong
TI Learning Physical Parameters and Detail Enhancement for Gaseous Scene
   Design Based on Data Guidance
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computational modeling; Data models; Numerical models; Parameter
   estimation; Couplings; Graphics; Deep learning; Computer graphics;
   physics-based modeling and simulation; neural networks for learning
   physical parameters and detail enhancement; gaseous scene design
ID SUPERRESOLUTION
AB This article articulates a novel learning framework for both parameter estimation and detail enhancement for Eulerian gas based on data guidance. The key motivation of this article is to devise a new hybrid, grid-based simulation that could inherit modeling and simulation advantages from both physically-correct simulation methods and powerful data-driven methods, while combating existing difficulties exhibited in both approaches. We first employ a convolutional neural network (CNN) to estimate the physical parameters of gaseous phenomena in Eulerian settings, then we can use the just-learnt parameters to re-simulate (with or without artists' guidance) for specific scenes with flexible coupling effects. Next, a second CNN is adopted to reconstruct the high-resolution velocity field to guide a fast re-simulation on the finer grid, achieving richer and more realistic details with little extra computational expense. From the perspective of physics-based simulation, our trained networks respect temporal coherence and physical constraints. From the perspective of the data-driven machine-learning approaches, our network design aims at extracting a meaningful parameters and reconstructing visually realistic details. Additionally, our implementation based on parallel acceleration could significantly enhance the computational performance of every involved module. Our comprehensive experiments confirm the controllability, effectiveness, and accuracy of our novel approach when producing various gaseous scenes with rich details for widespread graphics applications.
C1 [Li, Chen; Qiu, Sheng] East China Normal Univ, Shanghai 200062, Peoples R China.
   [Wang, Changbo] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 East China Normal University; East China Normal University; State
   University of New York (SUNY) System; Stony Brook University
RP Wang, CB (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200062, Peoples R China.
EM lichen2014gyx@163.com; 1147899155@qq.com; cbwang@cs.ecnu.edu.cn;
   qin@cs.stonybrook.edu
RI 李, 晨/LCE-3200-2024
OI QIN, HONG/0000-0001-7699-1355; Wang, Changbo/0000-0001-8940-6418
FU National Natural Science Foundation of China [61532002, 61672237];
   National Science Foundation of USA [IIS-1715985, IIS-1812606]
FX This article was supported in part by the National Natural Science
   Foundation of China (61532002 and 61672237) and the National Science
   Foundation of USA (IIS-1715985 and IIS-1812606). The authors would like
   to appreciate all the anonymous reviewers for their insightful comments
   and constructive suggestions to polish this article in high quality. C.
   Li and S. Qiu contributed equally to this article.
NR 57
TC 6
Z9 6
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2021
VL 27
IS 10
BP 3867
EP 3880
DI 10.1109/TVCG.2020.2991217
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL8JB
UT WOS:000692890200003
PM 32356752
OA Bronze
DA 2025-03-07
ER

PT J
AU Bhatia, H
   Kirby, RM
   Pascucci, V
   Bremer, PT
AF Bhatia, Harsh
   Kirby, Robert M.
   Pascucci, Valerio
   Bremer, Peer-Timo
TI Vector Field Decompositions Using Multiscale Poisson Kernel
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Feature extraction; Smoothing methods; Poisson equations; Computer
   graphics; Observers; Kernel; Data mining; Helmholtz-Hodge decomposition;
   flow analysis; multiscale features
ID PROPER ORTHOGONAL DECOMPOSITION; HELMHOLTZ-HODGE DECOMPOSITION;
   TURBULENCE; JET; DYNAMICS; FLAME
AB Extraction of multiscale features using scale-space is one of the fundamental approaches to analyze scalar fields. However, similar techniques for vector fields are much less common, even though it is well known that, for example, turbulent flows contain cascades of nested vortices at different scales. The challenge is that the ideas related to scale-space are based upon iteratively smoothing the data to extract features at progressively larger scale, making it difficult to extract overlapping features. Instead, we consider spatial regions of influence in vector fields as scale, and introduce a new approach for the multiscale analysis of vector fields. Rather than smoothing the flow, we use the natural Helmholtz-Hodge decomposition to split it into small-scale and large-scale components using progressively larger neighborhoods. Our approach creates a natural separation of features by extracting local flow behavior, for example, a small vortex, from large-scale effects, for example, a background flow. We demonstrate our technique on large-scale, turbulent flows, and show multiscale features that cannot be extracted using state-of-the-art techniques.
C1 [Bhatia, Harsh; Bremer, Peer-Timo] Lawrence Livermore Natl Lab, Ctr Appl Sci Comp, Livermore, CA 94550 USA.
   [Kirby, Robert M.; Pascucci, Valerio] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
C3 United States Department of Energy (DOE); Lawrence Livermore National
   Laboratory; Utah System of Higher Education; University of Utah
RP Bhatia, H (corresponding author), Lawrence Livermore Natl Lab, Ctr Appl Sci Comp, Livermore, CA 94550 USA.
EM hbhatia@llnl.gov; kirby@sci.utah.edu; pascucci@sci.utah.edu;
   ptbremer@llnl.gov
RI pascucci, Valerio/GXF-0616-2022; Bhatia, Harsh/K-7836-2019
OI pascucci, valerio/0000-0002-8877-2042; Kirby,
   Robert/0000-0001-5712-4141; Bremer, Peer-Timo/0000-0003-4107-3831
FU U.S. Department of Energy (DOE) by Lawrence Livermore National
   Laboratory (LLNL) [DE-AC52-07NA27344]; ARO [W911NF-15-1-0222]; DOE,
   National Nuclear Security Administration (NNSA) [DE-NA0002375]; Exascale
   Computing Project [17-SC-20-SC]; DOE Office of Science; NSF OAC
   [1941085, 1842042]; NSF CMMI [1629660]; NNSA; Div Of Civil, Mechanical,
   & Manufact Inn; Directorate For Engineering [1629660] Funding Source:
   National Science Foundation
FX This work was performed under the auspices of the U.S. Department of
   Energy (DOE) by Lawrence Livermore National Laboratory (LLNL) under
   contract DE-AC52-07NA27344. The second author acknowledge support from
   ARO W911NF-15-1-0222 (Program Manager Dr. Mike Coyle). This work was
   supported in part by the DOE, National Nuclear Security Administration
   (NNSA), under Award Number(s) DE-NA0002375. This research was supported
   in part by the Exascale Computing Project (17-SC-20-SC), a collaborative
   effort of the DOE Office of Science and the NNSA. NSF OAC award 1842042.
   NSF OAC award 1941085. NSF CMMI award 1629660. LLNL Release:
   LLNL-JRNL-743978.
NR 43
TC 0
Z9 0
U1 2
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2021
VL 27
IS 9
BP 3781
EP 3793
DI 10.1109/TVCG.2020.2984413
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TS3CP
UT WOS:000679532000014
PM 32248111
OA Bronze, Green Submitted
DA 2025-03-07
ER

PT J
AU Hong, S
   Choi, J
   Jeong, WK
AF Hong, Sumin
   Choi, Junyoung
   Jeong, Won-Ki
TI Distributed Interactive Visualization Using GPU-Optimized Spark
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Sparks; Graphics processing units; Computational
   modeling; Rendering (computer graphics); Programming; Task analysis;
   MapReduce; spark; GPU; distributed rendering; in-situ visualization
ID DOMAIN-SPECIFIC LANGUAGE; FRAMEWORK
AB With the advent of advances in imaging and computing technologies, large-scale data acquisition and processing have become commonplace in many science and engineering disciplines. Conventional workflows for large-scale data processing usually rely on in-house or commercial software that are designed for domain-specific computing tasks. Recent advances in MapReduce, which was originally developed for batch processing textual data via a simplified programming model of the map and reduce functions, have expanded its applications to more general tasks in big-data processing, such as scientific computing, and biomedical image processing. However, as shown in previous work, volume rendering and visualization using MapReduce is still considered challenging and impractical owing to the disk-based, batch-processing nature of its computing model. In this article, contrary to this common belief, we show that the MapReduce computing model can be effectively used for interactive visualization. Our proposed system is a novel extension of Spark, one of the most popular open-source MapReduce frameworks, which offers GPU-accelerated MapReduce computing. To minimize CPU-GPU communication and overcome slow, disk-based shuffle performance, the proposed system supports GPU in-memory caching and MPI-based direct communication between compute nodes. To allow for GPU-accelerated in-situ visualization using raster graphics in Spark, we leveraged the CUDA-OpenGL interoperability, resulting in faster processing speeds by several orders of magnitude compared to conventional MapReduce systems. We demonstrate the performance of our system via several volume processing and visualization tasks, such as direct volume rendering, iso-surface extraction, and numerical simulations with in-situ visualization.
C1 [Hong, Sumin; Choi, Junyoung] Ulsan Natl Inst Sci & Technol, Ulsan 44919, South Korea.
   [Jeong, Won-Ki] Korea Univ, Seoul 02841, South Korea.
C3 Ulsan National Institute of Science & Technology (UNIST); Korea
   University
RP Jeong, WK (corresponding author), Korea Univ, Seoul 02841, South Korea.
EM sumin246@unist.ac.kr; juny0603@unist.ac.kr; wkjeong@korea.ac.kr
RI Jeong, Won-Ki/F-8171-2011; Hong, Sumin/GLR-8286-2022; choi,
   junyoung/T-4389-2019
OI Choi, JunYoung/0000-0002-4255-4402; Jeong, Won-Ki/0000-0002-9393-6451;
   Hong, Sumin/0000-0002-3321-2072
FU National Research Foundation of Korea (NRF) - Ministry of Education
   [NRF-2017R1D1A1A09000841]; Ministry of Science and ICT
   [NRF-2019M3E5D2A01063819, NRF-2014K1A3A1A05034557]; Korea Health
   Industry Development Institute (KHIDI) - Ministry of Health Welfare
   [HI18C0316]
FX This work was partially supported by the National Research Foundation of
   Korea (NRF) funded by the Ministry of Education
   (NRF-2017R1D1A1A09000841) and the Ministry of Science and ICT
   (NRF-2019M3E5D2A01063819, NRF-2014K1A3A1A05034557), and by the Korea
   Health Industry Development Institute (KHIDI) funded by the Ministry of
   Health & Welfare (HI18C0316). The authors would like to thank Prof.
   Hanspeter Pfister and his group at Harvard University for providing the
   connectome dataset.
NR 43
TC 7
Z9 9
U1 1
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2021
VL 27
IS 9
BP 3670
EP 3684
DI 10.1109/TVCG.2020.2990894
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TS3CP
UT WOS:000679532000006
PM 32356751
DA 2025-03-07
ER

PT J
AU Zhou, W
   Jia, JY
   Jiang, WY
   Huang, CX
AF Zhou, Wen
   Jia, Jinyuan
   Jiang, Wenying
   Huang, Chenxi
TI Sketch Augmentation-Driven Shape Retrieval Learning Framework Based on
   Convolutional Neural Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Three-dimensional displays; Two dimensional displays; Solid
   modeling; Training; Task analysis; Neural networks; Sketch-based shape
   retrieval; convolutional neural network; learning framework; sketch
   augmentation; best view; joint Bayesian fusion
AB In this article, we present a deep learning approach to sketch-based shape retrieval that incorporates a few novel techniques to improve the quality of the retrieval results. First, to address the problem of scarcity of training sketch data, we present a sketch augmentation method that more closely mimics human sketches compared to simple image transformation. Our method generates more sketches from the existing training data by (i) removing a stroke, (ii) adjusting a stroke, and (iii) rotating the sketch. As such, we generate a large number of sketch samples for training our neural network. Second, we obtain the 2D renderings of each 3D model in the shape database by determining the view positions that best depict the 3D shape: i.e., avoiding self-occlusion, showing the most salient features, and following how a human would normally sketch the model. We use a convolutional neural network (CNN) to learn the best viewing positions of each 3D model and generates their 2D images for the next step. Third, our method uses a cross-domain learning strategy based on two Siamese CNNs that pair up sketches and the 2D shape images. A joint Bayesian measure is used to measure the output similarity from these CNNs to maximize inter-class similarity and minimize intra-class similarity. Extensive experiments show that our proposed approach comprehensively outperforms many existing state-of-the-art methods.
C1 [Zhou, Wen; Jiang, Wenying] Anhui Normal Univ, Sch Comp & Informat, Wuhu 241002, Anhui, Peoples R China.
   [Jia, Jinyuan] Tongji Univ, Sch Software Engn, Shanghai 201804, Peoples R China.
   [Huang, Chenxi] Xiamen Univ, Dept Comp Sci, Xiamen 361005, Fujian, Peoples R China.
C3 Anhui Normal University; Tongji University; Xiamen University
RP Zhou, W (corresponding author), Anhui Normal Univ, Sch Comp & Informat, Wuhu 241002, Anhui, Peoples R China.
EM w.zhou@ahnu.edu.cn; tjsse17@yeah.net; jiangwenying@ahnu.edu.cn;
   909813723@qq.com
RI Huang, Chenxi/AAC-6316-2019
OI jiang, wenying/0000-0001-7647-1202; ZHOU, WEN/0000-0002-1266-1864
FU National Natural Science Foundation of China (NSFC) [61902003,
   61976006]; Key Project of Natural Science Foundation of China
   [U19A2063]; Doctoral Scientific Research Foundation of Anhui Normal
   University
FX The authors would like to thank the comments and suggestions of all the
   anonymous reviewers, whose comments helped us to significantly improve
   this paper. We thank Wei Tsang Ooi from the National University of
   Singapore, who provided valuable inputs to our manuscript. This work was
   supported in part by the National Natural Science Foundation of China
   (NSFC) (Grants. 61902003, 61976006), the Key Project of Natural Science
   Foundation of China (Grant no. U19A2063) and the Doctoral Scientific
   Research Foundation of Anhui Normal University.
NR 44
TC 2
Z9 2
U1 1
U2 31
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2021
VL 27
IS 8
BP 3558
EP 3570
DI 10.1109/TVCG.2020.2975504
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TC7PK
UT WOS:000668831500016
PM 32092007
DA 2025-03-07
ER

PT J
AU Svirsky, Y
   Sharf, A
AF Svirsky, Yonatan
   Sharf, Andrei
TI A Non-Linear Differentiable CNN-Rendering Module for 3D Data Enhancement
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Rendering (computer graphics); Shape; Task
   analysis; Two dimensional displays; Clutter; Neural networks; 3D
   convolutional neural networks; shape modeling; noise removal
AB In this article we introduce a differentiable rendering module which allows neural networks to efficiently process 3D data. The module is composed of continuous piecewise differentiable functions defined as a sensor array of cells embedded in 3D space. Our module is learnable and can be easily integrated into neural networks allowing to optimize data rendering towards specific learning tasks using gradient based methods in an end-to-end fashion. Essentially, the module's sensor cells are allowed to transform independently and locally focus and sense different parts of the 3D data. Thus, through their optimization process, cells learn to focus on important parts of the data, bypassing occlusions, clutter, and noise. Since sensor cells originally lie on a grid, this equals to a highly non-linear rendering of the scene into a 2D image. Our module performs especially well in presence of clutter and occlusions as well as dealing with non-linear deformations to improve classification accuracy through proper rendering of the data. In our experiments, we apply our module in various learning tasks and demonstrate that using our rendering module we accomplish efficient classification, localization, and segmentation tasks on 2D/3D cluttered and non-cluttered data.
C1 [Svirsky, Yonatan; Sharf, Andrei] Ben Gurion Univ Negev, Dept Comp Sci, POB 653, IL-8410501 Beer Sheva, Israel.
C3 Ben Gurion University
RP Sharf, A (corresponding author), Ben Gurion Univ Negev, Dept Comp Sci, POB 653, IL-8410501 Beer Sheva, Israel.
EM svirskyy@post.bgu.ac.il; asharf@bgu.ac.il
RI Sharf, Andrei/F-1370-2012
OI Sharf, Andrei/0000-0002-3963-4508
NR 43
TC 2
Z9 2
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2021
VL 27
IS 7
BP 3238
EP 3249
DI 10.1109/TVCG.2020.2968062
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SK0PE
UT WOS:000655924400011
PM 31985422
DA 2025-03-07
ER

PT J
AU Gao, L
   Zhang, LX
   Meng, HY
   Ren, YH
   Lai, YK
   Kobbelt, L
AF Gao, Lin
   Zhang, Ling-Xiao
   Meng, Hsien-Yu
   Ren, Yi-Hui
   Lai, Yu-Kun
   Kobbelt, Leif
TI PRS-Net: Planar Reflective Symmetry Detection Net for 3D Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Shape; Geometry; Two dimensional displays;
   Feature extraction; Solid modeling; Computational modeling; Unsupervised
   learning; convolutional neural network; symmetry detection; 3D models;
   planar reflective symmetry
AB In geometry processing, symmetry is a universal type of high-level structural information of 3D models and benefits many geometry processing tasks including shape segmentation, alignment, matching, and completion. Thus it is an important problem to analyze various symmetry forms of 3D shapes. Planar reflective symmetry is the most fundamental one. Traditional methods based on spatial sampling can be time-consuming and may not be able to identify all the symmetry planes. In this article, we present a novel learning framework to automatically discover global planar reflective symmetry of a 3D shape. Our framework trains an unsupervised 3D convolutional neural network to extract global model features and then outputs possible global symmetry parameters, where input shapes are represented using voxels. We introduce a dedicated symmetry distance loss along with a regularization loss to avoid generating duplicated symmetry planes. Our network can also identify generalized cylinders by predicting their rotation axes. We further provide a method to remove invalid and duplicated planes and axes. We demonstrate that our method is able to produce reliable and accurate results. Our neural network based method is hundreds of times faster than the state-of-the-art methods, which are based on sampling. Our method is also robust even with noisy or incomplete input surfaces.
C1 [Gao, Lin; Zhang, Ling-Xiao; Ren, Yi-Hui] Chinese Acad Sci, Beijing Key Lab Mobile Comp & Pervas Device, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Gao, Lin; Zhang, Ling-Xiao; Ren, Yi-Hui] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
   [Meng, Hsien-Yu] Univ Maryland, College Pk, MD 20742 USA.
   [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, Wales.
   [Kobbelt, Leif] Rhein Westfal TH Aachen, Inst Comp Graph & Multimedia, D-52062 Aachen, Germany.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; University System of Maryland; University of Maryland College Park;
   Cardiff University; RWTH Aachen University
RP Gao, L (corresponding author), Chinese Acad Sci, Beijing Key Lab Mobile Comp & Pervas Device, Inst Comp Technol, Beijing 100190, Peoples R China.
EM gaolin@ict.ac.cn; zhanglingxiao@ict.ac.cn; mengxy19@cs.umd.edu;
   renyihui17@mails.ucas.ac.cn; LaiY4@cardiff.ac.uk;
   kobbelt@cs.rwth-aachen.de
RI Gao, Lin/JNF-0375-2023; Lai, Yu-Kun/D-2343-2010
OI Kobbelt, Leif/0000-0002-7880-9470; Lai, Yukun/0000-0002-2094-5680
FU National Natural Science Foundation of China [61872440, 61828204];
   Beijing Municipal Natural Science Foundation [L182016]; Royal Society
   Newton Advanced Fellowship [NAF\R2\192151]; Youth Innovation Promotion
   Association CAS; CCF-Tencent Open Fund; Tencent AI Lab Rhino-Bird
   Focused Research Program [JR202024]; Open Project Program of the
   National Laboratory of Pattern Recognition [201900055]
FX This work was supported by National Natural Science Foundation of China
   (No. 61872440 and No. 61828204), Beijing Municipal Natural Science
   Foundation (No. L182016), the Royal Society Newton Advanced Fellowship
   (No. NAF\R2\192151), Youth Innovation Promotion Association CAS,
   CCF-Tencent Open Fund, Tencent AI Lab RhinoBird Focused Research Program
   (No.JR202024) and Open Project Program of the National Laboratory of
   Pattern Recognition (No. 201900055).
NR 58
TC 23
Z9 25
U1 2
U2 26
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2021
VL 27
IS 6
BP 3007
EP 3018
DI 10.1109/TVCG.2020.3003823
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SA9KC
UT WOS:000649620700018
PM 32746265
OA Green Accepted, Green Submitted
DA 2025-03-07
ER

PT J
AU Zhang, J
   Tao, J
   Wang, JX
   Wang, CL
AF Zhang, Jun
   Tao, Jun
   Wang, Jian-Xun
   Wang, Chaoli
TI SurfRiver: Flattening Stream Surfaces for Comparative Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Rivers; Rendering (computer graphics); Three-dimensional
   displays; Streaming media; Computer science; Shape; Flow visualization;
   stream surfaces; visual transformation; comparative visualization
ID FLOW
AB We present SurfRiver, a new visual transformation approach that flattens stream surfaces in 3D to rivers in 2D for comparative visualization. Leveraging the TextFlow-like visual metaphor, SurfRiver untangles the convoluted individual stream surfaces along the flow direction and maps them along the horizontal direction of the abstract river view. It stacks multiple surfaces along the vertical direction of the river view. This visual mapping makes it easy for users to track along the flow direction and align stream surfaces for comparative study. Through brushing and linking, the river view is connected to the spatial surface view for collective reasoning. SurfRiver can be used to examine a single stream surface, investigate seeding sensitivity or variability of a family of surfaces from a group of related seeding curves, or explore a collection of representative surfaces. We describe our optimization solution to achieve the desirable mapping, present SurfRiver interface and interactions, and report results from different flow fields to demonstrate its efficacy. Feedback from a domain expert also indicates the promise of SurfRiver.
C1 [Zhang, Jun; Wang, Chaoli] Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
   [Zhang, Jun; Tao, Jun] Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou, Peoples R China.
   [Tao, Jun] Natl Supercomp Ctr Guangzhou, Guangzhou, Peoples R China.
   [Wang, Jian-Xun] Univ Notre Dame, Dept Aerosp & Mech Engn, Notre Dame, IN 46556 USA.
C3 University of Notre Dame; Sun Yat Sen University; University of Notre
   Dame
RP Tao, J (corresponding author), Sun Yat Sen Univ, Sch Comp Sci & Engn, Guangzhou, Peoples R China.; Tao, J (corresponding author), Natl Supercomp Ctr Guangzhou, Guangzhou, Peoples R China.
EM jzhang28@nd.edu; taoj23@mail.sysu.edu.cn; jwang33@nd.edu;
   chaoli.wang@nd.edu
RI Wang, Chaoli/AAJ-5173-2020; Wang, Ziyan/HII-8458-2022; Wang,
   Jian-Xun/B-4119-2017
OI Wang, Jian-Xun/0000-0002-9030-1733
FU U.S. National Science Foundation [IIS-1455886, DUE-1833129, IIS-1955395,
   CMMI-1934300]; Notre Dame Asia Research Collaboration Grant Program;
   National Natural Science Foundation of China [61902446]; National
   Numerical Windtunnel Project
FX This research was supported in part by the U.S. National Science
   Foundation through grants IIS-1455886, DUE-1833129, IIS-1955395,
   CMMI-1934300, Notre Dame Asia Research Collaboration Grant Program, the
   National Natural Science Foundation of China through grant 61902446, and
   the National Numerical Windtunnel Project. The authors would like to
   thank the anonymous reviewers for their insightful comments.
NR 36
TC 3
Z9 3
U1 2
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2021
VL 27
IS 6
BP 2783
EP 2795
DI 10.1109/TVCG.2021.3074585
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SA9KC
UT WOS:000649620700002
PM 33881996
DA 2025-03-07
ER

PT J
AU Kubo, H
   Jayasuriya, S
   Iwaguchi, T
   Funatomi, T
   Mukaigawa, Y
   Narasimhan, SG
AF Kubo, Hiroyuki
   Jayasuriya, Suren
   Iwaguchi, Takafumi
   Funatomi, Takuya
   Mukaigawa, Yasuhiro
   Narasimhan, Srinivasa G.
TI Programmable Non-Epipolar Indirect Light Transport: Capture and Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cameras; Lighting; Skin; Synchronization; Delays; Scattering; Light
   transport; indirect light; synchronized projector-camera system;
   computational photography
ID COMPONENTS; SEPARATION; REFLECTION
AB The decomposition of light transport into direct and global components, diffuse and specular interreflections, and subsurface scattering allows for new visualizations of light in everyday scenes. In particular, indirect light contains a myriad of information about the complex appearance of materials useful for computer vision and inverse rendering applications. In this paper, we present a new imaging technique that captures and analyzes components of indirect light via light transport using a synchronized projector-camera system. The rectified system illuminates the scene with epipolar planes corresponding to projector rows, and we vary two key parameters to capture plane-to-ray light transport between projector row and camera pixel: (1) the offset between projector row and camera row in the rolling shutter (implemented as synchronization delay), and (2) the exposure of the camera row. We describe how this synchronized rolling shutter performs illumination multiplexing, and develop a nonlinear optimization algorithm to demultiplex the resulting 3D light transport operator. Using our system, we are able to capture live short and long-range non-epipolar indirect light transport, disambiguate subsurface scattering, diffuse and specular interreflections, and distinguish materials according to their subsurface scattering properties. In particular, we show the utility of indirect imaging for capturing and analyzing the hidden structure of veins in human skin.
C1 [Kubo, Hiroyuki; Iwaguchi, Takafumi; Funatomi, Takuya; Mukaigawa, Yasuhiro] Nara Inst Sci & Technol, Ikoma, Nara 6300192, Japan.
   [Kubo, Hiroyuki; Iwaguchi, Takafumi] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Jayasuriya, Suren] Arizona State Univ, Sch Arts Media & Engn, Tempe, AZ 85281 USA.
   [Jayasuriya, Suren] Arizona State Univ, Sch Elect Comp & Energy Engn, Tempe, AZ 85281 USA.
   [Iwaguchi, Takafumi] Kyushu Univ, Fukuoka, Fukuoka 8190395, Japan.
   [Narasimhan, Srinivasa G.] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.
C3 Nara Institute of Science & Technology; Carnegie Mellon University;
   Arizona State University; Arizona State University-Tempe; Arizona State
   University; Arizona State University-Tempe; Kyushu University; Carnegie
   Mellon University
RP Kubo, H (corresponding author), Nara Inst Sci & Technol, Ikoma, Nara 6300192, Japan.
EM hkubo@is.naist.jp; sjayasur@asu.edu; iwaguchi@ait.kyushu-u.ac.jp;
   funatomi@is.naist.jp; mukaigawa@is.naist.jp; srinivas@cs.cmu.edu
RI Kubo, Hiroyuki/AAS-1487-2021; Funatomi, Takuya/K-5919-2018
OI Kubo, Hiroyuki/0000-0002-7061-7941; Narasimhan,
   Srinivasa/0000-0003-0389-1921; Mukaigawa, Yasuhiro/0000-0001-8689-3724;
   Jayasuriya, Suren/0000-0001-7143-4429
FU JSPS [G2802]; JST CREST [JPMJCR1764]; KAKENHI [JP15K16027, 19H04138];
   NAIST global collaboration program; Herberger Research Initiative in the
   Herberger Institute for Design and the Arts (HIDA); Fulton Schools of
   Engineering (FSE) at Arizona State University; NSF [IIS-1909192]; US
   National Science Foundation Expeditions in Computing on Computational
   Photo-scatterography [NSF CCF-1730147]; Defense Advanced Research
   Projects Agency (REVEAL Grant) [HR00111620021]; Grants-in-Aid for
   Scientific Research [19H04138] Funding Source: KAKEN
FX The authors would like to thank Supreeth Achar and Joe Bartels for help
   with Episcan3D prototype development, and both Vishwanath Saragadam and
   Reikou Kamiyama for their helpful comments. This work was sponsored by
   the JSPS program for advancing strategic international networks to
   accelerate the circulation of talented researchers (G2802) and JST CREST
   JPMJCR1764 to H. Kubo, T. Funatomi, and Y. Mukaigawa, KAKENHI Grant
   number JP15K16027 and 19H04138 to H. Kubo, the NAIST global
   collaboration program to T. Iwaguchi, joint support from both the
   Herberger Research Initiative in the Herberger Institute for Design and
   the Arts (HIDA) and the Fulton Schools of Engineering (FSE) at Arizona
   State University to S. Jayasuriya, NSF IIS-1909192 to S. Jayasuriya, the
   US National Science Foundation Expeditions in Computing on Computational
   Photo-scatterography (NSF CCF-1730147) to S. Narasimhan, and the Defense
   Advanced Research Projects Agency (REVEAL Grant HR00111620021) to S.
   Jayasuriya and S. Narasimhan.
NR 62
TC 5
Z9 6
U1 0
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2021
VL 27
IS 4
BP 2421
EP 2436
DI 10.1109/TVCG.2019.2946812
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO8XL
UT WOS:000623420400013
PM 31634839
OA hybrid
DA 2025-03-07
ER

PT J
AU Katzakis, N
   Chen, LH
   Ariza, O
   Teather, RJ
   Steinicke, F
AF Katzakis, Nikolaos
   Chen, Lihan
   Ariza, Oscar
   Teather, Robert J.
   Steinicke, Frank
TI Evaluation of 3D Pointing Accuracy in the Fovea and Periphery in
   Immersive Head-Mounted Display Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Task analysis; Three-dimensional displays; Estimation;
   Stereo image processing; Virtual reality; Feedback loop; Virtual
   reality; vision; periphery; 3D; pointing; visual feedback
AB The coupling between perception and action has seldom been explored in sophisticated motor behaviour such as 3D pointing. In this study, we investigated how 3D pointing accuracy, measured by a depth estimation task, could be affected by the target appearing in different visual eccentricities. Specifically, we manipulated the visual eccentricity of the target and its depth in virtual reality. Participants wore a head-mounted-display with an integrated eye-tracker and docked a cursor into a target. We adopted a within-participants factorial design with three variables. The first variable is Eccentricity: the location of the target on one of five horizontal eccentricities (left far periphery, left near periphery, foveal, right near periphery and right far periphery). The second variable is Depth at three levels and the third variable is Feedback Loop with two levels: open/closed. Eccentricity is refactored into Motion Correspondence between the starting location of the cursor and the target location with four levels: periphery to fovea, fovea to periphery, periphery to periphery, fovea to fovea. The results showed that the pointing accuracy is modulated mainly by the target locations rather than the initial locations of the effector (hand). Visible feedback during pointing improved performance.
C1 [Katzakis, Nikolaos; Ariza, Oscar; Steinicke, Frank] Univ Hamburg, Informat, D-20146 Hamburg, Germany.
   [Chen, Lihan] Peking Univ, Dept Psychol, Beijing 100871, Peoples R China.
   [Teather, Robert J.] Carleton Univ, Sch Informat Technol, Ottawa, ON K1S 5B6, Canada.
C3 University of Hamburg; Peking University; Carleton University
RP Katzakis, N (corresponding author), Univ Hamburg, Informat, D-20146 Hamburg, Germany.
EM nicholas.katzakis@uni-hamburg.de; CLH@pku.edu.cn;
   ariza@informatik.uni-hamburg.de; rob.teather@carleton.ca;
   frank.steinicke@uni-hamburg.de
RI Steinicke, Frank/AAC-2976-2020
FU German Research Foundation (DFG); National Science Foundation of China
   (NSFC) [TRR-169]
FX This work was partially funded by the German Research Foundation (DFG)
   and the National Science Foundation of China (NSFC) in project
   Crossmodal Learning, TRR-169.
NR 47
TC 5
Z9 5
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2021
VL 27
IS 3
BP 1929
EP 1936
DI 10.1109/TVCG.2019.2947504
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA QA9EF
UT WOS:000613744500005
PM 31634134
DA 2025-03-07
ER

PT J
AU Ortner, T
   Walch, A
   Nowak, R
   Barnes, R
   Hollt, T
   Groller, ME
AF Ortner, Thomas
   Walch, Andreas
   Nowak, Rebecca
   Barnes, Robert
   Hollt, Thomas
   Groller, M. Eduard
TI InCorr: Interactive Data-Driven Correlation Panels for Digital Outcrop
   Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Geographic / geospatial visualization; remote sensing geology; digital
   outcrop analysis; integration of spatial and non -spatial data
   visualization
ID VISUALIZATION
AB Geological analysis of 3D Digital Outcrop Models (DOMs) for reconstruction of ancient habitable environments is a key aspect of the upcoming ESA ExoMars 2022 Rosalind Franklin Rover and the NASA 2020 Rover Perseverance missions in seeking signs of past life on Mars. Geologists measure and interpret 3D DOMs, create sedimentary logs and combine them in 'correlation panels' to map the extents of key geological horizons, and build a stratigraphic model to understand their position in the ancient landscape. Currently, the creation of correlation panels is completely manual and therefore time-consuming, and inflexible. With InCorr we present a visualization solution that encompasses a 3D logging tool and an interactive data -driven correlation panel that evolves with the stratigraphic analysis. For the creation of InCorr we closely cooperated with leading planetary geologists in the form of a design study. We verify our results by recreating an existing correlation analysis with InCorr and validate our correlation panel against a manually created illustration. Further, we conducted a user-study with a wider circle of geologists. Our evaluation shows that InCorr efficiently supports the domain experts in tackling their research questions and that it has the potential to significantly impact how geologists work with digital outcrop representations in general.
C1 [Ortner, Thomas; Walch, Andreas; Nowak, Rebecca] VRVis Zentrum Virtual Real & Visualisieru Forsch, London, England.
   [Barnes, Robert] Imperial Coll London, London, England.
   [Groller, M. Eduard] Delft Univ Technol, Delft, Netherlands.
C3 Imperial College London; Delft University of Technology
RP Ortner, T (corresponding author), VRVis Zentrum Virtual Real & Visualisieru Forsch, London, England.
EM ortner@vrvis.at; walch@vrvis.at; rnowak@vrvis.at;
   robert.barnes@imperial.ac.uk; T.Hollt-1@tudelft.nl;
   meister@cg.tuwien.ac.at
OI Walch, Andreas/0000-0002-4567-7942
FU Austrian Research Promotion Agency under ESA PEA [4000105568,
   4000117520]; Vienna Business Agency in the scope of COMET - Competence
   Centers for Excellent Technologies [854174]
FX We wish to thank Sanjeev Gupta and Steve Banham from Imperial College of
   London for taking the time to share their knowledge about sedimentology.
   We also thank Marijn van Capelle for annotating the outcrop
   reconstructions provided by Joanneum Research and creating the manually
   illustrated correlation panel. This work was funded by ESA-PRODEX
   funding, supported by the Austrian Research Promotion Agency under ESA
   PEA Grants 4000105568 & 4000117520 and by BMK, BMDW, Styria, SFG and
   Vienna Business Agency in the scope of COMET - Competence Centers for
   Excellent Technologies (854174) which is managed by FFG.
NR 31
TC 0
Z9 0
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 755
EP 764
DI 10.1109/TVCG.2020.3030409
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100061
PM 33085617
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Popolin Neto, M
   Paulovich, FV
AF Popolin Neto, Mario
   Paulovich, Fernando V.
TI Explainable Matrix - Visualization for Global and Local Interpretability
   of Random Forest Classification Ensembles
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Random forests; Radio frequency; Decision trees;
   Scalability; Predictive models; Vegetation; Random forest visualization;
   logic rules visualization; classification model interpretability;
   explainable artificial intelligence
AB Over the past decades, classification models have proven to be essential machine learning tools given their potential and applicability in various domains. In these years, the north of the majority of the researchers had been to improve quantitative metrics, notwithstanding the lack of information about models' decisions such metrics convey. This paradigm has recently shifted, and strategies beyond tables and numbers to assist in interpreting models' decisions are increasing in importance. Part of this trend, visualization techniques have been extensively used to support classification models' interpretability, with a significant focus on rule-based models. Despite the advances, the existing approaches present limitations in terms of visual scalability, and the visualization of large and complex models, such as the ones produced by the Random Forest (RF) technique, remains a challenge. In this paper, we propose Explainable Matrix (ExMatrix), a novel visualization method for RF interpretability that can handle models with massive quantities of rules. It employs a simple yet powerful matrix-like visual metaphor, where rows are rules, columns are features, and cells are rules predicates, enabling the analysis of entire models and auditing classification results. ExMatrix applicability is confirmed via different examples, showing how it can be used in practice to promote RF models interpretability.
C1 [Popolin Neto, Mario] Fed Inst Sao Paulo IFSP, Sao Paulo, Brazil.
   [Popolin Neto, Mario; Paulovich, Fernando V.] Univ Sao Paulo, Sao Paulo, Brazil.
   [Paulovich, Fernando V.] Dalhousie Univ, Halifax, NS, Canada.
C3 Instituto Federal de Sao Paulo (IFSP); Universidade de Sao Paulo;
   Dalhousie University
RP Popolin Neto, M (corresponding author), Fed Inst Sao Paulo IFSP, Sao Paulo, Brazil.; Popolin Neto, M (corresponding author), Univ Sao Paulo, Sao Paulo, Brazil.
EM mariopopolin@ifsp.edu.br; paulovich@dal.ca
RI Popolin Neto, Mário/AFI-8722-2022; Paulovich, Fernando/G-1329-2010
OI Popolin Neto, Mario/0000-0002-8379-2458; Paulovich,
   Fernando/0000-0002-2316-760X
FU Qualification Program of the Federal Institute of Sao Paulo (IFSP);
   Natural Sciences and Engineering Research Council of Canada (NSERC)
FX The authors wish to thank the valuable comments and suggestions obtained
   from the reviewers, as well as the support received from the
   Qualification Program of the Federal Institute of Sao Paulo (IFSP). We
   acknowledge the support of the Natural Sciences and Engineering Research
   Council of Canada (NSERC).
NR 60
TC 71
Z9 76
U1 5
U2 48
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1427
EP 1437
DI 10.1109/TVCG.2020.3030354
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100122
PM 33048689
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Qu, BT
   Roy, L
   Zhang, Y
   Zhang, E
AF Qu, Botong
   Roy, Lawrence
   Zhang, Yue
   Zhang, Eugene
TI Mode Surfaces of Symmetric Tensor Fields: Topological Analysis and
   Seamless Extraction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Tensor field visualization; tensor field topology; traceless tensors;
   degenerate curve extraction; neutral surface extraction; mode surface
   extraction
ID VISUALIZATION; LINES
AB Mode surfaces are the generalization of degenerate curves and neutral surfaces, which constitute 3D symmetric tensor field topology. Efficient analysis and visualization of mode surfaces can provide additional insight into not only degenerate curves and neutral surfaces, but also how these features transition into each other. Moreover, the geometry and topology of mode surfaces can help domain scientists better understand the tensor fields in their applications. Existing mode surface extraction methods can miss features in the surfaces. Moreover, the mode surfaces extracted from neighboring cells have gaps, which make their subsequent analysis difficult. In this paper, we provide novel analysis on the topological structures of mode surfaces, including a common parameterization of all mode surfaces of a tensor field using 2D asymmetric tensors. This allows us to not only better understand the structures in mode surfaces and their interactions with degenerate curves and neutral surfaces, but also develop an efficient algorithm to seamlessly extract mode surfaces, including neutral surfaces. The seamless mode surfaces enable efficient analysis of their geometric structures, such as the principal curvature directions. We apply our analysis and visualization to a number of solid mechanics data sets.
C1 [Qu, Botong; Roy, Lawrence; Zhang, Yue; Zhang, Eugene] Oregon State Univ, Sch Elect Engn & Comp Sci, Corvallis, OR 97331 USA.
C3 Oregon State University
RP Qu, BT (corresponding author), Oregon State Univ, Sch Elect Engn & Comp Sci, Corvallis, OR 97331 USA.
EM qub@oregonstate.edu; royl@eecs.oregonstate.edu;
   zhangyue@oregonstate.edu; zhange@eecs.oregonstate.edu
RI Qu, Botong/AAU-6231-2021; Roy, Lawrence/GLN-6659-2022
FU NSF [1566236, 1619383]; Div Of Information & Intelligent Systems; Direct
   For Computer & Info Scie & Enginr [1566236] Funding Source: National
   Science Foundation; Div Of Information & Intelligent Systems; Direct For
   Computer & Info Scie & Enginr [1619383] Funding Source: National Science
   Foundation
FX We wish to thank our anonymous reviewers for their valuable suggestions.
   We thank Kyle Hiebel for making the voice recording of our video. This
   research is partially supported by NSF awards (#1566236) and (#1619383).
NR 40
TC 4
Z9 4
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 583
EP 592
DI 10.1109/TVCG.2020.3030431
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100045
PM 33052860
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Wald, I
   Zellmann, S
   Usher, W
   Morrical, N
   Lang, U
   Pascucci, V
AF Wald, Ingo
   Zellmann, Stefan
   Usher, Will
   Morrical, Nate
   Lang, Ulrich
   Pascucci, Valerio
TI Ray Tracing Structured AMR Data Using ExaBricks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Rendering (computer graphics); Computational modeling; Ray tracing; Data
   models; Adaptation models; Octrees; Adaptive mesh refinement;
   acceleration data structures; volume rendering; hardware ray tracing
AB Structured Adaptive Mesh Refinement (Structured AMR) enables simulations to adapt the domain resolution to save computation and storage, and has become one of the dominant data representations used by scientific simulations; however, efficiently rendering such data remains a challenge. We present an efficient approach for volume- and iso-surface ray tracing of Structured AMR data on GPU-equipped workstations, using a combination of two different data structures. Together, these data structures allow a ray tracing based renderer to quickly determine which segments along the ray need to be integrated and at what frequency, while also providing quick access to all data values required for a smooth sample reconstruction kernel. Our method makes use of the RTX ray tracing hardware for surface rendering, ray marching, space skipping, and adaptive sampling; and allows for interactive changes to the transfer function and implicit iso-surfacing thresholds. We demonstrate that our method achieves high performance with little memory overhead, enabling interactive high quality rendering of complex AMR data sets on individual GPU workstations.
C1 [Wald, Ingo] NVIDIA, Santa Clara, CA 95051 USA.
   [Zellmann, Stefan; Lang, Ulrich] Univ Cologne, Chair Comp Sci, Cologne, Germany.
   [Usher, Will; Morrical, Nate; Pascucci, Valerio] Univ Utah, Inst Sci, Salt Lake City, UT 84112 USA.
   [Usher, Will] Intel Corp, Salt Lake City, UT 84112 USA.
C3 Nvidia Corporation; University of Cologne; Utah System of Higher
   Education; University of Utah; Intel Corporation; Intel USA
RP Wald, I (corresponding author), NVIDIA, Santa Clara, CA 95051 USA.
EM iwald@nvidia.com
RI pascucci, Valerio/GXF-0616-2022; Müller, Thomas/AAD-3910-2019
OI pascucci, valerio/0000-0002-8877-2042
FU NSF OAC [1842042, 1941085]; NSF CMMI award [1629660]; LLNL LDRD project
   [SI-20-001]; Department of Energy (DoE), National Nuclear Security
   Administration (NNSA) [DE-NA0002375]; Exascale Computing Project
   [17-SC-20-SC]; DoE Office of Science; NNSA; DoE by Lawrence Livermore
   National Laboratory [DE-AC52-07NA27344]
FX The Landing Gear was graciously provided by Michael Barad, Cetin Kiris
   and Pat Moran of NASA. The Exajet was made available by Exa GmbH and Pat
   Moran. The TAC Molecular Cloud is courtesy of Daniel Seifried. The
   Stellar Cluster Wind is courtesy of Melinda Soares-Furtado. This work
   was supported in part by NSF OAC awards 1842042, 1941085, NSF CMMI award
   1629660, LLNL LDRD project SI-20-001 This material is based in part upon
   work supported by the Department of Energy (DoE), National Nuclear
   Security Administration (NNSA), under award DE-NA0002375. This research
   was supported in part by the Exascale Computing Project (17-SC-20-SC), a
   collaborative effort of the DoE Office of Science and the NNSA. This
   work was performed in part under the auspices of the DoE by Lawrence
   Livermore National Laboratory under Contract DE-AC52-07NA27344.
NR 41
TC 9
Z9 9
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 625
EP 634
DI 10.1109/TVCG.2020.3030470
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100049
PM 33048750
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Somarakis, A
   Van Unen, V
   Koning, F
   Lelieveldt, B
   Höllt, T
AF Somarakis, Antonios
   Van Unen, Vincent
   Koning, Frits
   Lelieveldt, Boudewijn
   Hollt, Thomas
TI ImaCytE: Visual Exploration of Cellular Micro-Environments for Imaging
   Mass Cytometry Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Proteins; Imaging; Spatial resolution; Tools; Task analysis;
   Visualization; Visual analytics; imaging mass cytometry; spatial omics
   data; high-dimensional images
ID VISUALIZATION; EXPRESSION
AB Tissue functionality is determined by the characteristics of tissue-resident cells and their interactions within their microenvironment. Imaging Mass Cytometry offers the opportunity to distinguish cell types with high precision and link them to their spatial location in intact tissues at sub-cellular resolution. This technology produces large amounts of spatially-resolved high-dimensional data, which constitutes a serious challenge for the data analysis. We present an interactive visual analysis workflow for the end-to-end analysis of Imaging Mass Cytometry data that was developed in close collaboration with domain expert partners. We implemented the presented workflow in an interactive visual analysis tool; ImaCytE. Our workflow is designed to allow the user to discriminate cell types according to their protein expression profiles and analyze their cellular microenvironments, aiding in the formulation or verification of hypotheses on tissue architecture and function. Finally, we show the effectiveness of our workflow and ImaCytE through a case study performed by a collaborating specialist.
C1 [Somarakis, Antonios; Lelieveldt, Boudewijn] Leiden Univ, Dept Radiol, Div Image Proc, Med Ctr, NL-2333 ZA Leiden, Netherlands.
   [Van Unen, Vincent; Koning, Frits] Leiden Univ, Dept Immunol, Med Ctr, NL-2333 ZA Leiden, Netherlands.
   [Hollt, Thomas] Leiden Univ, Leiden Computat Biol Ctr, Med Ctr, NL-2333 ZA Leiden, Netherlands.
   [Hollt, Thomas] Delft Univ Technol, Comp Graph & Visualizat Dept, NL-2628 CD Delft, Netherlands.
C3 Leiden University; Leiden University Medical Center (LUMC); Leiden
   University - Excl LUMC; Leiden University; Leiden University Medical
   Center (LUMC); Leiden University - Excl LUMC; Leiden University - Excl
   LUMC; Leiden University; Leiden University Medical Center (LUMC); Delft
   University of Technology
RP Somarakis, A (corresponding author), Leiden Univ, Dept Radiol, Div Image Proc, Med Ctr, NL-2333 ZA Leiden, Netherlands.
EM a.somarakis@lumc.nl; v.van_unen@lumc.nl; f.koning@lumc.nl;
   b.p.f.lelieveldt@lumc.nl; t.hoellt@lumc.nl
RI van Unen, Vincent/L-9105-2019; Lelieveldt, Boudewijn/B-6501-2008
OI van Unen, Vincent/0000-0001-9339-8430; Koning,
   Frits/0000-0002-4007-5715; Somarakis, Antonios/0000-0003-1020-1562;
   /0000-0001-8125-1650; Lelieveldt, Boudewijn/0000-0001-8269-7603
FU Leiden University Data Science Research Programme; H2020-Marie
   Skodowska-Curie Action Research and Innovation Staff Exchange (RISE)
   Grant [644373-PRISAR]
FX We would like to thank M.E. IJsselsteijn and Dr. N.F. Miranda for
   performing the biological experiments, B. van Lew for narrating the
   supplemental video, which can be found on the Computer Society Digital
   Library at http://doi.ieeecomputersociety.org/10.1109/TVCG.2019.2931299,
   N. Li and N Guo for their valuable comments on our tool. Moreover, we
   would like to thank the anonymous reviewers for their constructive
   criticism that lead to significant improvements of the paper and the
   application. This work received funding through Leiden University Data
   Science Research Programme. B.P.F.Lelieveldt received partial funding
   from H2020-Marie Skodowska-Curie Action Research and Innovation Staff
   Exchange (RISE) Grant 644373-PRISAR.
NR 44
TC 47
Z9 49
U1 0
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN 1
PY 2021
VL 27
IS 1
BP 98
EP 110
DI 10.1109/TVCG.2019.2931299
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OY4TT
UT WOS:000594242000008
PM 31369380
OA hybrid, Green Published
DA 2025-03-07
ER

PT J
AU Farias, R
   Kallmann, M
AF Farias, Renato
   Kallmann, Marcelo
TI Optimal Path Maps on the GPU
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Graphics processing units; Path planning; Microsoft Windows; Geometry;
   Navigation; Virtual environments; Computational efficiency; Graphics
   data structures and data types; path planning; shortest path maps;
   navigation; GPU programming
ID EUCLIDEAN SHORTEST PATHS; VORONOI DIAGRAMS; LINE SEGMENTS; GRAPH
AB We introduce a new method for computing optimal path maps on the GPU using OpenGL shaders. Our method explores GPU rasterization as a way to propagate optimal costs on a polygonal 2D environment, producing optimal path maps which can efficiently be queried at run-time. Our method is implemented entirely with GPU shaders, does not require pre-computation, addresses optimal path maps with multiple points and line segments as sources, and introduces a new optimal path map concept not addressed before: maps with weights at vertices representing possible changes in traversal speed. The produced maps offer new capabilities not explored by previous navigation representations and at the same time address paths with global optimality, a characteristic which has been mostly neglected in animated virtual environments. The proposed path maps partition the input environment into the regions sharing a same parent point along the shortest path to the closest source, taking into account possible speed changes at vertices. The proposed approach is particularly suitable for the animation of multiple agents moving toward the entrances or exits of a virtual environment, a situation which is efficiently represented with the proposed path maps.
C1 [Farias, Renato; Kallmann, Marcelo] Univ Calif, Comp Sci & Engn Dept, Merced, CA 95343 USA.
C3 University of California System; University of California Merced
RP Farias, R (corresponding author), Univ Calif, Comp Sci & Engn Dept, Merced, CA 95343 USA.
EM rfarias2@ucmerced.edu; mkallmann@ucmerced.edu
RI Kallmann, Marcelo/HSC-7222-2023
OI Kallmann, Marcelo/0000-0001-5138-0603
FU Army Research Office [W911NF-17-1-0463]
FX This research was sponsored by the Army Research Office and was
   accomplished under Grant Number W911NF-17-1-0463. The views and
   conclusions contained in this document are those of the authors and
   should not be interpreted as representing the official policies, either
   expressed or implied, of the Army Research Office or the U.S.
   Government. The U.S. Government is authorized to reproduce and
   distribute reprints for Government purposes notwithstanding any
   copyright notation herein.
NR 39
TC 8
Z9 8
U1 0
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2020
VL 26
IS 9
BP 2863
EP 2874
DI 10.1109/TVCG.2019.2904271
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MS7LW
UT WOS:000554457900010
PM 30869623
DA 2025-03-07
ER

PT J
AU Naik, H
   Bastien, R
   Navab, N
   Couzin, ID
AF Naik, Hemal
   Bastien, Renaud
   Navab, Nassir
   Couzin, Iain D.
TI Animals in Virtual Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 27th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 22-26, 2020
CL Atlanta, GA
SP IEEE, IEEE Comp Soc
DE Virtual environments; Visualization; Animal behavior; Robot sensing
   systems; Insects; animal behavior; computer vision; neuroscience;
   interactive experiments; evolution; ecology; ethology
ID HIPPOCAMPAL PLACE CELLS; OPTOMOTOR RESPONSES; CELLULAR RESOLUTION;
   REALITY; DROSOPHILA; STIMULI; DYNAMICS; TRACKING; BEHAVIOR; VISION
AB The core idea in an XR (VR/MR/AR) application is to digitally stimulate one or more sensory systems (e.g. visual, auditory, olfactory) of the human user in an interactive way to achieve an immersive experience. Since the early 2000s biologists have been using Virtual Environments (VE) to investigate the mechanisms of behavior in non-human animals including insects, fish, and mammals. VEs have become reliable tools for studying vision, cognition, and sensory-motor control in animals. In turn, the knowledge gained from studying such behaviors can be harnessed by researchers designing biologically inspired robots, smart sensors, and rnulti-agent artificial intelligence. VE for animals is becoming a widely used application of XR technology but such applications have not previously been reported in the technical literature related to XR. Biologists and computer scientists can benefit greatly from deepening interdisciplinary research in this emerging field and together we can develop new methods for conducting fundamental research in behavioral sciences and engineering. To support our argument we present this review which provides an overview of animal behavior experiments conducted in virtual environments.
C1 [Naik, Hemal; Bastien, Renaud; Couzin, Iain D.] Univ Konstanz, Max Planck Inst Anim Behav, Constance, Germany.
   [Naik, Hemal; Bastien, Renaud; Couzin, Iain D.] Univ Konstanz, Ctr Adv Study Collect Behav, Dept Biol, Constance, Germany.
   [Naik, Hemal; Navab, Nassir] Tech Univ Munich, Munich, Germany.
C3 Max Planck Society; University of Konstanz; University of Konstanz;
   Technical University of Munich
RP Naik, H (corresponding author), Univ Konstanz, Max Planck Inst Anim Behav, Constance, Germany.; Naik, H (corresponding author), Univ Konstanz, Ctr Adv Study Collect Behav, Dept Biol, Constance, Germany.; Naik, H (corresponding author), Tech Univ Munich, Munich, Germany.
EM hnaik@ab.mpg.de; rbastien@ab.mpg.de; nassir.navab@tum.de;
   icouzin@ab.mpg.de
RI Couzin, Iain/Q-6862-2018
FU DFG Centre of Excellence 2117 Centre for the Advanced Study of
   Collective Behaviour [422037984]
FX The authors wish to thank John Stowers, Yuji Oyamada, and Bianca Schell.
   This work is supported by funding from the DFG Centre of Excellence 2117
   Centre for the Advanced Study of Collective Behaviour (ID: 422037984).
NR 81
TC 31
Z9 32
U1 7
U2 61
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2020
VL 26
IS 5
BP 2073
EP 2083
DI 10.1109/TVCG.2020.2973063
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA LA1WU
UT WOS:000523746000026
PM 32070970
OA Green Submitted, hybrid
DA 2025-03-07
ER

PT J
AU Liu, MM
   Zhang, KX
   Zhu, J
   Wang, J
   Guo, J
   Guo, YW
AF Liu, Mingming
   Zhang, Kexin
   Zhu, Jie
   Wang, Jun
   Guo, Jie
   Guo, Yanwen
TI Data-Driven Indoor Scene Modeling from a Single Color Image with
   Iterative Object Segmentation and Model Retrieval
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Solid modeling; Three-dimensional displays; Image segmentation; Object
   segmentation; Computational modeling; Semantics; Layout; Indoor scene
   modeling; data-driven; object segmentation; model retrieval; layout
   estimation
AB We propose a new method for modeling the indoor scene from a single color image. With our system, the user only needs to drag a few semantic bounding boxes surrounding the objects of interest. Our system then automatically finds the most similar 3D models from the ShapeNet model repository and aligns them with the corresponding objects of interest. To achieve this, each 3D model is represented as a group of view-dependent representations generated from a set of synthesized views. We iteratively conduct object segmentation and 3D model retrieval, based on the observation that good segmentation of the objects of interest can significantly improve the accuracy of model retrieval and make it robust to cluttered background and occlusions, and in turn, the retrieved 3D models can be used to assist with object segmentation. Segmentation of all objects of interest is achieved simultaneously under a unified multi-labeling framework which fully utilizes the correspondences between the objects of interest and retrieved model images. Besides, we propose a new method to estimate the scene layout of the input image with the segmentation masks, which helps compose the resulting scene and further improves the modeling result remarkably. We verify the effectiveness of our approach through experimenting with a variety of indoor images and comparing against the relevant methods.
C1 [Liu, Mingming; Zhang, Kexin; Zhu, Jie; Guo, Jie; Guo, Yanwen] Nanjing Univ, Nal Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.
   [Guo, Yanwen] Sci & Technol Informat Syst Engn Lab, Nanjing, Peoples R China.
   [Wang, Jun] Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing 210016, Peoples R China.
C3 Nanjing University; Nanjing University of Aeronautics & Astronautics
RP Guo, YW (corresponding author), Nanjing Univ, Nal Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.; Guo, YW (corresponding author), Sci & Technol Informat Syst Engn Lab, Nanjing, Peoples R China.
EM ieliuming@163.com; claireikxiu@gmail.com; magickuang@126.com;
   wjun@nuaa.edu.cn; guojie@nju.edu.cn; ywguo@nju.edu.cn
RI Wang, Jun/AAM-6868-2021
OI Wang, Jun/0000-0001-9223-2615
FU National Natural Science Foundation of China [61772257, 61672279];
   Natural Science Foundation of Jiangsu Province [BK20150016]
FX The authors would like to thank the reviewers for their constructive
   comments which helped improve this paper greatly. This work was
   supported in part by the National Natural Science Foundation of China
   under Grants 61772257 and 61672279, the Natural Science Foundation of
   Jiangsu Province under Grants BK20150016.
NR 48
TC 9
Z9 9
U1 1
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2020
VL 26
IS 4
BP 1702
EP 1715
DI 10.1109/TVCG.2018.2880737
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KU2OG
UT WOS:000519547200007
PM 30418912
DA 2025-03-07
ER

PT J
AU Chao, QW
   Deng, ZG
   Xiao, YX
   He, DB
   Miao, QG
   Jin, XG
AF Chao, Qianwen
   Deng, Zhigang
   Xiao, Yangxi
   He, Dunbang
   Miao, Qiguang
   Jin, Xiaogang
TI Dictionary-based Fidelity Measure for Virtual Traffic
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computational modeling; Measurement; Solid modeling; Trajectory;
   Dictionaries; Data models; Benchmark testing; Traffic simulation; crowd
   animation; data-driven simulation; dictionary learning; user study
ID SPARSE; MODEL; CROWD; SIMULATION; ANIMATION; SELECTION
AB Aiming at objectively measuring the realism of virtual traffic flows and evaluating the effectiveness of different traffic simulation techniques, this paper introduces a general, dictionary-based learning method to evaluate the fidelity of any traffic trajectory data. First, a traffic pattern dictionary that characterizes common patterns of real-world traffic behavior is built offline from pre-collected ground truth traffic data. The corresponding learning error is set as the benchmark of the dictionary-based traffic representation. With the aid of the constructed dictionary, the realism of input simulated traffic flow data can be evaluated by comparing its dictionary-based reconstruction error with the dictionary error benchmark. This evaluation metric can be robustly applied to any simulated traffic flow data; in other words, it is independent of how the traffic data are generated. We demonstrated the effectiveness and robustness of this metric through many experiments on real-world traffic data and various simulated traffic data, comparisons with the state-of-the-art entropy-based similarity metric for aggregate crowd motions, and perceptual evaluation studies.
C1 [Chao, Qianwen; Miao, Qiguang] Xidian Univ, Dept Comp Sci, Xian 710038, Peoples R China.
   [Deng, Zhigang] Univ Houston, Comp Sci Dept, Houston, TX 77004 USA.
   [Xiao, Yangxi; He, Dunbang; Jin, Xiaogang] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
C3 Xidian University; University of Houston System; University of Houston;
   Zhejiang University
RP Chao, QW (corresponding author), Xidian Univ, Dept Comp Sci, Xian 710038, Peoples R China.
EM chaoqianwen15@gmail.com; zdeng4@uh.edu; x_x@zju.edu.cn;
   dunbanghe@gmail.com; qgmiao@mail.xidian.edu.cn; jin@cad.zju.edu.cn
RI Deng, Zhigang/MBH-0755-2025
OI Deng, Zhigang/0000-0003-2571-5865; Jin, Xiaogang/0000-0001-7339-2920;
   Deng, Zhigang/0000-0002-0452-8676; Miao, Qiguang/0000-0002-2872-388X
FU National Natural Science Foundation of China [61702393]; Fundamental
   Research Funds for the Central Universities [JBX170310, XJS17052,
   JB170306, JB170304]; US NSF [IIS-1524782]; National Key R&D Program of
   China [2017YFB1002600]; Artificial Intelligence Research Foundation of
   Baidu Inc.; National Natural Science Foundations of China [61772396,
   61472302, 61772392, 61672409]
FX The authors would like to thank Dr. Hen-wei Huang in ETH Z_urich for the
   discussion and support. Qianwen Chao was supported by the National
   Natural Science Foundation of China (Grant No. 61702393), and the
   Fundamental Research Funds for the Central Universities (Grant Nos.
   JBX170310, XJS17052). Zhigang Deng was in part supported by US NSF grant
   IIS-1524782. Xiaogang Jin was supported by the National Key R&D Program
   of China (Grant No. 2017YFB1002600) and Artificial Intelligence Research
   Foundation of Baidu Inc. Qiguang Miao was supported by the National
   Natural Science Foundations of China (Grant Nos. 61772396, 61472302,
   61772392, 61672409), and the Fundamental Research Funds for the Central
   Universities (Grant Nos. JB170306, JB170304).
NR 54
TC 1
Z9 2
U1 2
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2020
VL 26
IS 3
BP 1490
EP 1501
DI 10.1109/TVCG.2018.2873695
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KH5VW
UT WOS:000510719400006
PM 30295621
DA 2025-03-07
ER

PT J
AU Behrisch, M
   Schreck, T
   Pfister, H
AF Behrisch, Michael
   Schreck, Tobias
   Pfister, Hanspeter
TI GUIRO: User-Guided Matrix Reordering
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual Analytics; matrix; black-box algorithms; seriation; ordering;
   sorting; steerable algorithm; interaction; 2D projection
ID VISUALIZATION; SERIATION; SYSTEM
AB Matrix representations are one of the main established and empirically proven to be effective visualization techniques for relational (or network) data. However, matrices similar to node-link diagrams are most effective if their layout reveals the underlying data topology. Given the many developed algorithms, a practical problem arises: Which matrix reordering algorithm should I choose for my dataset at hand? To make matters worse, different reordering algorithms applied to the same dataset may let significantly different visual matrix patterns emerge. This leads to the question of trustworthiness and explainability of these fully automated, often heuristic, black-box processes. We present, a Visual Analytics system that helps novices, network analysts, and algorithm designers to open the black-box. Users can investigate the usefulness and expressiveness of 70 accessible matrix reordering algorithms. For network analysts, we introduce a novel model space representation and two interaction techniques for a user-guided reordering of rows or columns, and especially groups thereof (submatrix reordering). These novel techniques contribute to the understanding of the global and local dataset topology. We support algorithm designers by giving them access to 16 reordering quality metrics and visual exploration means for comparing reordering implementations on a row/column permutation level. We evaluated in a guided explorative user study with 12 subjects, a case study demonstrating its usefulness in a real-world scenario, and through an expert study gathering feedback on our design decisions. We found that our proposed methods help even inexperienced users to understand matrix patterns and allow a user-guided steering of reordering algorithms. helps to increase the transparency of matrix reordering algorithms, thus helping a broad range of users to get a better insight into the complex reordering process, in turn supporting data and reordering algorithm insights.
C1 [Behrisch, Michael; Pfister, Hanspeter] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
   [Schreck, Tobias] Graz Univ Technol, Graz, Austria.
C3 Harvard University; Graz University of Technology
RP Behrisch, M (corresponding author), Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
EM behrisch@seas.harvard.edu; tobias.schreck@cgv.tugraz.at;
   pfister@seas.harvard.edu
OI Pfister, Hanspeter/0000-0002-3620-2582
NR 83
TC 14
Z9 15
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 184
EP 194
DI 10.1109/TVCG.2019.2934300
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100017
PM 31442977
DA 2025-03-07
ER

PT J
AU Dasgupta, A
   Wang, H
   O'Brien, N
   Burrows, S
AF Dasgupta, Aritra
   Wang, Hong
   O'Brien, Nancy
   Burrows, Susannah
TI <i>Separating the Wheat from the Chaff</i>: Comparative Visual Cues for
   Transparent Diagnostics of Competing Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual comparison; Visual cues; Model evaluation; Transparency;
   Simulation
ID VISUALIZATION; DESIGN; TOOL
AB Experts in data and physical sciences have to regularly grapple with the problem of competing models. Be it analytical or physics-based models, a cross-cutting challenge for experts is to reliably diagnose which model outcomes appropriately predict or simulate real-world phenomena. Expert judgment involves reconciling information across many, and often, conflicting criteria that describe the quality of model outcomes. In this paper, through a design study with climate scientists, we develop a deeper understanding of the problem and solution space of model diagnostics, resulting in the following contributions: i) a problem and task characterization using which we map experts model diagnostics goals to multi-way visual comparison tasks, ii) a design space of comparative visual cues for letting experts quickly understand the degree of disagreement among competing models and gauge the degree of stability of model outputs with respect to alternative criteria, and iii) design and evaluation of MyriadCues, an interactive visualization interface for exploring alternative hypotheses and insights about good and bad models by leveraging comparative visual cues. We present case studies and subjective feedback by experts, which validate how MyriadCues enables more transparent model diagnostic mechanisms, as compared to the state of the art.
C1 [Dasgupta, Aritra] New Jersey Inst Technol, Newark, NJ 07102 USA.
   [Wang, Hong] Arizona State Univ, Tempe, AZ 85287 USA.
   [O'Brien, Nancy; Burrows, Susannah] Pacific Northwest Natl Lab, Richland, WA 99352 USA.
C3 New Jersey Institute of Technology; Arizona State University; Arizona
   State University-Tempe; United States Department of Energy (DOE);
   Pacific Northwest National Laboratory
RP Dasgupta, A (corresponding author), New Jersey Inst Technol, Newark, NJ 07102 USA.
EM aritra.dasgupta@njit.edu; hong.wang@asu.edu; nancy.obrien@pnnl.gov;
   susannah.burrows@pnnl.gov
RI Dasgupta, Aritra/AAV-8710-2020; Burrows, Susannah/A-7429-2011
OI Dasgupta, Aritra/0000-0002-5551-5103; Burrows,
   Susannah/0000-0002-0745-7252
FU Laboratory Directed Research and Development Program at PNNL
FX This work was partially supported by the Laboratory Directed Research
   and Development Program at PNNL, a multi-program national laboratory
   operated by Battelle. We would like to thank Feng Wang for developing
   the initial prototypes, and Phil Rasch, Yun Qian, and PoLun Ma for their
   feedback about MyriadCues. We are also grateful to the anonymous
   reviewers for their constructive comments, which helped refine the
   discussions in the paper.
NR 48
TC 11
Z9 11
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 1043
EP 1053
DI 10.1109/TVCG.2019.2934540
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100097
PM 31478858
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Lu, M
   Wang, SQ
   Lanir, J
   Fish, N
   Yue, Y
   Cohen-Or, D
   Huang, H
AF Lu, Min
   Wang, Shuaiqi
   Lanir, Joel
   Fish, Noa
   Yue, Yang
   Cohen-Or, Daniel
   Huang, Hui
TI Winglets: Visualizing Association with Uncertainty in Multi-class
   Scatterplots
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Scatterplot; Gestalt laws; Association; Uncertainty
ID PERCEPTION; MODEL
AB This work proposes Winglets, an enhancement to the classic scatterplot to better perceptually pronounce multiple classes by improving the perception of association and uncertainty of points to their related cluster. Designed as a pair of dual-sided strokes belonging to a data point, Winglets leverage the Gestalt principle of Closure to shape the perception of the form of the clusters, rather than use an explicit divisive encoding. Through a subtle design of two dominant attributes, length and orientation, Winglets enable viewers to perform a mental completion of the clusters. A controlled user study was conducted to examine the efficiency of Winglets in perceiving the cluster association and the uncertainty of certain points. The results show Winglets form a more prominent association of points into clusters and improve the perception of associating uncertainty.
C1 [Lu, Min; Wang, Shuaiqi; Yue, Yang; Cohen-Or, Daniel; Huang, Hui] Shenzhen Univ, Shenzhen, Peoples R China.
   [Lanir, Joel] Univ Haifa, Haifa, Israel.
   [Fish, Noa] Tel Aviv Univ, Tel Aviv, Israel.
C3 Shenzhen University; University of Haifa; Tel Aviv University
RP Huang, H (corresponding author), Shenzhen Univ, Shenzhen, Peoples R China.
EM lumin.vis@gmail.com; shuaiqiwang666@gmail.com; ylanir@is.haifa.il;
   noafish@gmail.com; yueyang@szu.edu.cn; cohenor@gmail.com;
   hhzhiyan@gmail.com
RI wang, shuaiqi/MDT-8158-2025; Huang, Hui/JGB-1049-2023
OI Huang, Hui/0000-0003-3212-0544; Yue, Yang/0000-0001-6934-2357
FU NSFC [61802265, 41671387, 61761146002, 61861130365]; LHTD [20170003];
   Guangdong Provincial Natural Science Foundation [2018A030310426,
   2015A030312015]; National Engineering Laboratory for Big Data System
   Computing Technology
FX We thank the reviewers for their valuable comments. This work is
   supported in parts by NSFC (61802265, 41671387, 61761146002,
   61861130365), LHTD (20170003), Guangdong Provincial Natural Science
   Foundation (2018A030310426, 2015A030312015), and the National
   Engineering Laboratory for Big Data System Computing Technology.
NR 42
TC 6
Z9 8
U1 1
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 770
EP 779
DI 10.1109/TVCG.2019.2934811
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100071
PM 31562094
DA 2025-03-07
ER

PT J
AU Qi, J
   Bloemen, V
   Wang, SH
   van Wijk, J
   van de Wetering, H
AF Qi, Ji
   Bloemen, Vincent
   Wang, Shihan
   van Wijk, Jarke
   van de Wetering, Huub
TI STBins: Visual Tracking and Comparison of Multiple Data Sequences Using
   Temporal Binning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; time series data; data sequence
ID EVENT SEQUENCES; VISUALIZATION; ANALYTICS; TOPICS
AB While analyzing multiple data sequences, the following questions typically arise: how does a single sequence change over time, how do multiple sequences compare within a period, and how does such comparison change over time. This paper presents a visual technique named STBins to answer these questions. STBins is designed for visual tracking of individual data sequences and also for comparison of sequences. The latter is done by showing the similarity of sequences within temporal windows. A perception study is conducted to examine the readability of alternative visual designs based on sequence tracking and comparison tasks. Also, two case studies based on real-world datasets are presented in detail to demonstrate usage of our technique.
C1 [Qi, Ji; van Wijk, Jarke; van de Wetering, Huub] Eindhoven Univ Technol, Dept Math & Comp Sci, Eindhoven, Netherlands.
   [Bloemen, Vincent] Univ Twente, Fac Elect Engn Math Comp Sci, Enschede, Netherlands.
   [Wang, Shihan] Univ Amsterdam, Amsterdam, Netherlands.
C3 Eindhoven University of Technology; University of Twente; University of
   Amsterdam
RP Qi, J (corresponding author), Eindhoven Univ Technol, Dept Math & Comp Sci, Eindhoven, Netherlands.
EM j.qi@tue.nl; v.bloemen@utwente.nl; s.w.wang@uva.nl; j.j.v.wijk@tue.nl;
   h.v.d.wetering@tue.nl
RI Bloemen, Veerle/B-2971-2019
OI van de Wetering, Huub/0000-0002-0517-1322
FU Big Software on the Run (BSR) project in Netherlands
FX This work was supported by the Big Software on the Run (BSR) project in
   Netherlands.
NR 46
TC 7
Z9 7
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 1054
EP 1063
DI 10.1109/TVCG.2019.2934289
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100098
PM 31425095
DA 2025-03-07
ER

PT J
AU Rapp, T
   Peters, C
   Dachsbacher, C
AF Rapp, Tobias
   Peters, Christoph
   Dachsbacher, Carsten
TI Void-and-Cluster Sampling of Large Scattered Data and Trajectories
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data reduction; sampling; blue noise; entropy-based sampling; scattered
   data; pathlines
ID VISUALIZATION; SIMULATION
AB We propose a data reduction technique for scattered data based on statistical sampling. Our void-and-cluster sampling technique finds a representative subset that is optimally distributed in the spatial domain with respect to the blue noise property. In addition, it can adapt to a given density function, which we use to sample regions of high complexity in the multivariate value domain more densely. Moreover, our sampling technique implicitly defines an ordering on the samples that enables progressive data loading and a continuous level-of-detail representation. We extend our technique to sample time-dependent trajectories, for example pathlines in a time interval, using an efficient and iterative approach. Furthermore, we introduce a local and continuous error measure to quantify how well a set of samples represents the original dataset. We apply this error measure during sampling to guide the number of samples that are taken. Finally, we use this error measure and other quantities to evaluate the quality, performance, and scalability of our algorithm.
C1 [Rapp, Tobias; Peters, Christoph; Dachsbacher, Carsten] Karlsruhe Inst Technol, Karlsruhe, Germany.
C3 Helmholtz Association; Karlsruhe Institute of Technology
RP Rapp, T (corresponding author), Karlsruhe Inst Technol, Karlsruhe, Germany.
EM tobias.rapp@kit.edu; christoph.peters@kit.edu; dachsbacher@kit.edu
OI Rapp, Tobias/0000-0002-5436-5553
NR 28
TC 14
Z9 16
U1 1
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 780
EP 789
DI 10.1109/TVCG.2019.2934335
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100072
PM 31425103
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Veras, R
   Collins, C
AF Veras, Rafael
   Collins, Christopher
TI Discriminability Tests for Visualization Effectiveness and Scalability
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Scalability; Discriminability; Simulation; Perception
ID QUALITY; INFORMATION; ATTENTION; MODEL
AB The scalability of a particular visualization approach is limited by the ability for people to discern differences between plots made with different datasets. Ideally, when the data changes, the visualization changes in perceptible ways. This relation breaks down when there is a mismatch between the encoding and the character of the dataset being viewed. Unfortunately, visualizations are often designed and evaluated without fully exploring how they will respond to a wide variety of datasets. We explore the use of an image similarity measure, the Multi-Scale Structural Similarity Index (MS-SSIM), for testing the discriminability of a data visualization across a variety of datasets. MS-SSIM is able to capture the similarity of two visualizations across multiple scales, including low level granular changes and high level patterns. Significant data changes that are not captured by the MS-SSIM indicate visualizations of low discriminability and effectiveness. The measures utility is demonstrated with two empirical studies. In the first, we compare human similarity judgments and MS-SSIM scores for a collection of scatterplots. In the second, we compute the discriminability values for a set of basic visualizations and compare them with empirical measurements of effectiveness. In both cases, the analyses show that the computational measure is able to approximate empirical results. Our approach can be used to rank competing encodings on their discriminability and to aid in selecting visualizations for a particular type of data distribution.
C1 [Veras, Rafael; Collins, Christopher] Ontario Tech Univ, Oshawa, ON, Canada.
RP Veras, R (corresponding author), Ontario Tech Univ, Oshawa, ON, Canada.
EM rafael.verasguimaraes@uoit.ca; christopher.collins@uoit.ca
RI Collins, Christopher/AAJ-6345-2020
FU Natural Sciences and Engineering Research Council of Canada (NSERC);
   Fundacao CAPES [907813-4]
FX We acknowledge the support of the Natural Sciences and Engineering
   Research Council of Canada (NSERC) and Fundacao CAPES (907813-4/Ciencia
   sem Fronteiras).
NR 48
TC 10
Z9 13
U1 1
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 749
EP 758
DI 10.1109/TVCG.2019.2934432
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100069
PM 31442981
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Wang, Y
   Jin, ZH
   Wang, QW
   Cui, WW
   Ma, TF
   Qu, HM
AF Wang, Yong
   Jin, Zhihua
   Wang, Qianwen
   Cui, Weiwei
   Ma, Tengfei
   Qu, Huamin
TI DeepDrawing: A Deep Learning Approach to Graph Drawing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Graph Drawing; Deep Learning; LSTM; Procrustes Analysis
ID LAYOUT; ALGORITHM
AB Node-link diagrams are widely used to facilitate network explorations. However, when using a graph drawing technique to visualize networks, users often need to tune different algorithm-specific parameters iteratively by comparing the corresponding drawing results in order to achieve a desired visual effect. This trial and error process is often tedious and time-consuming, especially for non-expert users. Inspired by the powerful data modelling and prediction capabilities of deep learning techniques, we explore the possibility of applying deep learning techniques to graph drawing. Specifically, we propose using a graph-LSTM-based approach to directly map network structures to graph drawings. Given a set of layout examples as the training dataset, we train the proposed graph-LSTM-based model to capture their layout characteristics. Then, the trained model is used to generate graph drawings in a similar style for new networks. We evaluated the proposed approach on two special types of layouts (i.e., grid layouts and star layouts) and two general types of layouts (i.e., ForceAtlas2 and PivotMDS) in both qualitative and quantitative ways. The results provide support for the effectiveness of our approach. We also conducted a time cost assessment on the drawings of small graphs with 20 to 50 nodes. We further report the lessons we learned and discuss the limitations and future work.
C1 [Wang, Yong; Jin, Zhihua; Wang, Qianwen; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Jin, Zhihua] Zhejiang Univ, Hangzhou, Peoples R China.
   [Cui, Weiwei] Microsoft Res Asia, Beijing, Peoples R China.
   [Ma, Tengfei] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
C3 Hong Kong University of Science & Technology; Zhejiang University;
   Microsoft; Microsoft China; Microsoft Research Asia; International
   Business Machines (IBM); IBM USA
RP Wang, Y (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
EM ywangct@cse.ust.hk; zjinak@cse.ust.hk; qwangbb@cse.ust.hk;
   weiwei.cui@microsoft.com; tengfei.ma1@ibm.com; huamin@cse.ust.hk
RI Wang, Yong/HKF-3903-2023; Wang, Qianwen/GRJ-9435-2022; Ma,
   Tengfei/L-6178-2018
FU MSRA [MRA19EG02]
FX The authors wish to thank Yao Ming, Qiaomu Shen and Daniel Archambault
   for the constructive discussions. The authors also thank the anonymous
   reviewers for their valuable comments. This work is partially supported
   by a grant from MSRA (code: MRA19EG02).
NR 82
TC 30
Z9 30
U1 2
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 676
EP 686
DI 10.1109/TVCG.2019.2934798
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100063
PM 31443020
OA Green Accepted, Green Submitted
DA 2025-03-07
ER

PT J
AU Zhao, JQ
   Karimzadeh, M
   Snyder, LS
   Surakitbanharn, C
   Qian, ZC
   Ebert, DS
AF Zhao, Jieqiong
   Karimzadeh, Morteza
   Snyder, Luke S.
   Surakitbanharn, Chittayong
   Qian, Zhenyu Cheryl
   Ebert, David S.
TI MetricsVis: A Visual Analytics System for Evaluating Employee
   Performance in Public Safety Agencies
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Organizational performance analysis; multi-dimensional data;
   hierarchical relationships; visual analytics
ID HUMAN-RESOURCE MANAGEMENT; ORGANIZATIONAL PERFORMANCE; EXPLORATION;
   IMPACT
AB Evaluating employee performance in organizations with varying workloads and tasks is challenging. Specifically, it is important to understand how quantitative measurements of employee achievements relate to supervisor expectations, what the main drivers of good performance are, and how to combine these complex and flexible performance evaluation metrics into an accurate portrayal of organizational performance in order to identify shortcomings and improve overall productivity. To facilitate this process, we summarize common organizational performance analyses into four visual exploration task categories. Additionally, we develop MetricsVis, a visual analytics system composed of multiple coordinated views to support the dynamic evaluation and comparison of individual, team, and organizational performance in public safety organizations. MetricsVis provides four primary visual components to expedite performance evaluation: (1) a priority adjustment view to support direct manipulation on evaluation metrics; (2) a reorderable performance matrix to demonstrate the details of individual employees; (3) a group performance view that highlights aggregate performance and individual contributions for each group; and (4) a projection view illustrating employees with similar specialties to facilitate shift assignments and training. We demonstrate the usability of our framework with two case studies from medium-sized law enforcement agencies and highlight its broader applicability to other domains.
C1 [Zhao, Jieqiong; Snyder, Luke S.; Qian, Zhenyu Cheryl; Ebert, David S.] Purdue Univ, W Lafayette, IN 47907 USA.
   [Karimzadeh, Morteza] Univ Colorado, Purdue Univ, Boulder, CO 80309 USA.
   [Surakitbanharn, Chittayong] Hivemapper, Burlingame, CA USA.
C3 Purdue University System; Purdue University; University of Colorado
   System; University of Colorado Boulder
RP Zhao, JQ (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.
EM zhao413@purdue.edu; karimzadeh@colorado.edu; snyde238@purdue.edu;
   chittayong.surakitbanharn@gmail.com; qianz@purdue.edu; ebertd@purdue.edu
RI Qian, Cheryl Zhenyu/AAD-8705-2022; Zhao, Jieqiong/HLH-8586-2023;
   Karimzadeh, Morteza/AAE-8300-2020
OI Zhao, Jieqiong/0000-0002-4303-7722; Ebert, David/0000-0001-6177-1296;
   Karimzadeh, Morteza/0000-0002-6498-1763; QIAN, Cheryl
   Zhenyu/0000-0002-7310-8608
FU U.S. Department of Homeland Security VACCINE Center
   [2009-ST-061-CI0003]; Research Projects Agency-Energy (ARPA-E), U.S.
   Department of Energy [DE-AR0000593]
FX The authors wish to thank Steve Hawthorn, Patrick J. Flannelly,
   Christina A. Stober, and Hao Chen. This work is funded in part by the
   U.S. Department of Homeland Security VACCINE Center under Award Number
   2009-ST-061-CI0003 and the Research Projects Agency-Energy (ARPA-E),
   U.S. Department of Energy, under Award Number DE-AR0000593.
NR 43
TC 4
Z9 5
U1 1
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 1193
EP 1203
DI 10.1109/TVCG.2019.2934603
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA KB0CF
UT WOS:000506166100110
PM 31425117
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Erat, O
   Hoell, M
   Haubenwallner, K
   Pirchheim, C
   Schmalstieg, D
AF Erat, Okan
   Hoell, Markus
   Haubenwallner, Karl
   Pirchheim, Christian
   Schmalstieg, Dieter
TI Real-Time View Planning for Unstructured Lumigraph Modeling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 18th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 14-18, 2019
CL Beijing, PEOPLES R CHINA
SP IEEE
DE Lumigraph; virtual reality; rendering; real-time; view planning;
   keyframe selection; multi-view
ID MULTIVIEW STEREO; QUALITY
AB We propose an algorithm for generating an unstructured lumigraph in real-time from an image stream. This problem has important applications in mixed reality, such as telepresence, interior design or as-built documentation. Unlike conventional texture optimization in structure from motion, our method must choose views from the input stream in a strictly incremental manner, since only a small number of views can be stored or transmitted. This requires formulating an online variant of the well-known view-planning problem, which must take into account what parts of the scene have already been seen and how the lumigraph sample distribution could improve in the future. We address this highly unconstrained problem by regularizing the scene structure using a regular grid structure. Upon the grid structure, we define a coverage metric describing how well the lumigraph samples cover the grid in terms of spatial and angular resolution, and we greedily keep incoming views if they improve the coverage. We evaluate the performance of our algorithm quantitatively and qualitatively on a variety of synthetic and real scenes, and demonstrate visually appealing results obtained at real-time frame rates (in the range of 3Hz-100Hz per incoming image, depending on configuration).
C1 [Erat, Okan; Hoell, Markus; Haubenwallner, Karl; Pirchheim, Christian; Schmalstieg, Dieter] Graz Univ Technol, Graz, Austria.
C3 Graz University of Technology
RP Erat, O (corresponding author), Graz Univ Technol, Graz, Austria.
EM okan.erat@icg.tugraz.at; hoell@icg.tugraz.at;
   karl.haubenwallner@icg.tugraz.at; pirchheim@icg.tugraz.at;
   schmalstieg@icg.tugraz.at
NR 51
TC 6
Z9 6
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2019
VL 25
IS 11
BP 3063
EP 3072
DI 10.1109/TVCG.2019.2932237
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA JD2VM
UT WOS:000489833000004
PM 31403421
DA 2025-03-07
ER

PT J
AU Meuschke, M
   Oeltze-Jafra, S
   Beuing, O
   Preim, B
   Lawonn, K
AF Meuschke, Monique
   Oeltze-Jafra, Steffen
   Beuing, Oliver
   Preim, Bernhard
   Lawonn, Kai
TI Classification of Blood Flow Patterns in Cerebral Aneurysms
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Medical visualizations; aneurysms; blood flow; parametrization;
   classification
ID WALL SHEAR-STRESS; COMBINED VISUALIZATION; RUPTURE; HEMODYNAMICS;
   EXPLORATION; PATIENT; RISK; THICKNESS; MAPS
AB We present a Cerebral Aneurysm Vortex Classification (CAVOCLA) that allows to classify blood flow in cerebral aneurysms. Medical studies assume a strong relation between the progression and rupture of aneurysms and flow patterns. To understand how flow patterns impact the vessel morphology, they are manually classified according to predefined classes. However, manual classifications are time-consuming and exhibit a high inter-observer variability. In contrast, our approach is more objective and faster than manual methods. The classification of integral lines, representing steady or unsteady blood flow, is based on a mapping of the aneurysm surface to a hemisphere by calculating polar-based coordinates. The lines are clustered and for each cluster a representative is calculated. Then, the polar-based coordinates are transformed to the representative as basis for the classification. Classes are based on the flow complexity. The classification results are presented by a detail-on-demand approach using a visual transition from the representative over an enclosing surface to the associated lines. Based on seven representative datasets, we conduct an informal interview with five domain experts to evaluate the system. They confirmed that CAVOCLA allows for a robust classification of intra-aneurysmal flow patterns. The detail-on-demand visualization enables an efficient exploration and interpretation of flow patterns.
C1 [Meuschke, Monique; Preim, Bernhard] Univ Magdeburg, Dept Simulat & Graph, D-39106 Magdeburg, Germany.
   [Meuschke, Monique; Beuing, Oliver; Preim, Bernhard] Res Campus STIMULATE, D-39106 Magdeburg, Germany.
   [Oeltze-Jafra, Steffen] Univ Leipzig, Res Grp Digital Patient & Proc Model, Innovat Ctr Comp Assisted Surg, D-04109 Leipzig, Germany.
   [Beuing, Oliver] Univ Hosp Magdeburg, Inst Neuroradiol, D-39120 Magdeburg, Germany.
   [Lawonn, Kai] Univ Koblenz Landau, Med Visualizat, D-55118 Mainz, Germany.
C3 Otto von Guericke University; Leipzig University; University Hospital
   Magdeburg; University of Koblenz & Landau
RP Meuschke, M (corresponding author), Univ Magdeburg, Dept Simulat & Graph, D-39106 Magdeburg, Germany.; Meuschke, M (corresponding author), Res Campus STIMULATE, D-39106 Magdeburg, Germany.
EM meuschke@isg.cs.uni-magdeburg.de;
   Steffen.Oeltze-Jafra@medizin.uni-leipzig.de; oliver.beuing@med.ovgu.de;
   bernhard@isg.cs.uni-magdeburg.de; lawonn@uni-koblenz.de
RI Preim, Bernhard/AAF-6565-2021; Oeltze-Jafra, Steffen/W-8088-2019
FU BMBF [STIMULATE:13GW0095A]
FX This work was partially funded by the BMBF (STIMULATE:13GW0095A). The
   authors like to thank Samuel Voss and Philipp Berg for the fruitful
   discussions on these topics.
NR 64
TC 14
Z9 16
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2019
VL 25
IS 7
BP 2404
EP 2418
DI 10.1109/TVCG.2018.2834923
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IA8WG
UT WOS:000469838700006
PM 29994310
DA 2025-03-07
ER

PT J
AU Xie, X
   Cai, XW
   Zhou, JP
   Cao, N
   Wu, YC
AF Xie, Xiao
   Cai, Xiwen
   Zhou, Junpei
   Cao, Nan
   Wu, Yingcai
TI A Semantic-Based Method for Visualizing Large Image Collections
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image visualization; semantic layout; CNN; image captioning
ID INTERACTIVE EXPLORATION; TEXT COLLECTIONS; ANALYTICS
AB Interactive visualization of large image collections is important and useful in many applications, such as personal album management and user profiling on images. However, most prior studies focus on using low-level visual features of images, such as texture and color histogram, to create visualizations without considering the more important semantic information embedded in images. This paper proposes a novel visual analytic system to analyze images in a semantic-aware manner. The system mainly comprises two components: a semantic information extractor and a visual layout generator. The semantic information extractor employs an image captioning technique based on convolutional neural network (CNN) to produce descriptive captions for images, which can be transformed into semantic keywords. The layout generator employs a novel co-embedding model to project images and the associated semantic keywords to the same 2D space. Inspired by the galaxy metaphor, we further turn the projected 2D space to a galaxy visualization of images, in which semantic keywords and images are visually encoded as stars and planets. Our system naturally supports multi-scale visualization and navigation, in which users can immediately see a semantic overview of an image collection and drill down for detailed inspection of a certain group of images. Users can iteratively refine the visual layout by integrating their domain knowledge into the co-embedding process. Two task-based evaluations are conducted to demonstrate the effectiveness of our system.
C1 [Xie, Xiao; Cai, Xiwen; Zhou, Junpei; Wu, Yingcai] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
   [Xie, Xiao; Wu, Yingcai] Alibaba Zhejiang Univ, Joint Inst Frontier Technol, Hangzhou 310027, Zhejiang, Peoples R China.
   [Cao, Nan] Tongji Univ, Coll Design & Innovat, Shanghai 200092, Peoples R China.
C3 Zhejiang University; Tongji University
RP Xie, X (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.; Xie, X (corresponding author), Alibaba Zhejiang Univ, Joint Inst Frontier Technol, Hangzhou 310027, Zhejiang, Peoples R China.
EM xxie@zju.edu.cn; xwcai@zju.edu.cn; zhoujunpei@zju.edu.cn;
   nan.cao@gmail.com; ycwu@zju.edu.cn
RI Cao, Nan/O-5397-2014; Cai, Xiwen/AIE-0886-2022
OI Xie, Xiao/0000-0002-2135-5236; Zhou, Junpei/0000-0001-8156-1464; Cao,
   Nan/0000-0003-1316-7515
FU National Key R&D Program of China [2018YFB1004302]; NSFC [61761136020,
   61502416]; NSFC-Zhejiang Joint Fund for the Integration of
   Industrialization and Informatization [U1609217]; Zhejiang Provincial
   Natural Science Foundation [LR18F020001]; 100 Talents Program of
   Zhejiang University; Microsoft Research Asia
FX The work was supported by National Key R&D Program of China
   (2018YFB1004302), NSFC (61761136020, 61502416), NSFC-Zhejiang Joint Fund
   for the Integration of Industrialization and Informatization (U1609217),
   Zhejiang Provincial Natural Science Foundation (LR18F020001) and the 100
   Talents Program of Zhejiang University. This project is also partially
   funded by Microsoft Research Asia.
NR 54
TC 31
Z9 34
U1 0
U2 31
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2019
VL 25
IS 7
BP 2362
EP 2377
DI 10.1109/TVCG.2018.2835485
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IA8WG
UT WOS:000469838700003
PM 29993720
DA 2025-03-07
ER

PT J
AU Windhager, F
   Federico, P
   Schreder, G
   Glinka, K
   Dörk, M
   Miksch, S
   Mayr, E
AF Windhager, Florian
   Federico, Paolo
   Schreder, Guenther
   Glinka, Katrin
   Doerk, Marian
   Miksch, Silvia
   Mayr, Eva
TI Visualization of Cultural Heritage Collection Data: State of the Art and
   Future Challenges
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Information visualization; introductory and survey; digital libraries;
   arts and humanities
ID INFORMATION VISUALIZATION; VISUAL ANALYTICS; DESIGN; STORIES; SUPPORT;
   REALITY; MODEL
AB After decades of digitization, large cultural heritage collections have emerged on the web, which contain massive stocks of content from galleries, libraries, archives, and museums. This increase in digital cultural heritage data promises new modes of analysis and increased levels of access for academic scholars and casual users alike. Going beyond the standard representations of searchcentric and grid-based interfaces, a multitude of approaches has recently started to enable visual access to cultural collections, and to explore them as complex and comprehensive information spaces by the means of interactive visualizations. In contrast to conventional web interfaces, we witness a widening spectrum of innovative visualization types specially designed for rich collections from the cultural heritage sector. This new class of information visualizations gives rise to a notable diversity of interaction and representation techniques while lending currency and urgency to a discussion about principles such as serendipity, generosity, and criticality in connection with visualization design. With this survey, we review information visualization approaches to digital cultural heritage collections and reflect on the state of the art in techniques and design choices. We contextualize our survey with humanist perspectives on the field and point out opportunities for future research.
C1 [Windhager, Florian; Schreder, Guenther; Mayr, Eva] Danube Univ Krems, Dept Knowledge & Commun Management, A-3500 Krems An Der Donau, Austria.
   [Federico, Paolo; Miksch, Silvia] Vienna Univ Technol, Inst Software Technol & Interact Syst, A-1040 Vienna, Austria.
   [Glinka, Katrin] Stiftung Preuss Kulturbesitz, D-10785 Berlin, Germany.
   [Doerk, Marian] Univ Appl Sci, Urban Complex Lab, D-14469 Potsdam, Germany.
C3 Danube University Krems; Technische Universitat Wien
RP Windhager, F (corresponding author), Danube Univ Krems, Dept Knowledge & Commun Management, A-3500 Krems An Der Donau, Austria.
EM florian.windhager@donau-uni.ac.at; federico@ifs.tuwien.ac.at;
   guenther.schreder@donau-uni.ac.at; k.glinka@smb.spk-berlin.de;
   doerk@fh-potsdam.de; miksch@ifs.tuwien.ac.at; eva.mayr@donau-uni.ac.at
RI Schreder, Günther/HOF-2032-2023; Federico, Paolo/J-3152-2016
OI Windhager, Florian/0000-0002-5170-2243; Dork,
   Marian/0000-0002-3469-7841; Schreder, Gunther/0000-0002-3034-5195; Mayr,
   Eva/0000-0001-8402-5990; Glinka, Katrin/0000-0002-4232-8907; Miksch,
   Silvia/0000-0003-4427-5703
FU Austrian Science Fund (FWF) [P28363-24]; German Federal Ministry of
   Education and Research (BMBF); Austrian Science Fund (FWF) [P28363]
   Funding Source: Austrian Science Fund (FWF)
FX This work was supported by a grant from the Austrian Science Fund (FWF)
   Project No. P28363-24, and by a grant from the German Federal Ministry
   of Education and Research (BMBF) for the project VIKUS -Visualisierung
   kultureller Sammlungen. The authors wish to thank Florian Krautli, Devon
   Schiller, and Florian Wienczek for their support.
NR 153
TC 98
Z9 105
U1 38
U2 214
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2019
VL 25
IS 6
BP 2311
EP 2330
DI 10.1109/TVCG.2018.2830759
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA HW8AC
UT WOS:000466910200015
PM 29994026
OA Bronze
DA 2025-03-07
ER

PT J
AU Stotko, P
   Krunnpen, S
   Hullin, MB
   Weinnnann, M
   Klein, R
AF Stotko, Patrick
   Krunnpen, Stefan
   Hullin, Matthias B.
   Weinnnann, Michael
   Klein, Reinhard
TI SLAMCast: Large-Scale, Real-Time 3D Reconstruction and Streaming for
   lmmersive Multi-Client Live Telepresence
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Remote collaboration; live telepresence; real-time reconstruction; voxel
   hashing; RGB-D; real-time streaming
ID INTEGRATION; SYSTEM
AB Real-time 3D scene reconstruction from RGB-D sensor data, as well as the exploration of such data in VRiAR settings, has seen tremendous progress in recent years. The combination of both these components into telepresence systems, however, comes with significant technical challenges. All approaches proposed so far are extremely demanding on input and output devices, compute resources and transmission bandwidth, and they do not reach the level of immediacy required for applications such as remote collaboration. Here, we introduce what we believe is the first practical client-server system for real-time capture and many-user exploration of static 3D scenes. Our system is based on the observation that interactive frame rates are sufficient for capturing and reconstruction, and real-time performance is only required on the client site to achieve lag-free view updates when rendering the 3D model. Starting from this insight, we extend previous voxel block hashing frameworks by introducing a novel thread-safe GPU hash map data structure that is robust under massively concurrent retrieval, insertion and removal of entries on a thread level. We further propose a novel transmission scheme for volume data that is specifically targeted to Marching Cubes geometry reconstruction and enables a 90% reduction in bandwidth between server and exploration clients. The resulting system poses very moderate requirements on network bandwidth, latency and client-side computation, which enables it to rely entirely on consumer-grade hardware, including mobile devices. We demonstrate that our technique achieves state-of-the-art representation accuracy while providing, for any number of clients, an immersive and fluid lag-free viewing experience even during network outages.
C1 [Stotko, Patrick; Krunnpen, Stefan; Hullin, Matthias B.; Weinnnann, Michael; Klein, Reinhard] Univ Bonn, Bonn, Germany.
C3 University of Bonn
RP Stotko, P (corresponding author), Univ Bonn, Bonn, Germany.
EM stotko@cs.uni-bonn.de; krumpen@cs.uni-bonn.de; hullin@cs.uni-bonn.de;
   mw@cs.uni-bonn.de; rk@cs.uni-bonn.de
RI Weinmann, Michael/AAK-9163-2020
OI Weinmann, Michael/0000-0003-3634-0093
FU DFG (DFG Research Unit Anticipating Human Behavior) [KL 1142/11-1, FOR
   2535]; DFG (DFG Research Unit Mapping on Demand) [KL 1142/9-2, FOR 1505]
FX This work was supported by the DFG projects KL 1142/11-1 (DFG Research
   Unit FOR 2535 Anticipating Human Behavior) and KL 1142/9-2 (DFG Research
   Unit FOR 1505 Mapping on Demand).
NR 52
TC 55
Z9 58
U1 0
U2 30
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2019
VL 25
IS 5
BP 2102
EP 2112
DI 10.1109/TVCG.2019.2899231
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HR3EI
UT WOS:000463019100029
PM 30794183
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Hu, YF
   Shi, L
   Liu, QS
AF Hu, Yifan
   Shi, Lei
   Liu, Qingsong
TI A Coloring Algorithm for Disambiguating Graph and Map Drawings
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Graph drawing; virtual maps; edge coloring; branch-and-bound algorithm;
   global optimization
ID EDGE; VISUALIZATION
AB Drawings of non-planar graphs always result in edge crossings. When there are many edges crossing at small angles, it is often difficult to follow these edges, because of the multiple visual paths resulted from the crossings that slow down eye movements. In this paper we propose an algorithm that disambiguates the edges with automatic selection of distinctive colors. Our proposed algorithm computes a near optimal color assignment of a dual collision graph, using a novel branch-and-bound procedure applied to a space decomposition of the color gamut. We give examples demonstrating this approach in real world graphs and maps, as well as a user study to establish its effectiveness and limitations.
C1 [Hu, Yifan] Yahoo Res, Sunnyvale, CA 94089 USA.
   [Shi, Lei; Liu, Qingsong] Chinese Acad Sci, Inst Software, SKLCS, Beijing 100049, Peoples R China.
   [Shi, Lei; Liu, Qingsong] UCAS, Beijing 100049, Peoples R China.
C3 Yahoo! Inc; Chinese Academy of Sciences; Institute of Software, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Shi, L (corresponding author), Chinese Acad Sci, Inst Software, SKLCS, Beijing 100049, Peoples R China.; Shi, L (corresponding author), UCAS, Beijing 100049, Peoples R China.
EM yifanhu@yahoo.com; shil@ios.ac.cn; liuqs@ios.ac.cn
FU China National 973 Project [2014CB340301]; NSFC [61379088, 61772504];
   Key Research Program of Frontier Sciences, CAS [QYZDY-SSW-JSC041]
FX Lei Shi was supported by China National 973 Project 2014CB340301, NSFC
   Grants 61379088, 61772504, and the Key Research Program of Frontier
   Sciences, CAS (Grant No. QYZDY-SSW-JSC041).
NR 30
TC 0
Z9 0
U1 0
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2019
VL 25
IS 2
BP 1321
EP 1335
DI 10.1109/TVCG.2018.2798631
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG5ZY
UT WOS:000455062000007
PM 29994354
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Reach, AM
   North, C
AF Reach, Andrew McCaleb
   North, Chris
TI Smooth, Efficient, and Interruptible Zooming and Panning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Zooming and panning; hyperbolic geometry; information visualization
AB This paper introduces a novel technique for smooth and efficient zooming and panning based on dynamical systems in hyperbolic space. Unlike the technique of van Wijk and Nuij, the animations produced by our technique are smooth at the end-points and when interrupted by a change of target. To analyze the results of our technique, we introduce world/screen diagrams, a novel technique for visualizing zooming and panning animations.
C1 [Reach, Andrew McCaleb; North, Chris] Virginia Tech, Blacksburg, VA 24061 USA.
C3 Virginia Polytechnic Institute & State University
RP Reach, AM (corresponding author), Virginia Tech, Blacksburg, VA 24061 USA.
EM caleb.reach@vt.edu; north@cs.vt.edu
FU NSF [IIS-1447416]
FX This research was supported in part by NSF grant IIS-1447416.
NR 19
TC 5
Z9 5
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2019
VL 25
IS 2
BP 1421
EP 1434
DI 10.1109/TVCG.2018.2800013
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG5ZY
UT WOS:000455062000014
PM 29994563
OA Bronze, Green Submitted
DA 2025-03-07
ER

PT J
AU Shih, M
   Rozhon, C
   Ma, KL
AF Shih, Min
   Rozhon, Charles
   Ma, Kwan-Liu
TI A Declarative Grammar of Flexible Volume Visualization Pipelines
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Volume visualization; direct volume rendering; declarative
   specification; multivariate/ultimodal volume data; animation
ID DOMAIN-SPECIFIC LANGUAGE; SHADER FRAMEWORK; ENVIRONMENT; DIDEROT;
   SYSTEM; VEGA
AB This paper presents a declarative grammar for conveniently and effectively specifying advanced volume visualizations. Existing methods for creating volume visualizations either lack the flexibility to specify sophisticated visualizations or are difficult to use for those unfamiliar with volume rendering implementation and parameterization. Our design provides the ability to quickly create expressive visualizations without knowledge of the volume rendering implementation. It attempts to capture aspects of those difficult but powerful methods while remaining flexible and easy to use. As a proof of concept. our current implementation of the grammar allows users to combine multiple data variables in various ways and define transfer functions for diverse input data. The grammar also has the ability to describe advanced shading effects and create animations. We demonstrate the power and flexibility of our approach using multiple practical volume visualizations.
C1 [Shih, Min; Rozhon, Charles; Ma, Kwan-Liu] Univ Calif Davis, Davis, CA 95616 USA.
C3 University of California System; University of California Davis
RP Shih, M (corresponding author), Univ Calif Davis, Davis, CA 95616 USA.
EM minshih@ucdavis.edu; carozhon@ucdavis.edu; ma@cs.ucdavis.edu
FU U.S. Department of Energy [DE-SC00012610]; U.S. National Science
   Foundation [IIS-1528203]
FX The authors wish to thank Chris Ye. Kelvin Li, and Franz Sauer for
   invaluable discussions. Thanks also go to Dr. Jacqueline H. Chen, Sandia
   National Laboratories. and Dr. Angelique Y. Louie, University of
   California, Davis, for providing us with their datasets. This research
   is supported in part by the U.S. Department of Energy through grant
   DE-SC00012610 and U.S. National Science Foundation through grant
   IIS-1528203.
NR 38
TC 6
Z9 9
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 1050
EP 1059
DI 10.1109/TVCG.2018.2864841
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000100
PM 30130223
DA 2025-03-07
ER

PT J
AU Xu, KJ
   Chen, GN
AF Xu, Kaoji
   Chen, Guoning
TI Hexahedral Mesh Structure Visualization and Evaluation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE hexahedral mesh; base complex; sheet decomposition; complexity analysis
ID GENERATION; REFINEMENT; LAYOUTS
AB Understanding hexahedral (hex-) mesh structures is important for a number of hex-mesh generation and optimization tasks. However, due to various configurations of the singularities in a valid pure hex-mesh, the structure (or base complex) of the mesh can be arbitrarily complex. In this work, we present a first and effective method to help meshing practitioners understand the possible configurations in a valid 3D base complex for the characterization of their complexity. In particular, we propose a strategy to decompose the complex hex-mesh structure into multi-level sub-structures so that they can be studied separately, from which we identify a small set of the sub-structures that can most efficiently represent the whole mesh structure. Furthermore, from this set of sub-structures, we attempt to define the first metric for the quantification of the complexity of hex-mesh structure. To aid the exploration of the extracted multi-level structure information, we devise a visual exploration system coupled with a matrix view to help alleviate the common challenge of 3D data exploration (e.g., clutter and occlusion). We have applied our tool and metric to a large number of hex-meshes generated with different approaches to reveal different characteristics of these methods in terms of the mesh structures they can produce. We also use our metric to assess the existing structure simplification techniques in terms of their effectiveness.
C1 [Xu, Kaoji; Chen, Guoning] Univ Houston, Houston, TX 77004 USA.
C3 University of Houston System; University of Houston
RP Xu, KJ (corresponding author), Univ Houston, Houston, TX 77004 USA.
EM cotrikxu@gmail.com; gchen16@uh.edu
OI Chen, Guoning/0000-0003-0581-6415
FU NSF [IIS 1553329]
FX We thank all anonymous reviewers for their constructive comments. We
   thank Brian Kline for proofreading the paper. This research was
   supported by NSF IIS 1553329.
NR 42
TC 8
Z9 11
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 1173
EP 1182
DI 10.1109/TVCG.2018.2864827
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000112
PM 30130218
DA 2025-03-07
ER

PT J
AU Bork, F
   Schnelzer, C
   Eck, U
   Navab, N
AF Bork, Felix
   Schnelzer, Christian
   Eck, Ulrich
   Navab, Nassir
TI Towards Efficient Visual Guidance in Limited Field-of-View Head-Mounted
   Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 17th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 16-20, 2018
CL Munich, GERMANY
SP IEEE, IEEE Comp Soc, IEEE VGTC, ACM SIGCHI, Mozilla, Apple, Intel, DAQRI, PTC, Amazon, Facebook, Qualcomm, Umajin, Disney Res, Univ S Australia Ventures Pty Ltd, REFLEKT, Occipital, Envisage AR, KHRONOS Grp, TUM, ETH Zurich
DE Mixed / Augmented reality; Visualization design and evaluation methods
ID STRATEGIES; TAXONOMY
AB Understanding, navigating, and performing goal-oriented actions in Mixed Reality (MR) environments is a challenging task and requires adequate information conveyance about the location of all virtual objects in a scene. Current Head-Mounted Displays (HMDs) have a limited field-of-view where augmented objects may be displayed. Furthermore, complex MR environments may be comprised of a large number of objects which can be distributed in the extended surrounding space of the user. This paper presents two novel techniques for visually guiding the attention of users towards out-of-view objects in HMD-based MR: the 3D Radar and the Mirror Ball. We evaluate our approaches against existing techniques during three different object collection scenarios, which simulate real-world exploratory and goal-oriented visual search tasks. To better understand how the different visualizations guide the attention of users, we analyzed the head rotation data for all techniques and introduce a novel method to evaluate and classify head rotation trajectories. Our findings provide supporting evidence that the type of visual guidance technique impacts the way users search for virtual objects in MR.
C1 [Bork, Felix; Schnelzer, Christian; Eck, Ulrich; Navab, Nassir] Tech Univ Munich, Munich, Germany.
C3 Technical University of Munich
RP Bork, F (corresponding author), Tech Univ Munich, Munich, Germany.
EM felix.bork@tum.de; christian.schnelzer@tum.de; ulrich.eck@tum.de;
   nassir.navab@tum.de
OI Eck, Ulrich/0000-0002-5322-4724
NR 42
TC 59
Z9 64
U1 1
U2 28
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2018
VL 24
IS 11
BP 2983
EP 2992
DI 10.1109/TVCG.2018.2868584
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GZ0TM
UT WOS:000449077900016
PM 30188832
DA 2025-03-07
ER

PT J
AU Hamasaki, T
   Itoh, Y
   Hiroi, Y
   Iwai, D
   Sugimoto, M
AF Hamasaki, Takumi
   Itoh, Yuta
   Hiroi, Yuichi
   Iwai, Daisuke
   Sugimoto, Maki
TI HySAR: Hybrid Material Rendering by an Optical See-Through Head-Mounted
   Display with Spatial Augmented Reality Projection
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 25th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 18-22, 2018
CL Reutlingen, GERMANY
SP IEEE, IEEE Comp Soc, IEEE Comp Soc Visualizat & Graph Tech Comm, VICON, Digital Project, ART, Haption, MiddleVR, VR ON, VISCON, BARCO, WorldViz, Disney Res, Chinese Acad Sci, Comp Network Informat Ctr, KUKA
DE Optical see-through displays; hybrid material rendering; spatial
   augmented reality
ID COLOR
AB Spatial augmented reality (SAR) pursues realism in rendering materials and objects. To advance this goal, we propose a hybrid SAR (HySAR) that combines a projector with optical see-through head-mounted displays (OST-HMD). In an ordinary SAR scenario with co-located viewers, the viewers perceive the same virtual material on physical surfaces. In general, the material consists of two components: a view-independent (VI) component such as diffuse reflection, and a view-dependent (VD) component such as specular reflection. The VI component is static over viewpoints, whereas the VD should change for each viewpoint even if a projector can simulate only one viewpoint at one time. In HySAR, a projector only renders the static VI components. In addition, the OST-HMD renders the dynamic VD components according to the viewer's current viewpoint. Unlike conventional SAR, the HySAR concept theoretically allows an unlimited number of co-located viewers to see the correct material over different viewpoints. Furthermore, the combination enhances the total dynamic range, the maximum intensity, and the resolution of perceived materials. With proof-of-concept systems, we demonstrate HySAR both qualitatively and quantitatively with real objects. First, we demonstrate HySAR by rendering synthetic material properties on a real object from different viewpoints. Our quantitative evaluation shows that our system increases the dynamic range by 2.24 times and the maximum intensity by 2.12 times compared to an ordinary SAR system. Second, we replicate the material properties of a real object by SAR and HySAR, and show that HySAR outperforms SAR in rendering VD specular components.
C1 [Hamasaki, Takumi; Itoh, Yuta; Hiroi, Yuichi; Sugimoto, Maki] Keio Univ, Tokyo, Japan.
   [Itoh, Yuta] Tokyo Inst Technol, Tokyo, Japan.
   [Itoh, Yuta] RIKEN, Wako, Saitama, Japan.
   [Iwai, Daisuke] Osaka Univ, Suita, Osaka, Japan.
C3 Keio University; Institute of Science Tokyo; Tokyo Institute of
   Technology; RIKEN; Osaka University
RP Hamasaki, T (corresponding author), Keio Univ, Tokyo, Japan.
EM hamasaki@imlab.ics.keio.ac.jp; y.hiroi@imlab.ics.keio.ac.jp;
   yuta.itoh@c.titech.ac.jp; daisuke.iwai@sys.es.osaka-u.ac.jp;
   sugimoto@imlab.ics.keio.ac.jp
RI Hiroi, Yuichi/KGM-7451-2024
OI Itoh, Yuta/0000-0002-5901-797X; Iwai, Daisuke/0000-0002-3493-5635
FU JSPS KAKENHI Grant, Japan [JP15H05925, JP17H04692]; JST CREST Grant,
   Japan [JPMJCR14E1]; JST PRESTO Grant, Japan [JPMJPR17J2]; Grants-in-Aid
   for Scientific Research [15H05925, 17H04692] Funding Source: KAKEN
FX This work was partially supported by JSPS KAKENHI Grant Numbers
   JP15H05925 and JP17H04692, Japan; by JST CREST Grant Number JPMJCR14E1,
   Japan; and by JST PRESTO Grant Number JPMJPR17J2, Japan.
NR 37
TC 10
Z9 10
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2018
VL 24
IS 4
BP 1457
EP 1466
DI 10.1109/TVCG.2018.2793659
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FZ6BS
UT WOS:000427682500008
PM 29543164
DA 2025-03-07
ER

PT J
AU Kim, H
   Gabbard, JL
   Anon, AM
   Misu, T
AF Kim, Hyungil
   Gabbard, Joseph L.
   Anon, Alexandre Miranda
   Misu, Teruhisa
TI Driver Behavior and Performance with Augmented Reality Pedestrian
   Collision Warning: An Outdoor User Study
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 25th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 18-22, 2018
CL Reutlingen, GERMANY
SP IEEE
DE Augmented reality; human performance; automotive; depth cues;
   three-dimensional displays
ID DEPTH; SIZE; DISPLAYS; MOTION
AB This article investigates the effects of visual warning presentation methods on human performance in augmented reality (AR) driving. An experimental user study was conducted in a parking lot where participants drove a test vehicle while braking for any cross traffic with assistance from AR visual warnings presented on a monoscopic and volumetric head-up display (HUD). Results showed that monoscopic displays can be as effective as volumetric displays for human performance in AR braking tasks. The experiment also demonstrated the benefits of conformal graphics, which are tightly integrated into the real world, such as their ability to guide drivers' attention and their positive consequences on driver behavior and performance. These findings suggest that conformal graphics presented via monoscopic HUDs can enhance driver performance by leveraging the effectiveness of monocular depth cues. The proposed approaches and methods can be used and further developed by future researchers and practitioners to better understand driver performance in AR as well as inform usability evaluation of future automotive AR applications.
C1 [Kim, Hyungil; Gabbard, Joseph L.] Virginia Tech, Blacksburg, VA 24061 USA.
   [Anon, Alexandre Miranda; Misu, Teruhisa] Honda Res Inst, Columbus, OH USA.
C3 Virginia Polytechnic Institute & State University; Honda Motor Company;
   Honda USA
RP Kim, H (corresponding author), Virginia Tech, Blacksburg, VA 24061 USA.
EM hci.kim@vt.edu; jgabbard@vt.edu; amiranda@honda-ri.com;
   tmisu@honda-ri.com
RI Kim, Hyungil/K-6944-2019; Misu, Teruhisa/KCY-4391-2024
OI Gabbard, Joseph/0000-0002-7488-676X; Kim, Hyungil/0000-0001-9014-7484
NR 52
TC 46
Z9 52
U1 3
U2 69
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2018
VL 24
IS 4
BP 1515
EP 1524
DI 10.1109/TVCG.2018.2793680
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FZ6BS
UT WOS:000427682500014
PM 29543169
DA 2025-03-07
ER

PT J
AU Xie, B
   Zhang, YQ
   Huang, HK
   Ogawa, E
   You, TJ
   Yu, LF
AF Xie, Biao
   Zhang, Yongqi
   Huang, Haikun
   Ogawa, Elisa
   You, Tongjian
   Yu, Lap-Fai
TI Exercise Intensity-driven Level Design
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 25th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 18-22, 2018
CL Reutlingen, GERMANY
SP IEEE
DE Virtual Reality; Level Design; Procedural Modeling; Exergaming
ID OLDER-ADULTS; HEART-RATE; BALANCE
AB Games and experiences designed for virtual or augmented reality usually require the player to move physically to play. This poses substantial challenge for level designers because the player's physical experience in a level will need to be considered, otherwise the level may turn out to be too exhausting or not challenging enough. This paper presents a novel approach to optimize level designs by considering the physical challenge imposed upon the player in completing a level of motion-based games. A game level is represented as an assembly of chunks characterized by the exercise intensity levels they impose on players. We formulate game level synthesis as an optimization problem, where the chunks are assembled in a way to achieve an optimized level of intensity. To allow the synthesis of game levels of varying lengths, we solve the trans-dimensional optimization problem with a Reversible-jump Markov chain Monte Carlo technique. We demonstrate that our approach can be applied to generate game levels for s of motion-based virtual reality games. A user evaluation validates the effectiveness of our approach in generating levels with the desired amount of physical challenge.
C1 [Xie, Biao; Zhang, Yongqi; Huang, Haikun; Ogawa, Elisa; You, Tongjian; Yu, Lap-Fai] Univ Massachusetts Boston, Boston, MA 02125 USA.
C3 University of Massachusetts System; University of Massachusetts Boston
RP Xie, B (corresponding author), Univ Massachusetts Boston, Boston, MA 02125 USA.
EM Biao.Xie001@umb.edu; Yongqi.Zhang001@umb.edu; Haikun.Huang001@umb.edu;
   Elisa.Ogawa001@umb.edu; Tongjian.You@umb.edu; craigyu@cs.umb.edu
RI Huang, Haikun/AAL-2838-2021
FU Div Of Information & Intelligent Systems; Direct For Computer & Info
   Scie & Enginr [1565978] Funding Source: National Science Foundation
NR 37
TC 22
Z9 24
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2018
VL 24
IS 4
BP 1661
EP 1670
DI 10.1109/TVCG.2018.2793618
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FZ6BS
UT WOS:000427682500029
PM 29553931
DA 2025-03-07
ER

PT J
AU Forbes, AG
   Burks, A
   Lee, K
   Li, X
   Boutillier, P
   Krivine, J
   Fontana, W
AF Forbes, Angus G.
   Burks, Andrew
   Lee, Kristine
   Li, Xing
   Boutillier, Pierre
   Krivine, Jean
   Fontana, Walter
TI Dynamic Influence Networks for Rule-based Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Dynamic networks; biological data visualization; rule-based modeling;
   protein-protein interaction networks
ID VERSATILE
AB We introduce the Dynamic Influence Network (DIN), a novel visual analytics technique for representing and analyzing rule based models of protein-protein interaction networks. Rule-based modeling has proved instrumental in developing biological models that are concise, comprehensible, easily extensible, and that mitigate the combinatorial complexity of multi-state and multi-component biological molecules. Our technique visualizes the dynamics of these rules as they evolve over time. Using the data produced by KaSim, an open source stochastic simulator of rule-based models written in the Kappa language, DINs provide a node-link diagram that represents the influence that each rule has on the other rules. That is, rather than representing individual biological components or types. we instead represent the rules about them (as nodes) and the current influence of these rules (as links). Using our interactive DIN-Viz software tool, researchers are able to query this dynamic network to find meaningful patterns about biological processes, and to identify salient aspects of complex rule-based models. To evaluate the effectiveness of our approach, we investigate a simulation of a circadian clock model that illustrates the oscillatory behavior of the KaiC protein phosphorylation cycle.
C1 [Forbes, Angus G.] Univ Calif Santa Cruz, Santa Cruz, CA 95064 USA.
   [Burks, Andrew; Lee, Kristine; Li, Xing] Univ Illinois, Chicago, IL USA.
   [Krivine, Jean] Univ Paris Diderot, Paris, France.
   [Boutillier, Pierre; Fontana, Walter] Halvord Med Sch, Boston, MA 02115 USA.
C3 University of California System; University of California Santa Cruz;
   University of Illinois System; University of Illinois Chicago;
   University of Illinois Chicago Hospital; Universite Paris Cite
RP Forbes, AG (corresponding author), Univ Calif Santa Cruz, Santa Cruz, CA 95064 USA.
EM angus@ucsc.edu; aburks@uic.edu; khlee2@uic.edu; xli227@uic.edu;
   pierre_boutillier@hms.harvard.edu; jean.krivine@irif.fr;
   walter@hms.harvard.edu
OI Burks, Andrew/0000-0003-0813-1200; Forbes, Angus/0000-0002-8700-7795
FU DARPA Big Mechanism Program under ARO [W911NF-14-1-0367,
   W911NF-14-1-0395]
FX This work is funded by the DARPA Big Mechanism Program under ARO
   contracts W911NF-14-1-0367 and W911NF-14-1-0395.
NR 66
TC 18
Z9 21
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 184
EP 194
DI 10.1109/TVCG.2017.2745280
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400020
PM 28866584
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Mohammed, H
   Al-Awami, AK
   Beyer, J
   Cali, C
   Magistretti, P
   Pfister, H
   Hadwiger, M
AF Mohammed, Haneen
   Al-Awami, Ali K.
   Beyer, Johanna
   Cali, Corrado
   Magistretti, Pierre
   Pfister, Hanspeter
   Hadwiger, Markus
TI Abstractocyte: A Visual Tool for Exploring Nanoscale Astroglial Cells
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Connectomics; Neuroscience; Data Abstraction; Interactive 3D
   Visualization
ID 3D MODELS; BRAIN
AB This paper presents Abstractocyte, a system for the visual analysis of astrocytes and their relation to neurons, in nanoscale volumes of brain tissue. Astrocytes are glial cells, i.e., non-neuronal cells that support neurons and the nervous system. The study of astrocytes has immense potential for understanding brain function. However, their complex and widely-branching structure requires high-resolution electron microscopy imaging and makes visualization and analysis challenging. Furthermore, the structure and function of astrocytes is very different from neurons, and therefore requires the development of new visualization and analysis tools. With Abstractocyte, biologists can explore the morphology of astrocytes using various visual abstraction levels, while simultaneously analyzing neighboring neurons and their connectivity. We define a novel, conceptual 2D abstraction space for jointly visualizing astrocytes and neurons. Neuroscientists can choose a specific joint visualization as a point in this space. Interactively moving this point allows them to smoothly transition between different abstraction levels in an intuitive manner. In contrast to simply switching between different visualizations, this preserves the visual context and correlations throughout the transition. Users can smoothly navigate from concrete, highly-detailed 3D views to simplified and abstracted 2D views. In addition to investigating astrocytes, neurons, and their relationships, we enable the interactive analysis of the distribution of glycogen, which is of high importance to neuroscientists. We describe the design of Abstractocyte, and present three case studies in which neuroscientists have successfully used our system to assess astrocytic coverage of synapses, glycogen distribution in relation to synapses, and astrocytic-mitochondria coverage.
C1 [Mohammed, Haneen; Al-Awami, Ali K.; Cali, Corrado; Magistretti, Pierre; Hadwiger, Markus] KAUST, Thuwal 239556900, Saudi Arabia.
   [Beyer, Johanna; Pfister, Hanspeter] Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
C3 King Abdullah University of Science & Technology; Harvard University
RP Mohammed, H (corresponding author), KAUST, Thuwal 239556900, Saudi Arabia.
EM haneen.mohammed@kaust.edu.sa; ali.awami@kaust.edu.sa;
   jbeyer@seas.harvard.edu; corrado.cali@kaust.edu.sa;
   pierre.magistretti@kaust.edu.sa; pfister@seas.harvard.edu;
   markus.hadwiger@kaust.edu.sa
RI Calì, Corrado/AAS-9183-2021; Magistretti, Pierre/ABV-1846-2022
OI Beyer, Johanna/0000-0002-3505-9171; Cali, Corrado/0000-0003-4856-0835;
   Hadwiger, Markus/0000-0003-1239-4871; Pfister,
   Hanspeter/0000-0002-3620-2582; Al-Awami, Ali/0000-0002-8725-1958;
   Magistretti, Pierre Julius/0000-0002-6678-320X; Mohammed,
   Haneen/0000-0002-4535-1926
FU King Abdullah University of Science and Technology (KAUST); KAUST
   [OSR-2015-CCF-2533-01]; Div Of Information & Intelligent Systems; Direct
   For Computer & Info Scie & Enginr [1607800, 1447344] Funding Source:
   National Science Foundation
FX This work was supported by funding from King Abdullah University of
   Science and Technology (KAUST) and KAUST award OSR-2015-CCF-2533-01.
NR 39
TC 36
Z9 42
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 853
EP 861
DI 10.1109/TVCG.2017.2744278
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400084
PM 28866534
DA 2025-03-07
ER

PT J
AU Poco, J
   Mayhua, A
   Heer, J
AF Poco, Jorge
   Mayhua, Angela
   Heer, Jeffrey
TI Extracting and Retargeting Color Mappings from Bitmap Images of
   Visualizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Visualization; color; chart understanding; information extraction;
   redesign; computer vision
AB Visualization designers regularly use color to encode quantitative or categorical data. However, visualizations "in the wild" often violate perceptual color design principles and may only be available as bitmap images. In this work, we contribute a method to semi-automatically extract color encodings from a bitmap visualization image. Given an image and a legend location, we classify the legend as describing either a discrete or continuous color encoding, identify the colors used, and extract legend text using OCR methods. We then combine this information to recover the specific color mapping. Users can also correct interpretation errors using an annotation interface. We evaluate our techniques using a corpus of images extracted from scientific papers and demonstrate accurate automatic inference of color mappings across a variety of chart types. In addition, we present two applications of our method: automatic recoloring to improve perceptual effectiveness, and interactive overlays to enable improved reading of static visualizations.
C1 [Poco, Jorge; Heer, Jeffrey] Univ Washington, Seattle, WA 98195 USA.
   [Poco, Jorge; Mayhua, Angela] Univ Catolica San Pablo, Arequipa, Peru.
C3 University of Washington; University of Washington Seattle; Universidad
   Catolica San Pablo
RP Poco, J (corresponding author), Univ Washington, Seattle, WA 98195 USA.; Poco, J (corresponding author), Univ Catolica San Pablo, Arequipa, Peru.
EM jpocom@ucsp.edu; angela.mayhua@ucsp.edu.pe; jheer@uw.edu
RI Quispe, Angela/AAQ-1143-2020; Poco, Jorge/F-3344-2016
OI Poco, Jorge/0000-0001-9096-6287
FU Paul G. Allen Emily Foundation Distinguished Investigator Award; Moore
   Foundation Data-Driven Discovery Investigator program; CONCYTEC
FX This work was supported by a Paul G. Allen Emily Foundation
   Distinguished Investigator Award and the Moore Foundation Data-Driven
   Discovery Investigator program. The second author gratefully
   acknowledges CONCYTEC for a scholarship in support of graduate studies.
NR 27
TC 51
Z9 54
U1 0
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 637
EP 646
DI 10.1109/TVCG.2017.2744320
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400063
PM 28866538
DA 2025-03-07
ER

PT J
AU Vuillemot, R
   Boy, J
AF Vuillemot, Romain
   Boy, Jeremy
TI Structuring Visualization Mock-ups at the Graphical Level by Dividing
   the Display Space
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Design Methodologies; Rapid Prototyping; Graphic Design; Mock-Ups;
   Toolkit Design
ID DESIGN; ELEMENTS; MODEL
AB Mock-ups are rapid, low fidelity prototypes, that are used in many design-related fields to generate and share ideas. While their creation is supported by many mature methods and tools, surprisingly few are suited for the needs of information visualization. In this article, we introduce a novel approach to creating visualizations mock-ups, based on a dialogue between graphic design and parametric toolkit explorations. Our approach consists in iteratively subdividing the display space, while progressively informing each division with realistic data. We show that a wealth of mock-ups can easily be created using only temporary data attributes, as we wait for more realistic data to become available. We describe the implementation of this approach in a D3-based toolkit, which we use to highlight its generative power, and we discuss the potential for transitioning towards higher fidelity prototypes.
C1 [Vuillemot, Romain] Unit Lyon, Ecole Cent Lyon, CNRS, UMR5205,LIRIS, F-69134 Lyon, France.
   [Boy, Jeremy] UN Global Pulse, New York, NY 10017 USA.
C3 Centre National de la Recherche Scientifique (CNRS); Institut National
   des Sciences Appliquees de Lyon - INSA Lyon; Ecole Centrale de Lyon
RP Vuillemot, R (corresponding author), Unit Lyon, Ecole Cent Lyon, CNRS, UMR5205,LIRIS, F-69134 Lyon, France.
EM romain@vuillemot.net; myjyby@gmail.com
RI Vuillemot, Romain/ABE-5719-2020
NR 61
TC 7
Z9 7
U1 0
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 424
EP 434
DI 10.1109/TVCG.2017.2743998
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400043
PM 28866513
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Sakhaee, E
   Entezari, A
AF Sakhaee, Elham
   Entezari, Alireza
TI A Statistical Direct Volume Rendering Framework for Visualization of
   Uncertain Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Uncertainty visualization; direct volume rendering; interpolation of
   distributions; ray casting; transfer function classification; data
   reduction; spline
ID NONPARAMETRIC MODELS; DENSITY-FUNCTION; RECONSTRUCTION; INTERPOLATION;
   SURFACES; LATTICE; ERRORS
AB With uncertainty present in almost all modalities of data acquisition, reduction, transformation, and representation, there is a growing demand for mathematical analysis of uncertainty propagation in data processing pipelines. In this paper, we present a statistical framework for quantification of uncertainty and its propagation in the main stages of the visualization pipeline. We propose a novel generalization of Irwin-Hall distributions from the statistical viewpoint of splines and box-splines, that enables interpolation of random variables. Moreover, we introduce a probabilistic transfer function classification model that allows for incorporating probability density functions into the volume rendering integral. Our statistical framework allows for incorporating distributions from various sources of uncertainty which makes it suitable in a wide range of visualization applications. We demonstrate effectiveness of our approach in visualization of ensemble data, visualizing large datasets at reduced scale, iso-surface extraction, and visualization of noisy data.
C1 [Sakhaee, Elham; Entezari, Alireza] Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32611 USA.
C3 State University System of Florida; University of Florida
RP Sakhaee, E (corresponding author), Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32611 USA.
EM e.sakhaee@ufl.edu; entezari@ufl.edu
FU ONR grant [N00014-14-1-0762]; US National Science Foundation
   [IIS-1617101]; UFII Graduate Fellowship
FX We would like to thank Voreen team that have provided their
   visualization software publicly. We also would like to thank the
   reviewers for their recommendations on improvements on this paper. This
   research was supported in part by ONR grant N00014-14-1-0762, the US
   National Science Foundation grant IIS-1617101, and UFII Graduate
   Fellowship.
NR 45
TC 20
Z9 25
U1 0
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2017
VL 23
IS 12
BP 2509
EP 2520
DI 10.1109/TVCG.2016.2637333
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6YZ
UT WOS:000414393700003
PM 27959812
OA hybrid
DA 2025-03-07
ER

PT J
AU Siegl, C
   Lange, V
   Stamminger, M
   Bauer, F
   Thies, J
AF Siegl, Christian
   Lange, Vanessa
   Stamminger, Marc
   Bauer, Frank
   Thies, Justus
TI FaceForge: Markerless Non-Rigid Face Multi-Projection Mapping
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY SEP 09-13, 2017
CL Nantes, FRANCE
SP IEEE
DE Face Projection; Mixed Reality; Multi-Projection Mapping; Non-Rigid Face
   Tracking
AB Recent publications and art performances demonstrate amazing results using projection mapping. To our knowledge, there exists no multi-projection system that can project onto non-rigid target geometries. This constrains the applicability and quality for live performances with multiple spectators. Given the cost and complexity of current systems, we present a low-cost easy-to-use markerless non-rigid face multi-projection system. It is based on a non-rigid, dense face tracker and a real-time multi-projection solver adapted to imprecise tracking, geometry and calibration. Using this novel system we produce compelling results with only consumer-grade hardware.
C1 [Siegl, Christian; Lange, Vanessa; Stamminger, Marc; Bauer, Frank; Thies, Justus] Univ Erlangen Nurnberg, Comp Graph Grp, Erlangen, Germany.
C3 University of Erlangen Nuremberg
RP Siegl, C (corresponding author), Univ Erlangen Nurnberg, Comp Graph Grp, Erlangen, Germany.
EM Christian.Siegl@fau.de; Vanessa.Lange@fau.de; Marc.Stamminger@fau.de;
   Frank.Bauer@fau.de; Justus.Thies@fau.de
OI Klein, Vanessa/0000-0002-8974-8933
FU German Research Foundation (DFG) [GRK-1773]; Heterogeneous Image Systems
FX We would like to thank Chen Cao and Kun Zhou for the blendshape models,
   as well as Volker Blanz, Thomas Vetter, and Oleg Alexander for the
   provided face data. The facial landmark tracker was kindly provided by
   True Vision Solution. This research is partially funded by the German
   Research Foundation (DFG), grant GRK-1773 Heterogeneous Image Systems.
NR 26
TC 21
Z9 24
U1 1
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2017
VL 23
IS 11
BP 2440
EP 2446
DI 10.1109/TVCG.2017.2734428
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FI7XT
UT WOS:000412214000010
PM 28809691
DA 2025-03-07
ER

PT J
AU Tan, DJ
   Navab, N
   Tombari, F
AF Tan, David Joseph
   Navab, Nassir
   Tombari, Federico
TI Looking Beyond the Simple Scenarios: Combining Learners and Optimizers
   in 3D Temporal Tracking
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY SEP 09-13, 2017
CL Nantes, FRANCE
SP IEEE
DE 3DTracking; Random Forest; 6D Pose Estimation
ID OBJECT TRACKING; REGISTRATION
AB 3D object temporal trackers estimate the 3D rotation and 3D translation of a rigid object by propagating the transformation from one frame to the next. To confront this task, algorithms either learn the transformation between two consecutive frames or optimize an energy function to align the object to the scene. The motivation behind our approach stems from a consideration on the nature of learners and optimizers. Throughout the evaluation of different types of objects and working conditions, we observe their complementary nature - on one hand, learners are more robust when undergoing challenging scenarios, while optimizers are prone to tracking failures due to the entrapment at local minima; on the other, optimizers can converge to a better accuracy and minimize jitter. Therefore, we propose to bridge the gap between learners and optimizers to attain a robust and accurate RGB-D temporal tracker that runs at approximately 2 ms per frame using one CPU core. Our work is highly suitable for Augmented Reality (AR), Mixed Reality (MR) and Virtual Reality (VR) applications due to its robustness, accuracy, efficiency and low latency. Aiming at stepping beyond the simple scenarios used by current systems, often constrained by having a single object in the absence of clutter, averting to touch the object to prevent close-range partial occlusion or selecting brightly colored objects to easily segment them individually, we demonstrate the capacity to handle challenging cases under clutter, partial occlusion and varying lighting conditions.
C1 [Tan, David Joseph; Navab, Nassir; Tombari, Federico] Tech Univ Munich, Munich, Germany.
C3 Technical University of Munich
RP Tan, DJ (corresponding author), Tech Univ Munich, Munich, Germany.
EM tanda@in.tum.de; navab@cs.tum.edu; tombari@in.tum.de
RI Tan, David/HIR-2214-2022
NR 30
TC 18
Z9 20
U1 0
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2017
VL 23
IS 11
BP 2399
EP 2409
DI 10.1109/TVCG.2017.2734539
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FI7XT
UT WOS:000412214000006
PM 28809695
DA 2025-03-07
ER

PT J
AU Baldacci, A
   Ganovelli, F
   Corsini, M
   Scopigno, R
AF Baldacci, Andrea
   Ganovelli, Fabio
   Corsini, Massimiliano
   Scopigno, Roberto
TI Presentation of 3D Scenes Through Video Example
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Multimedia content production; video similarity; 2D vector field
   comparison; computer animation
AB Using synthetic videos to present a 3D scene is a common requirement for architects, designers, engineers or Cultural Heritage professionals however it is usually time consuming and, in order to obtain high quality results, the support of a film maker/computer animation expert is necessary. We introduce an alternative approach that takes the 3D scene of interest and an example video as input, and automatically produces a video of the input scene that resembles the given video example. In other words, our algorithm allows the user to "replicate" an existing video, on a different 3D scene. We build on the intuition that a video sequence of a static environment is strongly characterized by its optical flow, or, in other words, that two videos are similar if their optical flows are similar. We therefore recast the problem as producing a video of the input scene whose optical flow is similar to the optical flow of the input video. Our intuition is supported by a user-study specifically designed to verify this statement. We have successfully tested our approach on several scenes and input videos, some of which are reported in the accompanying material of this paper.
C1 [Baldacci, Andrea; Ganovelli, Fabio; Corsini, Massimiliano; Scopigno, Roberto] Ist Sci & Tecnol Informaz Alessandro Faedo, I-56127 Pisa, Italy.
C3 Consiglio Nazionale delle Ricerche (CNR); Istituto di Scienza e
   Tecnologie dell'Informazione "Alessandro Faedo" (ISTI-CNR)
RP Baldacci, A (corresponding author), Ist Sci & Tecnol Informaz Alessandro Faedo, I-56127 Pisa, Italy.
EM andrea.baldacci@isti.cnr.it; fabio.ganovelli@gmail.com;
   massimiliano.corsini@isti.cnr.it; roberto.scopigno@isti.cnr.it
RI scopigno, roberto/AAH-7645-2020; Corsini, Massimiliano/B-6375-2015
OI Corsini, Massimiliano/0000-0003-0543-1638
FU EU INFRA Project Ariadne [313193]
FX The research leading to these results was partially funded by EU INFRA
   Project Ariadne (GA n. 313193).
NR 32
TC 2
Z9 2
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2017
VL 23
IS 9
BP 2096
EP 2107
DI 10.1109/TVCG.2016.2608828
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FC7XA
UT WOS:000407054600005
PM 28113668
DA 2025-03-07
ER

PT J
AU Wang, J
   Xu, K
AF Wang, Jun
   Xu, Kai
TI Shape Detection from Raw LiDAR Data with Subspace Modeling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Urban building; raw LiDAR scan; modeling; reconstruction; substructure
   modeling
ID ROBUST; CLOUDS
AB LiDAR scanning has become a prevalent technique for digitalizing large-scale outdoor scenes. However, the raw LiDAR data often contain imperfections, e.g., missing large regions, anisotropy of sampling density, and contamination of noise and outliers, which are the major obstacles that hinder its more ambitious and higher level applications in digital city modeling. Observing that 3D urban scenes can be locally described with several low dimensional subspaces, we propose to locally classify the neighborhoods of the scans to model the substructures of the scenes. The key enabler is the adaptive kernel-scale scoring, filtering and clustering of substructures, making it possible to recover the local structures at all points simultaneously, even in the presence of severe data imperfections. Integrating the local analyses leads to robust shape detection from raw LiDAR data. On this basis, we develop several urban scene applications and verify them on a number of LiDAR scans with various complexities and styles, which demonstrates the effectiveness and robustness of our methods.
C1 [Wang, Jun] Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing 210016, Jiangsu, Peoples R China.
   [Xu, Kai] Natl Univ Def Technol, Sch Comp Sci, Changsha 410073, Hunan, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; National University of
   Defense Technology - China
RP Wang, J (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Mech & Elect Engn, Nanjing 210016, Jiangsu, Peoples R China.
EM junwang@outlook.com; kevin.kai.xu@gmail.com
RI Wang, Jun/AAM-6868-2021
OI Wang, Jun/0000-0001-9223-2615
FU Fundamental Research Funds for the Central Universities [NE2014402,
   NE2016004]; Natural Science Foundation of Jiangsu Province [BK2014833];
   Jiangsu Specially-Appointed Professorship; National Natural Science
   Foundation of China [61402224, 61572507, 61532003, 61622212]
FX We thank the reviewers for their comments and suggestions for improving
   the paper. Special thanks to Florent Lafarge, Aron Monszpart for their
   source code and datasets. This work was supported in part by the
   Fundamental Research Funds for the Central Universities (NE2014402,
   NE2016004), the Natural Science Foundation of Jiangsu Province (No.
   BK2014833), the Jiangsu Specially-Appointed Professorship, and the
   National Natural Science Foundation of China (No. 61402224, 61572507,
   61532003, 61622212).
NR 41
TC 29
Z9 31
U1 0
U2 31
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2017
VL 23
IS 9
BP 2137
EP 2150
DI 10.1109/TVCG.2016.2601915
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FC7XA
UT WOS:000407054600008
PM 28113588
DA 2025-03-07
ER

PT J
AU Kol, TR
   Klehm, O
   Seidel, HP
   Eisemann, E
AF Kol, Timothy R.
   Klehm, Oliver
   Seidel, Hans-Peter
   Eisemann, Elmar
TI Expressive Single Scattering for Light Shaft Stylization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Interactive stylization; artist control; single scattering
ID ARTISTIC CONTROL
AB Light scattering in participating media is a natural phenomenon that is increasingly featured in movies and games, as it is visually pleasing and lends realism to a scene. In art, it may further be used to express a certain mood or emphasize objects. Here, artists often rely on stylization when creating scattering effects, not only because of the complexity of physically correct scattering, but also to increase expressiveness. Little research, however, focuses on artistically influencing the simulation of the scattering process in a virtual 3D scene. We propose novel stylization techniques, enabling artists to change the appearance of single scattering effects such as light shafts. Users can add, remove, or enhance light shafts using occluder manipulation. The colors of the light shafts can be stylized and animated using easily modifiable transfer functions. Alternatively, our system can optimize a light map given a simple user input for a number of desired views in the 3D world. Finally, we enable artists to control the heterogeneity of the underlying medium. Our stylized scattering solution is easy to use and compatible with standard rendering pipelines. It works for animated scenes and can be executed in real time to provide the artist with quick feedback.
C1 [Kol, Timothy R.; Eisemann, Elmar] Delft Univ Technol, Dept Intelligent Syst, Comp Graph & Visualizat Grp, Delft, Zuid Holland, Netherlands.
   [Klehm, Oliver; Seidel, Hans-Peter] MPI Informat, Dept Comp Graph, Saarbrucken, Saarland, Germany.
C3 Delft University of Technology; Max Planck Society
RP Kol, TR (corresponding author), Delft Univ Technol, Dept Intelligent Syst, Comp Graph & Visualizat Grp, Delft, Zuid Holland, Netherlands.
EM t.r.kol@tudelft.nl; oklehm@mpi-inf.mpg.de; hpseidel@mpi-inf.mpg.de;
   e.eisemann@tudelft.nl
FU Intel Visual Computing Institute at Saarland University
FX This work was partly supported by the Intel Visual Computing Institute
   at Saarland University.
NR 37
TC 2
Z9 2
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2017
VL 23
IS 7
BP 1753
EP 1766
DI 10.1109/TVCG.2016.2554114
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EW7OV
UT WOS:000402705000003
PM 27101611
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Bach, B
   Riche, NH
   Hurter, C
   Marriott, K
   Dwyer, T
AF Bach, Benjamin
   Riche, Nathalie Henry
   Hurter, Christophe
   Marriott, Kim
   Dwyer, Tim
TI Towards Unambiguous Edge Bundling: Investigating Confluent Drawings for
   Network Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Network visualization; edge compression; confluent; power graph;
   bundling
ID CLUSTER-DETECTION; GRAPH; READABILITY
AB In this paper, we investigate Confluent Drawings (CD), a technique for bundling edges in node-link diagrams based on network connectivity. Edge-bundling techniques are designed to reduce edge clutter in node-link diagrams by coalescing lines into common paths or bundles. Unfortunately, traditional bundling techniques introduce ambiguity since edges are only bundled by spatial proximity, rather than network connectivity; following an edge from its source to its target can lead to the perception of incorrect connectivity if edges are not clearly separated within the bundles. Contrary, CDs bundle edges based on common sources or targets. Thus, a smooth path along a confluent bundle indicates precise connectivity. While CDs have been described in theory, practical investigation and application to real-world networks (i.e., networks beyond those with certain planarity restrictions) is currently lacking. Here, we provide the first algorithm for constructing CDs from arbitrary directed and undirected networks and present a simple layout method, embedded in a sand box environment providing techniques for interactive exploration. We then investigate patterns and artifacts in CDs, which we compare to other common edge-bundling techniques. Finally, we present the first user study that compares edge-compression techniques, including CD, power graphs, metro-style, and common edge bundling. We found that users without particular expertise in visualization or network analysis are able to read small CDs without difficulty. Compared to existing bundling techniques, CDs are more likely to allow people to correctly perceive connectivity.
C1 [Bach, Benjamin] Microsoft Research Inria Joint Ctr, Paris, France.
   [Riche, Nathalie Henry] Microsoft Res, Redmond, WA USA.
   [Hurter, Christophe] ENAC, Toulouse, France.
   [Marriott, Kim] Monash Univ, Melbourne, Vic, Australia.
   [Dwyer, Tim] Monash Univ, Melbourne, Vic, Australia.
C3 Microsoft; Universite Federale Toulouse Midi-Pyrenees (ComUE);
   Universite de Toulouse; Ecole Nationale de l'Aviation Civile (ENAC);
   Monash University; Monash University
RP Bach, B (corresponding author), Microsoft Research Inria Joint Ctr, Paris, France.
EM benj.bach@gmail.com; nath@microsoft.com; christophe.hurter@enac.fr;
   kim.marriott@monash.edu; tim.dwyer@monash.edu
RI Hurter, Christophe/AHB-0811-2022
OI Dwyer, Tim/0000-0002-9076-9571; Hurter, Christophe/0000-0003-4318-6717
NR 44
TC 38
Z9 40
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 541
EP 550
DI 10.1109/TVCG.2016.2598958
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600057
PM 27875170
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Feng, M
   Deng, C
   Peck, EM
   Harrison, L
AF Feng, Mi
   Deng, Cheng
   Peck, Evan M.
   Harrison, Lane
TI HindSight: Encouraging Exploration through Direct Encoding of Personal
   Interaction History
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Visualization; Interaction; History
AB Physical and digital objects often leave markers of our use. Website links turn purple after we visit them, for example, showing us information we have yet to explore. These "footprints" of interaction offer substantial benefits in information saturated environments - they enable us to easily revisit old information, systematically explore new information, and quickly resume tasks after interruption. While applying these design principles have been successful in HCI contexts, direct encodings of personal interaction history have received scarce attention in data visualization. One reason is that there is little guidance for integrating history into visualizations where many visual channels are already occupied by data. More importantly, there is not firm evidence that making users aware of their interaction history results in benefits with regards to exploration or insights. Following these observations, we propose HindSight - an umbrella term for the design space of representing interaction history directly in existing data visualizations. In this paper, we examine the value of HindSight principles by augmenting existing visualizations with visual indicators of user interaction history (e. g. How the Recession Shaped the Economy in 255 Charts, NYTimes). In controlled experiments of over 400 participants, we found that HindSight designs generally encouraged people to visit more data and recall different insights after interaction. The results of our experiments suggest that simple additions to visualizations can make users aware of their interaction history, and that these additions significantly impact users' exploration and insights.
C1 [Feng, Mi; Deng, Cheng; Harrison, Lane] Worcester Polytech Inst, Worcester, MA 01609 USA.
   [Peck, Evan M.] Bucknell Univ, Lewisburg, PA 17837 USA.
C3 Worcester Polytechnic Institute; Bucknell University
RP Feng, M (corresponding author), Worcester Polytech Inst, Worcester, MA 01609 USA.
EM mfeng2@wpi.edu; cdeng@wpi.edu; evan.peck@bucknell.edu; lane@cs.wpi.edu
RI chen, long/AAV-6276-2021
OI Harrison, Lane/0000-0003-3029-2799
NR 36
TC 26
Z9 29
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 351
EP 360
DI 10.1109/TVCG.2016.2599058
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600038
PM 27875151
DA 2025-03-07
ER

PT J
AU Goodwin, S
   Mears, C
   Dwyer, T
   de la Banda, MG
   Tack, G
   Wallace, M
AF Goodwin, Sarah
   Mears, Christopher
   Dwyer, Tim
   de la Banda, Maria Garcia
   Tack, Guido
   Wallace, Mark
TI What do Constraint Programming Users Want to See? Exploring the role of
   Visualisation in Profiling of Models and Search
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE visual analytics; user-centred design; profiling; constraint
   programming; tree visualisations
ID TREE VISUALIZATION; REQUIREMENTS; TIME; CREATIVITY; PATTERNS; SYSTEM
AB Constraint programming allows difficult combinatorial problems to be modelled declaratively and solved automatically. Advances in solver technologies over recent years have allowed the successful use of constraint programming in many application areas. However, when a particular solver's search for a solution takes too long, the complexity of the constraint program execution hinders the programmer's ability to profile that search and understand how it relates to their model. Therefore, effective tools to support such profiling and allow users of constraint programming technologies to refine their model or experiment with different search parameters are essential. This paper details the first user-centred design process for visual profiling tools in this domain. We report on: our insights and opportunities identified through an on-line questionnaire and a creativity workshop with domain experts carried out to elicit requirements for analytical and visual profiling techniques; our designs and functional prototypes realising such techniques; and case studies demonstrating how these techniques shed light on the behaviour of the solvers in practice.
C1 [Goodwin, Sarah; Dwyer, Tim] Monash Univ, Adapt Visualisat Lab, Clayton, Vic 3800, Australia.
   [Mears, Christopher; de la Banda, Maria Garcia; Tack, Guido; Wallace, Mark] Monash Univ, Fac Informat Technol, Clayton, Vic 3800, Australia.
C3 Monash University; Monash University
RP Goodwin, S (corresponding author), Monash Univ, Adapt Visualisat Lab, Clayton, Vic 3800, Australia.
EM sarah.goodwin@monash.edu; chris.mears@monash.edu; tim.dwyer@monash.edu;
   mariagarciadelabanda@monash.edu; guido.tack@monash.edu;
   mark.wallace@monash.edu
OI Wallace, Mark/0000-0001-7326-8110; Dwyer, Tim/0000-0002-9076-9571;
   Garcia de la Banda, Maria/0000-0002-6666-514X; Goodwin,
   Sarah/0000-0001-8894-8282
FU Australian Research Council [DP140100058]
FX This research was sponsored by the Australian Research Council grant
   DP140100058. We thank Maxim Shishmarev for his profiling software
   architecture, the questionnaire and workshop participants, Bart Demoen
   for models of the addition chain problem, as well as Sara Jones, Ethan
   Kerzner, Jason Dykes, Miriah Meyer and Graham Dove for their reflection
   and feedback on the workshop structure.
NR 38
TC 18
Z9 22
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 281
EP 290
DI 10.1109/TVCG.2016.2598545
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600031
PM 27875144
DA 2025-03-07
ER

PT J
AU Takeda, S
   Iwai, D
   Sato, K
AF Takeda, Shoichi
   Iwai, Daisuke
   Sato, Kosuke
TI Inter-reflection Compensation of Immersive Projection Display by
   Spatio-Temporal Screen Reflectance Modulation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE Virtual Reality Conference (IEEE VR)
CY MAR 19-23, 2016
CL Greenville, SC
SP IEEE, IEEE Comp Soc, IEEE Comp Soc Visualizat & Graph Tech Comm, Clemson Univ
DE Reverse radiosity; inter-reflection compensation; immersive projection
   display
AB We propose a novel inter-reflection compensation technique for immersive projection displays wherein we spatially modulate the reflectance pattern on the screen to improve the compensation performance of conventional methods. As the luminance of light reflected on a projection surface is mathematically represented as the multiplication of the illuminance of incident light and the surface reflectance, we can reduce undesirable intensity elevation because of inter -reflections by decreasing surface reflectance. Based on this principle, we improve conventional inter -reflection compensation techniques by applying reflectance pattern modulation. We realize spatial reflectance modulation of a projection screen by painting it with a photochromic compound, which changes its color (i.e.. the reflectance of the screen) when ultraviolet (UV) light is applied and by controlling UV irradiation with a UV LED array placed behind the screen. The main contribution of this paper is a computational model to optimize a reflectance pattern for the accurate reproduction of a target appearance by decreasing the intensity elevation caused by inter -reflection while maintaining the maximum intensity of the target appearance. Through simulation and physical experiments, we demonstrate the feasibility of the proposed model and confirm its advantage over conventional methods.
C1 [Takeda, Shoichi; Iwai, Daisuke; Sato, Kosuke] Osaka Univ, Suita, Osaka 565, Japan.
C3 Osaka University
RP Takeda, S; Iwai, D; Sato, K (corresponding author), Osaka Univ, Suita, Osaka 565, Japan.
EM shoichi.takeda@sens.sys.es.osaka-u.ac.jp;
   daisuke.iwai@sys.es.osaka-u.ac.jp; sato@sys.es.osaka-u.ac.jp
RI Iwai, Daisuke/R-8174-2019
OI Iwai, Daisuke/0000-0002-3493-5635
FU JSPS KAKENHI [15H05925]; Grants-in-Aid for Scientific Research
   [15H05925] Funding Source: KAKEN
FX This work was supported by JSPS KAKENHI Grant Number 15H05925
   (Grant-in-Aid for Scientific Research on Innovative Areas Innovative
   SHITSUKAN Science and Technology).
NR 31
TC 12
Z9 14
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2016
VL 22
IS 4
BP 1424
EP 1431
DI 10.1109/TVCG.2016.2518136
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DH5RN
UT WOS:000372849600012
PM 26780805
DA 2025-03-07
ER

PT J
AU Hu, WC
   Chen, ZG
   Pan, H
   Yu, YZ
   Grinspun, E
   Wang, WP
AF Hu, Wenchao
   Chen, Zhonggui
   Pan, Hao
   Yu, Yizhou
   Grinspun, Eitan
   Wang, Wenping
TI Surface Mosaic Synthesis with Irregular Tiles
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Simulated mosaics; irregular packing; polygon containment; surface
   tessellation
ID POLYGON CONTAINMENT; OBJECTS
AB Mosaics are widely used for surface decoration to produce appealing visual effects. We present a method for synthesizing digital surface mosaics with irregularly shaped tiles, which are a type of tiles often used for mosaics design. Our method employs both continuous optimization and combinatorial optimization to improve tile arrangement. In the continuous optimization step, we iteratively partition the base surface into approximate Voronoi regions of the tiles and optimize the positions and orientations of the tiles to achieve a tight fit. Combination optimization performs tile permutation and replacement to further increase surface coverage and diversify tile selection. The alternative applications of these two optimization steps lead to rich combination of tiles and high surface coverage. We demonstrate the effectiveness of our solution with extensive experiments and comparisons.
C1 [Hu, Wenchao; Pan, Hao; Yu, Yizhou; Wang, Wenping] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Chen, Zhonggui] Xiamen Univ, Dept Comp Sci, Xiamen 361005, Peoples R China.
   [Grinspun, Eitan] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
C3 University of Hong Kong; Xiamen University; Columbia University
RP Chen, ZG (corresponding author), Xiamen Univ, Dept Comp Sci, Xiamen 361005, Peoples R China.
EM wchu@cs.hku.hk; chenzhonggui@xmu.edu.cn; hpan@cs.hku.hk; yzyu@cs.hku.hk;
   eitan@cs.columbia.edu; wenping@cs.hku.hk
RI Hu, Wenchao/LSM-1924-2024
OI Chen, Zhonggui/0000-0002-9960-4896
FU National Natural Science Foundation of China [61472332]; Fundamental
   Research Funds for the Central Universities [20720140520]; GRF project
   of Hong Kong Research Grant Council [17208214]; NSFC [61272019,
   61572021, 61332015]; Shenzhen Science and Technology project
   [JCYJ20140903112959962]
FX Zhonggui Chen's work is supported partially by National Natural Science
   Foundation of China (61472332), and the Fundamental Research Funds for
   the Central Universities (20720140520). Wenping Wang's research is
   supported partially by the GRF project (17208214) of Hong Kong Research
   Grant Council, NSFC projects (61272019, 61572021 and 61332015) and
   Shenzhen Science and Technology project (JCYJ20140903112959962).
   Zhonggui Chen is the corresponding author.
NR 40
TC 16
Z9 20
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2016
VL 22
IS 3
BP 1302
EP 1313
DI 10.1109/TVCG.2015.2498620
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE2DC
UT WOS:000370435700011
PM 26561463
DA 2025-03-07
ER

PT J
AU Al-Awami, AK
   Beyer, J
   Haehn, D
   Kasthuri, N
   Lichtman, JW
   Pfister, H
   Hadwiger, M
AF Al-Awami, Ali K.
   Beyer, Johanna
   Haehn, Daniel
   Kasthuri, Narayanan
   Lichtman, Jeff W.
   Pfister, Hanspeter
   Hadwiger, Markus
TI NeuroBlocks - Visual Tracking of Segmentation and Proofreading for Large
   Connectomics Projects
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Neuroscience; Segmentation; Proofreading; Data and Provenance Tracking
ID VISUALIZATION; ANNOTATION; PROVENANCE
AB In the field of connectomics, neuroscientists acquire electron microscopy volumes at nanometer resolution in order to reconstruct a detailed wiring diagram of the neurons in the brain. The resulting image volumes, which often are hundreds of terabytes in size, need to be segmented to identify cell boundaries, synapses, and important cell organelles. However, the segmentation process of a single volume is very complex, time-intensive, and usually performed using a diverse set of tools and many users. To tackle the associated challenges, this paper presents NeuroBlocks, which is a novel visualization system for tracking the state, progress, and evolution of very large volumetric segmentation data in neuroscience. NeuroBlocks is a multi-user web-based application that seamlessly integrates the diverse set of tools that neuroscientists currently use for manual and semi-automatic segmentation, proofreading, visualization, and analysis. NeuroBlocks is the first system that integrates this heterogeneous tool set, providing crucial support for the management, provenance, accountability, and auditing of large-scale segmentations. We describe the design of NeuroBlocks, starting with an analysis of the domain-specific tasks, their inherent challenges, and our subsequent task abstraction and visual representation. We demonstrate the utility of our design based on two case studies that focus on different user roles and their respective requirements for performing and tracking the progress of segmentation and proofreading in a large real-world connectomics project.
C1 [Al-Awami, Ali K.; Hadwiger, Markus] King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.
   [Beyer, Johanna; Haehn, Daniel; Pfister, Hanspeter] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
   [Kasthuri, Narayanan] Boston Univ, Sch Med, Boston, MA 02215 USA.
   [Lichtman, Jeff W.] Harvard Univ, Ctr Brain Sci, Cambridge, MA 02138 USA.
C3 King Abdullah University of Science & Technology; Harvard University;
   Boston University; Harvard University
RP Al-Awami, AK (corresponding author), King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.
EM ali.awami@kaust.edu.sa; jbeyer@seas.harvard.edu; haehn@seas.harvard.edu;
   bkasthur@bu.edu; jeff@mcb.harvard.edu; pfister@seas.harvard.edu;
   markus.hadwiger@kaust.edu.sa
RI Haehn, Daniel/V-6773-2019
OI Al-Awami, Ali/0000-0002-8725-1958; Haehn, Daniel/0000-0001-9144-3461;
   Hadwiger, Markus/0000-0003-1239-4871; Beyer,
   Johanna/0000-0002-3505-9171; Pfister, Hanspeter/0000-0002-3620-2582
FU King Abdullah University of Science and Technology; NSF [OIA-1125087]
FX This work was partially supported by King Abdullah University of Science
   and Technology and NSF grant OIA-1125087.
NR 40
TC 32
Z9 36
U1 0
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 738
EP 746
DI 10.1109/TVCG.2015.2467441
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400079
PM 26529725
DA 2025-03-07
ER

PT J
AU Athawale, T
   Sakhaee, E
   Entezari, A
AF Athawale, Tushar
   Sakhaee, Elham
   Entezari, Alireza
TI Isosurface Visualization of Data with Nonparametric Models for
   Uncertainty
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Uncertainty quantification; linear interpolation; isosurface extraction;
   marching cubes; nonparametric statistics
ID INTERPOLATION; CHALLENGES
AB The problem of isosurface extraction in uncertain data is an important research problem and may be approached in two ways. One can extract statistics (e.g., mean) from uncertain data points and visualize the extracted field. Alternatively, data uncertainty, characterized by probability distributions, can be propagated through the isosurface extraction process. We analyze the impact of data uncertainty on topology and geometry extraction algorithms. A novel, edge-crossing probability based approach is proposed to predict underlying isosurface topology for uncertain data. We derive a probabilistic version of the midpoint decider that resolves ambiguities that arise in identifying topological configurations. Moreover, the probability density function characterizing positional uncertainty in isosurfaces is derived analytically for a broad class of nonparametric distributions. This analytic characterization can be used for efficient closed-form computation of the expected value and variation in geometry. Our experiments show the computational advantages of our analytic approach over Monte-Carlo sampling for characterizing positional uncertainty. We also show the advantage of modeling underlying error densities in a nonparametric statistical framework as opposed to a parametric statistical framework through our experiments on ensemble datasets and uncertain scalar fields.
C1 [Athawale, Tushar; Sakhaee, Elham; Entezari, Alireza] Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32611 USA.
C3 State University System of Florida; University of Florida
RP Athawale, T (corresponding author), Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32611 USA.
EM athawale@cise.ufl.edu; esakhaee@cise.ufl.edu; entezari@cise.ufl.edu
RI Athawale, Tushar/W-4008-2019
FU AFOSR [FA9550-12-1-0304]; NSF [CCF-1018149]; Direct For Computer & Info
   Scie & Enginr; Div Of Information & Intelligent Systems [1617101]
   Funding Source: National Science Foundation; Division of Computing and
   Communication Foundations; Direct For Computer & Info Scie & Enginr
   [1018149] Funding Source: National Science Foundation
FX The temperature dataset is courtesy of DEMETER project [27]. The fuel
   dataset and the bonsai tree dataset are courtesy of volvis.org. We also
   acknowledge ijkmcube package and Meshlab, which we use for isosurface
   visualization. This research was supported in part by the AFOSR grant
   FA9550-12-1-0304 and NSF grant CCF-1018149.
NR 44
TC 42
Z9 53
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 777
EP 786
DI 10.1109/TVCG.2015.2467958
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400083
PM 26529727
DA 2025-03-07
ER

PT J
AU Kappe, CN
   Schütz, L
   Gunther, S
   Hufnagel, L
   Lemke, S
   Leitte, H
AF Kappe, Christopher N.
   Schuetz, Lucas
   Gunther, Stefan
   Hufnagel, Lars
   Lemke, Steffen
   Leitte, Heike
TI Reconstruction and Visualization of Coordinated 3D Cell Migration Based
   on Optical Flow
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Cell migration; vector field; 3D; time-dependent; LIC; tracking;
   validation
ID TRACKING; EMBRYOS
AB Animal development is marked by the repeated reorganization of cells and cell populations, which ultimately determine form and shape of the growing organism. One of the central questions in developmental biology is to understand precisely how cells reorganize, as well as how and to what extent this reorganization is coordinated. While modern microscopes can record video data for every cell during animal development in 3D+t, analyzing these videos remains a major challenge: reconstruction of comprehensive cell tracks turned out to be very demanding especially with decreasing data quality and increasing cell densities. In this paper, we present an analysis pipeline for coordinated cellular motions in developing embryos based on the optical flow of a series of 3D images. We use numerical integration to reconstruct cellular long-term motions in the optical flow of the video, we take care of data validation, and we derive a LIC-based, dense flow visualization for the resulting pathlines. This approach allows us to handle low video quality such as noisy data or poorly separated cells, and it allows the biologists to get a comprehensive understanding of their data by capturing dynamic growth processes in stills. We validate our methods using three videos of growing fruit fly embryos.
C1 [Kappe, Christopher N.; Leitte, Heike] Heidelberg Univ, IWR, D-69115 Heidelberg, Germany.
   [Gunther, Stefan; Hufnagel, Lars] EMBL, Heidelberg, Germany.
   [Schuetz, Lucas; Lemke, Steffen] Heidelberg Univ, COS, D-69115 Heidelberg, Germany.
C3 Ruprecht Karls University Heidelberg; European Molecular Biology
   Laboratory (EMBL); Ruprecht Karls University Heidelberg
RP Kappe, CN (corresponding author), Heidelberg Univ, IWR, Bergheimer Str 58, D-69115 Heidelberg, Germany.
EM christoper.kappe@iwr.uni-heidelberg.de;
   lucas.schuetz@cos.uni-heidelberg.de; guenther@embl.de; hufnagel@embl.de;
   steffen.lemke@cos.uni-heidelberg.de; heike.leitte@iwr.uni-heidelberg.de
RI Hufnagel, Levente/A-1773-2011; Günther, Stefan/A-3200-2009; Lemke,
   Steffen/A-2463-2017
OI Leitte, Heike/0000-0002-7112-2190; Gunther, Stefan/0000-0003-4080-5041;
   Schutz, Lucas/0000-0003-2514-226X
FU Heidelberg Graduate School of Mathematical and Computational Methods for
   the Sciences (HGS MathComp); Hartmut-Hoffman-Berling International
   Graduate School; DFG [LE 2787/1-1]; NVIDIA Corporation
FX This research was supported by a stipend of the Heidelberg Graduate
   School of Mathematical and Computational Methods for the Sciences (HGS
   MathComp). And also by a fellowship from the Hartmut-Hoffman-Berling
   International Graduate School (L.S.), DFG grant LE 2787/1-1 (S.L.), and
   NVIDIA Corporation (donation of a Tesla K40 GPU).
NR 56
TC 5
Z9 6
U1 0
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 995
EP 1004
DI 10.1109/TVCG.2015.2467291
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400105
PM 26529743
DA 2025-03-07
ER

PT J
AU Löwe, T
   Förster, EC
   Albuquerque, G
   Kreiss, JP
   Magnor, M
AF Loewe, Thomas
   Foerster, Emmy-Charlotte
   Albuquerque, Georgia
   Kreiss, Jens-Peter
   Magnor, Marcus
TI Visual Analytics for Development and Evaluation of Order Selection
   Criteria for Autoregressive Processes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Visual analytics; time series analysis; order selection
ID REGRESSION
AB Order selection of autoregressive processes is an active research topic in time series analysis, and the development and evaluation of automatic order selection criteria remains a challenging task for domain experts. We propose a visual analytics approach, to guide the analysis and development of such criteria. A flexible synthetic model generator combined with specialized responsive visualizations allows comprehensive interactive evaluation. Our fast framework allows feedback-driven development and fine-tuning of new order selection criteria in real-time. We demonstrate the applicability of our approach in three use-cases for two general as well as a real-world example.
C1 [Loewe, Thomas; Foerster, Emmy-Charlotte; Albuquerque, Georgia; Magnor, Marcus] TU Braunschweig, Comp Graph Lab, Braunschweig, Germany.
   [Kreiss, Jens-Peter] TU Braunschweig, Inst Math Stochast, Braunschweig, Germany.
C3 Braunschweig University of Technology; Braunschweig University of
   Technology
RP Löwe, T (corresponding author), TU Braunschweig, Comp Graph Lab, Braunschweig, Germany.
EM loewe@cg.cs.tu-bs.de; foerster@cg.cs.tu-bs.de; georgia@cg.cs.tu-bs.de;
   j.kreiss@tu-bs.de; magnor@cg.cs.tu-bs.de
OI Magnor, Marcus/0000-0003-0579-480X; Kreiss,
   Jens-Peter/0000-0001-6446-4558
FU German Science Foundation [DFG MA2555/6-2]
FX The authors thank Matthias Ueberheide for valuable suggestions and
   discussions regarding implementation of the elementary symmetric
   function. The authors gratefully acknowledge funding by the German
   Science Foundation from project DFG MA2555/6-2 within the strategic
   research initiative on Scalable Visual Analytics.
NR 44
TC 6
Z9 6
U1 1
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 151
EP 159
DI 10.1109/TVCG.2015.2467612
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400020
PM 26529695
DA 2025-03-07
ER

PT J
AU Li, JT
   Liu, XQ
   Lu, GD
AF Li, Jituo
   Liu, Xinqi
   Lu, Guodong
TI Learning Pose Controllable Human Reconstruction With Dynamic Implicit
   Fields From a Single Image
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image reconstruction; Biological system modeling; Shape; Image color
   analysis; Task analysis; Avatars; Clothing; Clothed human
   reconstruction; dynamic implicit fields; neural implicit representation;
   single image; human avatar dataset
AB Recovering a user-special and controllable human model from a single RGB image is a nontrivial challenge. Existing methods usually generate static results with an image consistent subject's pose. Our work aspires to achieve pose-controllable human reconstruction from a single image by learning a dynamic (multi-pose) implicit field. We first construct a feature-embedded human model (FEHM) as a bridge to propagate image features to different pose spaces. Based on FEHM, we then encode three pose-decoupled features. Global image features represent user-specific shapes in images and replace widely used pixel-aligned ways to avoid unwanted shape-pose entanglement. Spatial color features propagate FEHM-embedded image cues into 3D pose space to provide spatial high-frequency guidance. Spatial geometry features improve reconstruction robustness by using the surface shape of the FEHM as the prior. Finally, new implicit functions are designed to predict the dynamic human implicit fields. For effective supervision, a realistic human avatar dataset, SimuSCAN, with 1000+ models is constructed using a low-cost hierarchical mesh registration method. Extensive experiments demonstrate that our method achieves the state-of-the-art reconstruction level.
C1 [Li, Jituo] Zhejiang Univ, Inst Design Engn, Sch Mech Engn, Hangzhou 310027, Peoples R China.
   Zhejiang Univ, Robot Inst, Hangzhou 310027, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Li, JT (corresponding author), Zhejiang Univ, Inst Design Engn, Sch Mech Engn, Hangzhou 310027, Peoples R China.
EM jituo_li@zju.edu.cn; liuxinqi@zju.edu.cn; lugd@zju.edu.cn
RI li, jituo/LMP-9875-2024; Gao, Zihao/KIL-4959-2024
OI Liu, Xinqi/0000-0002-9105-2417
FU National Key R&D Program of China [2022YFB3303103]; National Natural
   Science Foundation of China [52275276]; Pioneer and "Leading Goose" R&D
   Program of Zhejiang [2023C01067]; Research Funding of Zhejiang
   University Robotics Institute
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2022YFB3303103, in part by the National Natural Science
   Foundation of China under Grant 52275276, in part by the "Pioneer" and
   "Leading Goose" R&D Program of Zhejiang under Grant 2023C01067, and in
   part by the Research Funding of Zhejiang University Robotics Institute.
NR 54
TC 0
Z9 0
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2025
VL 31
IS 2
BP 1389
EP 1401
DI 10.1109/TVCG.2024.3363493
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA R6W2Y
UT WOS:001392823200017
PM 38324440
DA 2025-03-07
ER

PT J
AU Chen, XM
   Hu, ZZ
   Zhao, GX
   Li, HS
   Chung, V
   Quigley, A
AF Chen, Xiaoming
   Hu, Zeke Zexi
   Zhao, Guangxin
   Li, Haisheng
   Chung, Vera
   Quigley, Aaron
TI Video2Haptics: Converting Video Motion to Dynamic Haptic Feedback with
   Bio-Inspired Event Processing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Haptic interfaces; Cameras; Tracking; Visualization; Force; Estimation;
   Streaming media; Bio-inspired sensing; event camera; immersive video;
   haptics
ID EXPERIENCE; TRACKING; SYSTEM
AB In cinematic VR applications, haptic feedback can significantly enhance the sense of reality and immersion for users. The increasing availability of emerging haptic devices opens up possibilities for future cinematic VR applications that allow users to receive haptic feedback while they are watching videos. However, automatically rendering haptic cues from real-time video content, particularly from video motion, is a technically challenging task. In this article, we propose a novel framework called "Video2Haptics" that leverages the emerging bio-inspired event camera to capture event signals as a lightweight representation of video motion. We then propose efficient event-based visual processing methods to estimate force or intensity from video motion in the event domain, rather than the pixel domain. To demonstrate the application of Video2Haptics, we convert the estimated force or intensity to dynamic vibrotactile feedback on emerging haptic gloves, synchronized with the corresponding video motion. As a result, Video2Haptics allows users not only to view the video but also to perceive the video motion concurrently. Our experimental results show that the proposed event-based processing methods for force and intensity estimation are one to two orders of magnitude faster than conventional methods. Our user study results confirm that the proposed Video2Haptics framework can considerably enhance the users' video experience.
C1 [Chen, Xiaoming; Zhao, Guangxin; Li, Haisheng] Beijing Technol & Business Univ, Sch Comp & Artificial Intelligence, Beijing 102401, Peoples R China.
   [Hu, Zeke Zexi; Chung, Vera] Univ Sydney, Sch Comp Sci, Darlington, NSW 2008, Australia.
   [Quigley, Aaron] CSIRO, Data61, Eveleigh, NSW 2015, Australia.
C3 Beijing Technology & Business University; University of Sydney;
   Commonwealth Scientific & Industrial Research Organisation (CSIRO)
RP Li, HS (corresponding author), Beijing Technol & Business Univ, Sch Comp & Artificial Intelligence, Beijing 102401, Peoples R China.; Chung, V (corresponding author), Univ Sydney, Sch Comp Sci, Darlington, NSW 2008, Australia.
EM xiaoming.chen@btbu.edu.cn; zexi.hu@sydney.edu.au;
   2030702053@st.btbu.edu.cn; lihsh@btbu.edu.cn; vera.chung@sydney.edu.au;
   aaron.quigley@data61.csiro.au
RI Quigley, Aaron/JHS-5032-2023; LI, Haisheng/AAM-5232-2020; Hu,
   Zeke/Y-5032-2019
OI Chen, Xiaoming/0000-0002-7503-3021; Quigley, Aaron/0000-0002-5274-6889;
   LI, Haisheng/0000-0003-4861-0513; Hu, Zeke Zexi/0000-0003-4947-4832
FU National Natural Science Foundation of China [62177001, 62277001];
   Beijing Natural Science Foundation [4222003, L233026]
FX This work was supported in part by National Natural Science Foundation
   of China under Grant 62177001 and Grant 62277001, and Beijing Natural
   Science Foundation under Grant 4222003 and Grant L233026.
NR 80
TC 0
Z9 0
U1 10
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2024
VL 30
IS 12
BP 7717
EP 7735
DI 10.1109/TVCG.2024.3360468
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K8D1L
UT WOS:001346124800002
PM 38294913
DA 2025-03-07
ER

PT J
AU Feld, N
   Bimberg, P
   Weyers, B
   Zielasko, D
AF Feld, Nico
   Bimberg, Pauline
   Weyers, Benjamin
   Zielasko, Daniel
TI Simple and Efficient? Evaluation of Transitions for Task-Driven
   Cross-Reality Experiences
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Games; Visualization; Portals; Films; Virtual
   environments; User experience; Artificial; augmented; graphical user
   interfaces; user interfaces; virtual realities
ID QUESTIONNAIRES
AB The inquiry into the impact of diverse transitions between cross-reality environments on user experience remains a compelling research endeavor. Existing work often offers fragmented perspectives on various techniques or confines itself to a singular segment of the reality-virtuality spectrum, be it virtual reality or augmented reality. This study embarks on bridging this knowledge gap by systematically assessing the effects of six prevalent transitions while users remain immersed in tasks spanning both virtual and physical domains. In particular, we investigate the effect of different transitions while the user is continuously engaged in a demanding task instead of purely focusing on a given transition. As a preliminary step, we evaluate these six transitions within the realm of pure virtual reality to establish a baseline. Our findings reveal a clear preference among participants for brief and efficient transitions in a task-driven experience, instead of transitions that prioritize interactivity and continuity. Subsequently, we extend our investigation into a cross-reality context, encompassing transitions between virtual and physical environments. Once again, our results underscore the prevailing preference for concise and effective transitions. Furthermore, our research offers intriguing insights about the potential mitigation of visual incoherence between virtual and augmented reality environments by utilizing different transitions.
C1 [Feld, Nico; Bimberg, Pauline; Weyers, Benjamin; Zielasko, Daniel] Trier Univ, Trier, Germany.
C3 Universitat Trier
RP Feld, N (corresponding author), Trier Univ, Trier, Germany.
EM feldn@uni-trier.de; bimberg@uni-trier.de; weyers@uni-trier.de;
   daniel.zielasko@rwth-aachen.de
RI Feld, Nico/KSM-3998-2024
OI Bimberg, Pauline/0000-0003-1481-8359; Feld, Nico/0000-0002-1638-9837;
   Weyers, Benjamin/0000-0003-4785-708X; Zielasko,
   Daniel/0000-0003-3451-4977
FU Ministry for Science and Health of Rhineland-Palatinate; German Research
   Foundation (Deutsche Forschungsgemeinschaft, DFG) [528403131]
FX This work was supported in part by the Ministry for Science and Health
   of Rhineland-Palatinate (Ministerium fur Wissenschaft und Gesundheit,
   MWG) as part of the research project "Flexi Teams" and in part by the
   German Research Foundation (Deutsche Forschungsgemeinschaft, DFG) under
   Grant 528403131.
NR 90
TC 3
Z9 3
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2024
VL 30
IS 12
BP 7601
EP 7618
DI 10.1109/TVCG.2024.3356949
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K8D1L
UT WOS:001346124800005
PM 38252566
DA 2025-03-07
ER

PT J
AU Asish, SM
   Kulshreshth, AK
   Borst, CW
   Sutradhar, S
AF Asish, Sarker M.
   Kulshreshth, Arun K.
   Borst, Christoph W.
   Sutradhar, Shaon
TI Classification of Internal and External Distractions in an Educational
   VR Environment Using Multimodal Features
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Electroencephalography; Feature extraction; Gaze tracking; Brain
   modeling; Physiology; Avatars; Convolutional neural networks; Machine
   Learning; Human-centered computing; EEG; Eye-tracking
ID ATTENTION; MEMORY; MIND; AGE
AB Virtual reality (VR) can potentially enhance student engagement and memory retention in the classroom. However, distraction among participants in a VR-based classroom is a significant concern. Several factors, including mind wandering, external noise, stress, etc., can cause students to become internally and/or externally distracted while learning. To detect distractions, single or multi-modal features can be used. A single modality is found to be insufficient to detect both internal and external distractions, mainly because of individual variability. In this work, we investigated multi-modal features: eye tracking and EEG data, to classify the internal and external distractions in an educational VR environment. We set up our educational VR environment and equipped it for multi-modal data collection. We implemented different machine learning (ML) methods, including k-nearest-neighbors (kNN), Random Forest (RF), one-dimensional convolutional neural network - long short-term memory (1 D-CNN-LSTM), and two-dimensional convolutional neural networks (2D-CNN) to classify participants' internal and external distraction states using the multi-modal features. We performed cross-subject, cross-session, and gender-based grouping tests to evaluate our models. We found that the RF classifier achieves the highest accuracy over 83% in the cross-subject test, around 68% to 78% in the cross-session test, and around 90% in the gender-based grouping test compared to other models. SHAP analysis of the extracted features illustrated greater contributions from the occipital and prefrontal regions of the brain, as well as gaze angle, gaze origin, and head rotation features from the eye tracking data.
C1 [Asish, Sarker M.] Florida Polytech Univ, Lakeland, FL 33805 USA.
   [Kulshreshth, Arun K.; Borst, Christoph W.] Univ Louisiana, Lafayette, LA 70504 USA.
   [Sutradhar, Shaon] AIMEN, AI & Data Analyt Lab, O Porrino, Spain.
C3 Florida Polytechnical University; University of Louisiana Lafayette
RP Asish, SM (corresponding author), Florida Polytech Univ, Lakeland, FL 33805 USA.
EM asish.sust@gmail.com; arunkul@louisiana.edu; cwborst@gmail.com;
   shaon.cuet@gmail.com
RI Asish, Sarker Monojit/AAE-1811-2022
OI Sutradhar, Shaon/0000-0002-0755-1855
FU National Science Foundation [1815976]; Louisiana Board of Regents
   [LEQSF(2022-25)-RD-A-24]
FX This work was supported by the National Science Foundation under Grant
   No. 1815976 and the Louisiana Board of Regents under contract No.
   LEQSF(2022-25)-RD-A-24.
NR 50
TC 0
Z9 0
U1 9
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7332
EP 7342
DI 10.1109/TVCG.2024.3456140
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300021
PM 39255100
DA 2025-03-07
ER

PT J
AU Mahmud, MR
   Cordova, A
   Quarles, J
AF Mahmud, M. Rasel
   Cordova, Alberto
   Quarles, John
TI Multimodal Feedback Methods for Advancing the Accessibility of Immersive
   Virtual Reality for People With Balance Impairments Due to Multiple
   Sclerosis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Virtual environments; Stability analysis; Multiple
   sclerosis; Motors; Usability; Tactile sensors; Multimodal feedback;
   immersive VR; HMDs; Multiple Sclerosis
AB Maintaining balance in immersive virtual reality (VR) environments poses a significant challenge for users, particularly affecting those with pre-existing balance disorders. This study investigates the efficacy of multimodal feedback-comprising auditory, vibrotactile, and visual stimuli-in mitigating balance issues within VR. A sample of 68 participants, divided equally between individuals with balance deficits related to multiple sclerosis and those without, was evaluated. The research explored the impact of various feedback conditions on balance performance. The results demonstrated that the multimodal feedback condition significantly enhanced balance control compared to other conditions, with statistical analysis confirming this improvement (p <. 001). These findings underscore the potential of integrated sensory feedback in addressing balance-related difficulties in VR, thereby improving the overall accessibility and user experience for individuals affected by balance impairments. This research contributes valuable insights into optimizing VR environments for enhanced stability and user comfort.
C1 [Mahmud, M. Rasel] Kennesaw State Univ, Dept Comp Sci, Kennesaw, GA 30144 USA.
   [Cordova, Alberto] Univ Texas San Antonio, Dept Hlth & Kinesiol, San Antonio, TX USA.
   [Quarles, John] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX USA.
C3 University System of Georgia; Kennesaw State University; University of
   Texas System; University of Texas at San Antonio (UTSA); University of
   Texas System; University of Texas at San Antonio (UTSA)
RP Mahmud, MR (corresponding author), Kennesaw State Univ, Dept Comp Sci, Kennesaw, GA 30144 USA.
EM mmahmud2@kennesaw.edu; Alberto.Cordova@utsa.edu; John.Quarles@utsa.edu
NR 29
TC 0
Z9 0
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7193
EP 7202
DI 10.1109/TVCG.2024.3456139
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300023
DA 2025-03-07
ER

PT J
AU Fan, PM
   Thanyadit, S
   Pong, TC
AF Fan, Pak Ming
   Thanyadit, Santawat
   Pong, Ting-Chuen
TI VisTA-LIVE: A Visualization Tool for Assessment of Laboratories in
   Virtual Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Education technology; extended reality; computer-assisted instruction;
   virtual laboratory; Education technology; extended reality;
   computer-assisted instruction; virtual laboratory
AB A Virtual Reality Laboratory (VR Lab) experiment refers to an experiment session that is being conducted in the virtual environment through Virtual Reality (VR) and aims to deliver procedural knowledge to students similar to that in a physical lab environment. While VR Lab is becoming more popular among education institutes as a learning tool for students, existing designs are mostly considered from a student's perspective. Instructors could only receive limited information on how the students are performing and could not provide useful feedback to aid the students' learning and evaluate their performance. This motivated us to create VisTA-LIVE: a Visualization Tool for Assessment of Laboratories In Virtual Environments. In this article, we present in detail the design thinking approach that was applied to create VisTA-LIVE. The tool is deployed in an Extended Reality (XR) environment, and we report the evaluation results with domain experts and discuss issues related to monitoring and assessing a live VR lab session which lay potential directions for future work. We also describe how the resulting design of the tool could be used as a reference for other education developers who wish to develop similar applications.
C1 [Fan, Pak Ming; Pong, Ting-Chuen] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Thanyadit, Santawat] King Mongkuts Univ Technol Thonburi, Bangkok 10140, Thailand.
C3 Hong Kong University of Science & Technology; King Mongkuts University
   of Technology Thonburi
RP Fan, PM (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
EM leofan@connect.ust.hk; santawat.t@esicplus.co.th; tcpong@ust.hk
OI Fan, Pak Ming/0000-0003-1259-7572
FU Research Grant Council of the Hong Kong SAR GRF [16209621]
FX This work was supported by the Research Grant Council of the Hong Kong
   SAR GRF under Grant 16209621.
NR 51
TC 1
Z9 1
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6813
EP 6825
DI 10.1109/TVCG.2023.3341079
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600014
PM 38079370
DA 2025-03-07
ER

PT J
AU Chandio, Y
   Bashir, N
   Interrante, V
   Anwar, FM
AF Chandio, Yasra
   Bashir, Noman
   Interrante, Victoria
   Anwar, Fatima M.
TI Investigating the Correlation Between Presence and Reaction Time in
   Mixed Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual environments; Virtual reality; Time measurement; Correlation;
   Particle measurements; Mixed reality; Atmospheric measurements; presence
ID VIRTUAL ENVIRONMENTS; SPATIAL PRESENCE; PERFORMANCE; EXPERIENCE; SENSE;
   BEHAVIOR; ILLUSION
AB Measuring presence is critical to improving user involvement and performance in Mixed Reality (MR). Presence, a crucial aspect of MR, is traditionally gauged using subjective questionnaires, leading to a lack of time-varying responses and susceptibility to user bias. Inspired by the existing literature on the relationship between presence and human performance, the proposed methodology systematically measures a user's reaction time to a visual stimulus as they interact within a manipulated MR environment. We explore the user reaction time as a quantity that can be easily measured using the systemic tools available in modern MR devices. We conducted an exploratory study (N = 40) with two experiments designed to alter the users' sense of presence by manipulating place illusion and plausibility illusion. We found a significant correlation between presence scores and reaction times with a correlation coefficient -0.65, suggesting that users with a higher sense of presence responded more swiftly to stimuli. We develop a model that estimates a user's presence level using the reaction time values with high accuracy of up to 80%. While our study suggests that reaction time can be used as a measure of presence, further investigation is needed to improve the accuracy of the model.
C1 [Chandio, Yasra; Bashir, Noman; Interrante, Victoria; Anwar, Fatima M.] Univ Massachusetts Amherst, Amherst, MA 01003 USA.
C3 University of Massachusetts System; University of Massachusetts Amherst
RP Chandio, Y (corresponding author), Univ Massachusetts Amherst, Amherst, MA 01003 USA.
EM ychandio@umass.edu; nbashir@umass.edu; interran@umn.edu;
   fanwar@umass.edu
OI Bashir, Noman/0000-0001-9304-910X; Chandio, Yasra/0000-0002-3436-6452;
   Interrante, Victoria/0000-0002-3313-6663
FU NSF [2237485]
FX This work was supported by NSF under Grant 2237485.
NR 91
TC 6
Z9 7
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 5976
EP 5992
DI 10.1109/TVCG.2023.3319563
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000027
PM 37751337
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Hilasaca, GM
   Marcilio-Jr, WE
   Eler, DM
   Martins, RM
   Paulovich, FV
AF Hilasaca, Gladys M.
   Marcilio-Jr, Wilson E.
   Eler, Danilo M.
   Martins, Rafael M.
   Paulovich, Fernando V.
TI A Grid-Based Method for Removing Overlaps of Dimensionality Reduction
   Scatterplot Layouts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Dimensionality reduction; multidimensional projection; scatterplots;
   overlap removal
ID NEIGHBORHOOD PRESERVATION; VISUAL ANALYSIS; ADJUSTMENT
AB Dimensionality Reduction (DR) scatterplot layouts have become a ubiquitous visualization tool for analyzing multidimensional datasets. Despite their popularity, such scatterplots suffer from occlusion, especially when informative glyphs are used to represent data instances, potentially obfuscating critical information for the analysis under execution. Different strategies have been devised to address this issue, either producing overlap-free layouts that lack the powerful capabilities of contemporary DR techniques in uncovering interesting data patterns or eliminating overlaps as a post-processing strategy. Despite the good results of post-processing techniques, most of the best methods typically expand or distort the scatterplot area, thus reducing glyphs' size (sometimes) to unreadable dimensions, defeating the purpose of removing overlaps. This article presents Distance Grid (DGrid), a novel post-processing strategy to remove overlaps from DR layouts that faithfully preserves the original layout's characteristics and bounds the minimum glyph sizes. We show that DGrid surpasses the state-of-the-art in overlap removal (through an extensive comparative evaluation considering multiple different metrics) while also being one of the fastest techniques, especially for large datasets. A user study with 51 participants also shows that DGrid is consistently ranked among the top techniques for preserving the original scatterplots' visual characteristics and the aesthetics of the final results.
C1 [Hilasaca, Gladys M.] Fed Univ So Paulo UNIFESP, BR-05508220 Sao Paulo, Brazil.
   [Marcilio-Jr, Wilson E.] Sao Paulo State Univ, BR-05508070 Sao Paulo, Brazil.
   [Eler, Danilo M.] Sao Paulo State Univ, S-35252 Sao Paulo, Brazil.
   [Martins, Rafael M.; Paulovich, Fernando V.] Linnaeus Univ, NL-5612 AZ Vaxjo, Sweden.
C3 Universidade Estadual Paulista; Universidade Estadual Paulista; Linnaeus
   University
RP Paulovich, FV (corresponding author), Linnaeus Univ, NL-5612 AZ Vaxjo, Sweden.
EM gladysmarleny@gmail.com; wilson_jr@outlook.com; danilo.eler@unesp.br;
   rafael.martins@lnu.se; f.paulovich@tue.nl
RI Eler, Danilo/M-3320-2019; Martins, Rafael/H-9192-2019; Paulovich,
   Fernando/G-1329-2010
OI Paulovich, Fernando/0000-0002-2316-760X; Eler,
   Danilo/0000-0002-9493-145X
FU Natural Sciences and Engineering Research Council of Canada (NSERC);
   Emerging Leaders in the Americas Program (ELAP); Government of Canada;
   CAPES-Brazil
FX The authors would like to thank all reviewers who dedicated time to this
   article. Your comments and feedback were beneficial and indeed led to
   substantial improvement in the quality and clarity of this manuscript.
   We acknowledge the support of the Natural Sciences and Engineering
   Research Council of Canada(NSERC), CAPES-Brazil, and the Emerging
   Leaders in the Americas Program (ELAP) with the support of the
   Government of Canada.
NR 61
TC 1
Z9 1
U1 1
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5733
EP 5749
DI 10.1109/TVCG.2023.3309941
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400072
PM 37647195
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Panavas, L
   Crnovrsanin, T
   Adams, JL
   Ullman, J
   Sargavad, A
   Tory, M
   Dunne, C
AF Panavas, Liudas
   Crnovrsanin, Tarik
   Adams, Jane Lydia
   Ullman, Jonathan
   Sargavad, Ali
   Tory, Melanie
   Dunne, Cody
TI Investigating the Visual Utility of Differentially Private Scatterplots
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Differential privacy; data study; scatterplots; visual utility
ID FRAMEWORK; HISTOGRAM
AB Increasingly, visualization practitioners are working with, using, and studying private and sensitive data. There can be many stakeholders interested in the resulting analyses-but widespread sharing of the data can cause harm to individuals, companies, and organizations. Practitioners are increasingly turning to differential privacy to enable public data sharing with a guaranteed amount of privacy. Differential privacy algorithms do this by aggregating data statistics with noise, and this now-private data can be released visually with differentially private scatterplots. While the private visual output is affected by the algorithm choice, privacy level, bin number, data distribution, and user task, there is little guidance on how to choose and balance the effect of these parameters. To address this gap, we had experts examine 1,200 differentially private scatterplots created with a variety of parameter choices and tested their ability to see aggregate patterns in the private output (i.e. the visual utility of the chart). We synthesized these results to provide easy-to-use guidance for visualization practitioners releasing private data through scatterplots. Our findings also provide a ground truth for visual utility, which we use to benchmark automated utility metrics from various fields. We demonstrate how multi-scale structural similarity (MS-SSIM), the metric most strongly correlated with our study's utility results, can be used to optimize parameter selection.
C1 [Panavas, Liudas; Crnovrsanin, Tarik; Adams, Jane Lydia; Ullman, Jonathan; Tory, Melanie; Dunne, Cody] Northeastern Univ, Boston, MA 02115 USA.
   [Sargavad, Ali] Univ Massachusetts, Amherst, MA 01003 USA.
C3 Northeastern University; University of Massachusetts System; University
   of Massachusetts Amherst
RP Panavas, L (corresponding author), Northeastern Univ, Boston, MA 02115 USA.
EM panavas.l@northeastern.edu; t.crnovrsanin@northeastern.edu;
   adams.jan@northeastern.edu; j.ullman@northeastern.edu;
   asarv@cs.umass.edu; m.tory@northeastern.edu; c.dunne@northeastern.edu
RI Dunne, Cody/M-4444-2019
OI Sarvghad, Ali/0000-0003-3718-7043; Dunne, Cody/0000-0002-1609-9776;
   Crnovrsanin, Tarik/0000-0002-4397-5532; Panavas,
   Liudas/0000-0003-0428-5579; Tory, Melanie/0000-0002-6806-9253; Adams,
   Jane/0000-0002-7826-3500
FU NIEHS Superfund Research Program [P42ES017198]
FX This work was supported by NIEHS Superfund Research Program under Grant
   P42ES017198 (PROTECT). Recommended for acceptance by Peer-Timo Bremer.
NR 71
TC 0
Z9 0
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5370
EP 5385
DI 10.1109/TVCG.2023.3292391
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400005
PM 37405888
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhao, ZM
   Xie, W
   Zuo, BH
   Wang, YG
AF Zhao, Zimeng
   Xie, Wei
   Zuo, Binghui
   Wang, Yangang
TI Skeleton Extraction for Articulated Objects With the Spherical
   Unwrapping Profiles
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Skeleton; Three-dimensional displays; Feature extraction; Heating
   systems; Topology; Point cloud compression; Task analysis; Skeleton
   embedding; spherical unwrapping; surface-to-image representation
ID HUMAN POSE ESTIMATION; ALGORITHM
AB Embedding unified skeletons into unregistered scans is fundamental to finding correspondences, depicting motions, and capturing underlying structures among the articulated objects in the same category. Some existing approaches rely on laborious registration to adapt a predefined LBS model to each input, while others require the input to be set to a canonical pose, e.g., T-pose or A-pose. However, their effectiveness is always influenced by the water-tightness, face topology, and vertex density of the input mesh. At the core of our approach lies a novel unwrapping method, named SUPPLE (Spherical UnwraPping ProfiLEs), which maps a surface into image planes independent of mesh topologies. Based on this lower-dimensional representation, a learning-based framework is further designed to localize and connect skeletal joints with fully convolutional architectures. Experiments demonstrate that our framework yields reliable skeleton extractions across a broad range of articulated categories, from raw scans to online CADs.
C1 [Zhao, Zimeng; Xie, Wei; Zuo, Binghui; Wang, Yangang] Southeast Univ, Sch Automat, Nanjing 210096, Jiangsu, Peoples R China.
C3 Southeast University - China
RP Wang, YG (corresponding author), Southeast Univ, Sch Automat, Nanjing 210096, Jiangsu, Peoples R China.
EM zzmpassion@gmail.com; xiewei.xw@outlook.com; z_binghui@163.com;
   yangangwang@seu.edu.cn
RI Wang, Yangang/IVH-8352-2023
OI Zhao, Zimeng/0000-0001-6570-0620; Xie, Wei/0000-0001-7632-2987; Wang,
   Yangang/0000-0002-1325-9252
FU National Natural Science Foundation of China [62076061]; Natural Science
   Foundation of Jiangsu Province [BK20220127]; Young Elite Scientists
   Sponsorship Program by CAST [YES20200025]; Program of Southeast
   University [2242021R41083]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62076061, in part by the Natural Science
   Foundation of Jiangsu Province under Grant BK20220127, by the "Young
   Elite Scientists Sponsorship Program by CAST" under Grant YES20200025
   and by the "Zhishan Young Scholar" Program of Southeast University under
   Grant 2242021R41083.
NR 151
TC 0
Z9 0
U1 8
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3731
EP 3748
DI 10.1109/TVCG.2023.3239370
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700011
PM 37022000
DA 2025-03-07
ER

PT J
AU Combe, T
   Fribourg, R
   Detto, L
   Norm, JM
AF Combe, Theo
   Fribourg, Rebecca
   Detto, Lucas
   Norm, Jean-Marie
TI Exploring the Influence of Virtual Avatar Heads in Mixed Reality on
   Social Presence, Performance and User Experience in Collaborative Tasks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Avatars; Collaboration; Task analysis; Faces; Resists; Mixed reality;
   Cameras; Mixed Reality; Avatar Representation
ID COMMUNICATION; ENVIRONMENTS
AB In Mixed Reality (MR), users' heads are largely (if not completely) occluded by the MR Head-Mounted Display (HMD) they are wearing. As a consequence, one cannot see their facial expressions and other communication cues when interacting locally. In this paper, we investigate how displaying virtual avatars' heads on-top of the (HMD-occluded) heads of participants in a Video See-Through (VST) Mixed Reality local collaborative task could improve their collaboration as well as social presence. We hypothesized that virtual heads would convey more communicative cues (such as eye direction or facial expressions) hidden by the MR HMDs and lead to better collaboration and social presence. To do so, we conducted a between-subject study ($\mathrm{n}=88$) with two independent variables: the type of avatar (CartoonAvatar/RealisticAvatar/NoAvatar) and the level of facial expressions provided (HighExpr/LowExpr). The experiment involved two dyadic communication tasks: (i) the "20-question" game where one participant asks questions to guess a hidden word known by the other participant and (ii) a urban planning problem where participants have to solve a puzzle by collaborating. Each pair of participants performed both tasks using a specific type of avatar and facial animation. Our results indicate that while adding an avatar's head does not necessarily improve social presence, the amount of facial expressions provided through the social interaction does have an impact. Moreover, participants rated their performance higher when observing a realistic avatar but rated the cartoon avatars as less uncanny. Taken together, our results contribute to a better understanding of the role of partial avatars in local MR collaboration and pave the way for further research exploring collaboration in different scenarios, with different avatar types or MR setups.
C1 [Combe, Theo; Fribourg, Rebecca; Norm, Jean-Marie] Nantes Univ, Ecole Cent Nantes, LS2N PACCE, UMR 6004, Nantes, France.
   [Detto, Lucas] Nantes Univ, ENSA Nantes, Ecole Cent Nantes, CNRS,AAU CRENAU,UMR 1563, Nantes, France.
C3 Nantes Universite; Ecole Centrale de Nantes; Centre National de la
   Recherche Scientifique (CNRS); CNRS - Institute for Humanities & Social
   Sciences (INSHS); Nantes Universite; Ecole Centrale de Nantes
RP Combe, T (corresponding author), Nantes Univ, Ecole Cent Nantes, LS2N PACCE, UMR 6004, Nantes, France.
EM theo.combe@ec-nantes.fr; rebecca.fribourg@ec-nantes.fr;
   lucas.detto@eleves.ec-nantes.fr; jean-marie.normand@ec-nantes.fr
OI Combe, Theo/0000-0003-1209-5580
NR 68
TC 0
Z9 0
U1 3
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2206
EP 2216
DI 10.1109/TVCG.2024.3372051
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400060
PM 38437082
DA 2025-03-07
ER

PT J
AU Wen, EL
   Gupta, C
   Sasikumar, P
   Billinghurst, M
   Wilmott, J
   Skow, E
   Dey, A
   Nanayakkara, S
AF Wen, Elliott
   Gupta, Chitralekha
   Sasikumar, Prasanth
   Billinghurst, Mark
   Wilmott, James
   Skow, Emily
   Dey, Arindam
   Nanayakkara, Suranga
TI VR.net: A Real-world Dataset for Virtual Reality Motion Sickness
   Research
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Motion Sickness; Virtual Reality; Machine Learning
ID VISUALLY-INDUCED MOTION; FIELD-OF-VIEW; SIMULATOR SICKNESS; HEART-RATE;
   CYBERSICKNESS; VECTION; DEPTH; ANGLE; PITCH; GAZE
AB Researchers have used machine learning approaches to identify motion sickness in VR experience. These approaches would certainly benefit from an accurately labeled, real-world, diverse dataset that enables the development of generalizable ML models. We introduce 'VR.net', a dataset comprising 165-hour gameplay videos from 100 real-world games spanning ten diverse genres, evaluated by 500 participants. VR.net accurately assigns 24 motion sickness-related labels for each video frame, such as camera/object movement, depth of field, and motion flow. Building such a dataset is challenging since manual labeling would require an infeasible amount of time. Instead, we implement a tool to automatically and precisely extract ground truth data from 3D engines' rendering pipelines without accessing VR games' source code. We illustrate the utility of VR.net through several applications, such as risk factor detection and sickness level prediction. We believe that the scale, accuracy, and diversity of VR.net can offer unparalleled opportunities for VR motion sickness research and beyond.We also provide access to our data collection tool, enabling researchers to contribute to the expansion of VR.net.
C1 [Wen, Elliott; Billinghurst, Mark] Univ Auckland, Auckland, New Zealand.
   [Gupta, Chitralekha; Sasikumar, Prasanth; Nanayakkara, Suranga] Natl Univ Singapore, Singapore, Singapore.
   [Wilmott, James; Skow, Emily; Dey, Arindam] Meta Real Labs, Burlingame, CA USA.
C3 University of Auckland; National University of Singapore
RP Wen, EL (corresponding author), Univ Auckland, Auckland, New Zealand.
EM jq.elliott.wen@gmail.com; Chitralekha@ahlab.org; Prasanth@ahlab.org;
   mark.billinghurst@auckland.ac.nz; jwilmott@meta.com; emilyskow@meta.com;
   aridey@meta.com; suranga@ahlab.org
RI Gupta, Chitralekha/AAF-8617-2019; Billinghurst, Mark/AAJ-4236-2020;
   Wilmott, James/AGA-8227-2022
OI NANAYAKKARA, SURANGA/0000-0001-7441-5493
NR 64
TC 2
Z9 2
U1 6
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2330
EP 2336
DI 10.1109/TVCG.2024.3372044
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400053
PM 38437109
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhang, WH
   Yue, YT
   Pan, H
   Chen, ZG
   Wang, C
   Pfister, H
   Wang, WP
AF Zhang, Wenhua
   Yue, Yating
   Pan, Hao
   Chen, Zhonggui
   Wang, Chuan
   Pfister, Hanspeter
   Wang, Wenping
TI Marching Windows: Scalable Mesh Generation for Volumetric Data With
   Multiple Materials
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Topology; Mesh generation; Three-dimensional displays; Imaging; Data
   visualization; Memory management; Measurement; Large volumetric data;
   multiple material; marching windows; mesh simplification; topology
   guarantee
ID TETRAHEDRAL MESHES; SIMPLIFICATION; ALGORITHM; SURFACES
AB Volumetric data abounds in medical imaging and other fields. With the improved imaging quality and the increased resolution, volumetric datasets are getting so large that the existing tools have become inadequate for processing and analyzing the data. Here we consider the problem of computing tetrahedral meshes to represent large volumetric datasets with labeled multiple materials, which are often encountered in medical imaging or microscopy optical slice tomography. Such tetrahedral meshes are a more compact and expressive geometric representation so are in demand for efficient visualization and simulation of the data, which are impossible if the original large volumetric data are used directly due to the large memory requirement. Existing methods for meshing volumetric data are not scalable for handling large datasets due to their sheer demand on excessively large run-time memory or failure to produce a tet-mesh that preserves the multi-material structure of the original volumetric data. In this article we propose a novel approach, called Marching Windows, that uses a moving window and a disk-swap strategy to reduce the run-time memory footprint, devise a new scheme that guarantees to preserve the topological structure of the original dataset, and adopt an error-guided optimization technique to improve both geometric approximation error and mesh quality. Extensive experiments show that our method is capable of processing very large volumetric datasets beyond the capability of the existing methods and producing tetrahedral meshes of high quality.
C1 [Zhang, Wenhua; Yue, Yating; Wang, Chuan] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Pan, Hao] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Chen, Zhonggui] Xiamen Univ, Sch Informat, Xiamen 361005, Peoples R China.
   [Pfister, Hanspeter] Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
   [Wang, Wenping] Texas A&M Univ, Dept Visualizat, College Stn, TX 77843 USA.
C3 University of Hong Kong; Microsoft; Microsoft China; Microsoft Research
   Asia; Xiamen University; Harvard University; Texas A&M University
   System; Texas A&M University College Station
RP Zhang, WH (corresponding author), Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
EM winniezhangcoding@gmail.com; ytyue@cs.hku.hk; haopan@microsoft.com;
   chenzhonggui@xmu.edu.cn; cwang.hku@gmail.com; pfister@g.harvard.edu;
   wenping@tamu.edu
RI Zhang, Wenhua/HOC-9630-2023
OI Pfister, Hanspeter/0000-0002-3620-2582; Zhang,
   Wenhua/0000-0001-5411-0684; Chen, Zhonggui/0000-0002-9960-4896; PAN,
   Hao/0000-0003-3628-9777
FU National Natural Science Foundation of China
FX No Statement Available
NR 59
TC 0
Z9 0
U1 1
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2024
VL 30
IS 3
BP 1728
EP 1742
DI 10.1109/TVCG.2022.3225526
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IN0A9
UT WOS:001166876500006
PM 36455093
DA 2025-03-07
ER

PT J
AU Floricel, C
   Wentzel, A
   Mohamed, A
   Fuller, CD
   Canahuate, G
   Marai, GE
AF Floricel, Carla
   Wentzel, Andrew
   Mohamed, Abdallah
   Fuller, C. David
   Canahuate, Guadalupe
   Marai, G. Elisabeta
TI Roses Have Thorns: Understanding the Downside of Oncological Care
   Delivery Through Visual Analytics and Sequential Rule Mining
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Cancer; Computational modeling; Data visualization; Data
   mining; Analytical models; Data models; Temporal Data; Life Sciences;
   Mixed Initiative Human-Machine Analysis; Data Clustering and Aggregation
ID SYMPTOM CLUSTERS; EVENT SEQUENCES; CANCER-PATIENTS; NECK-CANCER;
   ASSOCIATION; VALIDATION; PREDICTION; COHORT; HEAD
AB Personalized head and neck cancer therapeutics have greatly improved survival rates for patients, but are often leading to understudied long-lasting symptoms which affect quality of life. Sequential rule mining (SRM) is a promising unsupervised machine learning method for predicting longitudinal patterns in temporal data which, however, can output many repetitive patterns that are difficult to interpret without the assistance of visual analytics. We present a data-driven, human-machine analysis visual system developed in collaboration with SRM model builders in cancer symptom research, which facilitates mechanistic knowledge discovery in large scale, multivariate cohort symptom data. Our system supports multivariate predictive modeling of post-treatment symptoms based on during-treatment symptoms. It supports this goal through an SRM, clustering, and aggregation back end, and a custom front end to help develop and tune the predictive models. The system also explains the resulting predictions in the context of therapeutic decisions typical in personalized care delivery. We evaluate the resulting models and system with an interdisciplinary group of modelers and head and neck oncology researchers. The results demonstrate that our system effectively supports clinical and symptom research.
C1 [Floricel, Carla; Wentzel, Andrew; Marai, G. Elisabeta] Univ Illinois, Chicago, IL 60607 USA.
   [Canahuate, Guadalupe] Univ Iowa, Iowa City, IA USA.
   [Mohamed, Abdallah; Fuller, C. David] Univ Texas MD Anderson Canc Ctr, Austin, TX USA.
C3 University of Illinois System; University of Illinois Chicago;
   University of Illinois Chicago Hospital; University of Iowa; University
   of Texas System; UTMD Anderson Cancer Center
RP Floricel, C (corresponding author), Univ Illinois, Chicago, IL 60607 USA.
EM cflori3@uic.edu; awentze2@uic.edu; asmohamed@mdanderson.org;
   cdfuller@mdanderson.org; guadalupe-canahuate@uiowa.edu; gmarai@uic.edu
RI Fuller, Clifton/AAB-4012-2019
OI Fuller, Clifton D./0000-0002-5264-3994
FU NIH
FX No Statement Available
NR 82
TC 1
Z9 1
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 1227
EP 1237
DI 10.1109/TVCG.2023.3326939
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500037
PM 38015695
OA Green Submitted, Green Accepted
DA 2025-03-07
ER

PT J
AU Morrical, N
   Zellmann, S
   Sahistan, A
   Shriwise, P
   Pascucci, V
AF Morrical, Nate
   Zellmann, Stefan
   Sahistan, Alper
   Shriwise, Patrick
   Pascucci, Valerio
TI Attribute-Aware RBFs: Interactive Visualization of Time Series Particle
   Volumes Using RT Core Range Queries
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Ray Tracing; Volume Rendering; Particle Volumes; Radial Basis Functions;
   Scientific Visualization
ID MODELS
AB Smoothed-particle hydrodynamics (SPH) is a mesh-free method used to simulate volumetric media in fluids, astrophysics, and solid mechanics. Visualizing these simulations is problematic because these datasets often contain millions, if not billions of particles carrying physical attributes and moving over time. Radial basis functions (RBFs) are used to model particles, and overlapping particles are interpolated to reconstruct a high-quality volumetric field; however, this interpolation process is expensive and makes interactive visualization difficult. Existing RBF interpolation schemes do not account for color-mapped attributes and are instead constrained to visualizing just the density field. To address these challenges, we exploit ray tracing cores in modern GPU architectures to accelerate scalar field reconstruction. We use a novel RBF interpolation scheme to integrate per-particle colors and densities, and leverage GPU-parallel tree construction and refitting to quickly update the tree as the simulation animates over time or when the user manipulates particle radii. We also propose a Hilbert reordering scheme to cluster particles together at the leaves of the tree to reduce tree memory consumption. Finally, we reduce the noise of volumetric shadows by adopting a spatially temporal blue noise sampling scheme. Our method can provide a more detailed and interactive view of these large, volumetric, time-series particle datasets than traditional methods, leading to new insights into these physics simulations.
C1 [Morrical, Nate; Sahistan, Alper; Pascucci, Valerio] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
   [Zellmann, Stefan] Univ Cologne, Cologne, Germany.
   [Shriwise, Patrick] Argonne Natl Lab, Argonne, IL USA.
C3 Utah System of Higher Education; University of Utah; University of
   Cologne; United States Department of Energy (DOE); Argonne National
   Laboratory
RP Morrical, N (corresponding author), Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
EM natemorrical@gmail.com
RI pascucci, Valerio/GXF-0616-2022; Müller, Thomas/AAD-3910-2019
OI Sahistan, Alper/0000-0002-3480-7713; Shriwise,
   Patrick/0000-0002-3979-7665; pascucci, valerio/0000-0002-8877-2042;
   Zellmann, Stefan/0000-0003-2880-9090; Morrical,
   Nathan/0000-0002-2262-6974
FU UChicago Argonne, LLC
FX No Statement Available
NR 48
TC 0
Z9 0
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 1150
EP 1160
DI 10.1109/TVCG.2023.3327366
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500028
PM 37878450
DA 2025-03-07
ER

PT J
AU Scimone, A
   Eckelt, K
   Streit, M
   Hinterreiter, A
AF Scimone, Anna
   Eckelt, Klaus
   Streit, Marc
   Hinterreiter, Andreas
TI Marjorie: Visualizing Type 1 Diabetes Data to Support Pattern
   Exploration
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Design study; task analysis; diabetes; time series data; visual
   analytics; clustering
ID TIME-SERIES DATA
AB In this work we propose Marjorie, a visual analytics approach to address the challenge of analyzing patients' diabetes data during brief regular appointments with their diabetologists. Designed in consultation with diabetologists, Marjorie uses a combination of visual and algorithmic methods to support the exploration of patterns in the data. Patterns of interest include seasonal variations of the glucose profiles, and non-periodic patterns such as fluctuations around mealtimes or periods of hypoglycemia (i.e., glucose levels below the normal range). We introduce a unique representation of glucose data based on modified horizon graphs and hierarchical clustering of adjacent carbohydrate or insulin entries. Semantic zooming allows the exploration of patterns on different levels of temporal detail. We evaluated our solution in a case study, which demonstrated Marjorie's potential to provide valuable insights into therapy parameters and unfavorable eating habits, among others. The study results and informal feedback collected from target users suggest that Marjorie effectively supports patients and diabetologists in the joint exploration of patterns in diabetes data, potentially enabling more informed treatment decisions. A free copy of this paper and all supplemental materials are available at https://osf.io/34t8c/.
C1 [Scimone, Anna; Eckelt, Klaus; Streit, Marc; Hinterreiter, Andreas] Johannes Kepler Univ Linz, Linz, Austria.
C3 Johannes Kepler University Linz
RP Scimone, A (corresponding author), Johannes Kepler Univ Linz, Linz, Austria.
EM anna_scimone@live.de; klaus.eckelt@jku.at; marc.streit@jku.at;
   andreas.hinterreiter@jku.at
FU Austrian Research Promotion Agency
FX No Statement Available
NR 50
TC 0
Z9 0
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 1216
EP 1226
DI 10.1109/TVCG.2023.3326936
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500053
PM 37874710
OA Green Submitted, hybrid
DA 2025-03-07
ER

PT J
AU Wu, YF
   Guo, ZY
   Mamakos, M
   Hartline, J
   Hullman, J
AF Wu, Yifan
   Guo, Ziyang
   Mamakos, Michalis
   Hartline, Jason
   Hullman, Jessica
TI The Rational Agent Benchmark for Data Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Task analysis; Behavioral sciences; Visualization;
   Benchmark testing; Bayes methods; Uncertainty; Evaluation;
   decision-making; rational agent; scoring rule
ID UNCERTAINTY
AB Understanding how helpful a visualization is from experimental results is difficult because the observed performance is confounded with aspects of the study design, such as how useful the information that is visualized is for the task. We develop a rational agent framework for designing and interpreting visualization experiments. Our framework conceives two experiments with the same setup: one with behavioral agents (human subjects), and the other one with a hypothetical rational agent. A visualization is evaluated by comparing the expected performance of behavioral agents to that of a rational agent under different assumptions. Using recent visualization decision studies from the literature, we demonstrate how the framework can be used to pre-experimentally evaluate the experiment design by bounding the expected improvement in performance from having access to visualizations, and post-experimentally to deconfound errors of information extraction from errors of optimization, among other analyses.
C1 [Wu, Yifan; Guo, Ziyang; Mamakos, Michalis; Hartline, Jason; Hullman, Jessica] Northwestern Univ, Evanston, IL 60208 USA.
C3 Northwestern University
RP Wu, YF (corresponding author), Northwestern Univ, Evanston, IL 60208 USA.
EM yifan.wu@u.northwestern.edu; ziyangguo2027@u.northwestern.edu;
   michailmamakos2022@u.northwestern.edu; hartline@northwestern.edu;
   jhullman@northwestern.edu
RI Hartline, Jason/B-7167-2009; Hullman, Jessica/P-7130-2018
OI Hullman, Jessica/0000-0001-6826-3550; Hartline,
   Jason/0000-0001-5505-6819; Wu, Yifan/0000-0002-4299-8169; Guo,
   Ziyang/0009-0004-4200-6774
NR 33
TC 1
Z9 1
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 338
EP 347
DI 10.1109/TVCG.2023.3326513
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500063
PM 37871058
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Adhikari, A
   Zielasko, D
   Aguilar, I
   Bretin, A
   Kruijff, E
   von der Heyde, M
   Riecke, BE
AF Adhikari, Ashu
   Zielasko, Daniel
   Aguilar, Ivan
   Bretin, Alexander
   Kruijff, Ernst
   von der Heyde, Markus
   Riecke, Bernhard E.
TI Integrating Continuous and Teleporting VR Locomotion into a Seamless
   HyperJump Paradigm
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Teleportation; Cybersickness; Optical flow; Task analysis; Navigation;
   Animation; Virtual environments; Virtual reality; spatial updating;
   leaning; teleportation; locomotion; semi-continuous locomotion
ID VIRTUAL ENVIRONMENTS; SPATIAL ORIENTATION; PATH-INTEGRATION; NAVIGATION;
   REALITY; TRAVEL
AB Continuous locomotion in VR provides uninterrupted optical flow, which mimics real-world locomotion and supports path integration. However, optical flow limits the maximum speed and acceleration that can be effectively used without inducing cybersickness. In contrast, teleportation provides neither optical flow nor acceleration cues, and users can jump to any length without increasing cybersickness. However, teleportation cannot support continuous spatial updating and can increase disorientation. Thus, we designed 'HyperJump' in an attempt to merge benefits from continuous locomotion and teleportation. HyperJump adds iterative jumps every half a second on top of the continuous movement and was hypothesized to facilitate faster travel without compromising spatial awareness/orientation. In a user study, Participants travelled around a naturalistic virtual city with and without HyperJump (equivalent maximum speed). They followed waypoints to new landmarks, stopped near them and pointed back to all previously visited landmarks in random order. HyperJump was added to two continuous locomotion interfaces (controller- and leaning-based). Participants had better spatial awareness/orientation with leaning-based interfaces compared to controller-based (assessed via rapid pointing). With HyperJump, participants travelled significantly faster, while staying on the desired course without impairing their spatial knowledge. This provides evidence that optical flow can be effectively limited such that it facilitates faster travel without compromising spatial orientation. In future design iterations, we plan to utilize audio-visual effects to support jumping metaphors that help users better anticipate and interpret jumps, and use much larger virtual environments requiring faster speeds, where cybersickness will become increasingly prevalent and thus teleporting will become more important.
C1 [Adhikari, Ashu; Aguilar, Ivan; Bretin, Alexander; Kruijff, Ernst; von der Heyde, Markus; Riecke, Bernhard E.] Simon Fraser Univ, Sch Interact Arts & Technol, Burnaby, BC V5A 1S6, Canada.
   [Zielasko, Daniel] Univ Trier, D-54296 Trier, Germany.
   [Kruijff, Ernst] Bonn Rhein Sieg Univ Appl Sci, Inst Visual Comp, D-53757 St Augustin, Germany.
   [von der Heyde, Markus] vdH IT, D-99425 Weimar, Germany.
C3 Simon Fraser University; Universitat Trier; Hochschule Bonn Rhein Sieg
RP Adhikari, A (corresponding author), Simon Fraser Univ, Sch Interact Arts & Technol, Burnaby, BC V5A 1S6, Canada.
EM ashua@sfu.ca; zielasko@uni-trier.de; ivan_aguilar@sfu.ca;
   alexanderbretin@sfu.ca; ernst.kruijff@h-brs.de; info@vdh-it.de;
   ber1@sfu.ca
RI von der Heyde, Markus/HJA-0319-2022; Aguilar, Ivan/AAO-4316-2020;
   Adhikari, Ashu/MIJ-8756-2025; Riecke, Bernhard/C-6399-2011
OI von der Heyde, Markus/0000-0002-6026-082X; Zielasko,
   Daniel/0000-0003-3451-4977; Aguilar, Ivan/0000-0002-8735-4041; Adhikari,
   Ashu/0000-0002-2540-6344; Kruijff, Ernst/0000-0003-1625-0955; Riecke,
   Bernhard/0000-0001-7974-0850
NR 58
TC 7
Z9 7
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5265
EP 5281
DI 10.1109/TVCG.2022.3207157
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300031
PM 36112551
DA 2025-03-07
ER

PT J
AU Cao, AQ
   Lan, J
   Xie, X
   Chen, H
   Zhang, XL
   Zhang, H
   Wu, YC
AF Cao, Anqi
   Lan, Ji
   Xie, Xiao
   Chen, Hongyu
   Zhang, Xiaolong
   Zhang, Hui
   Wu, Yingcai
TI Team-Builder: Toward More Effective Lineup Selection in Soccer
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Sports; Data visualization; Analytical models; Trajectory; Computational
   modeling; Visual analytics; Videos; Sports visualization; lineup
   selection; design study
ID PLAYER SELECTION; VISUAL ANALYTICS; VISUALIZATION; SYSTEM; SPORT
AB Lineup selection is an essential and important task in soccer matches. To win a match, coaches must consider various factors and select appropriate players for a planned formation. Computation-based tools have been proposed to help coaches on this complex task, but they are usually based on over-simplified models on player performances, do not support interactive analysis, and overlook the inputs by coaches. In this article, we propose a method for visual analytics of soccer lineup selection by tackling two challenges: characterizing essential factors involved in generating optimal lineup, and supporting coach-driven visual analytics of lineup selection. We develop a lineup selection model that integrates such important factors, such as spatial regions of player actions and defensive interactions with opponent players. A visualization system, Team-Builder, is developed to help coaches control the process of lineup generation, explanation, and comparison through multiple coordinated views. The usefulness and effectiveness of our system are demonstrated by two case studies on a real-world soccer event dataset.
C1 [Cao, Anqi; Lan, Ji; Chen, Hongyu; Wu, Yingcai] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
   [Xie, Xiao; Zhang, Hui] Zhejiang Univ, Dept Sports Sci, Hangzhou 310027, Peoples R China.
   [Zhang, Xiaolong] Penn State Univ, Coll Informat Sci & Technol, State Coll, PA 16801 USA.
C3 Zhejiang University; Zhejiang University; Pennsylvania Commonwealth
   System of Higher Education (PCSHE); Pennsylvania State University
RP Xie, X (corresponding author), Zhejiang Univ, Dept Sports Sci, Hangzhou 310027, Peoples R China.
EM caoanqi@zju.edu.cn; lanjizju@zju.edu.cn; xxie@zju.edu.cn;
   chenhy7820@zju.edu.cn; lzhang@ist.psu.edu; zhang_hui@zju.edu.cn;
   ycwu@zju.edu.cn
RI zhang, hui/GXH-6098-2022; LAN, JI/M-2006-2018
OI LAN, JI/0000-0002-8658-8620; , Hui/0000-0003-0601-3905; Cao,
   Anqi/0000-0003-1794-4510
FU NSFC [62072400]; Collaborative Innovation Center of Artificial
   Intelligence by MOE and Zhejiang ProvincialGovernment (ZJU); Zhejiang
   Lab [2021KE0AC02]
FX The work was supported in part by NSFC under Grant 62072400, in part by
   the Collaborative Innovation Center of Artificial Intelligence by MOE
   and Zhejiang ProvincialGovernment (ZJU), and in part by Zhejiang Lab
   (2021KE0AC02). (Corresponding author: Xiao Xie.)
NR 61
TC 3
Z9 3
U1 8
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5178
EP 5193
DI 10.1109/TVCG.2022.3207147
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300025
PM 36112553
DA 2025-03-07
ER

PT J
AU Erickson, A
   Bruder, G
   Welch, GF
AF Erickson, Austin
   Bruder, Gerd
   Welch, Gregory F.
TI Analysis of the Saliency of Color-Based Dichoptic Cues in Optical
   See-Through Augmented Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Task analysis; Observers; Image color analysis; Optical
   saturation; Optical imaging; Color; Augmented reality; optical
   see-through displays; visual perception; attention cues; preattentive
   cues; dichoptic cues; human-computer interaction (HCI)
ID VISUAL-SEARCH; EYE; BINOCULARITY; CONTRAST
AB In a future of pervasive augmented reality (AR), AR systems will need to be able to efficiently draw or guide the attention of the user to visual points of interest in their physical-virtual environment. Since AR imagery is overlaid on top of the user's view of their physical environment, these attention guidance techniques must not only compete with other virtual imagery, but also with distracting or attention-grabbing features in the user's physical environment. Because of the wide range of physical-virtual environments that pervasive AR users will find themselves in, it is difficult to design visual cues that "pop out" to the user without performing a visual analysis of the user's environment, and changing the appearance of the cue to stand out from its surroundings. In this article, we present an initial investigation into the potential uses of dichoptic visual cues for optical see-through AR displays, specifically cues that involve having a difference in hue, saturation, or value between the user's eyes. These types of cues have been shown to be preattentively processed by the user when presented on other stereoscopic displays, and may also be an effective method of drawing user attention on optical see-through AR displays. We present two user studies: one that evaluates the saliency of dichoptic visual cues on optical see-through displays, and one that evaluates their subjective qualities. Our results suggest that hue-based dichoptic cues or "Forbidden Colors" may be particularly effective for these purposes, achieving significantly lower error rates in a pop out task compared to value-based and saturation-based cues.
C1 [Erickson, Austin; Bruder, Gerd; Welch, Gregory F.] Univ Cent Florida, Orlando, FL 32816 USA.
C3 State University System of Florida; University of Central Florida
RP Erickson, A (corresponding author), Univ Cent Florida, Orlando, FL 32816 USA.
EM ericksona@knights.ucf.edu; gerd.bruder@ucf.edu; welch@ucf.edu
RI Erickson, Austin/AAV-9677-2020
OI Welch, Gregory/0000-0002-8243-646X; Erickson, Austin/0000-0002-3146-8023
FU National Science Foundation [1800961]; Office of Naval Research
   [N00014-21-1-2578]; Advent Health Endowed Chair in Healthcare Simulation
FX This work was supported in part by National Science Foundation under
   Grant 1800961 (Dr. Ephraim P. Glinert, IIS) in part by the Office of
   Naval Research under Grant N00014-21-1-2578 (Dr. Peter Squire, Code 34),
   and in part by Advent Health Endowed Chair in Healthcare Simulation
   (Prof. Welch).
NR 55
TC 0
Z9 0
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 4936
EP 4950
DI 10.1109/TVCG.2022.3195111
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300009
PM 35905060
DA 2025-03-07
ER

PT J
AU Liu, SG
   Hao, JQ
AF Liu, Shiguang
   Hao, Jiaqi
TI Generating Talking Face With Controllable Eye Movements by Disentangled
   Blinking Feature
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Faces; Videos; Mouth; Training; Three-dimensional displays; Feature
   extraction; Decoding; Talking face generation; eye blink generation;
   blink feature; virtual character
ID QUALITY
AB In virtual reality, talking face generation is committed to using voice and face images to generate real face speech videos to improve the communication experience in the case of limited user information exchange. In a real video, blinking is an action often accompanied by speech, and it is also one of the indispensable actions in real face speech videos. However, the current methods either do not pay attention to the generation of eye movements, or cannot control the blinking in the generated results. To this end, this article proposes a novel system which produces vivid talking face with controllable eye blinks driven by the joint features including identity feature, audio feature, and blink feature. In order to disentangle the blinking action, we designed three independent features to individually drive the main components in the generated frame, namely the facial appearance, mouth movements, and eye movements. Through the adversarial training of the identity encoder, we filter out the information of the eye state from the identity feature, thereby strengthening the independence of the blinking feature. We introduced the blink score as the leading information of the blink feature, and through training, the value can be consistent with human perception to form a complete and independent control of the eyes. Experimental results on multiple datasets show that our method can not only reproduce real talking faces, but also ensure that the blinking pattern and time are fully controllable.
C1 [Liu, Shiguang; Hao, Jiaqi] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
C3 Tianjin University
RP Liu, SG (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
EM lsg@tju.edu.cn; 2019216063@tju.edu.cn
FU Natural Science Foundation of China [62072328]
FX This work was supported by the Natural Science Foundation of China under
   Grant 62072328.
NR 55
TC 2
Z9 3
U1 3
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5050
EP 5061
DI 10.1109/TVCG.2022.3199412
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300016
PM 35976839
DA 2025-03-07
ER

PT J
AU Reda, K
AF Reda, Khairi
TI Rainbow Colormaps: What are They <i>Good</i> and <i>Bad</i> for?
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image color analysis; Task analysis; Encoding; Costs; Observers;
   Standards; Sensitivity; Quantitative color encoding; rainbow colormaps;
   scalar fields; perception
ID RUSSIAN BLUES; INFERENCE; MAPS; VISUALIZATION; GUIDELINES; SCHEMES
AB Guidelines for color use in quantitative visualizations have strongly discouraged the use of rainbow colormaps, arguing instead for smooth designs that do not induce visual discontinuities or implicit color categories. However, the empirical evidence behind this argument has been mixed and, at times, even contradictory. In practice, rainbow colormaps are widely used, raising questions about the true utility or dangers of such designs. We study how color categorization impacts the interpretation of scalar fields. We first introduce an approach to detect latent categories in colormaps. We hypothesize that the appearance of color categories in scalar visualizations can be beneficial in that they enhance the perception of certain features, although at the cost of rendering other features less noticeable. In three crowdsourced experiments, we show that observers are more likely to discriminate global, distributional features when viewing colorful scales that induce categorization (e.g., rainbow or diverging schemes). Conversely, when seeing the same data through a less colorful representation, observers are more likely to report localized features defined by small variations in the data. Participants showed awareness of these different affordances, and exhibited bias for exploiting the more discriminating colormap, given a particular feature type. Our results demonstrate costs and benefits for rainbows (and similarly colorful schemes), suggesting that their complementary utility for analyzing scalar data should not be dismissed. In addition to explaining potentially valid uses of rainbow, our study provides actionable guidelines, including on when such designs can be more harmful than useful. Data and materials are available at https://osf.io/xjhtf
C1 [Reda, Khairi] Indiana Univ Purdue Univ, Indianapolis, IN 46208 USA.
C3 Purdue University System; Purdue University
RP Reda, K (corresponding author), Indiana Univ Purdue Univ, Indianapolis, IN 46208 USA.
EM redak@iu.edu
OI Reda, Khairi/0000-0002-8096-658X
FU National Science Foundation [1942429]
FX This work was supported by the National Science Foundation under Grant
   1942429.
NR 55
TC 2
Z9 2
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5496
EP 5510
DI 10.1109/TVCG.2022.3214771
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300047
PM 36240035
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Shao, H
   Huang, LB
   Michels, DL
AF Shao, Han
   Huang, Libo
   Michels, Dominik L.
TI A Current Loop Model for the Fast Simulation of Ferrofluids
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computational modeling; Mathematical models; Force; Numerical models;
   Surface tension; Magnetomechanical effects; Magnetic forces;
   Computational electromagnetics; divergence-free SPH (DFSPH);
   ferrofluids; fluid mechanics; implicit incompressible SPH (IISPH);
   large-scale simulations; magnetic fluids; maxwell's equations; natural
   phenomena; navier-stokes equations; numerical simulations; smoothed
   particle hydrodynamics (SPH)
ID MAGNETIC FLUID
AB Ferrofluids are oil-based liquids containing magnetic particles that interact with magnetic fields without solidifying. Leveraging the exploration of new applications of these promising materials (such as in optics, medicine and engineering) requires high fidelity modeling and simulation capabilities in order to accurately explore ferrofluids in silico. While recent work addressed themacroscopic simulation of large-scale ferrofluids using smoothed-particle hydrodynamics (SPH), such simulations are computationally expensive. In their work, the Kelvin force model has been used to calculate interactions between different SPH particles. The application of this model results in a force pointing outwards with respect to the fluid surface causing significant levitation problems. This drawback limits the application of more advanced and efficient SPH frameworks such as divergence-free SPH(DFSPH) or implicit incompressible SPH(IISPH). In this contribution, we propose a current loopmagnetic force model which enables the fast macroscopic simulation of ferrofluids. Our new force model results in a force term pointing inwards allowing for more stable and fast simulations of ferrofluids using DFSPH and IISPH.
C1 [Shao, Han; Huang, Libo; Michels, Dominik L.] KAUST, Computat Sci Grp, Visual Comp Ctr, Thuwal 23955, Saudi Arabia.
C3 King Abdullah University of Science & Technology
RP Michels, DL (corresponding author), KAUST, Computat Sci Grp, Visual Comp Ctr, Thuwal 23955, Saudi Arabia.
EM han.shao@kaust.edu.sa; libo.huang@kaust.edu.sa;
   dominik.michels@kaust.edu.sa
FU KAUST through the baseline funding of the Computational Sciences Group
   within the Visual Computing Center
FX The authors would like to thank Jan Bender for publishing the DFSPH
   framework open-source. This work was supported and funded in part by
   KAUST through the baseline funding of the Computational Sciences Group
   within the Visual Computing Center.
NR 58
TC 4
Z9 4
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5394
EP 5405
DI 10.1109/TVCG.2022.3211414
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300040
PM 36191100
OA hybrid, Green Submitted
DA 2025-03-07
ER

PT J
AU Yang, FM
   Tompkin, J
   Harrison, L
   Laidlaw, DH
AF Yang, Fumeng
   Tompkin, James
   Harrison, Lane
   Laidlaw, David H.
TI Visual Cue Effects on a Classification Accuracy Estimation Task in
   Immersive Scatterplots
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Task analysis; Data visualization; Three-dimensional
   displays; Neural networks; Monitoring; Training; Virtual reality;
   cluster perception; information visualization; immersive analytics;
   dimension reduction; classification
ID ECOLOGICAL VALIDITY; BINOCULAR VISION; 3D SHAPE; PERFORMANCE;
   PERCEPTION; VISUALIZATION; INFORMATION; DEPTH; DISPLAY; STEREO
AB Immersive visualization in virtual reality (VR) allows us to exploit visual cues for perception in 3D space, yet few existing studies have measured the effects of visual cues. Across a desktop monitor and a head-mounted display (HMD), we assessed scatterplot designs which vary their use of visual cues-motion, shading, perspective (graphical projection), and dimensionality-on two sets of data. We conducted a user study with a summary task in which 32 participants estimated the classification accuracy of an artificial neural network from the scatterplots. With Bayesian multilevel modeling, we capture the intricate visual effects and find that no cue alone explains all the variance in estimation error. Visual motion cues generally reduce participants' estimation error; besides this motion, using other cues may increase participants' estimation error. Using an HMD, adding visual motion cues, providing a third data dimension, or showing a more complicated dataset leads to longer response times. We speculate that most visual cues may not strongly affect perception in immersive analytics unless they change people's mental model about data. In summary, by studying participants as they interpret the output from a complicated machine learning model, we advance our understanding of how to use the visual cues in immersive analytics.
C1 [Yang, Fumeng] Northwestern Univ, Dept Comp Sci, Evanston, IL 60208 USA.
   [Tompkin, James; Laidlaw, David H.] Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
   [Harrison, Lane] Worcester Polytech Inst, Dept Comp Sci, Worcester, MA 01609 USA.
C3 Northwestern University; Brown University; Worcester Polytechnic
   Institute
RP Laidlaw, DH (corresponding author), Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
EM fy@northwestern.edu; james_tompkin@brown.edu; ltharrison@wpi.edu;
   dhl@cs.brown.edu
RI Yang, Fumeng/HME-2828-2023
OI Tompkin, James/0000-0003-2218-2899; Laidlaw, David/0000-0002-3411-7376;
   Harrison, Lane/0000-0003-3029-2799
FU NSF [IIS-2107409, 2127309]
FX This work was supported in part by hardware donations from NVIDIA, in
   part by NSF under Grant IIS-2107409, and in part by the NSF under Grant
   2127309 to the Computing Research Association for the CIFellows Project.
   This work involved human subjects or animals in its research. Approval
   of all ethical and experimental procedures and protocols was granted by
   the Brown University Human Research Protection Program under Application
   No. #0005990214, and performed in line with the federal regulations (45
   CFR 46.110).
NR 128
TC 0
Z9 0
U1 5
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 4858
EP 4873
DI 10.1109/TVCG.2022.3192364
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300004
PM 35857736
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Lee, SU
   Kim, J
   Lee, J
AF Lee, Seung Un
   Kim, Jinwook
   Lee, Jeongmi
TI Effects of Reward Schedule and Avatar Visibility on Joint Agency During
   VR Collaboration
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 17th International Conference on High Speed Machining (HSM)
CY OCT 25-28, 2023
CL Nanjing, PEOPLES R CHINA
DE Joint Agency; Performance; Reward Schedule; Avatar Visibility; Virtual
   Reality; Collaboration
ID SENSE; STRENGTHENS; FEEDBACK
AB Joint agency, a group-level sense of agency, has been studied as an essential social cognitive element while engaging in collaborative tasks. The joint agency has been actively investigated in diverse contexts (e.g., performance, reward schedules, and predictability), yet the studies were mostly conducted in traditional 2D computer environments. Since virtual reality (VR) is an emerging technology for remote collaboration, we aimed to probe the effects of traditional reward schedule factors along with novel VR features (i.e., avatar visibility) on joint agency during remote collaboration. In this study, we implemented an experiment based on a card-matching game to test the effects of the reward schedule (fair or equal) and the counterpart's avatar hand visibility (absent or present) on the sense of joint agency. The results showed that participants felt a higher sense of joint agency when the reward was distributed equally regardless of the individual performance and when the counterpart's avatar hand was present. Moreover, the effects of reward schedule and avatar hand visibility interacted, with a bigger amount of deficit for the absent avatar hand when the reward was distributed differentially according to performance. Interestingly, the sense of joint agency was strongly correlated to the level of collaborative performance, as well as to perceptions of other social cognitive factors, including cooperativeness, reward fairness, and social presence. These results contribute to the understanding of joint agency perceptions during VR collaboration and provide design guidelines for remote collaborative tasks and environments for users' optimal social experience and performance.
C1 [Lee, Seung Un; Kim, Jinwook; Lee, Jeongmi] Korea Adv Inst Sci & Technol, Daejeon, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Lee, J (corresponding author), Korea Adv Inst Sci & Technol, Daejeon, South Korea.
EM seungun.lee@kaist.ac.kr; jinwook.kim31@kaist.ac.kr; jeongmi@kaist.ac.kr
RI Lee, Jeongmi/D-1912-2018
OI Lee, Jeongmi/0000-0002-3403-8117; Kim, Jinwook/0000-0002-1962-5815
FU National Research Council of Science and Technology (NST) [CRC21011];
   National Research Foundation of Korea (NRF) - Ministry of Science and
   ICT (MSIT), Republic of Korea [2022R1A4A5033689]
FX This work was supported by a grant from the National Research Council of
   Science and Technology (NST) (CRC21011) and the National Research
   Foundation of Korea (NRF) (2022R1A4A5033689) funded by the Ministry of
   Science and ICT (MSIT), Republic of Korea.
NR 50
TC 0
Z9 0
U1 8
U2 29
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2023
VL 29
IS 11
BP 4372
EP 4382
DI 10.1109/TVCG.2023.3320221
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA X6ZW5
UT WOS:001099919100006
PM 37788201
DA 2025-03-07
ER

PT J
AU Cornel, D
   Zechmeister, S
   Gröller, E
   Waser, J
AF Cornel, Daniel
   Zechmeister, Silvana
   Groeller, Eduard
   Waser, Jurgen
TI Watertight Incremental Heightfield Tessellation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization techniques and methodologies; heightfield rendering;
   terrain rendering; level of detail; tessellation
ID TERRAIN; INTERPOLATION
AB In this article, we propose a method for the interactive visualization of medium-scale dynamic heightfields without visual artifacts. Our data fall into a category too large to be rendered directly at full resolution, but small enough to fit into GPU memory without pre-filtering and data streaming. We present the real-world use case of unfiltered flood simulation data of such medium scale that need to be visualized in real time for scientific purposes. Our solution facilitates compute shaders to maintain a guaranteed watertight triangulation in GPU memory that approximates the interpolated heightfields with view-dependent, continuous levels of detail. In each frame, the triangulation is updated incrementally by iteratively refining the cached result of the previous frame to minimize the computational effort. In particular, we minimize the number of heightfield sampling operations to make adaptive and higher-order interpolations viable options. We impose no restriction on the number of subdivisions and the achievable level of detail to allow for extreme zoom ranges required in geospatial visualization. Our method provides a stable runtime performance and can be executed with a limited time budget. We present a comparison of our method to three state-of-the-art methods, in which our method is competitive to previous non-watertight methods in terms of runtime, while outperforming them in terms of accuracy.
C1 [Cornel, Daniel; Zechmeister, Silvana; Waser, Jurgen] VRVis Forsch GmbH, A-1220 Vienna, Austria.
   [Groeller, Eduard] TU Wien, A-1040 Vienna, Austria.
C3 Technische Universitat Wien
RP Cornel, D (corresponding author), VRVis Forsch GmbH, A-1220 Vienna, Austria.
EM cornel@vrvis.at; zechmeister@vrvis.at; groeller@cg.tuwien.ac.at;
   jwaser@vrvis.at
OI Cornel, Daniel/0000-0002-2481-6720; Rauer-Zechmeister,
   Silvana/0000-0002-4715-0351
FU BMK; BMDW; Styria; SFG; Tyrol and Vienna Business Agency in the scope of
   COMET - Competence Centers for Excellent Technologies [879730]
FX This work was supported by BMK, BMDW, Styria, SFG, Tyrol and Vienna
   Business Agency in the scope of COMET - Competence Centers for Excellent
   Technologies under Grant 879730 which is managed by FFG.& nbsp;
NR 34
TC 1
Z9 1
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2023
VL 29
IS 9
BP 3888
EP 3899
DI 10.1109/TVCG.2022.3173081
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA O2BA8
UT WOS:001041912300012
PM 35522629
DA 2025-03-07
ER

PT J
AU Chen, W
   Wei, YT
   Wang, ZY
   Zhou, SY
   Lin, BR
   Zhou, ZG
AF Chen, Wei
   Wei, Yating
   Wang, Zhiyong
   Zhou, Shuyue
   Lin, Bingru
   Zhou, Zhiguang
TI Federated Visualization: A Privacy-Preserving Strategy for Aggregated
   Visual Query
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Data privacy; Privacy; Data models;
   Federated learning; Servers; Decentralized visualization; federated
   visualization; privacy-preserving visualization
ID MODEL
AB We present a novel privacy preservation strategy for aggregated visual query of decentralized data. The key idea is to imitate the flowchart of the federated learning framework, and reformulate the visualization process within a federated infrastructure. The federation of visualization is fulfilled by leveraging a shared global module that composes the encrypted externalizations of transformed visual features of data pieces in local modules. We design two implementations of federated visualization: a prediction-based scheme, and a query-based scheme. We demonstrate the effectiveness of our approach with a set of visual forms, and verify its robustness with evaluations. We report the value of federated visualization in real scenarios with an expert review.
C1 [Chen, Wei; Wei, Yating] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [Wei, Yating] Zhejiang Met & Mat Co, Hangzhou 310003, Zhejiang, Peoples R China.
   [Wang, Zhiyong] Tencent, Guangzhou 510630, Peoples R China.
   [Zhou, Shuyue; Lin, Bingru] Alibaba Grp, Hangzhou 310058, Zhejiang, Peoples R China.
   [Zhou, Zhiguang] Hangzhou Dianzi Univ, Hangzhou 310018, Zhejiang, Peoples R China.
C3 Zhejiang University; Tencent; Alibaba Group; Hangzhou Dianzi University
RP Chen, W (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM chenvis@zju.edu.cn; weiyating@zju.edu.cn; zerowangzy@zju.edu.cn;
   tangmao.zsy@alibaba-inc.com; bingru.lbr@alibaba-inc.com;
   zhgzhou1983@163.com
RI Chen, Wei/AAR-9817-2020
OI Chen, Wei/0000-0002-8365-4741
FU National Natural Science Foundation of China [62132017, 62277013];
   Fundamental Research Funds for the Central Universities [226-2022-00235]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62132017 and 62277013, and in part by
   the Fundamental Research Funds for the Central Universities under Grant
   226-2022-00235.
NR 49
TC 0
Z9 0
U1 1
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2023
VL 29
IS 6
BP 2901
EP 2913
DI 10.1109/TVCG.2023.3261938
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F4DZ2
UT WOS:000981880500006
PM 37030803
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Jin, ZH
   Wang, Y
   Wang, QW
   Ming, Y
   Ma, TF
   Qu, HM
AF Jin, Zhihua
   Wang, Yong
   Wang, Qianwen
   Ming, Yao
   Ma, Tengfei
   Qu, Huamin
TI GNNLens: A Visual Analytics Approach for Prediction Error Diagnosis of
   Graph Neural Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Analytical models; Deep learning; Predictive models; Visual analytics;
   Data models; Convolutional neural networks; Task analysis; Graph neural
   networks; error diagnosis; visualization
AB Graph Neural Networks (GNNs) aim to extend deep learning techniques to graph data and have achieved significant progress in graph analysis tasks (e.g., node classification) in recent years. However, similar to other deep neural networks like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), GNNs behave like a black box with their details hidden from model developers and users. It is therefore difficult to diagnose possible errors of GNNs. Despite many visual analytics studies being done on CNNs and RNNs, little research has addressed the challenges for GNNs. This paper fills the research gap with an interactive visual analysis tool, GNNLens, to assist model developers and users in understanding and analyzing GNNs. Specifically, Parallel Sets View and Projection View enable users to quickly identify and validate error patterns in the set of wrong predictions; Graph View and Feature Matrix View offer a detailed analysis of individual nodes to assist users in forming hypotheses about the error patterns. Since GNNs jointly model the graph structure and the node features, we reveal the relative influences of the two types of information by comparing the predictions of three models: GNN, Multi-Layer Perceptron (MLP), and GNN Without Using Features (GNNWUF). Two case studies and interviews with domain experts demonstrate the effectiveness of GNNLens in facilitating the understanding of GNN models and their errors.
C1 [Jin, Zhihua; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Wang, Yong] Singapore Management Univ, Sch Comp & Informat Syst, Singapore 188065, Singapore.
   [Wang, Qianwen] Harvard Univ, Cambridge, MA 02140 USA.
   [Ming, Yao] Bloomberg LP, New York, NY 10022 USA.
   [Ma, Tengfei] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
C3 Hong Kong University of Science & Technology; Singapore Management
   University; Harvard University; Bloomberg L.P.; International Business
   Machines (IBM); IBM USA
RP Jin, ZH (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
EM zjinak@connect.ust.hk; yongwang@smu.edu.sg;
   qianwen_wang@hms.harvard.edu; ymingaa@connect.ust.hk;
   tengfei.ma1@ibm.com; huamin@cse.ust.hk
RI Wang, Qianwen/GRJ-9435-2022; Ma, Tengfei/L-6178-2018; Wang,
   Yong/HKF-3903-2023
OI Wang, Yong/0000-0002-0092-0793
FU Hong Kong Theme-based Research Scheme [T41-709/17N]
FX This work was supported in part by Hong Kong Theme-based Research Scheme
   under Grant T41-709/17N.
NR 67
TC 15
Z9 16
U1 1
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2023
VL 29
IS 6
BP 3024
EP 3038
DI 10.1109/TVCG.2022.3148107
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F4DZ2
UT WOS:000981880500015
PM 35120004
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Li, H
   Zhai, HJ
   Yang, XR
   Wu, ZR
   Zheng, YH
   Wang, HF
   Wu, JC
   Bao, HJ
   Zhang, GF
AF Li, Hai
   Zhai, Hongjia
   Yang, Xingrui
   Wu, Zhirong
   Zheng, Yihao
   Wang, Haofan
   Wu, Jianchao
   Bao, Hujun
   Zhang, Guofeng
TI ImTooth: Neural Implicit Tooth for Dental Augmented Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Artificial intelligence; Neural implicit representation; Dental Mixed /
   Augmented reality
ID VIRTUAL-REALITY
AB The combination of augmented reality (AR) and medicine is an important trend in current research. The powerful display and interaction capabilities of the AR system can assist doctors to perform more complex operations. Since the tooth itself is an exposed rigid body structure, dental AR is a relatively hot research direction with application potential. However, none of the existing dental AR solutions are designed for wearable AR devices such as AR glasses. At the same time, these methods rely on high-precision scanning equipment or auxiliary positioning markers, which greatly increases the operational complexity and cost of clinical AR. In this work, we propose a simple and accurate neural-implicit model-driven dental AR system, named ImTooth, and adapted for AR glasses. Based on the modeling capabilities and differentiable optimization properties of state-of-the-art neural implicit representations, our system fuses reconstruction and registration in a single network, greatly simplifying the existing dental AR solutions and enabling reconstruction, registration, and interaction. Specifically, our method learns a scale-preserving voxel-based neural implicit model from multi-view images captured from a textureless plaster model of the tooth. Apart from color and surface, we also learn the consistent edge feature inside our representation. By leveraging the depth and edge information, our system can register the model to real images without additional training. In practice, our system uses a single Microsoft HoloLens 2 as the only sensor and display device. Experiments show that our method can reconstruct high-precision models and accomplish accurate registration. It is also robust to weak, repeating and inconsistent textures. We also show that our system can be easily integrated into dental diagnostic and therapeutic procedures, such as bracket placement guidance.
C1 [Li, Hai; Zhai, Hongjia; Bao, Hujun; Zhang, Guofeng] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Peoples R China.
   [Yang, Xingrui] China Aerodynam Res & Dev Ctr, High Speed Aerodynam Inst, Mianyang, Peoples R China.
   [Wu, Zhirong; Zheng, Yihao; Wang, Haofan] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Peoples R China.
   [Wu, Jianchao] Hangzhou Mingzhou Hosp, Dept Stomatol, Hangzhou, Peoples R China.
   [Wu, Jianchao] Taizhou Stomatol Hosp, Taizhou, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Zhang, GF (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Peoples R China.; Wu, JC (corresponding author), Hangzhou Mingzhou Hosp, Dept Stomatol, Hangzhou, Peoples R China.; Wu, JC (corresponding author), Taizhou Stomatol Hosp, Taizhou, Peoples R China.
EM garyli@zju.edu.cn; zhj1999@zju.edu.cn; xingruiy@gmail.com;
   zhirongwu@zju.edu.cn; zhengyihao@zju.edu.cn; wanghaofan@zju.edu.cn;
   wujianchao555@163.com; baohujun@zju.edu.cn; zhangguofeng@zju.edu.cn
RI Zhang, Ge/K-9118-2019; Zheng, Yihao/J-5070-2019
OI Li, Hai/0000-0002-5114-6566; Zheng, Yihao/0000-0002-0346-3006; Zhang,
   Guofeng/0000-0001-5661-8430; Yang, Xingrui/0000-0001-6812-3072
FU NSF of China [61932003]
FX This work was partially supported by NSF of China (No.61932003).
NR 65
TC 3
Z9 3
U1 3
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2837
EP 2846
DI 10.1109/TVCG.2023.3247459
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D2HD7
UT WOS:000966975000001
PM 37027614
DA 2025-03-07
ER

PT J
AU Schott, D
   Kunz, M
   Wunderling, T
   Heinrich, F
   Braun-Dullaeus, R
   Hansen, C
AF Schott, Danny
   Kunz, Matthias
   Wunderling, Tom
   Heinrich, Florian
   Braun-Dullaeus, Ruediger
   Hansen, Christian
TI CardioGenesis4D: Interactive Morphological Transitions of Embryonic
   Heart Development in a Virtual Learning Environment
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Education; Heart; Embryo; Solid modeling;
   Virtual reality; Visualization; Virtual Reality; Immersive Learning
   Environment; Embryonic Heart Development; Anatomy Education
ID ANATOMY; MULTIMEDIA
AB In the embryonic human heart, complex dynamic shape changes take place in a short period of time on a microscopic scale, making this development difficult to visualize. However, spatial understanding of these processes is essential for students and future cardiologists to properly diagnose and treat congenital heart defects. Following a user centered approach, the most crucial embryological stages were identified and translated into a virtual reality learning environment (VRLE) to enable the understanding of the morphological transitions of these stages through advanced interactions. To address individual learning types, we implemented different features and evaluated the application regarding usability, perceived task load, and sense of presence in a user study. We also assessed spatial awareness and knowledge gain, and finally obtained feedback from domain experts. Overall, students and professionals rated the application positively. To minimize distraction from interactive learning content, such VRLEs should consider features for different learning types, allow for gradual habituation, and at the same time provide enough playful stimuli. Our work previews how VR can be integrated into a cardiac embryology education curriculum.
C1 [Schott, Danny; Wunderling, Tom; Hansen, Christian] Univ Magdeburg, Fac Comp Sci, Magdeburg, Germany.
   [Kunz, Matthias; Braun-Dullaeus, Ruediger] Univ Magdeburg, Clin Cardiol & Angiol, Magdeburg, Germany.
   [Heinrich, Florian] Univ Wurzburg, Human Comp Interact HCI Grp, Wurzburg, Germany.
C3 Otto von Guericke University; Otto von Guericke University; University
   of Wurzburg
RP Schott, D (corresponding author), Univ Magdeburg, Fac Comp Sci, Magdeburg, Germany.
EM danny.schott@ovgu.de; matthias.kunz@med.ovgu.de;
   florian.heinrich@uni-wuerzburg.de; r.braun-dullaeus@med.ovgu.de;
   hansen@isg.cs.uni-magdeburg.de
OI Hansen, Christian/0000-0002-5734-7529; Schott,
   Danny/0000-0002-2576-7799; Kunz, Matthias/0000-0002-0439-2254; Heinrich,
   Florian/0000-0002-8169-3157; Wunderling, Tom/0000-0002-8862-3549
FU German Federal Ministry for Economic Affairs and Climate Action
   [16KN093942]
FX This work was partially funded by the German Federal Ministry for
   Economic Affairs and Climate Action under Grant 16KN093942.
NR 43
TC 6
Z9 6
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2615
EP 2625
DI 10.1109/TVCG.2023.3247110
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D0XL8
UT WOS:000966040400001
PM 37027713
OA hybrid
DA 2025-03-07
ER

PT J
AU Weissker, T
   Bimberg, P
   Gokhale, AS
   Kuhlen, T
   Froehlich, B
AF Weissker, Tim
   Bimberg, Pauline
   Gokhale, Aalok Shashidhar
   Kuhlen, Torsten
   Froehlich, Bernd
TI Gaining the High Ground: Teleportation to Mid-Air Targets in Immersive
   Virtual Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual Reality; 3D User Interfaces; 3D Navigation; Head-Mounted
   Display; Flying; Mid-Air Navigation
AB Most prior teleportation techniques in virtual reality are bound to target positions in the vicinity of selectable scene objects. In this paper, we present three adaptations of the classic teleportation metaphor that enable the user to travel to mid-air targets as well. Inspired by related work on the combination of teleports with virtual rotations, our three techniques differ in the extent to which elevation changes are integrated into the conventional target selection process. Elevation can be specified either simultaneously, as a connected second step, or separately from horizontal movements. A user study with 30 participants indicated a trade-off between the simultaneous method leading to the highest accuracy and the two-step method inducing the lowest task load as well as receiving the highest usability ratings. The separate method was least suitable on its own but could serve as a complement to one of the other approaches. Based on these findings and previous research, we define initial design guidelines for mid-air navigation techniques.
C1 [Weissker, Tim; Kuhlen, Torsten] Rhein Westfal TH Aachen, Visual Comp Inst, Aachen, Germany.
   [Bimberg, Pauline] Univ Trier, Human Comp Interact Grp, Trier, Germany.
   [Gokhale, Aalok Shashidhar; Froehlich, Bernd] Bauhaus Univ Weimar, Virtual Real & Visualizat Res Grp, Weimar, Germany.
C3 RWTH Aachen University; Universitat Trier; Bauhaus-Universitat Weimar
RP Weissker, T (corresponding author), Rhein Westfal TH Aachen, Visual Comp Inst, Aachen, Germany.
EM me@tim-weissker.de; bimberg@uni-trier.de; aaloksg@gmail.com;
   kuhlen@vr.rwth-aachen.de; bernd.froehlich@uni-weimar.de
RI ; Kuhlen, Torsten/A-1059-2017
OI Froehlich, Bernd/0000-0002-9439-1959; Kuhlen,
   Torsten/0000-0003-2144-4367; Weissker, Tim/0000-0001-9119-811X
FU Ministry of Economic Affairs, Industry, Climate Action and Energy of the
   State of North Rhine-Westphalia [005-2108-0055]; German Ministry of
   Education and Research (BMBF) [16SV8716]; Thuringian Ministry of
   Economic Affairs, Science and Digital Society (TMWDDG) [5575/10-5]
FX We would like to thank Daniel Ruppfor his diligent assistance in
   conducting a large part of the user studies for this paper. This work
   has mainly received funding from the Ministry of Economic Affairs,
   Industry, Climate Action and Energy of the State of North
   Rhine-Westphalia under grant 005-2108-0055 (projectVITAMINE_5G). This
   work was also partially funded by the German Ministry of Education and
   Research (BMBF) under grant 16SV8716 (projectGoethe-Live-3D)and the
   Thuringian Ministry of Economic Affairs, Science and Digital Society
   (TMWDDG) under grant 5575/10-5 (project MetaReal)
NR 47
TC 7
Z9 7
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2467
EP 2477
DI 10.1109/TVCG.2023.3247114
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D0KN8
UT WOS:000965703400001
PM 37027708
DA 2025-03-07
ER

PT J
AU Sridharamurthy, R
   Natarajan, V
AF Sridharamurthy, Raghavendra
   Natarajan, Vijay
TI Comparative Analysis of Merge Trees Using Local Tree Edit Distance
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Merge tree; scalar field; local distance measure; persistence; edit
   distance; symmetry detection; feature tracking
ID SCALAR FIELDS; REEB GRAPHS; SYMMETRY; PERSISTENCE; STABILITY
AB Comparative analysis of scalar fields is an important problem with various applications including feature-directed visualization and feature tracking in time-varying data. Comparing topological structures that are abstract and succinct representations of the scalar fields lead to faster and meaningful comparison. While there are many distance or similarity measures to compare topological structures in a global context, there are no known measures for comparing topological structures locally. While the global measures have many applications, they do not directly lend themselves to fine-grained analysis across multiple scales. We define a local variant of the tree edit distance and apply it towards local comparative analysis of merge trees with support for finer analysis. We also present experimental results on time-varying scalar fields, 3D cryo-electron microscopy data, and other synthetic data sets to show the utility of this approach in applications like symmetry detection and feature tracking.
C1 [Sridharamurthy, Raghavendra; Natarajan, Vijay] Indian Inst Sci, Dept Comp Sci & Automation, Bangalore 560012, Karnataka, India.
C3 Indian Institute of Science (IISC) - Bangalore
RP Sridharamurthy, R (corresponding author), Indian Inst Sci, Dept Comp Sci & Automation, Bangalore 560012, Karnataka, India.
EM raghavendrag@iisc.ac.in; vijayn@iisc.ac.in
RI Sridharamurthy, Raghavendra/JNS-6719-2023
OI Sridharamurthy, Raghavendra/0000-0001-8463-0488; Natarajan,
   Vijay/0000-0002-7956-1470
FU Department of Science and Technology, India [DST/SJF/ETA-02/2015-16];
   MHRD, Government of India; Mindtree Chair Research Grant
FX This work was supported in part by a Swarnajayanti Fellowship from the
   Department of Science and Technology, India under Grant
   DST/SJF/ETA-02/2015-16, in part by, a scholarship from MHRD, Government
   of India and in part by a Mindtree Chair Research Grant.
NR 32
TC 4
Z9 4
U1 4
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2023
VL 29
IS 2
BP 1518
EP 1530
DI 10.1109/TVCG.2021.3122176
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7M2HO
UT WOS:000906475100018
PM 34699362
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Wei, MQ
   Chen, HH
   Zhang, YK
   Xie, HR
   Guo, YW
   Wang, J
AF Wei, Mingqiang
   Chen, Honghua
   Zhang, Yingkui
   Xie, Haoran
   Guo, Yanwen
   Wang, Jun
TI GeoDualCNN: Geometry-Supporting Dual Convolutional Neural Network for
   Noisy Point Clouds
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE GeoDualCNN; normal estimation; point cloud denoising; geometry domain
   knowledge; neural network
ID ROBUST NORMAL ESTIMATION; SURFACE RECONSTRUCTION; SHAPE; 3D
AB We propose a geometry-supporting dual convolutional neural network (GeoDualCNN) for both point cloud normal estimation and denoising. GeoDualCNN fuses the geometry domain knowledge that the underlying surface of a noisy point cloud is piecewisely smooth with the fact that a point normal is properly defined only when local surface smoothness is guaranteed. Centered around this insight, we define the homogeneous neighborhood (HoNe) which stays clear of surface discontinuities, and associate each HoNe with a point whose geometry and normal orientation is mostly consistent with that of HoNe. Thus, we not only obtain initial estimates of the point normals by performing PCA on HoNes, but also for the first time optimize these initial point normals by learning the mapping from two proposed geometric descriptors to the ground-truth point normals. GeoDualCNN consists of two parallel branches that remove noise using the first geometric descriptor (a homogeneous height map, which encodes the point-position information), while preserving surface features using the second geometric descriptor (a homogeneous normal map, which encodes the point-normal information). Such geometry-supporting network architectures enable our model to leverage previous geometry expertise and to benefit from training data. Experiments with noisy point clouds show that GeoDualCNN outperforms the state-of-the-art methods in terms of both noise-robustness and feature preservation.
C1 [Wei, Mingqiang] Nanjing Univ Aeronaut & Astronaut, Shenzhen Res Inst, Sch Comp Sci & Technol, Nanjing 210016, Peoples R China.
   [Chen, Honghua; Wang, Jun] Nanjing Univ Aeronaut & Astronaut, Sch Mech & Elect Engn, Nanjing 210016, Peoples R China.
   [Zhang, Yingkui] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen 518055, Peoples R China.
   [Xie, Haoran] Lingnan Univ, Dept Comp & Decis Sci, Hong Kong, Peoples R China.
   [Guo, Yanwen] Nanjing Univ, Dept Comp Sci & Technol, Nanjing 210093, Peoples R China.
C3 Nanjing University of Aeronautics & Astronautics; Nanjing University of
   Aeronautics & Astronautics; Chinese Academy of Sciences; Shenzhen
   Institute of Advanced Technology, CAS; Lingnan University; Nanjing
   University
RP Xie, HR (corresponding author), Lingnan Univ, Dept Comp & Decis Sci, Hong Kong, Peoples R China.; Guo, YW (corresponding author), Nanjing Univ, Dept Comp Sci & Technol, Nanjing 210093, Peoples R China.
EM mingqiang.wei@gmail.com; chenhonghuacn@gmail.com; yk.zhang1@siat.ac.cn;
   hrxie2@gmail.com; ywguo@nju.edu.cn; wjun@nuaa.edu.cn
RI YANG, LEI/GQH-4271-2022; Xie, Haoran/AFS-3515-2022
OI Xie, Haoran/0000-0003-0965-3617; Honghua, Chen/0000-0001-7473-1146
NR 71
TC 13
Z9 13
U1 5
U2 26
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2023
VL 29
IS 2
BP 1357
EP 1370
DI 10.1109/TVCG.2021.3113463
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7M2HO
UT WOS:000906475100006
PM 34546923
DA 2025-03-07
ER

PT J
AU Liu, KL
   Chen, DD
   Liao, J
   Zhang, WM
   Zhou, H
   Zhang, J
   Zhou, WB
   Yu, NH
AF Liu, Kunlin
   Chen, Dongdong
   Liao, Jing
   Zhang, Weiming
   Zhou, Hang
   Zhang, Jie
   Zhou, Wenbo
   Yu, Nenghai
TI JPEG Robust Invertible Grayscale
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Gray-scale; Transform coding; Image color analysis; Image restoration;
   Color; Training; Image coding; Invertible grayscale; adversarial
   training; JPEG robust
ID COLOR; CONVERSION; GREYSCALE; IMAGES
AB Invertible grayscale is a special kind of grayscale from which the original color can be recovered. Given an input color image, this seminal work tries to hide the color information into its grayscale counterpart while making it hard to recognize any anomalies. This powerful functionality is enabled by training a hiding sub-network and restoring sub-network in an end-to-end way. Despite its expressive results, two key limitations exist: 1) The restored color image often suffers from some noticeable visual artifacts in the smooth regions. 2) It is very sensitive to JPEG compression, i.e., the original color information cannot be well recovered once the intermediate grayscale image is compressed by JPEG. To overcome these two limitations, this article introduces adversarial training and JPEG simulator respectively. Specifically, two auxiliary adversarial networks are incorporated to make the intermediate grayscale images and final restored color images indistinguishable from normal grayscale and color images. And the JPEG simulator is utilized to simulate real JPEG compression during the online training so that the hiding and restoring sub-networks can automatically learn to be JPEG robust. Extensive experiments demonstrate that the proposed method is superior to the original invertible grayscale work both qualitatively and quantitatively while ensuring the JPEG robustness. We further show that the proposed framework can be applied under different types of grayscale constraints and achieve excellent results.
C1 [Liu, Kunlin; Zhang, Weiming; Zhou, Hang; Zhang, Jie; Zhou, Wenbo; Yu, Nenghai] Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230026, Anhui, Peoples R China.
   [Chen, Dongdong] Microsoft Res, Redmond, WA 98052 USA.
   [Liao, Jing] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Microsoft; City University of Hong Kong
RP Zhang, WM (corresponding author), Univ Sci & Technol China, Dept Elect Engn & Informat Sci, Hefei 230026, Anhui, Peoples R China.
EM lkl6949@cityu.edu.hk; cddlyf@gmail.com; jingliao@cityu.edu.hk;
   zhangwm@ustc.edu.cn; zh2991@cityu.edu.hk; zjzac@cityu.edu.hk;
   welbeckz@cityu.edu.hk; ynh@ustc.edu.cn
RI Zhang, Zhiyong/KPY-6346-2024; Zhou, Hang/AAI-5565-2021; Chen,
   Dongdong/AAR-4481-2020
OI Zhang, Jie/0000-0002-4230-1077; Zhou, Hang/0000-0001-7860-8452; Zhang,
   Weiming/0000-0001-5576-6108; Chen, Dongdong/0000-0002-4642-4373; LIAO,
   Jing/0000-0001-7014-5377
FU National Natural Science Foundation of China [62002334U20B2047,
   62072421]; Anhui Science Foundation of China [2008085QF296]; Anhui
   Initiative in Quantum Information Technologies [AHY15040]; Fundamental
   Research Funds for Central Universities of China [WK2100000018];
   Research Grants Council of the Hong Kong [CityU 21209119]; Natural
   Science Foundation of China [62072421, 62002334]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62002334U20B2047 and 62072421, in part
   by the Anhui Science Foundation of China under Grant 2008085QF296, in
   part by the Anhui Initiative in Quantum Information Technologies under
   Grant AHY15040, in part by the Fundamental Research Funds for Central
   Universities of China under Grant WK2100000018, in part by an ECS grant
   from the Research Grants Council of the Hong Kong (Project No. CityU
   21209119), and in part by the Natural Science Foundation of China
   (Project No. 62072421 and 62002334). Kunlin Liu and Dongdong Chen are
   co-first authors.
NR 53
TC 9
Z9 11
U1 1
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4403
EP 4417
DI 10.1109/TVCG.2021.3088531
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400032
PM 34115588
DA 2025-03-07
ER

PT J
AU Skarbez, R
   Gabbard, JL
   Bowman, DA
   Ogle, JT
   Tucker, T
AF Skarbez, Richard
   Gabbard, Joseph L.
   Bowman, Doug A.
   Ogle, J. Todd
   Tucker, Thomas
TI Virtual Replicas of Real Places: Experimental Investigations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Virtual environments; Human computer interaction;
   Coherence; Task analysis; Solid modeling; Lighting; Virtual reality;
   virtual environments; presence; psychophysics; user studies
ID ENHANCES REALISTIC RESPONSE; VISUAL REALISM; DISTANCE PERCEPTION;
   SIZE-CONSTANCY; ENVIRONMENTS; PLAUSIBILITY
AB As virtual reality (VR) technology becomes cheaper, higher-quality, and more widely available, it is seeing increasing use in a variety of applications including cultural heritage, real estate, and architecture. A common goal for all these applications is a compelling virtual recreation of a real place. Despite this, there has been very little research into how users perceive and experience such replicated spaces. This article reports the results from a series of three user studies investigating this topic. Results include that the scale of the room and large objects in it are most important for users to perceive the room as real and that non-physical behaviors such as objects floating in air are readily noticeable and have a negative effect even when the errors are small in scale.
C1 [Skarbez, Richard] La Trobe Univ, Melbourne, Vic 3086, Australia.
   [Gabbard, Joseph L.] Virginia Tech, Grad Dept Ind & Syst Engn, Blacksburg, VA 24061 USA.
   [Gabbard, Joseph L.; Bowman, Doug A.; Ogle, J. Todd; Tucker, Thomas] Virginia Tech, Ctr Human Comp Interact, Blacksburg, VA 24061 USA.
   [Bowman, Doug A.] Virginia Tech, Dept Comp Sci, Blacksburg, VA 24061 USA.
   [Ogle, J. Todd] Virginia Tech, Appl Res Immers Environm & Simulat, Blacksburg, VA 24061 USA.
   [Tucker, Thomas] Virginia Tech, Sch Visual Arts, Blacksburg, VA 24061 USA.
C3 La Trobe University; Virginia Polytechnic Institute & State University;
   Virginia Polytechnic Institute & State University; Virginia Polytechnic
   Institute & State University; Virginia Polytechnic Institute & State
   University; Virginia Polytechnic Institute & State University
RP Skarbez, R (corresponding author), La Trobe Univ, Melbourne, Vic 3086, Australia.
EM r.skarbez@latrobe.edu.au; jgabbard@vt.edu; bowman@vt.edu; jogle@vt.edu
RI Skarbez, Richard/S-7298-2019
OI Gabbard, Joseph/0000-0002-7488-676X; Ogle, Todd/0000-0003-2940-3594;
   Bowman, Doug/0000-0003-0491-5067; Skarbez, Richard/0000-0002-2783-5257
FU Facebook
FX The authors would like to thank Deborah Asabere, Shabi Mustafa, and
   Ahmed Salih for their considerable help in running these experiments.
   They would also like to thank their participants, without whom this work
   would not have been possible, and the anonymous reviewers, for their
   helpful feedback. This work was supported by a grant from Facebook.
NR 30
TC 5
Z9 5
U1 3
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4594
EP 4608
DI 10.1109/TVCG.2021.3096494
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400046
PM 34255629
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Chaudhary, AK
   Nair, N
   Bailey, RJ
   Pelz, JB
   Talathi, SS
   Diaz, GJ
AF Chaudhary, Aayush K.
   Nair, Nitinraj
   Bailey, Reynold J.
   Pelz, Jeff B.
   Talathi, Sachin S.
   Diaz, Gabriel J.
TI <i>Temporal RIT-Eyes</i>: From real infrared eye-images to synthetic
   sequences of gaze behavior
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 21st IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 17-22, 2022
CL Singapore, SINGAPORE
SP IEEE
DE Eye-tracking; image segmentation; synthetic dataset; temporal dataset;
   eye rendering; renderings for ML; AR/VR
AB Current methods for segmenting eye imagery into skin, sclera, pupil, and iris cannot leverage information about eye motion. This is because the datasets on which models are trained are limited to temporally non-contiguous frames. We present Temporal RIT-Eyes, a Blender pipeline that draws data from real eye videos for the rendering of synthetic imagery depicting natural gaze dynamics. These sequences are accompanied by ground-truth segmentation maps that may be used for training image-segmentation networks. Temporal RIT-Eyes relies on a novel method for the extraction of 3D eyelid pose (top and bottom apex of eyelids/eyeball boundary) from raw eye images for the rendering of gaze-dependent eyelid pose and blink behavior. The pipeline is parameterized to vary in appearance, eye/head/camera/illuminant geometry, and environment settings (indoor/outdoor). We present two open-source datasets of synthetic eye imagery: sGiW is a set of synthetic-image sequences whose dynamics are modeled on those of the Gaze in Wild dataset. and sOpenEDS2 is a series of temporally non-contiguous eye images that approximate the OpenEDS-2019 dataset. We also analyze and demonstrate the quality of the rendered dataset qualitatively and show significant overlap between latent-space representations of the source and the rendered datasets.
C1 [Chaudhary, Aayush K.; Nair, Nitinraj; Bailey, Reynold J.; Pelz, Jeff B.; Diaz, Gabriel J.] Rochester Inst Technol, Rochester, NY 14623 USA.
   [Talathi, Sachin S.] Real Res Labs, Menlo Pk, CA USA.
C3 Rochester Institute of Technology
RP Chaudhary, AK (corresponding author), Rochester Inst Technol, Rochester, NY 14623 USA.
EM akc5959@rit.edu; nrn2741@rit.edu; rjb@cs.rit.edu; pelz@cis.rit.edu;
   stalathi@fb.com; gabriel.diaz@rit.edu
RI Pelz, Jeff/A-8272-2009
OI Talathi, Sachin/0009-0004-4614-8534
FU Reality Research Labs (Meta)
FX The authors would like to acknowledge Artem Romanenko, Kade Kelsch,
   Chengyi Ma, Jackson Shuminski for helping with renderings in different
   phases of the project. The authors would also like to thank RC Research
   Computing [34]. The project was funded by Reality Research Labs (Meta).
NR 45
TC 3
Z9 4
U1 1
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2022
VL 28
IS 11
BP 3948
EP 3958
DI 10.1109/TVCG.2022.3203100
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 5M3ZS
UT WOS:000871038200037
PM 36044495
DA 2025-03-07
ER

PT J
AU Tong, J
   Wilcox, LM
   Allison, RS
AF Tong, Jonathan
   Wilcox, Laurie M.
   Allison, Robert S.
TI The impacts of lens and stereo camera separation on perceived slant in
   Virtual Reality head-mounted displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 21st IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 17-22, 2022
CL Singapore, SINGAPORE
SP IEEE
DE Virtual Reality; Interpupillary Distance; Depth Perception
ID ONE-HANDED CATCH; DEPTH; EXPOSURE
AB Stereoscopic AR and VR headsets have displays and lenses that are either fixed or adjustable to match a limited range of user inter-pupillary distances (IPDs). Projective geometry predicts a misperception of depth when either the displays or virtual cameras used to render images are misaligned with the eyes. However, misalignment between the eyes and lenses might also affect binocular convergence, which could further distort perceived depth. This possibility has been largely ignored in previous studies. Here, we evaluated this phenomenon in a VR headset in which the inter-lens and inter-axial camera separations are coupled and adjustable. In a baseline condition, both were matched to observers' IPDs. In two other conditions, the inter-lens and inter-axial camera separations were set to the maximum and minimum allowed by the headset. In each condition, observers were instructed to adjust a fold created by two intersecting, textured surfaces until it appeared to have an angle of 90 degrees. The task was performed at three randomly interleaved viewing distances, monocularly and binocularly. In the monocular condition, observers underestimated the fold angle and there was no effect of viewing distance on their settings. In the binocular conditions, we found that when the lens and camera separation were less than the viewer's IPD, they exhibited compression of perceived slant relative to baseline. The reverse pattern was seen when the lens and camera separation were larger than the viewer's IPD. These results were well explained by a geometric model that considers shifts in convergence due to lens and display misalignment with the eyes, as well as the relative contribution of monocular cues.
C1 [Tong, Jonathan; Wilcox, Laurie M.; Allison, Robert S.] York Univ, Ctr Vis Res, N York, ON, Canada.
C3 York University - Canada
RP Tong, J (corresponding author), York Univ, Ctr Vis Res, N York, ON, Canada.
EM tongj86@yorku.ca; lwilcox@yorku.ca; allison@eecs.yorku.ca
OI Allison, Robert/0000-0002-4485-2665; Wilcox, Laurie/0000-0002-3594-6192
FU NSERC Collaborative Research and Development (CRD) grant; Qualcomm
   Canada Inc; Canada First Research Excellence Fund (CFREF) for the
   Vision: Science to Application (VISTA) program
FX This work was funded by an NSERC Collaborative Research and Development
   (CRD) grant, in partnership with Qualcomm Canada Inc, and the Canada
   First Research Excellence Fund (CFREF) for the Vision: Science to
   Application (VISTA) program.
NR 29
TC 2
Z9 2
U1 1
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2022
VL 28
IS 11
BP 3759
EP 3766
DI 10.1109/TVCG.2022.3203098
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 5M3ZS
UT WOS:000871038200020
PM 36048994
DA 2025-03-07
ER

PT J
AU Shi, N
   Xu, JY
   Wurster, SW
   Guo, HQ
   Woodring, J
   Van Roekel, LP
   Shen, HW
AF Shi, Neng
   Xu, Jiayi
   Wurster, Skylar W.
   Guo, Hanqi
   Woodring, Jonathan
   Van Roekel, Luke P.
   Shen, Han-Wei
TI GNN-Surrogate: A Hierarchical and Adaptive Graph Neural Network for
   Parameter Space Exploration of Unstructured-Mesh Ocean Simulations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Parameter Space Exploration; Ensemble Visualization; Unstructured Mesh;
   Surrogate Modeling; Graph Neural Network; Adaptive Resolution
ID VISUALIZATION
AB We propose GNN-Surrogate, a graph neural network-based surrogate model to explore the parameter space of ocean climate simulations. Parameter space exploration is important for domain scientists to understand the influence of input parameters (e.g., wind stress) on the simulation output (e.g., temperature). The exploration requires scientists to exhaust the complicated parameter space by running a batch of computationally expensive simulations. Our approach improves the efficiency of parameter space exploration with a surrogate model that predicts the simulation outputs accurately and efficiently. Specifically, GNN-Surrogate predicts the output field with given simulation parameters so scientists can explore the simulation parameter space with visualizations from user-specified visual mappings. Moreover, our graph-based techniques are designed for unstructured meshes, making the exploration of simulation outputs on irregular grids efficient. For efficient training, we generate hierarchical graphs and use adaptive resolutions. We give quantitative and qualitative evaluations on the MPAS-Ocean simulation to demonstrate the effectiveness and efficiency of GNN-Surrogate.
C1 [Shi, Neng; Xu, Jiayi; Wurster, Skylar W.; Shen, Han-Wei] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
   [Guo, Hanqi] Argonne Natl Lab, Math & Comp Sci Div, Lemont, IL 60439 USA.
   [Woodring, Jonathan] Los Alamos Natl Lab, Appl Comp Sci Grp CCS 7, Los Alamos, NM 87544 USA.
   [Van Roekel, Luke P.] Los Alamos Natl Lab, Fluid Dynam & Solid Mech Grp T3, Los Alamos, NM 87544 USA.
C3 University System of Ohio; Ohio State University; United States
   Department of Energy (DOE); Argonne National Laboratory; United States
   Department of Energy (DOE); Los Alamos National Laboratory; United
   States Department of Energy (DOE); Los Alamos National Laboratory
RP Shi, N (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
EM shi.1337@osu.edu; xu.2205@osu.edu; wurster.18@osu.edu; hguo@anl.gov;
   woodring@lanl.gov; lvanroekel@lanl.gov; shen.94@osu.edu
RI Guo, Hanqi/AAL-1929-2021; Shen, Han-wei/A-4710-2012; Guo,
   Hanqi/ADW-4234-2022
OI Guo, Hanqi/0000-0001-7776-1834; Wurster, Skylar/0000-0001-6685-615X
FU National Science Foundation Division of Information and Intelligent
   Systems [1955764]; National Science Foundation Office of Advanced
   Cyberinfrastructure [2112606]; U.S. Department of Energy Los Alamos
   National Laboratory [47145]; UT-Battelle LLC [4000159447]; SciDAC
   program - U.S. Department of Energy, Office of Science, Advanced
   Scientific Computing Research; DOE Office of Science User Facility
   [DE-AC02-06CH11357]; Direct For Computer & Info Scie & Enginr; Div Of
   Information & Intelligent Systems [1955764] Funding Source: National
   Science Foundation; Direct For Computer & Info Scie & Enginr; Office of
   Advanced Cyberinfrastructure (OAC) [2112606] Funding Source: National
   Science Foundation
FX This work is supported in part by the National Science Foundation
   Division of Information and Intelligent Systems-1955764, the National
   Science Foundation Office of Advanced Cyberinfrastructure2112606, U.S.
   Department of Energy Los Alamos National Laboratory contract 47145, and
   UT-Battelle LLC contract 4000159447 program manager Margaret Lentz. This
   work is also supported in part by the SciDAC program funded by the U.S.
   Department of Energy, Office of Science, Advanced Scientific Computing
   Research. This research used resources of the Argonne Leadership
   Computing Facility, which is a DOE Office of Science User Facility
   supported under Contract DE-AC02-06CH11357.
NR 43
TC 17
Z9 20
U1 6
U2 29
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2022
VL 28
IS 6
BP 2301
EP 2313
DI 10.1109/TVCG.2022.3165345
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0Z1CH
UT WOS:000790817100004
PM 35389867
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhao, J
   Sun, MY
   Chen, F
   Chiu, P
AF Zhao, Jian
   Sun, Maoyuan
   Chen, Francine
   Chiu, Patrick
TI Understanding Missing Links in Bipartite Networks With MissBiN
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Missing link prediction; bipartite network; bi-clique; interactive
   visualization; visual analytics
ID VISUALIZATION; EXPLORATION; PREDICTION
AB The analysis of bipartite networks is critical in a variety of application domains, such as exploring entity co-occurrences in intelligence analysis and investigating gene expression in bio-informatics. One important task is missing link prediction, which infers the existence of unseen links based on currently observed ones. In this article, we propose a visual analysis system, MissBiN, to involve analysts in the loop for making sense of link prediction results. MissBiN equips a novel method for link prediction in a bipartite network by leveraging the information of bi-cliques in the network. It also provides an interactive visualization for understanding the algorithm outputs. The design of MissBiN is based on three high-level analysis questions (what, why, and how) regarding missing links, which are distilled from the literature and expert interviews. We conducted quantitative experiments to assess the performance of the proposed link prediction algorithm, and interviewed two experts from different domains to demonstrate the effectiveness of MissBiN as a whole. We also provide a comprehensive usage scenario to illustrate the usefulness of the tool in an application of intelligence analysis.
C1 [Zhao, Jian] Univ Waterloo, Sch Comp Sci, Waterloo, ON N2L 3G1, Canada.
   [Sun, Maoyuan] Northern Illinois Univ, Dept Comp Sci, De Kalb, IL 60115 USA.
   [Chen, Francine; Chiu, Patrick] FXPAL, Palo Alto, CA 94304 USA.
C3 University of Waterloo; Northern Illinois University
RP Zhao, J (corresponding author), Univ Waterloo, Sch Comp Sci, Waterloo, ON N2L 3G1, Canada.
EM jianzhao@uwaterloo.ca; smaoyuan@niu.edu; francine@acm.org;
   patrick_chiu@acm.org
RI Sun, Maoyuan/AAJ-4301-2020
OI Zhao, Jian/0000-0001-5008-4319; Sun, Maoyuan/0000-0002-0990-2620
FU NSERC Discovery Grant; US National Science Foundation [IIS-1850036,
   IIS-2002082]
FX This work was supported in part by the NSERC Discovery Grant and US
   National Science Foundation under Grants IIS-1850036 and IIS-2002082.
   Part of the work was completed while the authors were at FXPAL.
NR 64
TC 7
Z9 8
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2022
VL 28
IS 6
BP 2457
EP 2469
DI 10.1109/TVCG.2020.3032984
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0Z1CH
UT WOS:000790817100016
PM 33090955
OA Bronze
DA 2025-03-07
ER

PT J
AU Yin, TR
   Hoyet, L
   Christie, M
   Cani, MP
   Pettre, J
AF Yin, Tairan
   Hoyet, Ludovic
   Christie, Marc
   Cani, Marie-Paule
   Pettre, Julien
TI The One-Man-Crowd: Single User Generation of Crowd Motions Using Virtual
   Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data models; Solid modeling; Context modeling; Trajectory; Legged
   locomotion; Ethics; Decision making; Crowd motion data; human
   interaction; virtual reality
ID PEDESTRIAN MOVEMENT; PERSONAL-SPACE; FLOW; PERCEPTION; DYNAMICS;
   WALKING; INFLOW; SPEED
AB Crowd motion data is fundamental for understanding and simulating realistic crowd behaviours. Such data is usually collected through controlled experiments to ensure that both desired individual interactions and collective behaviours can be observed. It is however scarce, due to ethical concerns and logistical difficulties involved in its gathering, and only covers a few typical crowd scenarios. In this work, we propose and evaluate a novel Virtual Reality based approach lifting the limitations of real-world experiments for the acquisition of crowd motion data. Our approach immerses a single user in virtual scenarios where he/she successively acts each crowd member. By recording the past trajectories and body movements of the user, and displaying them on virtual characters, the user progressively builds the overall crowd behaviour by him/herself. We validate the feasibility of our approach by replicating three real experiments, and compare both the resulting emergent phenomena and the individual interactions to existing real datasets. Our results suggest that realistic collective behaviours can naturally emerge from virtual crowd data generated using our approach, even though the variety in behaviours is lower than in real situations. These results provide valuable insights to the building of virtual crowd experiences, and reveal key directions for further improvements.
C1 [Yin, Tairan; Hoyet, Ludovic; Christie, Marc; Pettre, Julien] Univ Rennes, IRISA, CNRS, INRIA, Rennes, France.
   [Cani, Marie-Paule] Ecole Polytech, IP Paris, CNRS LIX, Paris, France.
C3 Inria; Universite de Rennes; Centre National de la Recherche
   Scientifique (CNRS); Centre National de la Recherche Scientifique
   (CNRS); Institut Polytechnique de Paris; Ecole Polytechnique
RP Yin, TR (corresponding author), Univ Rennes, IRISA, CNRS, INRIA, Rennes, France.
EM tairan.yin@inria.fr
RI Hoyet, Ludovic/IWU-9100-2023; Pettré, Julien/AAB-2590-2022
OI Hoyet, Ludovic/0000-0002-7373-6049
FU European Union [860768]; Marie Curie Actions (MSCA) [860768] Funding
   Source: Marie Curie Actions (MSCA)
FX This work has received funding from the European Union's Horizon 2020
   research and innovation programme under the Marie SklodowskaCurie grant
   agreement No 860768 (CLIPE project).
NR 64
TC 13
Z9 13
U1 1
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY 1
PY 2022
VL 28
IS 5
BP 2245
EP 2255
DI 10.1109/TVCG.2022.3150507
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1R1AK
UT WOS:000803110400036
PM 35167473
OA Green Published
DA 2025-03-07
ER

PT J
AU Xin, HG
   Zheng, SK
   Xu, K
   Yan, LQ
AF Xin, Hanggao
   Zheng, Shaokun
   Xu, Kun
   Yan, Ling-Qi
TI Lightweight Bilateral Convolutional Neural Networks for Interactive
   Single-Bounce Diffuse Indirect Illumination
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Lighting; Neural networks; Rendering (computer graphics);
   Three-dimensional displays; Coherence; Monte Carlo methods; Lattices;
   Real-time rendering; global illumination
AB Physically correct, noise-free global illumination is crucial in physically-based rendering, but often takes a long time to compute. Recent approaches have exploited sparse sampling and filtering to accelerate this process but still cannot achieve interactive performance. It is partly due to the time-consuming ray sampling even at 1 sample per pixel, and partly because of the complexity of deep neural networks. To address this problem, we propose a novel method to generate plausible single-bounce indirect illumination for dynamic scenes in interactive framerates. In our method, we first compute direct illumination and then use a lightweight neural network to predict screen space indirect illumination. Our neural network is designed explicitly with bilateral convolution layers and takes only essential information as input (direct illumination, surface normals, and 3D positions). Also, our network maintains the coherence between adjacent image frames efficiently without heavy recurrent connections. Compared to state-of-the-art works, our method produces single-bounce indirect illumination of dynamic scenes with higher quality and better temporal coherence and runs at interactive framerates.
C1 [Xin, Hanggao; Zheng, Shaokun; Xu, Kun] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
   [Yan, Ling-Qi] UC Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.
C3 Tsinghua University; University of California System; University of
   California Santa Barbara
RP Xu, K (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
EM xhg18@mails.tsinghua.edu.cn; mango@live.cn; xukun@tsinghua.edu.cn;
   lingqi@cs.ucsb.edu
RI Xu, Kun/K-7134-2012
OI Zheng, Shaokun/0009-0003-7221-6521; Xin, Hanggao/0000-0003-1292-2427
FU National Natural Science Foundation of China [61822204, 61932003,
   61521002]; Beijing Higher Institution Engineering Research Center
FX This work was supported by the National Natural Science Foundation of
   China (Project Number 61822204, 61932003, 61521002), a research Grant
   from the Beijing Higher Institution Engineering Research Center.
NR 48
TC 8
Z9 9
U1 1
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2022
VL 28
IS 4
BP 1824
EP 1834
DI 10.1109/TVCG.2020.3023129
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZH9CR
UT WOS:000761227900009
PM 32915740
DA 2025-03-07
ER

PT J
AU Park, H
   Fussell, D
   Navratil, P
AF Park, Hyungman
   Fussell, Donald
   Navratil, Paul
TI Data-Aware Predictive Scheduling for Distributed-Memory Ray Tracing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Distributed databases; Rendering (computer graphics);
   Data visualization; Processor scheduling; Parallel processing; Ray
   tracing; Ray tracing; distributed data visualization
ID SIMULATION
AB Scientific ray tracing now can include realistic shading and material properties, but tracing rays of various depths to conclusion through partitioned data is inefficient. For such data, many ray scheduling methods have demonstrated improved rendering performance. However, synchronicity and non-adaptivity inherent in prior methods hinder further performance optimizations. In this paper, we attempt to relax these constraints. Specifically, we incorporate prediction models capable of dynamically adjusting levels of speculation in ray-data queries, making ray scheduling highly adaptable to a spectrum of scene characteristics. In addition, we organize rays in a tree of speculation nodes, where speculation is coordinated pairwise within a subtree of adaptive ray groups, facilitating concurrency and parallelism. Compared to prior non-predictive methods, we achieve up to three times higher throughput for volume and geometry rendering on a distributed system, making our method fit for both interactive and offline applications.
C1 [Park, Hyungman; Fussell, Donald; Navratil, Paul] Univ Texas Austin, Austin, TX 78712 USA.
C3 University of Texas System; University of Texas Austin
RP Park, H (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.
EM hyungman@utexas.edu
FU US NSF [ACI-1339863]; Intel Graphics and Visualization Institute of
   eXcellence award
FX We thank the anonymous reviewers for their thoughtful comments and
   suggestions during the review cycle. We also thank a number of people
   and organizations for providing us with the datasets and tools: the
   Noise volume generator provided by Greg Abram at the Texas Advanced
   Computing Center at the University of Texas at Austin; the RM dataset
   provided by the Lawrence Livermore National Laboratory; the DNS and
   Lambda2 datasets provided by Myoungkyu Lee at the Sandia National
   Laboratories; and the Exajet dataset provided by Dassault Syst`emes and
   the NASA Open Data Portal. This work was funded in part by US NSF award
   ACI-1339863; and an Intel Graphics and Visualization Institute of
   eXcellence award.
NR 33
TC 1
Z9 2
U1 5
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 1172
EP 1181
DI 10.1109/TVCG.2021.3114838
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000110
PM 34587067
DA 2025-03-07
ER

PT J
AU Brument, H
   Bruder, G
   Marchal, M
   Olivier, AH
   Argelaguet, F
AF Brument, Hugo
   Bruder, Gerd
   Marchal, Maud
   Olivier, Anne Helene
   Argelaguet, Ferran
TI Understanding, Modeling and Simulating Unintended Positional Drift
   during Repetitive Steering Navigation Tasks in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 20th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 04-08, 2021
CL ELECTR NETWORK
SP IEEE
DE Visualization; Virtual Reality; navigation; steering techniques; motion
   analysis
ID REDIRECTED WALKING
AB Virtual steering techniques enable users to navigate in larger Virtual Environments (VEs) than the physical workspace available. Even though these techniques do not require physical movement of the users (e.g. using a joystick and the head orientation to steer towards a virtual direction), recent work observed that users might unintentionally move in the physical workspace while navigating, resulting in Unintended Positional Drift (UPD). This phenomenon can be a safety issue since users may unintentionally reach the physical boundaries of the workspace while using a steering technique. In this context, as a necessary first step to improve the design of navigation techniques minimizing the UPD, this paper aims at analyzing and modeling the UPD during a virtual navigation task. In particular, we characterize and analyze the UPD for a dataset containing the positions and orientations of eighteen users performing a virtual slalom task using virtual steering techniques. Participants wore a head-mounted display and had to follow three different sinusoidal-like trajectories (with low, medium and high curvature) using a torso-steering navigation technique. We analyzed the performed motions and proposed two UPD models: the first based on a linear regression analysis and the second based on a Gaussian Mixture Model (GMM) analysis. Then, we assessed both models through a simulation-based evaluation where we reproduced the same navigation task using virtual agents. Our results indicate the feasibility of using simulation-based evaluations to study UPD. The paper concludes with a discussion of potential applications of the results in order to gain a better understanding of UPD during steering and therefore improve the design of navigation techniques by compensating for UPD.
C1 [Brument, Hugo] Univ Rennes, INRIA, IRISA, Rennes, France.
   [Bruder, Gerd] Univ Cent Florida, Orlando, FL 32816 USA.
   [Marchal, Maud] Univ Rennes, CNRS, INSA, INRIA, Rennes, France.
   [Marchal, Maud] IUF, Paris, France.
   [Olivier, Anne Helene] Univ Rennes, CNRS, INRIA, IRISA,M2S, Rennes, France.
   [Argelaguet, Ferran] Univ Rennes, CNRS, IRISA, INRIA, Rennes, France.
C3 Universite de Rennes; Inria; State University System of Florida;
   University of Central Florida; Centre National de la Recherche
   Scientifique (CNRS); Universite de Rennes; Inria; Universite de Rennes;
   Inria; Centre National de la Recherche Scientifique (CNRS); Inria;
   Universite de Rennes; Centre National de la Recherche Scientifique
   (CNRS)
RP Brument, H (corresponding author), Univ Rennes, INRIA, IRISA, Rennes, France.
EM hugo.brument@inria.fr; bruder@ucf.edu; maud.marchal@inria.fr;
   anne.olivier@inria.fr; ferran.argelaguet@inria.fr
RI Olivier, Anne-Hélène/AAH-7378-2020
OI Olivier, Anne-Helene/0000-0002-2833-020X
NR 48
TC 7
Z9 7
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2021
VL 27
IS 11
BP 4300
EP 4310
DI 10.1109/TVCG.2021.3106504
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA WN2ZT
UT WOS:000711642700022
PM 34449383
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Concha, A
   Burri, M
   Briales, J
   Forster, C
   Oth, L
AF Concha, Alejo
   Burri, Michael
   Briales, Jesus
   Forster, Christian
   Oth, Luc
TI Instant Visual Odometry Initialization for Mobile AR
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cameras; Simultaneous localization and mapping; Transmission line matrix
   methods; Sensors; Feature extraction; Visual odometry; Tracking;
   Monocular initialization; relative pose estimator; Visual Odometry; AR
   instant placement
AB Mobile AR applications benefit from fast initialization to display world-locked effects instantly. However, standard visual odometry or SLAM algorithms require motion parallax to initialize (see Figure 1) and, therefore, suffer from delayed initialization. In this paper, we present a 6-DoF monocular visual odometry that initializes instantly and without motion parallax. Our main contribution is a pose estimator that decouples estimating the 5-DoF relative rotation and translation direction from the 1-DoF translation magnitude. While scale is not observable in a monocular vision-only setting, it is still paramount to estimate a consistent scale over the whole trajectory (even if not physically accurate) to avoid AR effects moving erroneously along depth. In our approach, we leverage the fact that depth errors are not perceivable to the user during rotation-only motion. However, as the user starts translating the device, depth becomes perceivable and so does the capability to estimate consistent scale. Our proposed algorithm naturally transitions between these two modes. Our second contribution is a novel residual in the relative pose problem to further improve the results. The residual combines the Jacobians of the functional and the functional itself and is minimized using a Levenberg-Marquardt optimizer on the 5-DoF manifold. We perform extensive validations of our contributions with both a publicly available dataset and synthetic data. We show that the proposed pose estimator outperforms the classical approaches for 6-DoF pose estimation used in the literature in low-parallax configurations. Likewise, we show our relative pose estimator outperforms state-of-the-art approaches in an odometry pipeline configuration where we can leverage initial guesses. We release a dataset for the relative pose problem using real data to facilitate the comparison with future solutions for the relative pose problem. Our solution is either used as a full odometry or as a pre-SLAM component of any supported SLAM system (ARKit, ARCore) in world-locked AR effects on platforms such as Instagram and Facebook.
C1 [Concha, Alejo; Burri, Michael; Briales, Jesus; Forster, Christian; Oth, Luc] Facebook Zurich, Zurich, Switzerland.
RP Concha, A (corresponding author), Facebook Zurich, Zurich, Switzerland.
EM aconchabelenguer@gmail.com
NR 30
TC 6
Z9 6
U1 1
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2021
VL 27
IS 11
BP 4226
EP 4235
DI 10.1109/TVCG.2021.3106505
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WN2ZT
UT WOS:000711642700016
PM 34449384
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kreiser, J
   Hermosilla, P
   Ropinski, T
AF Kreiser, Julian
   Hermosilla, Pedro
   Ropinski, Timo
TI Void Space Surfaces to Convey Depth in Vessel Visualizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image color analysis; Visualization; Data visualization; Surface
   morphology; Shape; Rendering (computer graphics); Biomedical imaging;
   Depth perception; void space surface; chromadepth
ID OCCLUSION SHADING MODEL; VOLUME VISUALIZATION; ILLUSTRATION;
   ILLUMINATION
AB To enhance depth perception and thus data comprehension, additional depth cues are often used in 3D visualizations of complex vascular structures. There is a variety of different approaches described in the literature, ranging from chromadepth color coding over depth of field to glyph-based encodings. Unfortunately, the majority of existing approaches suffers from the same problem: As these cues are directly applied to the geometry's surface, the display of additional information on the vessel wall, such as other modalities or derived attributes, is impaired. To overcome this limitation we propose Void Space Surfaces which utilizes empty space in between vessel branches to communicate depth and their relative positioning. This allows us to enhance the depth perception of vascular structures without interfering with the spatial data and potentially superimposed parameter information. With this article, we introduce Void Space Surfaces, describe their technical realization, and show their application to various vessel trees. Moreover, we report the outcome of two user studies which we have conducted in order to evaluate the perceptual impact of Void Space Surfaces compared to existing vessel visualization techniques and discuss expert feedback.
C1 [Kreiser, Julian; Hermosilla, Pedro; Ropinski, Timo] Ulm Univ, Visual Comp Grp, D-89081 Ulm, Germany.
   [Ropinski, Timo] Linkoping Univ, Sci Visualizat Grp, S-58183 Linkoping, Sweden.
C3 Ulm University; Linkoping University
RP Kreiser, J (corresponding author), Ulm Univ, Visual Comp Grp, D-89081 Ulm, Germany.
EM julian.kreiser@uni-ulm.de; pedro-1.hermosilla-casajus@uni-ulm.de;
   timo.ropinski@uni-ulm.de
OI Kreiser, Julian/0000-0003-2743-8291; Ropinski, Timo/0000-0002-7857-5512
FU Deutsche Forschungsgemeinschaft (DFG) [RO 3-408/31]; Ulm University
   Center for Translational Imaging MoMAN
FX This work was funded in part by the Deutsche Forschungsgemeinschaft
   (DFG) under Grant RO 3-408/31 (Inviwo). The authors would like to thank
   the Ulm University Center for Translational Imaging MoMAN for its
   support. The proposed concepts have been realized using the Inviwo
   visualization framework (www.inviwo.org).
NR 40
TC 1
Z9 2
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2021
VL 27
IS 10
BP 3913
EP 3925
DI 10.1109/TVCG.2020.2993992
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL8JB
UT WOS:000692890200006
PM 32406840
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Skarbez, R
   Brooks, FP
   Whitton, MC
AF Skarbez, Richard
   Brooks, Frederick P., Jr.
   Whitton, Mary C.
TI Immersion and Coherence: Research Agenda and Early Results
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Coherence; Virtual environments; Task analysis; Physiology; Image color
   analysis; User experience; Measurement; Virtual reality; presence; place
   illusion (PI); plausibility illusion (Psi); immersion; coherence; user
   studies; physiological metrics; research agenda
ID VIRTUAL ENVIRONMENTS; PAIN-CONTROL; REALITY; SENSE
AB Presence has been studied in the context of virtual environments for nearly thirty years, but the field has yet to reach consensus on even basic issues of definition and measurement, and there are many open research questions. We gather many of these open research questions and systematically group them according to what we believe are five key constructs that inform user experience in virtual environments: immersion, coherence, Place Illusion, Plausibility Illusion, and presence. We also report on the design and results of a study that investigated the effects of immersion and coherence on user experience in a stressful virtual visual cliff environment. In this article, each participant experienced a given VE in one of four conditions chosen from a 2x2 design: high or low levels of immersion and high or low levels of coherence. We collected both questionnaire-based and physiological metrics. Several existing presence questionnaires could not reliably distinguish the effects of immersion from those of coherence. They did, however, indicate that high levels of both together result in higher presence, compared any of the other three conditions. This suggests that "breaks in PI" and "breaks in Psi" belong to a broader category of "breaks in experience," any of which result in a degraded user experience. Participants' heart rates responded markedly differently in the two coherence conditions; no such difference was observed across the immersion conditions. This indicates that a VE that exhibits unusual or confusing behavior can cause stress in a user that affects physiological responses, and that one must take care to eliminate such confusing behaviors if one is using physiological measurement as a proxy for subjective experience in a VE.
C1 [Skarbez, Richard] La Trobe Univ, Dept Comp Sci & Informat Technol, Melbourne, Vic 3086, Australia.
   [Brooks, Frederick P., Jr.; Whitton, Mary C.] Univ N Carolina, Comp Sci, Chapel Hill, NC 27599 USA.
C3 La Trobe University; University of North Carolina; University of North
   Carolina Chapel Hill
RP Skarbez, R (corresponding author), La Trobe Univ, Dept Comp Sci & Informat Technol, Melbourne, Vic 3086, Australia.
EM r.skarbez@latrobe.edu.au; brooks@cs.unc.edu; whitton@cs.unc.edu
RI Skarbez, Richard/S-7298-2019; Whitton, Mary/AAN-2378-2021
OI Skarbez, Richard/0000-0002-2783-5257
NR 58
TC 36
Z9 37
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2021
VL 27
IS 10
BP 3839
EP 3850
DI 10.1109/TVCG.2020.2983701
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL8JB
UT WOS:000692890200001
PM 32248110
DA 2025-03-07
ER

PT J
AU Ogawa, N
   Narumi, T
   Hirose, M
AF Ogawa, Nami
   Narumi, Takuji
   Hirose, Michitaka
TI Effect of Avatar Appearance on Detection Thresholds for Remapped Hand
   Movements
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Avatars; Visualization; Haptic interfaces; Shape; Psychology;
   Sensitivity; Virtual reality; body ownership; 3D interaction; hand
   interaction; hand redirection; hand retargeting; avatar
ID VISUAL CAPTURE; INTEGRATION; EXPERIENCE; OWNERSHIP; SENSE; BODY;
   INFORMATION; EMBODIMENT; ACCURACY; DISTANCE
AB Hand interaction techniques in virtual reality often exploit visual dominance over proprioception to remap physical hand movements onto different virtual movements. However, when the offset between virtual and physical hands increases, the remapped virtual hand movements are hardly self-attributed, and the users become aware of the remapping. Interestingly, the sense of self-attribution of a body is called the sense of body ownership (SoBO) in the field of psychology, and the realistic the avatar, the stronger is the SoBO. Hence, we hypothesized that realistic avatars (i.e., human hands) can foster self-attribution of the remapped movements better than abstract avatars (i.e., spherical pointers), thus making the remapping less noticeable. In this article, we present an experiment in which participants repeatedly executed reaching movements with their right hand while different amounts of horizontal shifts were applied. We measured the remapping detection thresholds for each combination of shift directions (left or right) and avatar appearances (realistic or abstract). The results show that realistic avatars increased the detection threshold (i.e., lowered sensitivity) by 31.3 percent than the abstract avatars when the leftward shift was applied (i.e., when the hand moved in the direction away from the body-midline). In addition, the proprioceptive drift (i.e., the displacement of self-localization toward an avatar) was larger with realistic avatars for leftward shifts, indicating that visual information was given greater preference during visuo-proprioceptive integration in realistic avatars. Our findings quantifiably show that realistic avatars can make remapping less noticeable for larger mismatches between virtual and physical movements and can potentially improve a wide variety of hand-remapping techniques without changing the mapping itself.
C1 [Ogawa, Nami; Narumi, Takuji; Hirose, Michitaka] Univ Tokyo, Tokyo 1138654, Japan.
   [Narumi, Takuji] JST PRESTO, Tokyo 1138654, Japan.
C3 University of Tokyo; Japan Science & Technology Agency (JST)
RP Ogawa, N (corresponding author), Univ Tokyo, Tokyo 1138654, Japan.
EM ogawa@cyber.t.u-tokyo.ac.jp; narumi@cyber.t.u-tokyo.ac.jp;
   hirose@cyber.t.u-tokyo.ac.jp
RI Narumi, Takuji/K-3925-2014
OI Narumi, Takuji/0000-0002-9010-1491
FU JST PRESTO [JPMJPR17J6]; Grants-in-Aid for Scientific Research
   [20K21801] Funding Source: KAKEN
FX The authors would like to thank Ferran Argelaguet and Hideaki Kuzuoka
   for for their comments on an earlier version of the manuscript. This
   work was partially supported by JST PRESTO (JPMJPR17J6).
NR 85
TC 38
Z9 41
U1 2
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2021
VL 27
IS 7
BP 3182
EP 3197
DI 10.1109/TVCG.2020.2964758
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SK0PE
UT WOS:000655924400007
PM 31940540
DA 2025-03-07
ER

PT J
AU Shen, YF
   Zhang, CG
   Fu, HB
   Zhou, K
   Zheng, YY
AF Shen, Yuefan
   Zhang, Changgeng
   Fu, Hongbo
   Zhou, Kun
   Zheng, Youyi
TI DeepSketchHair: Deep Sketch-Based 3D Hair Modeling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Hair; Three-dimensional displays; Solid modeling; Two dimensional
   displays; Computational modeling; Deep learning; Neural networks;
   Sketch-based hair modeling; 3D volumetric structure; deep learning;
   generative adversarial networks
AB We present DeepSketchHair, a deep learning based tool for modeling of 3D hair from 2D sketches. Given a 3D bust model as reference, our sketching system takes as input a user-drawn sketch (consisting of hair contour and a few strokes indicating the hair growing direction within a hair region), and automatically generates a 3D hair model, matching the input sketch. The key enablers of our system are three carefully designed neural networks, namely, S2ONet, which converts an input sketch to a dense 2D hair orientation field; O2VNet, which maps the 2D orientation field to a 3D vector field; and V2VNet, which updates the 3D vector field with respect to the new sketches, enabling hair editing with additional sketches in new views. All the three networks are trained with synthetic data generated from a 3D hairstyle database. We demonstrate the effectiveness and expressiveness of our tool using a variety of hairstyles and also compare our method with prior art.
C1 [Shen, Yuefan; Zhang, Changgeng; Zhou, Kun; Zheng, Youyi] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
   [Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
C3 Zhejiang University; City University of Hong Kong
RP Zhou, K; Zheng, YY (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
EM jhonve@zju.edu.cn; changezhang@zju.edu.cn; hongbofu@cityu.edu.hk;
   kunzhou@acm.org; youyizheng@zju.edu.cn
RI Shen, Yuefan/GWN-0324-2022; zhou, kun/KRP-1631-2024
OI FU, Hongbo/0000-0002-0284-726X; Shen, Yuefan/0000-0002-6049-7966
FU National Key Research & Development Program of China [2018YFE0100900];
   NSF China [U1609215]; Fundamental Research Funds for the Central
   Universities; RGC of HKSAR [CityU 11212119]; City University of Hong
   Kong [7005176]; Centre for Applied Computing and Interactive Media
   (ACIM) of School of Creative Media, CityU
FX The authors would like to thank the anonymous reviewers for their
   constructive comments. This work was supported in part by the National
   Key Research & Development Program of China (2018YFE0100900), NSF China
   (No. U1609215), and the Fundamental Research Funds for the Central
   Universities. Hongbo Fu was supported by a gift from Adobe and Grants
   from the RGC of HKSAR (Project No. CityU 11212119), City University of
   Hong Kong (Project No. 7005176), and the Centre for Applied Computing
   and Interactive Media (ACIM) of School of Creative Media, CityU. Yuefan
   Shen and Changgeng Zhang are co-first authors.
NR 57
TC 13
Z9 14
U1 3
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2021
VL 27
IS 7
BP 3250
EP 3263
DI 10.1109/TVCG.2020.2968433
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SK0PE
UT WOS:000655924400012
PM 31985423
OA Bronze, Green Submitted
DA 2025-03-07
ER

PT J
AU Zhang, Y
   Lai, YK
   Zhang, FL
AF Zhang, Yun
   Lai, Yu-Kun
   Zhang, Fang-Lue
TI Content-Preserving Image Stitching With Piecewise Rectangular Boundary
   Constraints
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Content-preserving image stitching; panoramic image; rectangling;
   polygon boolean operations; piecewise rectangular boundary
ID PANORAMIC VIDEO
AB This article proposes an approach to content-preserving image stitching with regular boundary constraints, which aims to stitch multiple images to generate a panoramic image with piecewise rectangular boundaries. Existing methods treat image stitching and rectangling as two separate steps, which may result in suboptimal results as the stitching process is not aware of the further warping needs for rectangling. We address these limitations by formulating image stitching with regular boundaries in a unified optimization framework. Starting from the initial stitching result produced by traditional warping-based optimization, we obtain the irregular boundary from the warped meshes by polygon Boolean operations which robustly handle arbitrary mesh compositions. By analyzing the irregular boundary, we construct a piecewise rectangular boundary. Based on this, we further incorporate line and regular boundary preservation constraints into the image stitching framework, and conduct iterative optimizations to obtain an optimal piecewise rectangular boundary. Thus we can make the boundary of the stitching result as close as possible to a rectangle, while reducing unwanted distortions. We further extend our method to video stitching, by integrating the temporal coherence into the optimization. Experiments show that our method efficiently produces visually pleasing panoramas with regular boundaries and unnoticeable distortions.
C1 [Zhang, Yun] Commun Univ Zhejiang, Coll Media Engn, Hangzhou 31008, Peoples R China.
   [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, Wales.
   [Zhang, Fang-Lue] Victoria Univ Wellington, Sch Engn & Comp Sci, Wellington 6012, New Zealand.
C3 Cardiff University; Victoria University Wellington
RP Zhang, Y (corresponding author), Commun Univ Zhejiang, Coll Media Engn, Hangzhou 31008, Peoples R China.
EM zhangyun@cuz.edu.cn; LaiY4@cardiff.ac.uk; fanglue.zhang@ecs.vuw.ac.nz
RI Lai, Yu-Kun/D-2343-2010
OI Lai, Yukun/0000-0002-2094-5680
FU National Natural Science Foundation of China [61602402]; Zhejiang
   Province Public Welfare Technology Application Research [LGG19F020001];
   Victoria Early Career Research Excellence Award [224525]; Royal Society
   [IES\R1\180126]
FX The authors would like to thank all anonymous reviewers for their
   valuable comments. This work was supported by the National Natural
   Science Foundation of China (No. 61602402), Zhejiang Province Public
   Welfare Technology Application Research (No. LGG19F020001), Victoria
   Early Career Research Excellence Award (No. 224525) and the Royal
   Society (No. IESnR1n180126).
NR 30
TC 31
Z9 32
U1 9
U2 108
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2021
VL 27
IS 7
BP 3198
EP 3212
DI 10.1109/TVCG.2020.2965097
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SK0PE
UT WOS:000655924400008
PM 31944958
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Han, J
   Zheng, H
   Xing, YH
   Chen, DZ
   Wang, CL
AF Han, Jun
   Zheng, Hao
   Xing, Yunhao
   Chen, Danny Z.
   Wang, Chaoli
TI V2V: A Deep Learning Approach to Variable-to-Variable Selection and
   Translation for Multivariate Time-Varying Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Deep learning; Feature extraction; Task analysis; Input variables;
   Generative adversarial networks; Three-dimensional displays;
   Multivariate time-varying data; variable selection and translation;
   generative adversarial network; data extrapolation
AB We present V2V, a novel deep learning framework, as a general-purpose solution to the variable-to-variable (V2V) selection and translation problem for multivariate time-varying data (MTVD) analysis and visualization. V2V leverages a representation learning algorithm to identify transferable variables and utilizes Kullback-Leibler divergence to determine the source and target variables. It then uses a generative adversarial network (GAN) to learn the mapping from the source variable to the target variable via the adversarial, volumetric, and feature losses. V2V takes the pairs of time steps of the source and target variable as input for training, Once trained, it can infer unseen time steps of the target variable given the corresponding time steps of the source variable. Several multivariate time-varying data sets of different characteristics are used to demonstrate the effectiveness of V2V, both quantitatively and qualitatively. We compare V2V against histogram matching and two other deep learning solutions (Pix2Pix and CycleGAN).
C1 [Han, Jun; Zheng, Hao; Chen, Danny Z.; Wang, Chaoli] Univ Notre Dame, Notre Dame, IN 46556 USA.
   [Xing, Yunhao] Sichuan Univ, Chengdu, Peoples R China.
C3 University of Notre Dame; Sichuan University
RP Han, J (corresponding author), Univ Notre Dame, Notre Dame, IN 46556 USA.
EM jhan5@nd.edu; hzheng3@nd.edu; yhxing98@gmail.com; dchen@nd.edu;
   chaoli.wang@nd.edu
RI zheng, hao/HHM-6949-2022; Wang, Chaoli/AAJ-5173-2020
OI Zheng, Hao/0000-0002-9790-7607; Han, Jun/0000-0002-7286-062X
FU U.S. National Science Foundation [IIS-1455886, CCILI 617735,
   CNS-1629914, DUE -1833129, HS -1955395]; NVIDIA GPU Grant Program
FX This research was supported in part by the U.S. National Science
   Foundation through grants IIS-1455886. CCILI 617735, CNS-1629914, DUE
   -1833129. and HS -1955395, and the NVIDIA GPU Grant Program. The authors
   would like to thank the anonymous reviewers for their insightful
   comments.
NR 47
TC 16
Z9 20
U1 2
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1290
EP 1300
DI 10.1109/TVCG.2020.3030346
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100111
PM 33074812
DA 2025-03-07
ER

PT J
AU Liu, Y
   Kale, A
   Althoff, T
   Heer, J
AF Liu, Yang
   Kale, Alex
   Althoff, Tim
   Heer, Jeffrey
TI Boba: Authoring and Visualizing Multiverse Analyses
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Multiverse Analysis; Statistical Analysis; Analytic Decisions;
   Reproducibility
ID SPECIFICATION; UNCERTAINTY
AB Multiverse analysis is an approach to data analysis in which all "reasonable" analytic decisions are evaluated in parallel and interpreted collectively, in order to foster robustness and transparency. However, specifying a multiverse is demanding because analysts must manage myriad variants from a cross-product of analytic decisions, and the results require nuanced interpretation. We contribute Baba: an integrated domain-specific language (DSL) and visual analysis system for authoring and reviewing multiverse analyses. With the Boba DSL, analysts write the shared portion of analysis code only once, alongside local variations defining alternative decisions, from which the compiler generates a multiplex of scripts representing all possible analysis paths. The Boba Visualizer provides linked views of model results and the multiverse decision space to enable rapid, systematic assessment of consequential decisions and robustness, including sampling uncertainty and model fit. We demonstrate Boba's utility through two data analysis case studies, and reflect on challenges and design opportunities for multiverse analysis software.
C1 [Liu, Yang; Kale, Alex; Althoff, Tim; Heer, Jeffrey] Univ Washington, Seattle, WA 98195 USA.
C3 University of Washington; University of Washington Seattle
RP Liu, Y (corresponding author), Univ Washington, Seattle, WA 98195 USA.
EM yliu0@uw.edu; kalea@uw.edu; althoff@uw.edu; jheer@uw.edu
FU NSF [1901386]; Div Of Information & Intelligent Systems; Direct For
   Computer & Info Scie & Enginr [1901386] Funding Source: National Science
   Foundation
FX We thank the anonymous reviewers, UW IDL members, Uri Simonsohn, Mike
   Merrill, Ge Zhang, Pierre Dragicevic, Yvonne Jansen, Matthew Kay, Brian
   Hall, Abhraneel Sarma, Fanny Chevalier, and Michael Moon for their help.
   This work was supported by NSF Award 1901386.
NR 60
TC 27
Z9 33
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1753
EP 1763
DI 10.1109/TVCG.2020.3028985
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100151
PM 33027002
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Lukasczyk, J
   Garth, C
   Maciejewski, R
   Tierny, J
AF Lukasczyk, Jonas
   Garth, Christoph
   Maciejewski, Ross
   Tierny, Julien
TI Localized Topological Simplification of Scalar Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Topological data analysis; scalar data; simplification; feature
   extraction; parallel computing
AB This paper describes a localized algorithm for the topological simplification of scalar data, an essential pre-processing step of topological data analysis (TDA). Given a scalar field f and a selection of extrema to preserve, the proposed localized topological simplification (LTS) derives a function g that is close to f and only exhibits the selected set of extrema. Specifically, sub- and superlevel set components associated with undesired extrema are first locally flattened and then correctly embedded into the global scalar field, such that these regions are guaranteed from a combinatorial perspective to no longer contain any undesired extrema. In contrast to previous global approaches, LTS only and independently processes regions of the domain that actually need to be simplified, which already results in a noticeable speedup. Moreover, due to the localized nature of the algorithm, LTS can utilize shared-memory parallelism to simplify regions simultaneously with a high parallel efficiency (70%). Hence, LTS significantly improves interactivity for the exploration of simplification parameters and their effect on subsequent topological analysis. For such exploration tasks, LTS brings the overall execution time of a plethora of TDA pipelines from minutes down to seconds, with an average observed speedup over state-of-the-art techniques of up to 36. Furthermore, in the special case where preserved extrema are selected based on topological persistence, an adapted version of LTS partially computes the persistence diagram and simultaneously simplifies features below a predefined persistence threshold. The effectiveness of LTS, its parallel efficiency, and its resulting benefits for TDA are demonstrated on several simulated and acquired datasets from different application domains, including physics, chemistry, and biomedical imaging.
C1 [Lukasczyk, Jonas; Maciejewski, Ross] Arizona State Univ, Tempe, AZ 85287 USA.
   [Garth, Christoph] Tech Univ Kaiserslautern, Kaiserslautern, Germany.
   [Tierny, Julien] Sorbonne Univ, Paris, France.
   [Tierny, Julien] CNRS, Paris, France.
C3 Arizona State University; Arizona State University-Tempe; University of
   Kaiserslautern; Sorbonne Universite; Centre National de la Recherche
   Scientifique (CNRS)
RP Lukasczyk, J (corresponding author), Arizona State Univ, Tempe, AZ 85287 USA.
EM jl@jluk.de; garth@cs.uni-kl.de; rmacieje@asu.edu;
   julien.tierny@sorbonne-universite.fr
RI Garth, Christoph/Q-5901-2018
OI Garth, Christoph/0000-0003-1669-8549
FU Department of Homeland Security [2017-ST-061-QA00111,
   17STQAC00001-03-03]; National Seience Foundation Program [1350575];
   European Commission [867454]; German research foundation (DFG) [TRTG
   2057]
FX This work was supported by the Department of Homeland Security under
   Grant Award 2017-ST-061-QA00111 and 17STQAC00001-03-03, and the National
   Seience Foundation Program under Award No. 1350575. The views and
   conclusions contained in this document are those of the authors and
   should not he interpreted as necessarily representing the official
   policies, either expressed or implied, of the U.S. Department of
   Homeland Security. This work was also partially supported by the
   European Commission grant ERC-2019-COG "TORI" (ref. 867454), aid the
   German research foundation (DFG) through the TRTG 2057. Julien Tierny
   would like to dedicate this paper to his son Maivin.
NR 59
TC 19
Z9 21
U1 1
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 572
EP 582
DI 10.1109/TVCG.2020.3030353
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100044
PM 33048688
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Meulemans, W
   Sondag, M
   Speckmann, B
AF Meulemans, Wouter
   Sondag, Max
   Speckmann, Bettina
TI A Simple Pipeline for Coherent Grid Maps
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Topology; Pipelines; Coherence; Data visualization;
   Visualization; Geometry; Grid maps; algorithms; tile maps; small
   multiples; geovisualization
ID MEDIAL AXIS; PARTS; DECOMPOSITION
AB Grid maps are spatial arrangements of simple tiles (often squares or hexagons), each of which represents a spatial element. They are an established, effective way to show complex data per spatial element, using visual encodings within each tile ranging from simple coloring to nested small-multiples visualizations. An effective grid map is coherent with the underlying geographic space: the tiles maintain the contiguity, neighborhoods and identifiability of the corresponding spatial elements, while the grid map as a whole maintains the global shape of the input. Of particular importance are salient local features of the global shape which need to be represented by tiles assigned to the appropriate spatial elements. State-of-the-art techniques can adequately deal only with simple cases, such as close-to-uniform spatial distributions or global shapes that have few characteristic features. We introduce a simple fully-automated 3-step pipeline for computing coherent grid maps. Each step is a well-studied problem: shape decomposition based on salient features, tile-based Mosaic Cartograms, and point-set matching. Our pipeline is a seamless composition of existing techniques for these problems and results in high-quality grid maps. We provide an implementation, demonstrate the efficacy of our approach on various complex datasets, and compare it to the state-of-the-art.
C1 [Meulemans, Wouter; Sondag, Max; Speckmann, Bettina] TU Eindhoven, Eindhoven, Netherlands.
C3 Eindhoven University of Technology
RP Meulemans, W (corresponding author), TU Eindhoven, Eindhoven, Netherlands.
EM w.meulemans@tue.nl; m.f.m.sondag@tue.nl; b.speckmann@tue.nl
RI sondag, max/KIJ-0026-2024
OI sondag, max/0000-0003-3309-638X; Speckmann, Bettina/0000-0002-8514-7858
FU Dutch Research Council (NWO) [639.023.20]
FX M. Sondag and B. Speckmann are (partially) supported by the Dutch
   Research Council (NWO) under project no. 639.023.20.
NR 52
TC 9
Z9 9
U1 1
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1236
EP 1246
DI 10.1109/TVCG.2020.3028953
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100106
PM 33026995
OA Green Submitted, Green Published
DA 2025-03-07
ER

PT J
AU Rautek, P
   Mlejnek, M
   Beyer, J
   Troidl, J
   Pfister, H
   Theussl, T
   Hadwiger, M
AF Rautek, Peter
   Mlejnek, Matej
   Beyer, Johanna
   Troidl, Jakob
   Pfister, Hanspeter
   Theussl, Thomas
   Hadwiger, Markus
TI Objective Observer-Relative Flow Visualization in Curved Spaces for
   Unsteady 2D Geophysical Flows
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Observers; Visualization; Two dimensional displays; Three-dimensional
   displays; Manifolds; Hurricanes; Earth; Flow visualization; observer
   fields; frames of reference; objectivity; symmetry groups; intrinsic
   covariant derivatives
ID HELMHOLTZ-HODGE DECOMPOSITION; VECTOR-FIELDS; VORTICES; DESIGN
AB Computing and visualizing features in fluid flow often depends on the observer, or reference frame, relative to which the input velocity field is given. A desired property of feature detectors is therefore that they are objective, meaning independent of the input reference frame. However, the standard definition of objectivity is only given for Euclidean domains and cannot be applied in curved spaces. We build on methods from mathematical physics and Riemannian geometry to generalize objectivity to curved spaces, using the powerful notion of symmetry groups as the basis for definition. From this, we develop a general mathematical framework for the objective computation of observer fields for curved spaces, relative to which other computed measures become objective. An important property of our framework is that it works intrinsically in 2D, instead of in the 3D ambient space. This enables a direct generalization of the 2D computation via optimization of observer fields in flat space to curved domains, without having to perform optimization in 3D. We specifically develop the case of unsteady 2D geophysical flows given on spheres, such as the Earth. Our observer fields in curved spaces then enable objective feature computation as well as the visualization of the time evolution of scalar and vector fields, such that the automatically computed reference frames follow moving structures like vortices in a way that makes them appear to be steady.
C1 [Rautek, Peter; Mlejnek, Matej; Troidl, Jakob; Hadwiger, Markus] King Abdullah Univ Sci & Technol KAUST, Visual Comp Ctr, Thuwal 239556900, Saudi Arabia.
   [Beyer, Johanna; Pfister, Hanspeter] Harvard Univ, Cambridge, MA 02138 USA.
   [Troidl, Jakob] TU Wien, Vienna, Austria.
   [Theussl, Thomas] King Abdullah Univ Sci & Technol KAUST, Core Labs, Thuwal 239556000, Saudi Arabia.
C3 King Abdullah University of Science & Technology; Harvard University;
   Technische Universitat Wien; King Abdullah University of Science &
   Technology
RP Rautek, P (corresponding author), King Abdullah Univ Sci & Technol KAUST, Visual Comp Ctr, Thuwal 239556900, Saudi Arabia.
EM peter.rautek@kaust.edu.sa; matej.mlejnek@kaust.edu.sa;
   jbeyer@seas.harvard.edu; jakob.troidl@googlemail.com;
   pfister@seas.harvard.edu; thomas.theussl@kaust.edu.sa;
   markus.hadwiger@kaust.edu.sa
OI Pfister, Hanspeter/0000-0002-3620-2582; Rautek,
   Peter/0000-0003-4821-7404; Hadwiger, Markus/0000-0003-1239-4871
FU King Abdullah University of Science and Technology (KAUST); KAUST Office
   of Sponsored Research (OSR) [OSR-2015-CCF-2533-01]
FX We thank Anna Fruhstuck for the illustrations and for help with the
   figures and the video. Hurricane Isabel data courtesy of EU Copernicus
   project, path from National Hurricane Center/Wikipedia. This work was
   supported by King Abdullah University of Science and Technology (KAUST),
   and the KAUST Office of Sponsored Research (OSR) award
   OSR-2015-CCF-2533-01. This research used resources of the Core Labs of
   KAUST.
NR 71
TC 11
Z9 12
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 283
EP 293
DI 10.1109/TVCG.2020.3030454
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100017
PM 33048741
DA 2025-03-07
ER

PT J
AU Reipschlager, P
   Flemisch, T
   Dachselt, R
AF Reipschlager, Patrick
   Flemisch, Tamara
   Dachselt, Raimund
TI Personal Augmented Reality for Information Visualization on Large
   Interactive Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Data analysis; Augmented reality; Three-dimensional
   displays; Navigation; Visualization; Augmented Reality; Information
   Visualization; InfoVis; Large Displays; Immersive Analytics; Physical
   Navigation; Multiple Coordinated Views
ID EXPLORATION; WALL; CHALLENGES
AB In this work we propose the combination of large interactive displays with personal head-mounted Augmented Reality (AR) for information visualization to facilitate data exploration and analysis. Even though large displays provide more display space, they are challenging with regard to perception, effective multi-user support, and managing data density and complexity. To address these issues and illustrate our proposed setup, we contribute an extensive design space comprising first, the spatial alignment of display, visualizations, and objects in AR space. Next, we discuss which parts of a visualization can be augmented. Finally, we analyze how AR can be used to display personal views in order to show additional information and to minimize the mutual disturbance of data analysts. Based on this conceptual foundation, we present a number of exemplary techniques for extending visualizations with AR and discuss their relation to our design space. We further describe how these techniques address typical visualization problems that we have identified during our literature research. To examine our concepts, we introduce a generic AR visualization framework as well as a prototype implementing several example techniques. In order to demonstrate their potential, we further present a use case walkthrough in which we analyze a movie data set. From these experiences, we conclude that the contributed techniques can be useful in exploring and understanding multivariate data. We are convinced that the extension of large displays with AR for information visualization has a great potential for data analysis and sense-making.
C1 [Reipschlager, Patrick; Flemisch, Tamara; Dachselt, Raimund] Tech Univ Dresden, Interact Media Lab, Dresden, Germany.
   [Dachselt, Raimund] Tech Univ Dresden, Ctr Tactile Internet CeTi, Dresden, Germany.
   [Dachselt, Raimund] Tech Univ Dresden, Cluster Excellence Phys Lile, Dresden, Germany.
C3 Technische Universitat Dresden; Technische Universitat Dresden;
   Technische Universitat Dresden
RP Reipschlager, P (corresponding author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany.
EM patrick.reipschlaeger@tu-dresden.de; tamara.fiemisch@tu-dresden.de;
   dachselt@acm.org
RI Dachselt, Raimund/B-2860-2017
OI Dachselt, Raimund/0000-0002-2176-876X
FU DFG [389792660, TRR 248]; DFG, Germany's Excellence Strategy Clusters of
   Excellence [EXC-2068 390729961, EXC 2050/1 390696704]
FX We want to thank Mats Ole Ellenberg, Severin Engert, and Remke Dirk
   Albrecht for their support in creating this publication. This work was
   funded by DFG grant 389792660 as part of TRR 248 (see
   https://perspicuous-computing.science) and the DFG as part of Germany's
   Excellence Strategy Clusters of Excellence EXC-2068 390729961 Physics of
   Life and EXC 2050/1 390696704 Centre for Tactile Internet with
   Human-in-the-Loop (CeTI) of TU Dresden.
NR 92
TC 69
Z9 76
U1 3
U2 29
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1182
EP 1192
DI 10.1109/TVCG.2020.3030460
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100101
PM 33052863
OA Green Submitted, hybrid
DA 2025-03-07
ER

PT J
AU Monica, R
   Aleotti, J
AF Monica, Riccardo
   Aleotti, Jacopo
TI Surfel-Based Incremental Reconstruction of the Boundary Between Known
   and Unknown Space
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Cloud computing; Robot sensing systems;
   Image reconstruction; Cameras; Surface reconstruction; Solid modeling;
   Surfel based mapping; dense multi-view 3D reconstruction; range sensing
ID SLAM
AB This article presents the first surfel-based method for multi-view 3D reconstruction of the boundary between known and unknown space. The proposed approach integrates multiple views from a moving depth camera and it generates a set of surfels that encloses observed empty space, i.e., it models both the boundary between empty and occupied space, and the boundary between empty and unknown space. One novelty of the method is that it does not require a persistent voxel map of the environment to distinguish between unknown and empty space. The problem is solved thanks to an incremental algorithm that computes the Boolean union of two surfel bounded volumes: the known volume from previous frames and the space observed from the current depth image. A number of strategies were developed to cope with errors in surfel position and orientation. The method, implemented on CPU and GPU, was evaluated on real data acquired in indoor scenarios, and it was compared against state of the art approaches. Results show that the proposed method has a low number of false positive and false negatives, it is faster than a standard volumetric algorithm, it has a lower memory consumption, and it scales better in large environments.
C1 [Monica, Riccardo; Aleotti, Jacopo] Univ Parma, Dept Engn & Architecture, Robot & Intelligent Machines Lab RIMLab, I-43124 Parma, PR, Italy.
C3 University of Parma
RP Monica, R (corresponding author), Univ Parma, Dept Engn & Architecture, Robot & Intelligent Machines Lab RIMLab, I-43124 Parma, PR, Italy.
EM riccardo.monica@unipr.it; jacopo.aleotti@unipr.it
OI Monica, Riccardo/0000-0002-1262-6348
NR 28
TC 1
Z9 1
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG. 1
PY 2020
VL 26
IS 8
BP 2683
EP 2695
DI 10.1109/TVCG.2020.2990315
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MG6BL
UT WOS:000546115000010
PM 32340950
DA 2025-03-07
ER

PT J
AU Peck, TC
   Tutar, A
AF Peck, Tabitha C.
   Tutar, Altan
TI The Impact of a Self-Avatar, Hand Collocation, and Hand Proximity on
   Embodiment and Stroop Interference
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 27th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 22-26, 2020
CL Atlanta, GA
SP IEEE, IEEE Comp Soc
DE Task analysis; Avatars; Interference; Visualization; Cognition; Sensors;
   Resists; Virtual reality; avatars; embodiment; user-studies; cognition;
   Stroop interference test
ID WORKING-MEMORY; ALTERS VISION; POSTURE; TOUCH; FEEL
AB Understanding the effects of hand proximity to objects and tasks is critical for hand-held and near-hand objects. Even though self-avatars have been shown to be beneficial for various tasks in virtual environments, little research has investigated the effect of avatar hand proximity on working memory. This paper presents a between-participants user study investigating the effects of self-avatars and physical hand proximity on a common working memory task, the Stroop interference task. Results show that participants felt embodied when a self-avatar was in the scene, and that the subjective level of embodiment decreased when a participant's hands were not collocated with the avatar's hands. Furthermore, a participant's physical hand placement was significantly related to Stroop interference: proximal hands produced a significant increase in accuracy compared to non-proximal hands. Surprisingly, Stroop interference was not mediated by the existence of a self-avatar or level of embodiment.
C1 [Peck, Tabitha C.; Tutar, Altan] Davidson Coll, Davidson, NC 28035 USA.
C3 Davidson College
RP Peck, TC (corresponding author), Davidson Coll, Davidson, NC 28035 USA.
EM tapeck@davidson.edu; altutar@davidson.edu
RI Peck, Tabitha/AAH-2032-2021
NR 34
TC 19
Z9 21
U1 1
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2020
VL 26
IS 5
BP 1964
EP 1971
DI 10.1109/TVCG.2020.2973061
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA LA1WU
UT WOS:000523746000015
PM 32070969
DA 2025-03-07
ER

PT J
AU Bader, R
   Sprenger, M
   Ban, N
   Rüdisühli, S
   Scär, C
   Günther, T
AF Bader, Robin
   Sprenger, Michael
   Ban, Nikolina
   Ruedisuehli, Stefan
   Schaer, Christoph
   Guenther, Tobias
TI Extraction and Visual Analysis of Potential Vorticity Banners around the
   Alps
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Scientific Visualization; potential vorticity; meteorology; feature
   extraction
ID OF-THE-ART; VORTEX; VISUALIZATION; IDENTIFICATION; CYCLOGENESIS;
   FEATURES; LEE
AB Potential vorticity is among the most important scalar quantities in atmospheric dynamics. For instance, potential vorticity plays a key role in particularly strong wind peaks in extratropical cyclones and it is able to explain the occurrence of frontal rain bands. Potential vorticity combines the key quantities of atmospheric dynamics, namely rotation and stratification. Under suitable wind conditions elongated banners of potential vorticity appear in the lee of mountains. Their role in atmospheric dynamics has recently raised considerable interest in the meteorological community for instance due to their influence in aviation wind hazards and maritime transport. In order to support meteorologists and climatologists in the analysis of these structures, we developed an extraction algorithm and a visual exploration framework consisting of multiple linked views. For the extraction we apply a predictor-corrector algorithm that follows streamlines and realigns them with extremal lines of potential vorticity. Using the agglomerative hierarchical clustering algorithm, we group banners from different sources based on their proximity. To visually analyze the time-dependent banner geometry, we provide interactive overviews and enable the query for detail on demand, including the analysis of different time steps, potentially correlated scalar quantities, and the wind vector field. In particular, we study the relationship between relative humidity and the banners for their potential in indicating the development of precipitation. Working with our method, the collaborating meteorologists gained a deeper understanding of the three-dimensional processes, which may spur follow-up research in the future.
C1 [Bader, Robin; Guenther, Tobias] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
   [Sprenger, Michael; Ban, Nikolina; Ruedisuehli, Stefan; Schaer, Christoph] Swiss Fed Inst Technol, Inst Atmospher & Climate Sci, Zurich, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich; Swiss Federal
   Institutes of Technology Domain; ETH Zurich
RP Bader, R (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM baderr@student.ethz.ch; michael.sprenger@env.ethz.ch;
   nikolina.ban@env.ethz.ch; stefan.ruedisuehli@env.ethz.ch;
   schaer@env.ethz.ch; tobias.guenther@inf.ethz.ch
RI Schar, Christoph/A-1033-2008
OI Sprenger, Michael/0000-0002-9317-8822; Ban,
   Nikolina/0000-0002-1672-3655; Gunther, Tobias/0000-0002-3020-0930
FU Swiss National Science Foundation (SNSF) Ambizione grant [PZ00P2
   180114]; Swiss National Science Foundation under Sinergia grant [CRSII2
   154486/1 crCLIM]; Swiss National Science Foundation (SNF)
   [PZ00P2_180114] Funding Source: Swiss National Science Foundation (SNF)
FX This work was supported by the Swiss National Science Foundation (SNSF)
   Ambizione grant no. PZ00P2 180114. The atmospheric model simulations
   have been supported by the Swiss National Science Foundation under
   Sinergia grant CRSII2 154486/1 crCLIM. In addition, we acknowledge PRACE
   for awarding us access to Piz Daint at CSCS (Switzerland) where the
   simulations are integrated.
NR 48
TC 10
Z9 10
U1 1
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 259
EP 269
DI 10.1109/TVCG.2019.2934310
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100024
PM 31425096
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Yu, BW
   Silva, CT
AF Yu, Bowen
   Silva, Claudio T.
TI FlowSense: A Natural Language Interface for Visual Data Exploration
   within a Dataflow System
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Natural language interface; dataflow visualization system; visual data
   exploration
ID VISUALIZATION; ENVIRONMENT
AB Dataflow visualization systems enable flexible visual data exploration by allowing the user to construct a dataflow diagram that composes query and visualization modules to specify system functionality. However learning dataflow diagram usage presents overhead that often discourages the user. In this work we design FlowSense, a natural language interface for dataflow visualization systems that utilizes state-of-the-art natural language processing techniques to assist dataflow diagram construction. FlowSense employs a semantic parser with special utterance tagging and special utterance placeholders to generalize to different datasets and dataflow diagrams. It explicitly presents recognized dataset and diagram special utterances to the user for dataflow context awareness. With FlowSense the user can expand and adjust dataflow diagrams more conveniently via plain English. We apply FlowSense to the VisFlow subset-flow visualization system to enhance its usability. We evaluate FlowSense by one case study with domain experts on a real-world data analysis problem and a formal user study.
C1 [Yu, Bowen; Silva, Claudio T.] NYU, New York, NY 10003 USA.
C3 New York University
RP Yu, BW (corresponding author), NYU, New York, NY 10003 USA.
EM bowen.yu@nyu.edu; csilva@nyu.edu
FU Moore-Sloan Data Science Environment at NYU; NASA; NSF [CNS-1229185,
   CCF-1533564, CNS-1544753, CNS-1730396, CNS-1828576]; DARPA MEMEX
   program; DARPA D3M program
FX We would like to thank BlindData.com for providing the user study
   dataset. This work was supported in part by: the Moore-Sloan Data
   Science Environment at NYU; NASA; NSF awards CNS-1229185, CCF-1533564,
   CNS-1544753, CNS-1730396, CNS-1828576. B. Yu and C. T. Silva are
   partially supported by the DARPA MEMEX and D3M programs. Any opinions,
   findings, and conclusions or recommendations expressed in this material
   are those of the authors and do not necessarily reflect the views of
   DARPA.
NR 48
TC 74
Z9 88
U1 0
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 1
EP 11
DI 10.1109/TVCG.2019.2934668
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100001
PM 31443010
DA 2025-03-07
ER

PT J
AU Kovacs, B
   O'Donovan, P
   Bala, K
   Hertzmann, A
AF Kovacs, Balazs
   O'Donovan, Peter
   Bala, Kavita
   Hertzmann, Aaron
TI Context-Aware Asset Search for Graphic Design
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Graphic design; machine learning; AB testing; image search; user
   interfaces; color
AB Graphic design tools provide powerful controls for expert-level design creation, but the options can often be overwhelming for novices. This paper proposes Context-Aware Asset Search tools that take the current state of the user's design into account, thereby providing search and selections that are compatible with the current design and better fit the user's needs. In particular, we focus on image search and color selection, two tasks that are central to design. We learn a model for compatibility of images and colors within a design, using crowdsourced data. We then use the learned model to rank image search results or color suggestions during design. We found counterintuitive behavior using conventional training with pairwise comparisons for image search, where models with and without compatibility performed similarly. We describe a data collection procedure that alleviates this problem. We show that our method outperforms baseline approaches in quantitative evaluation, and we also evaluate a prototype interactive design tool.
C1 [Kovacs, Balazs; Bala, Kavita] Cornell Univ, Ithaca, NY 14850 USA.
   [O'Donovan, Peter; Hertzmann, Aaron] Adobe Syst Inc, San Jose, CA 95110 USA.
C3 Cornell University; Adobe Systems Inc.
RP Hertzmann, A (corresponding author), Adobe Syst Inc, San Jose, CA 95110 USA.
EM kockafely@gmail.com; peter.odonovan@gmail.com; kb@cs.cornell.edu;
   hertzman@dgp.toronto.edu
OI Kovacs, Balazs/0000-0003-4265-6609; Hertzmann,
   Aaron/0000-0001-9667-0292; Bala, Kavita/0000-0001-9761-6503
FU US National Science Foundation [IIS-1617861]; Adobe
FX Thanks to Andy Edmonds for useful feedback. This work was supported by
   the US National Science Foundation (grant IIS-1617861), and Adobe.
NR 46
TC 7
Z9 8
U1 0
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2019
VL 25
IS 7
BP 2419
EP 2429
DI 10.1109/TVCG.2018.2842734
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IA8WG
UT WOS:000469838700007
PM 29993550
DA 2025-03-07
ER

PT J
AU Aksit, K
   Chakravarthula, P
   Rathinavel, K
   Jeong, Y
   Albert, R
   Fuchs, H
   Luebke, D
AF Aksit, Kaan
   Chakravarthula, Praneeth
   Rathinavel, Kishore
   Jeong, Youngmo
   Albert, Rachel
   Fuchs, Henry
   Luebke, David
TI Manufacturing Application-Driven Foveated Near-Eye Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Near-eye displays; See-through Displays; Application Adaptive Displays;
   Computational Displays; Augmented Reality Displays; 3D printed optical
   components; Waveguides; projection displays
ID RESOLUTION
AB Traditional optical manufacturing poses a great challenge to near-eye display designers due to large lead times in the order of multiple weeks. limiting the abilities of optical designers to iterate fast and explore beyond conventional designs. We present a complete near-eye display manufacturing pipeline with a day lead time using commodity hardware. Our novel manufacturing pipeline consists of several innovations including a rapid production technique to improve surface of a 3D printed component to optical quality suitable for near-eye display application, a computational design methodology using machine learning and ray tracing to create freeform static projection screen surfaces for near-eye displays that can represent arbitrary focal surfaces, and a custom projection lens design that distributes pixels non-uniformly for a foveated near-eye display hardware design candidate. We have demonstrated untethered augmented reality near-eye display prototypes to assess success of our technique, and show that a ski-goggles form factor, a large monocular field of view (30 degrees x 55 degrees), and a resolution of 12 cycles per degree can be achieved.
C1 [Aksit, Kaan; Jeong, Youngmo; Albert, Rachel; Luebke, David] Nvidia, Santa Clara, CA 95051 USA.
   [Chakravarthula, Praneeth; Rathinavel, Kishore; Fuchs, Henry] Univ N Carolina, Chapel Hill, NC USA.
C3 Nvidia Corporation; University of North Carolina; University of North
   Carolina Chapel Hill
RP Aksit, K (corresponding author), Nvidia, Santa Clara, CA 95051 USA.
EM kaksit@nvidia.com; fuchs@cs.unc.edu
RI Aksit, Kaan/AAY-6704-2020; Chakravarthula, Praneeth Kumar/AFZ-2211-2022
OI AKSIT, KAAN/0000-0002-5934-5500; Brown, Rachel/0000-0003-4779-7873
NR 93
TC 44
Z9 46
U1 0
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2019
VL 25
IS 5
BP 1928
EP 1939
DI 10.1109/TVCG.2019.2898781
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HR3EI
UT WOS:000463019100012
PM 30794179
DA 2025-03-07
ER

PT J
AU Bertel, T
   Campbell, NDF
   Richardt, C
AF Bertel, Tobias
   Campbell, Neill D. F.
   Richardt, Christian
TI MegaParallax: Casual 360° Panoramas with Motion Parallax
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Casual 360 degrees scene capture; plenoptic modeling; image-based
   rendering; novel-view synthesis; virtual reality
ID STEREO
AB The ubiquity of smart mobile devices, such as phones and tablets, enables users to casually capture 360 degrees panoramas with a single camera sweep to share and relive experiences. However. panoramas lack motion parallax as they do not provide different views for different viewpoints. The motion parallax induced by translational head motion is a crucial depth cue in daily life. Alternatives, such as omnidirectional stereo panoramas, provide different views for each eye (binocular disparity), but they also lack motion parallax as the left and right eye panoramas are stitched statically. Methods based on explicit scene geometry reconstruct textured 3D geometry, which provides motion parallax, but suffers from visible reconstruction artefacts. The core of our method is a novel multi-perspective panorama representation, which can be casually captured and rendered with motion parallax for each eye on the fly. This provides a more realistic perception of panoramic environments which is particularly useful for virtual reality applications. Our approach uses a single consumer video camera to acquire 200-400 views of a real 360 degrees environment with a single sweep. By using novel-view synthesis with flow-based blending, we show how to turn these input views into an enriched 360 degrees panoramic experience that can be explored in real time, without relying on potentially unreliable reconstruction of scene geometry. We compare our results with existing omnidirectional stereo and image-based rendering methods to demonstrate the benefit of our approach, which is the first to enable casual consumers to capture and view high-quality 360 degrees panoramas with motion parallax.
C1 [Bertel, Tobias; Campbell, Neill D. F.; Richardt, Christian] Univ Bath, Dept Comp Sci, Bath, Avon, England.
C3 University of Bath
RP Bertel, T (corresponding author), Univ Bath, Dept Comp Sci, Bath, Avon, England.
EM T.B.Bertel@bath.ac.uk; N.Campbell@bath.ac.uk; christian@richardt.name
OI Richardt, Christian/0000-0001-6716-9845; Campbell,
   Neill/0000-0003-2130-4903
FU EU Horizon 2020 MSCA grant FIRE [665992]; EPSRC Centre for Doctoral
   Training in Digital Entertainment [EP/L016540/1]; RCUK grant CAMERA
   [EP/M023281/1]; UKRI Innovation Fellowship [EP/S001050/1]; Rabin Ezra
   Scholarship; NVIDIA Corporation GPU Grant; EPSRC [EP/M023281/1] Funding
   Source: UKRI; ISCF [EP/S001050/1] Funding Source: UKRI
FX This work was supported by EU Horizon 2020 MSCA grant FIRE (665992), the
   EPSRC Centre for Doctoral Training in Digital Entertainment
   (EP/L016540/1), RCUK grant CAMERA (EP/M023281/1), a UKRI Innovation
   Fellowship (EP/S001050/1), a Rabin Ezra Scholarship and an NVIDIA
   Corporation GPU Grant.
NR 38
TC 30
Z9 32
U1 2
U2 32
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2019
VL 25
IS 5
BP 1828
EP 1835
DI 10.1109/TVCG.2019.2898799
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HR3EI
UT WOS:000463019100002
PM 30802864
OA Green Published
DA 2025-03-07
ER

PT J
AU Lobo, MJ
   Appert, C
   Pietriga, E
AF Lobo, Maria-Jesus
   Appert, Caroline
   Pietriga, Emmanuel
TI Animation Plans for Before-and-After Satellite Images
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Animation; blending; staging; remote sensing images
ID TRANSITIONS; VISUALIZATION; SURGE
AB Before-and-after image pairs show how entities in a given region have evolved over a specific period of time. Satellite images are a major source of such data, that capture how natural phenomena or human activity impact a geographical area. These images are used both for data analysis and to illustrate the resulting findings to diverse audiences. The simple techniques used to display them, including juxtaposing, swapping and monolithic blending, often fail to convey the underlying phenomenon in a meaningful manner. We introduce Baia, a framework to create advanced animated transitions, called animation plans, between before-and-after images. Baia relies on a pixel-based transition model that gives authors much expressive power, while keeping animations for common types of changes easy to create thanks to predefined animation primitives. We describe our model, the associated animation editor, and report on two user studies. In the first study, advanced transitions enabled by Baia were compared to monolithic blending, and perceived as more realistic and better at focusing viewer's attention on a region of interest than the latter. The second study aimed at gathering feedback about the usability of Baia's animation editor.
C1 [Lobo, Maria-Jesus; Appert, Caroline; Pietriga, Emmanuel] Univ Paris Sud, F-91400 Orsay, France.
   [Lobo, Maria-Jesus; Appert, Caroline; Pietriga, Emmanuel] Univ Paris Saclay, INRIA, CNRS, F-91400 Orsay, France.
C3 Universite Paris Saclay; Universite Paris Saclay; Centre National de la
   Recherche Scientifique (CNRS); Inria; Microsoft
RP Lobo, MJ (corresponding author), Univ Paris Sud, F-91400 Orsay, France.; Lobo, MJ (corresponding author), Univ Paris Saclay, INRIA, CNRS, F-91400 Orsay, France.
EM maria-jesus.lobo@inria.fr; appert@lri.fr; emmanuel.pietriga@inria.fr
OI Pietriga, Emmanuel/0000-0002-9762-0462; Appert,
   Caroline/0000-0002-3050-9284
FU ANR project Map-Muxing [ANR-14-CE24-0011-02]; Agence Nationale de la
   Recherche (ANR) [ANR-14-CE24-0011] Funding Source: Agence Nationale de
   la Recherche (ANR)
FX This research was partly supported by ANR project Map-Muxing
   (ANR-14-CE24-0011-02).
NR 37
TC 7
Z9 7
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2019
VL 25
IS 2
BP 1347
EP 1360
DI 10.1109/TVCG.2018.2796557
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA HG5ZY
UT WOS:000455062000009
PM 29994421
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Dowling, M
   Wenskovitch, J
   Fry, JT
   Leman, S
   House, L
   North, C
AF Dowling, Michelle
   Wenskovitch, John
   Fry, J. T.
   Leman, Scotland
   House, Leanna
   North, Chris
TI SIRIUS: Dual, Symmetric, Interactive Dimension Reductions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Dimension reduction; semantic interaction; exploratory data analysis;
   observation projection; attribute projection
ID SEMANTIC INTERACTION; VISUAL ANALYSIS; VISUALIZATION; EXPLORATION;
   MODEL; FIT
AB Much research has been done regarding how to visualize and interact with observations and attributes of high-dimensional data for exploratory data analysis. From the analyst's perceptual and cognitive perspective, current visualization approaches typically treat the observations of the high-dimensional dataset very differently from the attributes. Often, the attributes are treated as inputs (e.g., sliders), and observations as outputs (e.g., projection plots), thus emphasizing investigation of the observations. However, there are many cases in which analysts wish to investigate both the observations and the attributes of the dataset, suggesting a symmetry between how analysts think about attributes and observations. To address this, we define SIRIUS (Symmetric Interactive Representations In a Unified System), a symmetric, dual projection technique to support exploratory data analysis of high-dimensional data. We provide an example implementation of SIRIUS and demonstrate how this symmetry affords additional insights.
C1 [Dowling, Michelle; Wenskovitch, John; North, Chris] Virginia Tech, Dept Comp Sci, Blacksburg, VA 24061 USA.
   [Fry, J. T.; Leman, Scotland; House, Leanna] Virginia Tech, Dept Stat, Blacksburg, VA USA.
C3 Virginia Polytechnic Institute & State University; Virginia Polytechnic
   Institute & State University
RP Dowling, M (corresponding author), Virginia Tech, Dept Comp Sci, Blacksburg, VA 24061 USA.
EM dowlingm@cs.vt.edu; jw87@cs.vt.edu; fryjt1@vt.edu; leman@vt.edu;
   lhouse@vt.edu; north@cs.vt.edu
RI Wenskovitch, John/AAY-4371-2020
OI Wenskovitch, John/0000-0002-0573-6442; Dowling,
   Michelle/0000-0002-2572-1133
FU NSF [IIS-1447416]
FX This research was partially supported by NSF grant IIS-1447416. We also
   thank the BaVA @ VT research group members for their contributions in
   developing SIRIUS as well as the reviewers for helping improve this
   paper.
NR 59
TC 24
Z9 30
U1 1
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 172
EP 182
DI 10.1109/TVCG.2018.2865047
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000017
PM 30136978
DA 2025-03-07
ER

PT J
AU Raith, F
   Blecha, C
   Nagel, T
   Parisio, F
   Kolditz, O
   Günther, F
   Stommel, M
   Scheuermann, G
AF Raith, Felix
   Blecha, Christian
   Nagel, Thomas
   Parisio, Francesco
   Kolditz, Olaf
   Guenther, Fabian
   Stommel, Markus
   Scheuermann, Gerik
TI Tensor Field Visualization using Fiber Surfaces of Invariant Space
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE visualization; tensor field; invariants; fiber surface; interaction
AB Scientific visualization developed successful methods for scalar and vector fields. For tensor fields, however, effective, interactive visualizations are still missing despite progress over the last decades. We present a general approach for the generation of separating surfaces in symmetric, second-order, three-dimensional tensor fields. These surfaces are defined as fiber surfaces of the invariant space, i.e. as pre-images of surfaces in the range of a complete set of invariants. This approach leads to a generalization of the fiber surface algorithm by Klacansky et al. [16] to three dimensions in the range. This is due to the fact that the invariant space is three-dimensional for symmetric second-order tensors over a spatial domain. We present an algorithm for surface construction for simplicial grids in the domain and simplicial surfaces in the invariant space. We demonstrate our approach by applying it to stress fields from component design in mechanical engineering.
C1 [Raith, Felix; Blecha, Christian; Scheuermann, Gerik] Univ Leipzig, Inst Comp Sci, PF 100920, D-04009 Leipzig, Germany.
   [Nagel, Thomas; Parisio, Francesco; Kolditz, Olaf] Helmholtz Ctr Environm Res, Dept Environm Informat, Permoserstr 15, D-04318 Leipzig, Germany.
   [Guenther, Fabian; Stommel, Markus] TU Dortmund Univ, Fac Mech Engn, Leonhard Euler Str 5, D-44227 Dortmund, Germany.
C3 Leipzig University; Helmholtz Association; Helmholtz Center for
   Environmental Research (UFZ); Dortmund University of Technology
RP Raith, F (corresponding author), Univ Leipzig, Inst Comp Sci, PF 100920, D-04009 Leipzig, Germany.
RI Parisio, Francesco/ABE-7343-2021; Nagel, Thomas/T-8805-2019
OI Stommel, Markus/0000-0002-0406-5800; Nagel, Thomas/0000-0001-8459-4616;
   Raith, Felix/0000-0002-3505-6130; Kolditz, Olaf/0000-0002-8098-4905;
   Parisio, Francesco/0000-0002-1798-3993
FU German Federal Ministry of Education and Research within the project
   Competence Center for Scalable Data Services and Solutions (ScaDS)
   Dresden/Leipzig [BMBF 01IS14014B]
FX This work was funded by the German Federal Ministry of Education and
   Research within the project Competence Center for Scalable Data Services
   and Solutions (ScaDS) Dresden/Leipzig (BMBF 01IS14014B).
NR 35
TC 16
Z9 16
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 1122
EP 1131
DI 10.1109/TVCG.2018.2864846
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000107
PM 30176596
DA 2025-03-07
ER

PT J
AU Sacha, D
   Kraus, M
   Keim, DA
   Chen, M
AF Sacha, Dominik
   Kraus, Matthias
   Keim, Daniel A.
   Chen, Min
TI VIS4ML: An Ontology for Visual Analytics Assisted Machine Learning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual Analytics; Visualization; Machine Learning; Human-Computer
   Interaction; Ontology; VIS4ML
ID CLUSTER-ANALYSIS; VISUALIZATION; MODEL
AB While many VA workflows make use of machine-learned models to support analytical tasks. VA workflows have become increasingly important in understanding and improving Machine Learning (ML) processes. In this paper. we propose an ontology (VIS4ML) for a subarea of VA, namely "VA-assisted ML'. The purpose of VIS4ML is to describe and understand existing VA workflows used in ML as well as to detect gaps in ML processes and the potential of introducing advanced VA techniques to such processes. Ontologies have been widely used to map out the scope of a topic in biology, medicine, and many other disciplines. We adopt the scholarly methodologies for constructing VIS4ML, including the specification, conceptualization, formalization, implementation, and validation of ontologies. In particular, we reinterpret the traditional VA pipeline to encompass model-development workflows. We introduce necessary definitions, rules, syntaxes, and visual notations for formulating VIS4ML and make use of semantic web technologies for implementing it in the Web Ontology Language (OWL). VIS4ML captures the high-level knowledge about previous workflows where VA is used to assist in ML. It is consistent with the established VA concepts and will continue to evolve along with the future developments in VA and ML. While this ontology is an effort for building the theoretical foundation of VA, it can be used by practitioners in real-world applications to optimize model-development workflows by systematically examining the potential benefits that can be brought about by either machine or human capabilities. Meanwhile, VIS4ML is intended to be extensible and will continue to be updated to reflect future advancements in using VA for building high-quality data-analytical models or for building such models rapidly.
C1 [Sacha, Dominik; Kraus, Matthias; Keim, Daniel A.] Univ Konstanz, Constance, Germany.
   [Chen, Min] Univ Oxford, Oxford, England.
C3 University of Konstanz; University of Oxford
RP Sacha, D (corresponding author), Univ Konstanz, Constance, Germany.
EM sacha@dbvis.inf.uni-konstanz.de; kraus@dbvis.inf.uni-konstanz.de;
   keim@dbvis.inf.uni-konstanz.de; min.chen@oerc.ox.ac.uk
RI Keim, Daniel/X-7749-2019
NR 71
TC 66
Z9 81
U1 0
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 385
EP 395
DI 10.1109/TVCG.2018.2864838
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000037
PM 30130221
OA Green Submitted, Green Published
DA 2025-03-07
ER

PT J
AU Xie, C
   Xu, W
   Mueller, K
AF Xie, Cong
   Xu, Wei
   Mueller, Klaus
TI A Visual Analytics Framework for the Detection of Anomalous Call Stack
   Trees in High Performance Computing Applications
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Call Stack; Performance Visualization; Representation Learning; Active
   Learning; Anomaly Detection
AB Anomalous runtime behavior detection is one of the most important tasks for performance diagnosis in High Performance Computing (HPC). Most of the existing methods find anomalous executions based on the properties of individual functions, such as execution time. However, it is insufficient to identify abnormal behavior without taking into account the context of the executions, such as the invocations of children functions and the communications with other HPC nodes. We improve upon the existing anomaly detection approaches by utilizing the call stack structures of the executions, which record rich temporal and contextual information. With our call stack tree (CSTree) representation of the executions, we formulate the anomaly detection problem as finding anomalous tree structures in a call stack forest. The CSTrees are converted to vector representations using our proposed stack2vec embedding. Structural and temporal visualizations of CSTrees are provided to support users in the identification and verification of the anomalies during an active anomaly detection process. Three case studies of real-world HPC applications demonstrate the capabilities of our approach.
C1 [Xie, Cong; Mueller, Klaus] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Xu, Wei] Brookhaven Natl Lab, Computat Sci Initiat, Upton, NY 11973 USA.
C3 State University of New York (SUNY) System; Stony Brook University;
   United States Department of Energy (DOE); Brookhaven National Laboratory
RP Xie, C (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM coxie@cs.stonybrook.edu; xuw@bnl.gov; mueller@cs.stonybrook.edu
FU NSF [IIS 1527200]; BNL LDRD grant [16-041, 18-009]; MSIP (Ministry of
   Science, ICT and Future Planning), Korea under "IT Consilience Creative
   Program (ITCCP)"; ECP CODAR project [17-SC-20-SC]
FX We thank Shinjae Yoo, Wen Zhong and flailing Yan for helpful
   discussions. This research was partially supported by NSF grant IIS
   1527200, BNL LDRD grant 16-041 and 18-009, ECP CODAR project
   17-SC-20-SC, and the MSIP (Ministry of Science, ICT and Future
   Planning), Korea, under "IT Consilience Creative Program (ITCCP)"
   supervised by NIPA.
NR 55
TC 35
Z9 40
U1 1
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 215
EP 224
DI 10.1109/TVCG.2018.2865026
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000021
PM 30136972
DA 2025-03-07
ER

PT J
AU Xu, K
   Xia, M
   Mu, X
   Wang, Y
   Cao, N
AF Xu, Ke
   Xia, Meng
   Mu, Xing
   Wang, Yun
   Cao, Nan
TI EnsembleLens: Ensemble-based Visual Exploration of Anomaly Detection
   Algorithms with Multidimensional Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Algorithm Evaluation; Ensemble Analysis; Anomaly Detection; Visual
   Analysis; Multidimensional Data
ID OUTLIER DETECTION; VISUALIZATION; VIDEO; TOOL
AB The results of anomaly detection are sensitive to the choice of detection algorithms as they are specialized for different properties of data, especially for multidimensional data. Thus, it is vital to select the algorithm appropriately. To systematically select the algorithms, ensemble analysis techniques have been developed to support the assembly and comparison of heterogeneous algorithms. However, challenges remain due to the absence of the ground truth, interpretation, or evaluation of these anomaly detectors. In this paper, we present a visual analytics system named EnsembleLens that evaluates anomaly detection algorithms based on the ensemble analysis process. The system visualizes the ensemble processes and results by a set of novel visual designs and multiple coordinated contextual views to meet the requirements of correlation analysis, assessment and reasoning of anomaly detection algorithms. We also introduce an interactive analysis workflow that dynamically produces contextualized and interpretable data summaries that allow further refinements of exploration results based on user feedback. We demonstrate the effectiveness of EnsembleLens through a quantitative evaluation, three case studies with real-world data and interviews with two domain experts.
C1 [Xu, Ke; Xia, Meng; Mu, Xing; Wang, Yun] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Cao, Nan] Tongji Univ, IDVx Lab, Shanghai, Peoples R China.
C3 Hong Kong University of Science & Technology; Tongji University
RP Cao, N (corresponding author), Tongji Univ, IDVx Lab, Shanghai, Peoples R China.
EM kxuak@connect.ust.hk; iris.xia@connect.ust.hk; hecate.l.mu@gmail.com;
   ywangch@connect.ust.hk; nan.cao@tongji.edu.cn
RI XIA, MENG/HJI-4722-2023; Cao, Nan/O-5397-2014; Xu, Ke/HSH-1984-2023
OI Xia, Meng/0000-0002-2676-9032
FU Fundamental Research Funds for the Central Universities; NFSC [61602306]
FX We would like to thank all the reviewers and domain experts for their
   comments. This work is part of the research supported by NFSC
   Grants-61602306, and the Fundamental Research Funds for the Central
   Universities.
NR 80
TC 32
Z9 37
U1 0
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 109
EP 119
DI 10.1109/TVCG.2018.2864825
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000011
PM 30130216
DA 2025-03-07
ER

PT J
AU Pjanic, P
   Willi, S
   Iwai, D
   Grundhöfer, A
AF Pjanic, Petar
   Willi, Simon
   Iwai, Daisuke
   Grundhofer, Anselm
TI Seamless Multi-Projection Revisited
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 17th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 16-20, 2018
CL Munich, GERMANY
SP IEEE, IEEE Comp Soc, IEEE VGTC, ACM SIGCHI, Mozilla, Apple, Intel, DAQRI, PTC, Amazon, Facebook, Qualcomm, Umajin, Disney Res, Univ S Australia Ventures Pty Ltd, REFLEKT, Occipital, Envisage AR, KHRONOS Grp, TUM, ETH Zurich
DE Projector-camera systems; colorimetric calibration; 3D stereoscopic and
   multi-user entertainment
ID COLOR; CALIBRATION; DISPLAYS
AB This paper introduces a novel photometric compensation technique for inter-projector luminance and chrominance variations. Although it sounds as a classical technical issue, to the best of our knowledge there is no existing solution to alleviate the spatial non-uniformity among strongly heterogeneous projectors at perceptually acceptable quality. Primary goal of our method is increasing the perceived seamlessness of the projection system by automatically generating an improved and consistent visual quality. It builds upon the existing research of multi-projection systems, but instead of working with perceptually non-uniform color spaces such as CIEXYZ, the overall computation is carried out using the RLab [10, pp. 243-254] color appearance model which models the color processing in an adaptive, perceptual manner. Besides, we propose an adaptive color gamut acquisition, spatially varying gamut mapping, and optimization framework for edge blending. The paper describes the overall workflow and detailed algorithm of each component, followed by an evaluation validating the proposed method. The experimental results both qualitatively and quantitatively show the proposed method significant improved the visual quality of projected results of a multi-projection display with projectors with severely heterogeneous color processing.
C1 [Pjanic, Petar; Willi, Simon; Grundhofer, Anselm] Disney Resaerch, Los Angeles, CA 91201 USA.
   [Iwai, Daisuke] Osaka Univ, Osaka, Japan.
C3 Osaka University
RP Pjanic, P (corresponding author), Disney Resaerch, Los Angeles, CA 91201 USA.
EM petar@disneyresearch.com; simon@disneyresearch.com;
   daisuke.iwai@sys.es.osaka-u.ac.jp; anselm@disneyresearch.com
RI Iwai, Daisuke/R-8174-2019
OI Iwai, Daisuke/0000-0002-3493-5635
NR 37
TC 8
Z9 8
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2018
VL 24
IS 11
BP 2963
EP 2973
DI 10.1109/TVCG.2018.2868597
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GZ0TM
UT WOS:000449077900014
PM 30346290
DA 2025-03-07
ER

PT J
AU Lu, YF
   Wang, H
   Landis, S
   Maciejewski, R
AF Lu, Yafeng
   Wang, Hong
   Landis, Steven
   Maciejewski, Ross
TI A Visual Analytics Framework for Identifying Topic Drivers in Media
   Events
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Semantic similarity; media annotation; visual analytics; causality
   modeling; social media
ID TEXT DATA; VISUALIZATION
AB Media data has been the subject of large scale analysis with applications of text mining being used to provide overviews of media themes and information flows. Such information extracted from media articles has also shown its contextual value of being integrated with other data, such as criminal records and stock market pricing. In this work, we explore linking textual media data with curated secondary textual data sources through user-guided semantic lexical matching for identifying relationships and data links. In this manner, critical information can be identified and used to annotate media timelines in order to provide a more detailed overview of events that may be driving media topics and frames. These linked events are further analyzed through an application of causality modeling to model temporal drivers between the data series. Such causal links are then annotated through automatic entity extraction which enables the analyst to explore persons, locations, and organizations that may be pertinent to the media topic of interest. To demonstrate the proposed framework, two media datasets and an armed conflict event dataset are explored.
C1 [Lu, Yafeng; Wang, Hong; Maciejewski, Ross] Arizona State Univ, Tempe, AZ 85287 USA.
   [Landis, Steven] Univ Nevada, Las Vegas, NV 89154 USA.
C3 Arizona State University; Arizona State University-Tempe; Nevada System
   of Higher Education (NSHE); University of Nevada Las Vegas
RP Lu, YF (corresponding author), Arizona State Univ, Tempe, AZ 85287 USA.
EM lyafeng@asu.edu; hxwang@asu.edu; steven.landis@unlv.edu;
   rmacieje@asu.edu
FU US Department of Homeland Security's VACCINE Center
   [2009-ST-061-CI0001]; US National Science Foundation [1350573, 1639227];
   Division Of Computer and Network Systems; Direct For Computer & Info
   Scie & Enginr [1639227] Funding Source: National Science Foundation
FX This work was supported by the US Department of Homeland Security's
   VACCINE Center, Award 2009-ST-061-CI0001 and the US National Science
   Foundation, Grant Nos. 1350573 and 1639227.
NR 49
TC 18
Z9 22
U1 0
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2018
VL 24
IS 9
BP 2501
EP 2515
DI 10.1109/TVCG.2017.2752166
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GP3XC
UT WOS:000440787200004
PM 28920902
OA Bronze
DA 2025-03-07
ER

PT J
AU Lynch, SD
   Kulpa, R
   Meerhoff, LA
   Pettré, J
   Crétual, A
   Olivier, AH
AF Lynch, Sean Dean
   Kulpa, Richard
   Meerhoff, Laurentius Antonius
   Pettre, Julien
   Cretual, Armel
   Olivier, Anne-Helene
TI Collision Avoidance Behavior between Walkers: Global and Local Motion
   Cues
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Locomotion; interaction; collision avoidance; virtual reality; motion
   perception
ID VISUAL GUIDANCE; WALKING SPEED; PERCEPTION; DYNAMICS; DISTANCE;
   INFORMATION; BODY
AB Daily activities require agents to interact with each other, such as during collision avoidance. The nature of visual information that is used for a collision free interaction requires further understanding. We aim to manipulate the nature of visual information in two forms, global and local information appearances. Sixteen healthy participants navigated towards a target in an immersive computer-assisted virtual environment (CAVE) using a joystick. A moving passive obstacle crossed the participant's trajectory perpendicularly at various pre-defined risks of collision distances. The obstacle was presented with one of five virtual appearances, associated to global motion cues (i.e., a cylinder or a sphere), or local motion cues (i.e., only the legs or the trunk). A full body virtual walker, showing both local and global motion cues, used as a reference condition. The final crossing distance was affected by the global motion appearances, however, appearance had no qualitative effect on motion adaptations. These findings contribute towards further understanding what information people use when interacting with others.
C1 [Lynch, Sean Dean; Kulpa, Richard; Meerhoff, Laurentius Antonius; Pettre, Julien; Cretual, Armel; Olivier, Anne-Helene] INRIA Rennes, F-35042 Rennes, France.
   [Lynch, Sean Dean; Kulpa, Richard; Meerhoff, Laurentius Antonius; Cretual, Armel; Olivier, Anne-Helene] Univ Rennes 2, Lab M2S, F-35042 Rennes, France.
C3 Universite de Rennes; Universite Rennes 2; Universite de Rennes
RP Lynch, SD (corresponding author), INRIA Rennes, F-35042 Rennes, France.; Lynch, SD (corresponding author), Univ Rennes 2, Lab M2S, F-35042 Rennes, France.
EM sean.lynch@etudiant.univ-rennes2.fr; richard.kulpa@univ-rennes2.fr;
   laurentius.meerhoff@inria.fr; julien.pettre@inria.fr;
   armel.cretual@univ-rennes2.fr; anne-helene.olivier@univ-rennes2.fr
RI Cretual, Armel/AFM-5749-2022; Olivier, Anne-Hélène/AAH-7378-2020;
   Pettré, Julien/AAB-2590-2022; Meerhoff, Laurentius/P-1187-2018
OI Meerhoff, Laurentius/0000-0003-4386-0919; Olivier,
   Anne-Helene/0000-0002-2833-020X
FU French National Research Agency, project Percolation [ANR-13-JS02-0008]
FX The research leading to these results has received funding from the
   French National Research Agency, project Percolation (ANR-13-JS02-0008).
   The authors would like to thank the Immersia team of Inria, Rennes and
   Francois Massy de la Chesneraye for their technical support and the
   volunteers who participated in this study.
NR 44
TC 27
Z9 27
U1 0
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2018
VL 24
IS 7
BP 2078
EP 2088
DI 10.1109/TVCG.2017.2718514
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GH3SG
UT WOS:000433321900002
PM 28650816
DA 2025-03-07
ER

PT J
AU Karch, GK
   Beck, F
   Ertl, M
   Meister, C
   Schulte, K
   Weigand, B
   Ertl, T
   Sadlo, F
AF Karch, Grzegorz Karol
   Beck, Fabian
   Ertl, Moritz
   Meister, Christian
   Schulte, Kathrin
   Weigand, Bernhard
   Ertl, Thomas
   Sadlo, Filip
TI Visual Analysis of Inclusion Dynamics in Two-Phase Flow
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA Esperanto
DT Article
DE Flow visualization; two-phase flow; feature deformation; space-time
   analysis; feature tracking
ID OF-THE-ART; FEATURE TRACKING; VISUALIZATION; INTERFACE; SURFACE
AB In single-phase flow visualization, research focuses on the analysis of vector field properties. In two-phase flow, in contrast, analysis of the phase components is typically of major interest. So far, visualization research of two-phase flow concentrated on proper interface reconstruction and the analysis thereof. In this paper, we present a novel visualization technique that enables the investigation of complex two-phase flow phenomena with respect to the physics of breakup and coalescence of inclusions. On the one hand, we adapt dimensionless quantities for a localized analysis of phase instability and breakup, and provide detailed inspection of breakup dynamics with emphasis on oscillation and its interplay with rotational motion. On the other hand, we present a parametric tightly linked space-time visualization approach for an effective interactive representation of the overall dynamics. We demonstrate the utility of our approach using several two-phase CFD datasets.
C1 [Karch, Grzegorz Karol] Univ Stuttgart, Visualizat Res Ctr, D-70174 Stuttgart, Germany.
   [Beck, Fabian] Univ Duisburg Essen, Inst Comp Sci & Business Informat Syst, D-47057 Duisburg, Germany.
   [Ertl, Moritz; Meister, Christian; Schulte, Kathrin; Weigand, Bernhard] Univ Stuttgart, Inst Aerosp Thermodynam, D-70174 Stuttgart, Germany.
   [Ertl, Thomas] Univ Stuttgart, Inst Visualizat & Interact Syst, D-70174 Stuttgart, Germany.
   [Sadlo, Filip] Heidelberg Univ, IWR, D-69120 Heidelberg, Germany.
C3 University of Stuttgart; University of Duisburg Essen; University of
   Stuttgart; University of Stuttgart; Ruprecht Karls University Heidelberg
RP Karch, GK (corresponding author), Univ Stuttgart, Visualizat Res Ctr, D-70174 Stuttgart, Germany.
EM grzegorz.karch@visus.uni-stuttgart.de; fabian.beck@wiwinf.uni-due.de;
   moritz.ertl@itlr.uni-stuttgart.de;
   christian.meister@itlr.uni-stuttgart.de;
   kathrin.eisenschmidt@itlr.uni-stuttgart.de;
   bernhard.weigand@itlr.uni-stuttgart.de;
   thomas.ertl@vis.uni-stuttgart.de; sadlo@uni-heidelberg.de
RI Sadlo, Filip/B-1624-2019; Schulte, Kathrin/ABC-1544-2020
OI Schulte, Kathrin/0000-0001-8650-5840; Ertl, Moritz/0000-0002-1900-5122;
   Weigand, Bernhard/0000-0002-1469-079X; Ertl, Thomas/0000-0003-4019-2505
FU Cluster of Excellence in Simulation Technology [EXC 310/1]; University
   of Stuttgart [SFB-TRR 75]
FX This work was supported in part by the Cluster of Excellence in
   Simulation Technology (EXC 310/1) and the Collaborative Research Centre
   SFB-TRR 75 at the University of Stuttgart.
NR 45
TC 6
Z9 6
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2018
VL 24
IS 5
BP 1841
EP 1855
DI 10.1109/TVCG.2017.2692781
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE7EW
UT WOS:000431397100012
PM 28422684
DA 2025-03-07
ER

PT J
AU Adams, H
   Narasimham, G
   Rieser, J
   Creem-Regehr, S
   Stefanucci, J
   Bodenheimer, B
AF Adams, Haley
   Narasimham, Gayathri
   Rieser, John
   Creem-Regehr, Sarah
   Stefanucci, Jeanine
   Bodenheimer, Bobby
TI Locomotive Recalibration and Prism Adaptation of Children and Teens in
   Immersive Virtual Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 25th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 18-22, 2018
CL Reutlingen, GERMANY
SP IEEE
DE Virtual environments; perceptual-motor recalibration; perception;
   children
ID TRAFFIC-FILLED INTERSECTIONS; PEDESTRIAN BEHAVIORS; DISTANCE JUDGMENTS;
   SPATIAL ALIGNMENT; PERCEPTION; ADULTS; CALIBRATION; TIME; AFFORDANCES;
   VISUOMOTOR
AB As virtual reality expands in popularity, an increasingly diverse audience is gaining exposure to immersive virtual environments (IVEs). A significant body of research has demonstrated how perception and action work in such environments, but most of this work has been done studying adults. Less is known about how physical and cognitive development affect perception and action in IVEs, particularly as applied to preteen and teenage children. Accordingly, in the current study we assess how preteens (children aged 8-12 years) and teenagers (children aged 15-18 years) respond to mismatches between their motor behavior and the visual information presented by an IVE. Over two experiments, we evaluate how these individuals recalibrate their actions across functionally distinct systems of movement. The first experiment analyzed forward walking recalibration after exposure to an IVE with either increased or decreased visual flow. Visual flow during normal bipedal locomotion was manipulated to be either twice or half as fast as the physical gait. The second experiment leveraged a prism throwing adaptation paradigm to test the effect of recalibration on throwing movement. In the first experiment, our results show no differences across age groups, although subjects generally experienced a post-exposure effect of shortened distance estimation after experiencing visually faster flow and longer distance estimation after experiencing visually slower flow. In the second experiment, subjects generally showed the typical prism adaptation behavior of a throwing after-effect error. The error lasted longer for preteens than older children. Our results have implications for the design of virtual systems with children as a target audience.
C1 [Adams, Haley; Bodenheimer, Bobby] Vanderbilt Univ, Dept Elect Engn & Comp Sci, 221 Kirkland Hall, Nashville, TN 37235 USA.
   [Narasimham, Gayathri] Vanderbilt Inst Digital Learning, Nashville, TN USA.
   [Rieser, John] Vanderbilt Univ, Dept Psychol & Human Dev, 221 Kirkland Hall, Nashville, TN 37235 USA.
   [Creem-Regehr, Sarah; Stefanucci, Jeanine] Univ Utah, Dept Psychol, Salt Lake City, UT 84112 USA.
C3 Vanderbilt University; Vanderbilt University; Utah System of Higher
   Education; University of Utah
RP Adams, H (corresponding author), Vanderbilt Univ, Dept Elect Engn & Comp Sci, 221 Kirkland Hall, Nashville, TN 37235 USA.
EM haley.a.adams@vanderbilt.edu; gayathri.narasimham@vanderbilt.edu;
   j.rieser@vanderbilt.edu; sarah.creem@psych.utah.edu;
   jeanine.stefanucci@psycho.utah.edu; bobby.bodenheimer@vanderbilt.edu
FU National Science Foundation [1116988, 1116636, 1526448]; Direct For
   Computer & Info Scie & Enginr; Div Of Information & Intelligent Systems
   [1116988] Funding Source: National Science Foundation; Div Of
   Information & Intelligent Systems; Direct For Computer & Info Scie &
   Enginr [1116636] Funding Source: National Science Foundation
FX The authors would like to thank Richard Paris, Noorin Asjad, and Joe
   Huang for advice and help during the project. We also thank Michael
   Butler, Gordon Cooper, and Banning Day for developing the recalibration
   environment. This material is based upon work supported by the National
   Science Foundation under grants 1116988, 1116636, and 1526448.
NR 80
TC 21
Z9 24
U1 2
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2018
VL 24
IS 4
BP 1408
EP 1417
DI 10.1109/TVCG.2018.2794072
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FZ6BS
UT WOS:000427682500003
PM 29543159
DA 2025-03-07
ER

PT J
AU Ghosh, S
   Winston, L
   Panchal, N
   Kimura-Thollander, P
   Hotnog, J
   Cheong, D
   Reyes, G
   Abowd, GD
AF Ghosh, Sarthak
   Winston, Lauren
   Panchal, Nishant
   Kimura-Thollander, Philippe
   Hotnog, Jeff
   Cheong, Douglas
   Reyes, Gabriel
   Abowd, Gregory D.
TI NotifiVR: Exploring Interruptions and Notifications in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 25th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 18-22, 2018
CL Reutlingen, GERMANY
SP IEEE
DE Virtual Reality; Notifications; Interruptibility; Multi-Modal; Feedback;
   Context Awareness
AB The proliferation of high resolution and affordable virtual reality (VR) headsets is quickly making room-scale VR experiences available in our homes. Most VR experiences strive to achieve complete immersion by creating a disconnect from the real world. However, due to the lack of a standardized notification management system and minimal context awareness in VR, an immersed user may face certain situations such as missing an important phone call (digital scenario), tripping over wandering pets (physical scenario), or losing track of time (temporal scenario). In this paper, we present the results of 1) a survey across 61 VR users to understand common interruptions and scenarios that would benefit from some form of notifications; 2) a design exercise with VR professionals to explore possible notification methods; and 3) an empirical study on the noticeability and perception of 5 different VR interruption scenarios across 6 modality combinations (e.g., audio, visual, haptic, audio + haptic, visual + haptic, and audio + visual) implemented in Unity and presented using the HTC Vive headset. Finally, we combine key learnings from each of these steps along with participant feedback to present a set of observations and recommendations for notification design in VR.
C1 [Ghosh, Sarthak; Winston, Lauren; Panchal, Nishant; Kimura-Thollander, Philippe; Hotnog, Jeff; Cheong, Douglas; Reyes, Gabriel; Abowd, Gregory D.] Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology
RP Ghosh, S (corresponding author), Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA.
EM sarthak.ghosh@gatech.edu; lwinston@gatech.edu; nish@gatech.edu;
   philkt@gatech.edu; jhotnog3@gatech.edu; dcheong@gatech.edu;
   greyes@gatech.edu; abowd@gatech.edu
NR 40
TC 71
Z9 74
U1 1
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2018
VL 24
IS 4
BP 1447
EP 1456
DI 10.1109/TVCG.2018.2793698
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FZ6BS
UT WOS:000427682500007
PM 29543163
DA 2025-03-07
ER

PT J
AU Nagao, R
   Matsumoto, K
   Narumi, T
   Tanikawa, T
   Hirose, M
AF Nagao, Ryohei
   Matsumoto, Keigo
   Narumi, Takuji
   Tanikawa, Tomohiro
   Hirose, Michitaka
TI Ascending and Descending in Virtual Reality: Simple and Safe System
   using Passive Haptics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 25th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 18-22, 2018
CL Reutlingen, GERMANY
SP IEEE
DE Virtual reality; locomotion; haptic feedback; perception; stairs;
   staircase
ID SENSATION; WALKING
AB This paper presents a novel interactive system that provides users with virtual reality (VR) experiences, wherein users feel as if they are ascending/descending stairs through passive haptic feedback. The passive haptic stimuli are provided by small bumps under the feet of users; these stimuli are provided to represent the edges of the stairs in the virtual environment. The visual stimuli of the stairs and shoes, provided by head-mounted displays, evoke a visuo-haptic interaction that modifies a user's perception of the floor shape. Our system enables users to experience all types of stairs, such as half-turn and spiral stairs, in a VR setting. We conducted a preliminary user study and two experiments to evaluate the proposed technique. The preliminary user study investigated the effectiveness of the basic idea associated with the proposed technique for the case of a user ascending stairs. The results demonstrated that the passive haptic feedback produced by the small bumps enhanced the user's feeling of presence and sense of ascending. We subsequently performed an experiment to investigate an improved viewpoint manipulation method and the interaction of the manipulation and haptics for both the ascending and descending cases. The experimental results demonstrated that the participants had a feeling of presence and felt a steep stair gradient under the condition of haptic feedback and viewpoint manipulation based on the characteristics of actual stair walking data. However, these results also indicated that the proposed system may not be as effective in providing a sense of descending stairs without an optimization of the haptic stimuli. We then redesigned the shape of the small bumps, and evaluated the design in a second experiment. The results indicated that the best shape to present haptic stimuli is a right triangle cross section in both the ascending and descending cases. Although it is necessary to install small protrusions in the determined direction, by using this optimized shape the users feeling of presence of the stairs and the sensation of walking up and down was enhanced.
C1 [Nagao, Ryohei; Matsumoto, Keigo; Narumi, Takuji; Tanikawa, Tomohiro; Hirose, Michitaka] Univ Tokyo, Tokyo, Japan.
   [Narumi, Takuji] JST PREST, Tokyo, Japan.
C3 University of Tokyo; Japan Science & Technology Agency (JST)
RP Nagao, R (corresponding author), Univ Tokyo, Tokyo, Japan.
EM nagao@cyber.t.u-tokyo.ac.jp; matsumoto@cyber.t.u-tokyo.ac.jp;
   narumi@cyber.t.u-tokyo.ac.jp; tani@cyber.t.u-tokyo.ac.jp;
   hirose@cyber.t.u-tokyo.ac.jp
RI Narumi, Takuji/K-3925-2014
OI Narumi, Takuji/0000-0002-9010-1491; Matsumoto, Keigo/0000-0002-0038-0678
FU MEXT [16H05866]; JST PREST [JP-MJPR17J6(17939529)]; Grants-in-Aid for
   Scientific Research [15K12075, 16H05866] Funding Source: KAKEN
FX This work was partially supported by the MEXT, Grant-in-Aid for Young
   Scientists (A)(16H05866) and JST PREST JP-MJPR17J6(17939529).
NR 28
TC 43
Z9 45
U1 1
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2018
VL 24
IS 4
BP 1584
EP 1593
DI 10.1109/TVCG.2018.2793038
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FZ6BS
UT WOS:000427682500021
PM 29543176
DA 2025-03-07
ER

PT J
AU Glueck, M
   Naeini, MP
   Doshi-Velez, F
   Chevalier, F
   Khan, A
   Wigdor, D
   Brudno, M
AF Glueck, Michael
   Naeini, Mandi Pakdaman
   Doshi-Velez, Finale
   Chevalier, Fanny
   Khan, Azam
   Wigdor, Daniel
   Brudno, Michael
TI PhenoLines: Phenotype Comparison Visualizations for Disease Subtyping
   via Topic Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Developmental disorder; Human Phenotype Ontology (HPO); Phenotypes;
   Topic models; Topology simplification
AB PhenoLines is a visual analysis tool for the interpretation of disease subtypes, derived from the application of topic models to clinical data. Topic models enable one to mine cross-sectional patient comorbidity data (e.g., electronic health records) and construct disease subtypes each with its own temporally evolving prevalence and co-occurrence of phenotypes without requiring aligned longitudinal phenotype data for all patients. However, the dimensionality of topic models makes interpretation challenging, and de facto analyses provide little intuition regarding phenotype relevance or phenotype interrelationships. PhenoLines enables one to compare phenotype prevalence within and across disease subtype topics, thus supporting subtype characterization, a task that involves identifying a proposed subtype's dominant phenotypes, ages of effect, and clinical validity. We contribute a data transformation workflow that employs the Human Phenotype Ontology to hierarchically organize phenotypes and aggregate the evolving probabilities produced by topic models. We introduce a novel measure of phenotype relevance that can be used to simplify the resulting topology. The design of PhenoLines was motivated by formative interviews with machine learning and clinical experts. We describe the collaborative design process, distill high-level tasks, and report on initial evaluations with machine learning experts and a medical domain expert. These results suggest that PhenoLines demonstrates promising approaches to support the characterization and optimization of topic models.
C1 [Glueck, Michael; Khan, Azam] Autodesk Res, San Rafael, CA USA.
   [Glueck, Michael; Wigdor, Daniel; Brudno, Michael] Univ Toronto, Toronto, ON, Canada.
   [Naeini, Mandi Pakdaman; Doshi-Velez, Finale] Harvard Univ, Cambridge, MA 02138 USA.
   [Chevalier, Fanny] Inria, Le Chesnay, France.
   [Brudno, Michael] Hosp Sick Children, Toronto, ON, Canada.
C3 Autodesk, Inc.; University of Toronto; Harvard University; Inria;
   University of Toronto; Hospital for Sick Children (SickKids)
RP Glueck, M (corresponding author), Autodesk Res, San Rafael, CA USA.; Glueck, M (corresponding author), Univ Toronto, Toronto, ON, Canada.
EM mglueck@dgp.toronto.edu; pakdaman@g.harvard.edu;
   finale@seas.harvard.edu; fanny.chevalier@inria.fr;
   azam.khan@autodesk.com; daniel@dgp.toronto.edu; brudno@cs.toronto.edu
RI Glueck, Michael/AAK-3015-2020; Khan, Azam/JEF-7682-2023
OI Khan, Azam/0000-0002-1816-5714
FU Genome Canada; Ontario Genomics through a Bioinformatics/Computational
   Biology grant
FX This work was partially funded by Genome Canada and Ontario Genomics
   through a Bioinformatics/Computational Biology grant to Dr. Brudno. The
   authors thank Michelle Annett, Aryan Arbabi, Rafael Veras, Bruno De
   Araujo, John Hancock, the domain experts, as well as the anonymous
   reviewers for their thoughtful suggestions.
NR 56
TC 20
Z9 23
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 371
EP 381
DI 10.1109/TVCG.2017.2745118
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400038
PM 28866570
DA 2025-03-07
ER

PT J
AU Hurter, C
   Puechmorel, S
   Nicol, F
   Telea, A
AF Hurter, Christophe
   Puechmorel, Stephane
   Nicol, Florence
   Telea, Alexandru
TI Functional Decomposition for Bundled Simplification of Trail Sets
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE path visualization; trajectory visualization; edge bundles; functional
   decomposition; path generation; streamlines
ID EFFICIENT ALGORITHM; MEAN-SHIFT; EDGE; VISUALIZATION; GRAPH; DENSITY;
   SEQUENCE; CURVES
AB Bundling visually aggregates curves to reduce clutter and help finding important patterns in trail-sets or graph drawings. We propose a new approach to bundling based on functional decomposition of the underling dataset. We recover the functional nature of the curves by representing them as linear combinations of piecewise-polynomial basis functions with associated expansion coefficients. Next, we express all curves in a given cluster in terms of a centroid curve and a complementary term, via a set of so-called principal component functions. Based on the above, we propose a two-fold contribution: First, we use cluster centroids to design a new bundling method for 2D and 3D curve-sets. Secondly, we deform the cluster centroids and generate new curves along them, which enables us to modify the underlying data in a statistically-controlled way via its simplified (bundled) view. We demonstrate our method by applications on real-world 2D and 3D datasets for graph bundling, trajectory analysis, and vector field and tensor field visualization.
C1 [Hurter, Christophe; Puechmorel, Stephane; Nicol, Florence] ENAC, Toulouse, France.
   [Telea, Alexandru] Univ Groningen, Groningen, Netherlands.
C3 Universite Federale Toulouse Midi-Pyrenees (ComUE); Universite de
   Toulouse; Ecole Nationale de l'Aviation Civile (ENAC); University of
   Groningen
RP Hurter, C (corresponding author), ENAC, Toulouse, France.
EM christophe.hurter@enac.fr; stephane.puechmorel@enac.fr;
   florence.nicol@enac.fr; a.c.telea@rug.nl
RI Hurter, Christophe/AHB-0811-2022
FU French National Agency for Research (Agence Nationale de la Recherche
   ANR) [ANR-14-CE24-0006-01]; SESAR Research and Innovation Action Horizon
   2020 under project "MOTO" (The embodied reMOte TOwer); Agence Nationale
   de la Recherche (ANR) [ANR-14-CE24-0006] Funding Source: Agence
   Nationale de la Recherche (ANR)
FX The authors acknowledge the support of the French National Agency for
   Research (Agence Nationale de la Recherche ANR) under the grant
   ANR-14-CE24-0006-01 project "TERANOVA" and the SESAR Research and
   Innovation Action Horizon 2020 under project "MOTO" (The embodied reMOte
   TOwer).
NR 74
TC 12
Z9 13
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 500
EP 510
DI 10.1109/TVCG.2017.2744338
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400050
PM 28866541
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Qu, ZN
   Hultman, J
AF Qu, Zening
   Hultman, Jessica
TI Keeping Multiple Views Consistent: Constraints, Validations, and
   Exceptions in Visualization Authoring
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Visualization Design; Qualitative Study; Evaluation
AB Visualizations often appear in multiples, either in a single display (e.g., small multiples, dashboard) or across time or space (e.g., slideshow, set of dashboards). However, existing visualization design guidelines typically focus on single rather than multiple views. Solely following these guidelines can lead to effective yet inconsistent views (e.g., the same field has different axes domains across charts), making interpretation slow and error-prone. Moreover, little is known how consistency balances with other design considerations, making it difficult to incorporate consistency mechanisms in visualization authoring software. We present a wizard-of-oz study in which we observed how Tableau users achieve and sacrifice consistency in an exploration-to-presentation visualization design scenario. We extend (from our prior work) a set of encoding-specific constraints defining consistency across multiple views. Using the constraints as a checklist in our study, we observed cases where participants spontaneously maintained consistent encodings and warned cases where consistency was overlooked. In response to the warnings, participants either revised views for consistency or stated why they thought consistency should be overwritten. We categorize participants" actions and responses as constraint validations and exceptions, depicting the relative importance of consistency and other design considerations under various circumstances (e.g., data cardinality, available encoding resources, chart layout). We discuss automatic consistency checking as a constraint-satisfaction problem and provide design implications for communicating inconsistencies to users.
C1 [Qu, Zening; Hultman, Jessica] Univ Washington, Seattle, WA 98195 USA.
C3 University of Washington; University of Washington Seattle
RP Qu, ZN (corresponding author), Univ Washington, Seattle, WA 98195 USA.
EM zqu@uw.edu; jhullman@uw.edu
RI Hullman, Jessica/P-7130-2018
NR 33
TC 68
Z9 82
U1 1
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 468
EP 477
DI 10.1109/TVCG.2017.2744198
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400047
PM 28866529
DA 2025-03-07
ER

PT J
AU Sacha, D
   Kraus, M
   Bernard, J
   Behrisch, M
   Schreck, T
   Asano, Y
   Keim, DA
AF Sacha, Dominik
   Kraus, Matthias
   Bernard, Juergen
   Behrisch, Michael
   Schreck, Tobias
   Asano, Yuki
   Keim, Daniel A.
TI SOM Flow: Guided Exploratory Cluster Analysis with Self-Organizing Maps
   and Analytic Provenance
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Visual Analytics; Interaction; Visual Cluster Analysis; Quality Metrics;
   Guidance; Self -Organizing Maps; Time Series
ID VISUALIZATION; SEARCH
AB Clustering is a core building block for data analysis, aiming to extract otherwise hidden structures and relations from raw datasets, such as particular groups that can be effectively related, compared, and interpreted. A plethora of visual-interactive cluster analysis techniques has been proposed to date, however, arriving at useful clusterings often requires several rounds of user interactions to fine-tune the data preprocessing and algorithms. We present a multi-stage Visual Analytics (VA) approach for iterative cluster refinement together with an implementation (SOMFlow) that uses Self-Organizing Maps (SOM) to analyze time series data. It supports exploration by offering the analyst a visual platform to analyze intermediate results, adapt the underlying computations, iteratively partition the data, and to reflect previous analytical activities. The history of previous decisions is explicitly visualized within a flow graph, allowing to compare earlier cluster refinements and to explore relations. We further leverage quality and interestingness measures to guide the analyst in the discovery of useful patterns, relations, and data partitions. We conducted two pair analytics experiments together with a subject matter expert in speech intonation research to demonstrate that the approach is effective for interactive data analysis, supporting enhanced understanding of clustering results as well as the interactive process itself.
C1 [Sacha, Dominik; Kraus, Matthias; Behrisch, Michael; Keim, Daniel A.] Univ Konstanz, Constance, Germany.
   [Bernard, Juergen] Tech Univ Darmstadt, Darmstadt, Germany.
   [Schreck, Tobias] Graz Univ Technol, Graz, Austria.
   [Asano, Yuki] Univ Tubingen, Tubingen, Germany.
C3 University of Konstanz; Technical University of Darmstadt; Graz
   University of Technology; Eberhard Karls University of Tubingen
RP Sacha, D (corresponding author), Univ Konstanz, Constance, Germany.
EM dominik.sacha@uni-konstanz.de; juergen.bernard@gris.tu-darmstadt.de;
   tobias.schreck@cgv.tugraz.at; yuki.asano@es.uni-tuebingen.de
RI Bernard, Jürgen/M-5499-2019; Keim, Daniel/X-7749-2019
OI Schreck, Tobias/0000-0003-0778-8665; NASSREDDINE,
   Redhaounia/0000-0002-5436-6484
FU German Research Foundation (DFG) [SFB/Transregio 161]; German Research
   Foundation (DFG) within the Research Unit FOR 2111
FX We gratefully acknowledge the German Research Foundation (DFG) for
   financial support within the project A03 of SFB/Transregio 161 and
   within the Research Unit FOR 2111.
NR 58
TC 49
Z9 56
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 120
EP 130
DI 10.1109/TVCG.2017.2744805
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400014
PM 28866559
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Itoh, Y
   Hamasaki, T
   Sugimoto, M
AF Itoh, Yuta
   Hamasaki, Takumi
   Sugimoto, Maki
TI Occlusion Leak Compensation for Optical See-Through Displays using a
   Single layer Transmissive Spatial Light Modulator
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY SEP 09-13, 2017
CL Nantes, FRANCE
SP IEEE
DE Occlusion support; opticalsee-through HMD; occlusion leak; spatial light
   modulator; depth cue
ID HEAD-MOUNTED DISPLAYS; AUGMENTED REALITY; SYSTEM
AB We propose an occlusion compensation method for optical see-through head-mounted displays (OST-HMDs) equipped with a singlelayer transmissive spatial light modulator (SLM), in particular, a liquid crystal display (LCD). Occlusion is an important depth cue for 3D perception, yet realizing it on OST-HMDs is particularly difficult due to the displays' semitransparent nature. A key component for the occlusion support is the SLMa device that can selectively interfere with light rays passing through it. For example, an LCD is a transmissive SLM that can block or pass incoming light rays by turning pixels black or transparent. A straightforward solution places an LCD in front of an OST-HMD and drives the LCD to block light rays that could pass through rendered virtual objects at the viewpoint. This simple approach is, however, defective due to the depth mismatch between the LCD panel and the virtual objects, leading to blurred occlusion. This led existing OST-HMDs to employ dedicated hardware such as focus optics and multi-stacked SLMs. Contrary to these viable, yet complex and/or computationally expensive solutions, we return to the single-layer LCD approach for the hardware simplicity while maintaining fine occlusionwe compensate for a degraded occlusion area by overlaying a compensation image. We compute the image based on the HMD parameters and the background scene captured by a scene camera. The evaluation demonstrates that the proposed method reduced the occlusion leak error by 61.4% and the occlusion error by 85.7%.
C1 [Itoh, Yuta; Hamasaki, Takumi; Sugimoto, Maki] Keio Univ, Tokyo, Japan.
C3 Keio University
RP Itoh, Y (corresponding author), Keio Univ, Tokyo, Japan.
EM itoh@imlab.ics.keio.ac.jp; hamasaki@imlab.ics.keio.ac.jp;
   sugimoto@ics.keio.ac.jp
OI Itoh, Yuta/0000-0002-5901-797X
FU JSPS KAKENHI, Japan [JP17H04692, JP17K19985]; JST CREST, Japan
   [JPMJCR14E1]; Grants-in-Aid for Scientific Research [17K19985, 17H04692]
   Funding Source: KAKEN
FX This work was partially supported by JSPS KAKENHI Grant Numbers
   JP17H04692 and JP17K19985 and by JST CREST Grant Number JPMJCR14E1,
   Japan.
NR 50
TC 35
Z9 40
U1 0
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2017
VL 23
IS 11
BP 2463
EP 2473
DI 10.1109/TVCG.2017.2734427
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FI7XT
UT WOS:000412214000013
PM 28809690
DA 2025-03-07
ER

PT J
AU Matsui, Y
   Shiratori, T
   Aizawa, K
AF Matsui, Yusuke
   Shiratori, Takaaki
   Aizawa, Kiyoharu
TI DrawFromDrawings: 2D Drawing Assistance via Stroke Interpolation with a
   Sketch Database
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE interactive drawing; 2D shape interpolation
AB We present DrawFromDrawings, an interactive drawing system that provides users with visual feedback for assistance in 2D drawing using a database of sketch images. Following the traditional imitation and emulation training from art education, DrawFromDrawings enables users to retrieve and refer to a sketch image stored in a database and provides them with various novel strokes as suggestive or deformation feedback. Given regions of interest (ROIs) in the user and reference sketches, DrawFromDrawings detects as-long-as-possible (ALAP) stroke segments and the correspondences between user and reference sketches that are the key to computing seamless interpolations. The stroke-level interpolations are parametrized with the user strokes, the reference strokes, and new strokes created by warping the reference strokes based on the user and reference ROI shapes, and the user study indicated that the interpolation could produce various reasonable strokes varying in shapes and complexity. DrawFromDrawings allows users to either replace their strokes with interpolated strokes (deformation feedback) or overlays interpolated strokes onto their strokes (suggestive feedback). The other user studies on the feedback modes indicated that the suggestive feedback enabled drawers to develop and render their ideas using their own stroke style, whereas the deformation feedback enabled them to finish the sketch composition quickly.
C1 [Matsui, Yusuke; Aizawa, Kiyoharu] Univ Tokyo, Dept Informat & Commun Engn, Tokyo 1138654, Japan.
   [Shiratori, Takaaki] Facebook Inc, Oculus Res Pittsburgh, Pittsburgh, PA USA.
C3 University of Tokyo; Facebook Inc
RP Matsui, Y (corresponding author), Univ Tokyo, Dept Informat & Commun Engn, Tokyo 1138654, Japan.
EM matsui@hal.t.u-tokyo.ac.jp; takaaki.shiratori@oculus.com;
   aizawa@hal.t.u-tokyo.ac.jp
FU Strategic Information and Communications R&D Promotion Programme
   (SCOPE); JSPS KAKENHI [257696]; Academy of Finland (AKA) [257696]
   Funding Source: Academy of Finland (AKA)
FX The authors would like to thank David Wipf and Stephen Lin for useful
   discussions, and Yukihiro Takamatsu for editing the supplementary video,
   available online. This work was supported in part by the Strategic
   Information and Communications R&D Promotion Programme (SCOPE) and JSPS
   KAKENHI Grant Number 257696. This work was done while Takaaki Shiratori
   was affiliated with Microsoft Research and Yusuke Matsui was an intern
   there.
NR 35
TC 17
Z9 19
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2017
VL 23
IS 7
BP 1852
EP 1862
DI 10.1109/TVCG.2016.2554113
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EW7OV
UT WOS:000402705000010
PM 27101610
DA 2025-03-07
ER

PT J
AU Stüvel, SA
   Magnenat-Thalmann, N
   Thalmann, D
   van der Stappen, AF
   Egges, A
AF Stuvel, Sybren A.
   Magnenat-Thalmann, Nadia
   Thalmann, Daniel
   van der Stappen, A. Frank
   Egges, Arjan
TI Torso Crowds
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Crowd simulation; crowd animation; dense crowds; agent representation;
   holonomic motion
ID NAVIGATION; SPACE; MODEL
AB We present a novel dense crowd simulation method. In real crowds of high density, people manoeuvring the crowd need to twist their torso to pass between others. Our proposed method does not use the traditional disc-shaped agent, but instead employs capsule-shaped agents, which enables us to plan such torso orientations. Contrary to other crowd simulation systems, which often focus on the movement of the entire crowd, our method distinguishes between active agents that try to manoeuvre through the crowd, and passive agents that have no incentive to move. We introduce the concept of a focus point to influence crowd agent orientation. Recorded data from real human crowds are used for validation, which shows that our proposed model produces equivalent paths for 85 percent of the validation set. Furthermore, we present a character animation technique that uses the results from our crowd model to generate torso-twisting and side-stepping characters.
C1 [Stuvel, Sybren A.; van der Stappen, A. Frank; Egges, Arjan] Univ Utrecht, Virtual Human Technol Lab, Utrecht, Netherlands.
   [Magnenat-Thalmann, Nadia; Thalmann, Daniel] Nanyang Technol Univ, Inst Media Innovat, Singapore, Singapore.
C3 Utrecht University; Nanyang Technological University
RP Stüvel, SA (corresponding author), Univ Utrecht, Virtual Human Technol Lab, Utrecht, Netherlands.
EM s.a.stuvel@uu.nl; nadiathalmann@ntu.edu.sg; danielthalmann@ntu.edu.sg;
   a.f.vanderstappen@uu.nl; j.egges@uu.nl
RI Thalmann, Daniel/A-4347-2008; Thalmann, Nadia/AAK-5195-2021
OI Stuvel, Sybren/0000-0002-6460-5979; van der Stappen,
   Frank/0000-0001-7965-2818; Thalmann, Nadia/0000-0002-1459-5960
FU Dutch nationally funded project COMMIT/
FX This research is supported by the Dutch nationally funded project
   COMMIT/. Part of this work has been performed during the stay of Sybren
   A. Stuvel at Nanyang Technological University, Singapore. We thank the
   group of Roland Geraerts for the use of their Explicit Corridor Map
   framework, which we used to calculate the Voronoi diagrams. We also
   thank the Blender community for their help.
NR 46
TC 21
Z9 25
U1 1
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2017
VL 23
IS 7
BP 1823
EP 1837
DI 10.1109/TVCG.2016.2545670
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EW7OV
UT WOS:000402705000008
PM 28113857
OA Green Published
DA 2025-03-07
ER

PT J
AU Rhee, T
   Petikam, L
   Allen, B
   Chalmers, A
AF Rhee, Taehyun
   Petikam, Lohit
   Allen, Benjamin
   Chalmers, Andrew
TI MR360: Mixed Reality Rendering for 360° Panoramic Videos
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 19th IEEE Virtual Reality Conference (VR)
CY MAR 18-22, 2017
CL Los Angeles, CA
SP IEEE, IEEE Comp Soc, IEEE Comp Soc, Visualizat & Graph Tech Comm
DE Mixed reality rendering; image based lighting; image based shadowing;
   360 degrees panoramic video
AB This paper presents a novel immersive system called MR360 that provides interactive mixed reality (MR) experiences using a conventional low dynamic range (LDR) 360 panoramic video (360-video) shown in head mounted displays (HMDs). MR360 seamlessly composites 3D virtual objects into a live 360-video using the input panoramic video as the lighting source to illuminate the virtual objects. Image based lighting (IBL) is perceptually optimized to provide fast and believable results using the LDR 360-video as the lighting source. Regions of most salient lights in the input panoramic video are detected to optimize the number of lights used to cast perceptible shadows. Then, the areas of the detected lights adjust the penumbra of the shadow to provide realistic soft shadows. Finally, our real-time differential rendering synthesizes illumination of the virtual 3D objects into the 360-video. MR360 provides the illusion of interacting with objects in a video, which are actually 3D virtual objects seamlessly composited into the background of the 360-video. MR360 was implemented in a commercial game engine and tested using various 360-videos. Since our MR360 pipeline does not require any pre-computation. it can synthesize an interactive MR scene using a live 360-video stream while providing realistic high performance rendering suitable for HMDs.
C1 [Rhee, Taehyun; Petikam, Lohit; Allen, Benjamin; Chalmers, Andrew] Victoria Univ Wellington, Wellington, New Zealand.
C3 Victoria University Wellington
RP Rhee, T; Petikam, L; Allen, B; Chalmers, A (corresponding author), Victoria Univ Wellington, Wellington, New Zealand.
EM taehyun.rhee@ecs.vuw.ac.nz; Lohit.Petikam@ecs.vuw.ac.nz;
   benjamin.allen@ecs.vuw.ac.nz; Andrew.Chalmers@ecs.vuw.ac.nz
RI Chalmers, Andrew/AAM-5135-2021; Allen, Benjamin/F-1829-2011
OI Petikam, Lohit/0000-0001-6629-7490; Rhee, Taehyun/0000-0002-6150-0637
FU HDI4D project - MBIE in New Zealand; NRF in Korea
   [NRF-2014K1A3A1A17073365]
FX This research was supported in part by the HDI4D project funded by MBIE
   in New Zealand and NRF in Korea (NRF-2014K1A3A1A17073365).
NR 44
TC 82
Z9 89
U1 0
U2 46
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2017
VL 23
IS 4
BP 1302
EP 1311
DI 10.1109/TVCG.2017.2657178
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EO0QU
UT WOS:000396403800006
PM 28129172
DA 2025-03-07
ER

PT J
AU Yao, CY
   Hung, SH
   Li, GW
   Chen, IY
   Adhitya, R
   Lai, YC
AF Yao, Chih-Yuan
   Hung, Shih-Hsuan
   Li, Guo-Wei
   Chen, I-Yu
   Adhitya, Reza
   Lai, Yu-Chi
TI Manga Vectorization and Manipulation with Procedural Simple Screentone
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Manga; semantic components; vectorization; screentone; procedural
   shaders
ID TEXTURE; CLASSIFICATION
AB Manga are a popular artistic form around the world, and artists use simple line drawing and screentone to create all kinds of interesting productions. Vectorization is helpful to digitally reproduce these elements for proper content and intention delivery on electronic devices. Therefore, this study aims at transforming scanned Manga to a vector representation for interactive manipulation and real-time rendering with arbitrary resolution. Our system first decomposes the patch into rough Manga elements including possible borders and shading regions using adaptive binarization and screentone detector. We classify detected screentone into simple and complex patterns: our system extracts simple screentone properties for refining screentone borders, estimating lighting, compensating missing strokes inside screentone regions, and later resolution independently rendering with our procedural shaders. Our system treats the others as complex screentone areas and vectorizes them with our proposed line tracer which aims at locating boundaries of all shading regions and polishing all shading borders with the curve-based Gaussian refiner. A user can lay down simple scribbles to cluster Manga elements intuitively for the formation of semantic components, and our system vectorizes these components into shading meshes along with embedded Bezier curves as a unified foundation for consistent manipulation including pattern manipulation, deformation, and lighting addition. Our system can real-time and resolution independently render the shading regions with our procedural shaders and drawing borders with the curve-based shader. For Manga manipulation, the proposed vector representation can be not only magnified without artifacts but also deformed easily to generate interesting results.
C1 [Yao, Chih-Yuan; Hung, Shih-Hsuan; Li, Guo-Wei; Chen, I-Yu; Adhitya, Reza; Lai, Yu-Chi] NTUST, CSIE, Taipei, Taiwan.
C3 National Taiwan University of Science & Technology
RP Yao, CY (corresponding author), NTUST, CSIE, Taipei, Taiwan.
EM cyuan.yao@gmail.com; kn810609@gmail.com; kn810@gmail.com;
   karls820210@gmail.com; azerdarkblade@gmail.com; cheeryuchi@gmail.com
OI Lai, Yu-Chi/0000-0001-8578-3101
FU Ministry of Science and Technology of Taiwan
   [NSC-104-2221-E-011-029-MY3, NSC103-2221-E-011-114-MY2,
   NSC-104-2218-E-011-006, NSC-103-2218-E-011-014, NSC-104-2221-E-011-092,
   NSC-103-2221-E-011-076]
FX We thank Pin-Ci Yeh and International Games System (IGS) to provide us
   the Manga patches for study, participants of all user studies, and the
   support of IGS Inc. This work was also supported by
   NSC-104-2221-E-011-029-MY3, NSC103-2221-E-011-114-MY2,
   NSC-104-2218-E-011-006, NSC-103-2218-E-011-014, NSC-104-2221-E-011-092
   and NSC-103-2221-E-011-076, Taiwan. This work was supported by Ministry
   of Science and Technology of Taiwan under Grants
   NSC-104-2221-E-011-029-MY3, NSC103-2221-E-011-114-MY2,
   NSC-104-2218-E-011-006, NSC-103-2218-E-011-014, NSC-104-2221-E-011-092
   and NSC-103-2221-E-011-076. Yu-Chi Lai is the corresponding author.
NR 31
TC 14
Z9 18
U1 0
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2017
VL 23
IS 2
BP 1070
EP 1084
DI 10.1109/TVCG.2016.2525774
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM8CG
UT WOS:000395538200007
PM 26863665
DA 2025-03-07
ER

PT J
AU Behrisch, M
   Bach, B
   Hund, M
   Delz, M
   von Rüden, L
   Fekete, JD
   Schreck, T
AF Behrisch, Michael
   Bach, Benjamin
   Hund, Michael
   Delz, Michael
   von Rueden, Laura
   Fekete, Jean-Daniel
   Schreck, Tobias
TI Magnostics: Image-based Search of Interesting Matrix Views for Guided
   Network Exploration
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Matrix Visualization; Visual Quality Measures; Quality Metrics; Feature
   Detection/Selection; Relational Data
ID TEXTURAL FEATURES; VISUALIZATION; RETRIEVAL; METRICS
AB In this work we address the problem of retrieving potentially interesting matrix views to support the exploration of networks. We introduce Matrix Diagnostics (or MAGNOSTICS), following in spirit related approaches for rating and ranking other visualization techniques, such as Scagnostics for scatter plots. Our approach ranks matrix views according to the appearance of specific visual patterns, such as blocks and lines, indicating the existence of topological motifs in the data, such as clusters, bi-graphs, or central nodes. MAGNOSTICS can be used to analyze, query, or search for visually similar matrices in large collections, or to assess the quality of matrix reordering algorithms. While many feature descriptors for image analyzes exist, there is no evidence how they perform for detecting patterns in matrices. In order to make an informed choice of feature descriptors for matrix diagnostics, we evaluate 30 feature descriptors-27 existing ones and three new descriptors that we designed specifically for MAGNOSTICS-with respect to four criteria: pattern response, pattern variability, pattern sensibility, and pattern discrimination. We conclude with an informed set of six descriptors as most appropriate for MAGNOSTICS and demonstrate their application in two scenarios; exploring a large collection of matrices and analyzing temporal networks.
C1 [Behrisch, Michael; Hund, Michael; Delz, Michael] Univ Konstanz, Constance, Germany.
   [Bach, Benjamin] Microsoft Res Inria Joint Ctr, Saclay, France.
   [von Rueden, Laura] Capgemini, Paris, France.
   [von Rueden, Laura] Rhein Westfal TH Aachen, Aachen, Germany.
   [Fekete, Jean-Daniel] Inria, Saclay, France.
   [Schreck, Tobias] Graz Univ Technol, Graz, Austria.
C3 University of Konstanz; Capgemini; RWTH Aachen University; Inria; Graz
   University of Technology
RP Behrisch, M (corresponding author), Univ Konstanz, Constance, Germany.
EM michael.behrisch@uni-konstanz.de; benj.bach@gmail.com;
   michael.hund@uni-konstanz.de; michael.delz@uni-konstanz.de;
   laura.von.rueden@rwth-aachen.de; jean-daniel.fekete@inria.fr;
   tobias.schreck@cgv.tugraz.at
RI Fekete, Jean-Daniel/N-9175-2018
OI Fekete, Jean-Daniel/0000-0003-3770-8726; Behrisch,
   Michael/0000-0002-1102-103X; Schreck, Tobias/0000-0003-0778-8665
FU German Research Foundation (DFG) [SFB/Transregio 161]
FX The authors wish to thank Nayeem Khan for the discussions that
   contributed to this work. We also thank Bianca Orita, Manuel Hotz and
   Raffael Wagner for the development of the block feature descriptor and
   the statistical noise descriptor. The authors thank the German Research
   Foundation (DFG) for financial support within project A03
   "Quantification of Visual Analytics Transformations and Mappings" of
   SFB/Transregio 161.
NR 49
TC 21
Z9 23
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 31
EP 40
DI 10.1109/TVCG.2016.2598467
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600006
PM 27514053
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Besançon, L
   Issartel, P
   Ammi, M
   Isenberg, T
AF Besancon, Lonni
   Issartel, Paul
   Ammi, Mehdi
   Isenberg, Tobias
TI Hybrid Tactile/Tangible Interaction for 3D Data Exploration
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Interaction; tactile input; tangible input; 3D data visualization
ID DIRECT-TOUCH INTERACTION; SCIENTIFIC VISUALIZATION; STRATEGIES; DESIGN
AB We present the design and evaluation of an interface that combines tactile and tangible paradigms for 3D visualization. While studies have demonstrated that both tactile and tangible input can be efficient for a subset of 3D manipulation tasks, we reflect here on the possibility to combine the two complementary input types. Based on a field study and follow-up interviews, we present a conceptual framework of the use of these different interaction modalities for visualization both separately and combined-focusing on free exploration as well as precise control. We present a prototypical application of a subset of these combined mappings for fluid dynamics data visualization using a portable, position-aware device which offers both tactile input and tangible sensing. We evaluate our approach with domain experts and report on their qualitative feedback.
C1 [Besancon, Lonni] Inria Saclay, Paris, France.
   [Besancon, Lonni; Issartel, Paul] Univ Paris, Paris, France.
   [Ammi, Mehdi] CNRS, Limsi, F-75700 Paris, France.
   [Isenberg, Tobias] Inria, Paris, France.
C3 Universite Paris Cite; Centre National de la Recherche Scientifique
   (CNRS); Universite Paris Saclay; Inria
RP Besançon, L (corresponding author), Inria Saclay, Paris, France.; Besançon, L (corresponding author), Univ Paris, Paris, France.
EM lonni.besancon@gmail.com; paul.issartel@u-psud.fr; mehdi.ammi@limsi.fr;
   tobias.isenberg@inria.fr
RI Isenberg, Tobias/A-7575-2008; Besancon, Lonni/N-1856-2017
OI Isenberg, Tobias/0000-0001-7953-8644; Besancon,
   Lonni/0000-0002-7207-1276
NR 89
TC 45
Z9 48
U1 1
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 881
EP 890
DI 10.1109/TVCG.2016.2599217
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600091
PM 27875202
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Günther, T
   Theisel, H
AF Guenther, Tobias
   Theisel, Holger
TI Backward Finite-Time Lyapunov Exponents in Inertial Flows
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Inertial particles; finite-time Lyapunov exponents; backward
   integration; preferential particle settling
ID LAGRANGIAN COHERENT STRUCTURES; PREFERENTIAL CONCENTRATION;
   AEROSOL-PARTICLES; VISUALIZATION; FLUID; MOTION; TURBULENCE; CURVES;
   SPHERE
AB Inertial particles are finite-sized objects that are carried by fluid flows and in contrast to massless tracer particles they are subject to inertia effects. In unsteady flows, the dynamics of tracer particles have been extensively studied by the extraction of Lagrangian coherent structures (LCS), such as hyperbolic LCS as ridges of the Finite-Time Lyapunov Exponent (FTLE). The extension of the rich LCS framework to inertial particles is currently a hot topic in the CFD literature and is actively under research. Recently, backward FTLE on tracer particles has been shown to correlate with the preferential particle settling of small inertial particles. For larger particles, inertial trajectories may deviate strongly from (massless) tracer trajectories, and thus for a better agreement, backward FTLE should be computed on inertial trajectories directly. Inertial backward integration, however, has not been possible until the recent introduction of the influence curve concept, which - given an observation and an initial velocity - allows to recover all sources of inertial particles as tangent curves of a derived vector field. In this paper, we show that FTLE on the influence curve vector field is in agreement with preferential particle settling and more importantly it is not only valid for small (near-tracer) particles. We further generalize the influence curve concept to general equations of motion in unsteady spatio-velocity phase spaces, which enables backward integration with more general equations of motion. Applying the influence curve concept to tracer particles in the spatio-velocity domain emits streaklines in massless flows as tangent curves of the influence curve vector field. We demonstrate the correlation between inertial backward FTLE and the preferential particle settling in a number of unsteady vector fields.
C1 [Guenther, Tobias; Theisel, Holger] Univ Magdeburg, Visual Comp Grp, D-39106 Magdeburg, Germany.
C3 Otto von Guericke University
RP Günther, T (corresponding author), Univ Magdeburg, Visual Comp Grp, D-39106 Magdeburg, Germany.
EM tobias@isg.cs.ovgu.de; theisel@ovgu.de
OI Gunther, Tobias/0000-0002-3020-0930
FU DFG [TH 692/8-1]
FX This work was supported by DFG grant number TH 692/8-1.
NR 72
TC 7
Z9 7
U1 1
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 970
EP 979
DI 10.1109/TVCG.2016.2599016
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600100
PM 27875210
DA 2025-03-07
ER

PT J
AU Rocha, A
   Alim, U
   Silva, JD
   Sousa, MC
AF Rocha, Allan
   Alim, Usman
   Silva, Julio Daniel
   Sousa, Mario Costa
TI Decal-maps: Real-Time Layering of Decals on Surfaces for Multivariate
   Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Multivariate; Visualization; Real-time; Decal; Surface; Layering; Design
ID BLOOD-FLOW
AB We introduce the use of decals for multivariate visualization design. Decals are visual representations that are used for communication; for example, a pattern, a text, a glyph, or a symbol, transferred from a 2D-image to a surface upon contact. By creating what we define as decal-maps, we can design a set of images or patterns that represent one or more data attributes. We place decals on the surface considering the data pertaining to the locations we choose. We propose a (texture mapping) local parametrization that allows placing decals on arbitrary surfaces interactively, even when dealing with a high number of decals. Moreover, we extend the concept of layering to allow the co-visualization of an increased number of attributes on arbitrary surfaces. By combining decal-maps, color-maps and a layered visualization, we aim to facilitate and encourage the creative process of designing multivariate visualizations. Finally, we demonstrate the general applicability of our technique by providing examples of its use in a variety of contexts.
C1 [Rocha, Allan; Alim, Usman; Silva, Julio Daniel; Sousa, Mario Costa] Univ Calgary, Calgary, AB T2N 1N4, Canada.
C3 University of Calgary
RP Rocha, A (corresponding author), Univ Calgary, Calgary, AB T2N 1N4, Canada.
EM rocha.allanc@gmail.com; ualim@ucalgary.ca; uliodaniel@gmail.com;
   smcosta@ucalgary.ca
OI Costa Sousa, Mario/0000-0002-4347-0884; Rocha, Allan/0000-0002-1868-993X
FU NSERC/ AITF/ FCMG IRC program in Scalable Reservoir Visualization
FX We wish to thank the anonymous reviewers for their constructive
   comments, Hamidreza Hamdi for providing the reservoir data, and Andrew
   Owens and Sowmya Somanath for their valuable feedback. This research was
   supported in part by the NSERC/ AITF/ FCMG IRC program in Scalable
   Reservoir Visualization.
NR 59
TC 17
Z9 18
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 821
EP 830
DI 10.1109/TVCG.2016.2598866
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600085
PM 27875196
DA 2025-03-07
ER

PT J
AU Song, H
   Lee, J
   Kim, TJ
   Lee, KH
   Kim, B
   Seo, J
AF Song, Hyunjoo
   Lee, Jeongjin
   Kim, Tae Jung
   Lee, Kyoung Ho
   Kim, Bohyoung
   Seo, Jinwook
TI GazeDx: Interactive Visual Analytics Framework for Comparative Gaze
   Analysis with Volumetric Medical Images
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Eye tracking; gaze visualization; gaze pattern comparison; volumetric
   medical images; context-embedded interactive scatterplot; interactive
   temporal chart
ID VISUALIZATION; EXPERIENCE; PERCEPTION; CT
AB We present an interactive visual analytics framework, GazeDx (abbr. of GazeDiagnosis), for the comparative analysis of gaze data from multiple readers examining volumetric images while integrating important contextual information with the gaze data. Gaze pattern comparison is essential to understanding how radiologists examine medical images, and to identifying factors influencing the examination. Most prior work depended upon comparisons with manually juxtaposed static images of gaze tracking results. Comparative gaze analysis with volumetric images is more challenging due to the additional cognitive load on 3D perception. A recent study proposed a visualization design based on direct volume rendering (DVR) for visualizing gaze patterns in volumetric images; however, effective and comprehensive gaze pattern comparison is still challenging due to a lack of interactive visualization tools for comparative gaze analysis. We take the challenge with GazeDx while integrating crucial contextual information such as pupil size and windowing into the analysis process for more in-depth and ecologically valid findings. Among the interactive visualization components in GazeDx, a context-embedded interactive scatterplot is especially designed to help users examine abstract gaze data in diverse contexts by embedding medical imaging representations well known to radiologists in it. We present the results from two case studies with two experienced radiologists, where they compared the gaze patterns of 14 radiologists reading two patients' volumetric CT images.
C1 [Song, Hyunjoo; Seo, Jinwook] Seoul Natl Univ, Seoul, South Korea.
   [Lee, Jeongjin] Soongsil Univ, Seoul, South Korea.
   [Kim, Tae Jung] Samsung Med Ctr, Seoul, South Korea.
   [Lee, Kyoung Ho] Seoul Natl Univ, Bundang Hosp, Seoul, South Korea.
   [Kim, Bohyoung] Hankuk Univ Foreign Studies, Seoul, South Korea.
C3 Seoul National University (SNU); Soongsil University; Sungkyunkwan
   University (SKKU); Samsung Medical Center; Seoul National University
   (SNU); Seoul National University Hospital; Hankuk University Foreign
   Studies
RP Song, H (corresponding author), Seoul Natl Univ, Seoul, South Korea.
EM hjsong@hcil.snu.ac.kr; leejeongjin@ssu.ac.kr; taejung.kiml@gmail.com;
   kholee@snubh.org; bkim@hufs.ac.kr; jseo@snuac.kr
RI Kim, Jae/J-5431-2012; Lee, Kyoung/J-5570-2012; Song,
   hyunjoo/GWC-1292-2022
OI Lee, Kyoung Ho/0000-0001-6045-765X
FU National Research Foundation of Korea (NRF) - Korea government (MSIP)
   [NRF-2014R1A2A2A03006998]; Seoul National University Bundang Hospital
   Research Fund [13-2014-001]; Hankuk University of Foreign Studies
   Research Fund
FX This work was supported by the National Research Foundation of Korea
   (NRF) grant funded by the Korea government (MSIP) (No.
   NRF-2014R1A2A2A03006998), by Seoul National University Bundang Hospital
   Research Fund (No. 13-2014-001), and by the Hankuk University of Foreign
   Studies Research Fund of 2016. Bohyoung Kim and Jinwook Seo are the
   corresponding authors.
NR 32
TC 20
Z9 23
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 311
EP 320
DI 10.1109/TVCG.2016.2598796
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600034
PM 27875147
DA 2025-03-07
ER

PT J
AU Wu, YH
   Cao, N
   Archambault, D
   Shen, QM
   Qu, HM
   Cui, WW
AF Wu, Yanhong
   Cao, Nan
   Archambault, Daniel
   Shen, Qiaomu
   Qu, Huamin
   Cui, Weiwei
TI Evaluation of Graph Sampling: A Visualization Perspective
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Graph visualization; graph sampling; empirical evaluation
ID INFORMATION VISUALIZATION; SCALE-FREE
AB Graph sampling is frequently used to address scalability issues when analyzing large graphs. Many algorithms have been proposed to sample graphs, and the performance of these algorithms has been quantified through metrics based on graph structural properties preserved by the sampling: degree distribution, clustering coefficient, and others. However, a perspective that is missing is the impact of these sampling strategies on the resultant visualizations. In this paper, we present the results of three user studies that investigate how sampling strategies influence node-link visualizations of graphs. In particular, five sampling strategies widely used in the graph mining literature are tested to determine how well they preserve visual features in node-link diagrams. Our results show that depending on the sampling strategy used different visual features are preserved. These results provide a complimentary view to metric evaluations conducted in the graph mining literature and provide an impetus to conduct future visualization studies.
C1 [Wu, Yanhong; Shen, Qiaomu; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
   [Cao, Nan] New York Univ, Shanghai, Peoples R China.
   [Archambault, Daniel] Swansea Univ, Swansea, W Glam, Wales.
   [Cui, Weiwei] Microsoft Res Asia, Beijing, Peoples R China.
C3 Hong Kong University of Science & Technology; Swansea University;
   Microsoft Research Asia; Microsoft; Microsoft China
RP Cui, WW (corresponding author), Microsoft Res Asia, Beijing, Peoples R China.
EM ywubk@ust.hk; nan.cao@gmail.com; d.w.archambault@swansea.ac.uk;
   qshen@ust.hk; huamin@ust.hk; weiwei.cui@microsoft.com
RI Cao, Nan/O-5397-2014; Shen, Qiaomu/JRW-9498-2023
OI Shen, Qiaomu/0000-0002-6510-0964; Wu, Yanhong/0000-0003-3404-7467
FU National Basic Research Program of China (973 Program) [2014CB340304];
   Microsoft Research Asia; EPSRC [EP/N005724/1] Funding Source: UKRI
FX We gratefully thank all the anonymous reviewers for their valuable
   comments. This research was supported in part by the National Basic
   Research Program of China (973 Program) under Grant No. 2014CB340304 and
   a grant from Microsoft Research Asia.
NR 52
TC 47
Z9 62
U1 2
U2 27
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 401
EP 410
DI 10.1109/TVCG.2016.2598867
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600043
PM 27875156
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Markovic, D
   Antonacci, F
   Sarti, A
   Tubaro, S
AF Markovic, Dejan
   Antonacci, Fabio
   Sarti, Augusto
   Tubaro, Stefano
TI 3D Beam Tracing Based on Visibility Lookup for Interactive Acoustic
   Modeling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Acoustic simulation; room acoustics; beam tracing; visibility
   computation
ID SOUND-PROPAGATION; IMAGE METHOD; ROOM; COMPLEX; DIFFRACTION; SIMULATION;
   PREDICTION; RADIOSITY; ROBUST
AB We present a method for accelerating the computation of specular reflections in complex 3D enclosures, based on acoustic beam tracing. Our method constructs the beam tree on the fly through an iterative lookup process of a precomputed data structure that collects the information on the exact mutual visibility among all reflectors in the environment (region-to-region visibility). This information is encoded in the form of visibility regions that are conveniently represented in the space of acoustic rays using the Plucker coordinates. During the beam tracing phase, the visibility of the environment from the source position (the beam tree) is evaluated by traversing the precomputed visibility data structure and testing the presence of beams inside the visibility regions. The Plucker parameterization simplifies this procedure and reduces its computational burden, as it turns out to be an iterative intersection of linear subspaces. Similarly, during the path determination phase, acoustic paths are found by testing their presence within the nodes of the beam tree data structure. The simulations show that, with an average computation time per beam in the order of a dozen of microseconds, the proposed method can compute a large number of beams at rates suitable for interactive applications with moving sources and receivers.
C1 [Markovic, Dejan; Antonacci, Fabio; Sarti, Augusto; Tubaro, Stefano] Politecn Milan, Dip Elettron Informaz & Bioingn, Piazza Leonardo da Vinci 32, I-20133 Milan, Italy.
C3 Polytechnic University of Milan
RP Markovic, D (corresponding author), Politecn Milan, Dip Elettron Informaz & Bioingn, Piazza Leonardo da Vinci 32, I-20133 Milan, Italy.
EM dejan.markovic@polimi.it; fabio.antonacci@polimi.it;
   augusto.sarti@polimi.it; stefano.tubaro@polimi.it
RI Antonacci, Fabio/O-2505-2015
OI Antonacci, Fabio/0000-0003-4545-0315
NR 69
TC 11
Z9 13
U1 0
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2016
VL 22
IS 10
BP 2262
EP 2274
DI 10.1109/TVCG.2016.2515612
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV1IM
UT WOS:000382674500006
PM 26761820
OA Green Published
DA 2025-03-07
ER

PT J
AU Szafir, DA
   Sarikaya, A
   Gleicher, M
AF Szafir, Danielle Albers
   Sarikaya, Alper
   Gleicher, Michael
TI Lightness Constancy in Surface Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Color; shading; shadow; lightness constancy; molecular visualization;
   surface visualization; visual perception
ID COLOR CONSTANCY; PERCEPTUAL ORGANIZATION; ANCHORING THEORY;
   ILLUMINATION; SHAPE; CONTRAST; IMPROVES; WORLD
AB Color is a common channel for displaying data in surface visualization, but is affected by the shadows and shading used to convey surface depth and shape. Understanding encoded data in the context of surface structure is critical for effective analysis in a variety of domains, such as in molecular biology. In the physical world, lightness constancy allows people to accurately perceive shadowed colors; however, its effectiveness in complex synthetic environments such as surface visualizations is not well understood. We report a series of crowdsourced and laboratory studies that confirm the existence of lightness constancy effects for molecular surface visualizations using ambient occlusion. We provide empirical evidence of how common visualization design decisions can impact viewers' abilities to accurately identify encoded surface colors. These findings suggest that lightness constancy aids in understanding color encodings in surface visualization and reveal a correlation between visualization techniques that improve color interpretation in shadow and those that enhance perceptions of surface depth. These results collectively suggest that understanding constancy in practice can inform effective visualization design.
C1 [Szafir, Danielle Albers; Sarikaya, Alper; Gleicher, Michael] Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.
C3 University of Wisconsin System; University of Wisconsin Madison
RP Szafir, DA (corresponding author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.
EM dalbers@cs.wisc.edu; sarikaya@cs.wisc.edu; gleicher@cs.wisc.edu
FU US National Science Foundation (NSF) [IIS-1162037, CMMI-0941013]; NIH
   [5R01AI077376-07]; Direct For Computer & Info Scie & Enginr; Div Of
   Information & Intelligent Systems [1162037] Funding Source: National
   Science Foundation
FX The authors wish to thank Charles D. Hansen for his input on this
   project and Adrian Mayorga for his help with the protein surface
   visualization implementations. This work was supported in part by the US
   National Science Foundation (NSF) awards IIS-1162037, CMMI-0941013 and
   NIH award 5R01AI077376-07.
NR 66
TC 5
Z9 6
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2016
VL 22
IS 9
BP 2107
EP 2121
DI 10.1109/TVCG.2015.2500240
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA DU4FD
UT WOS:000382166900004
PM 26584495
OA Green Accepted, Bronze
DA 2025-03-07
ER

PT J
AU Thom, D
   Krüger, R
   Ertl, T
AF Thom, Dennis
   Krueger, Robert
   Ertl, Thomas
TI Can Twitter Save Lives? A Broad-Scale Study on Visual Social Media
   Analytics for Public Safety
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual analytics; geographic visualization; social media; twitter; user
   study; evaluation
AB The use of social media monitoring for public safety is on the brink of commercialization and practical adoption. To close the gap between research and application, this paper presents results of a two-phase study on visual analytics of social media for public safety. For the first phase, we conducted a large field study, in which 29 practitioners from disaster response and critical infrastructure management were asked to investigate crisis intelligence tasks based on Twitter data recorded during the 2013 German Flood. To this end, the ScatterBlogs visual analytics system, a platform that provides reference implementations of tools and techniques popular in research, was given to them as an integrated toolbox. We reviewed the domain experts' individual performances with the system as well as their comments about the usefulness of techniques. In the second phase, we built on this feedback about ScatterBlogs in order to sketch out a system and create additional tools specifically adapted to the collected requirements. The performance of the old lab prototype is finally compared against the re-design in a controlled user study.
C1 [Thom, Dennis; Krueger, Robert; Ertl, Thomas] Univ Suttgart, Inst Visualizat & Interact Syst, Stuttgart, Germany.
RP Thom, D (corresponding author), Univ Suttgart, Inst Visualizat & Interact Syst, Stuttgart, Germany.
EM dennis.thom@vis.uni-stuttgart.de; Robert.Krueger@vis.uni-stuttgart.de;
   Thomas.Ertl@vis.uni-stuttgart.de
OI Krueger, Robert/0000-0002-6468-8356; Ertl, Thomas/0000-0003-4019-2505
FU BMBF VASA project; Horizon project CIMPLEX [641191]
FX The authors would like to thank all participants of the studies and
   Rezzakul Haider for his help during the user study. This work was
   supported by the BMBF VASA project and the Horizon 2020 project CIMPLEX,
   grant no. 641191.
NR 36
TC 20
Z9 22
U1 0
U2 33
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2016
VL 22
IS 7
BP 1816
EP 1829
DI 10.1109/TVCG.2015.2511733
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DO0NI
UT WOS:000377475200004
PM 26841398
DA 2025-03-07
ER

PT J
AU Rungta, A
   Schissler, C
   Mehra, R
   Malloy, C
   Lin, M
   Manocha, D
AF Rungta, Atul
   Schissler, Carl
   Mehra, Ravish
   Malloy, Chris
   Lin, Ming
   Manocha, Dinesh
TI SynCoPation: Interactive Synthesis-Coupled Sound Propagation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE Virtual Reality Conference (IEEE VR)
CY MAR 19-23, 2016
CL Greenville, SC
SP IEEE, IEEE Comp Soc, IEEE Comp Soc Visualizat & Graph Tech Comm, Clemson Univ
DE Sound Synthesis; Sound Propagation; Physically-based Modeling
ID SIMULATION
AB Recent research in sound simulation has focused on either sound synthesis or sound propagation, and many standalone algorithms have been developed for each domain. We present a novel technique for coupling sound synthesis with sound propagation to automatically generate realistic aural content for virtual environments. Our approach can generate sounds from rigid-bodies based on the vibration modes and radiation coefficients represented by the single-point multipole expansion. We present a mode-adaptive propagation algorithm that uses a perceptual Hankel function approximation technique to achieve interactive runtime performance. The overall approach allows for high degrees of dynamism - it can support dynamic sources, dynamic listeners, and dynamic directivity simultaneously. We have integrated our system with the Unity game engine and demonstrate the effectiveness of this fully-automatic technique for audio content creation in complex indoor and outdoor scenes. We conducted a preliminary, online user-study to evaluate whether our Hankel function approximation causes any perceptible loss of audio quality. The results indicate that the subjects were unable to distinguish between the audio rendered using the approximate function and audio rendered using the full Hankel function in the Cathedral. Tuscany, and the Game benchmarks.
C1 [Rungta, Atul; Schissler, Carl; Mehra, Ravish; Lin, Ming; Manocha, Dinesh] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC USA.
C3 University of North Carolina; University of North Carolina Chapel Hill
RP Rungta, A; Schissler, C; Mehra, R; Lin, M; Manocha, D (corresponding author), Univ N Carolina, Dept Comp Sci, Chapel Hill, NC USA.
EM rungta@cs.unc.edu; schissle@cs.unc.edu; ravish.mehra07@gmail.com;
   chris.p.malloy@gmail.com; lin@cs.unc.edu; dm@cs.unc.edu
FU NSF [1320644, 1456299]; Link Foundation Fellowship in Advanced
   Simulation and Training; Direct For Computer & Info Scie & Enginr; Div
   Of Information & Intelligent Systems [1320644] Funding Source: National
   Science Foundation; Directorate For Engineering [1456299] Funding
   Source: National Science Foundation; Div Of Industrial Innovation &
   Partnersh [1456299] Funding Source: National Science Foundation
FX The authors would like to thank Alok Meshram, Nic Morales, and
   Priyadarshi Sharma for valuable insights and help at various stages of
   the project. The authors would also like to thank the anonymous subjects
   who took part in the user-study. The work was supported in part by NSF
   grants 1320644 and 1456299 (under subcontract to Impulsonic Inc.) and
   Link Foundation Fellowship in Advanced Simulation and Training.
NR 51
TC 6
Z9 8
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2016
VL 22
IS 4
BP 1346
EP 1355
DI 10.1109/TVCG.2016.2518421
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DH5RN
UT WOS:000372849600004
PM 26780813
DA 2025-03-07
ER

PT J
AU Andrews, S
   Teichmann, M
   Kry, PG
AF Andrews, Sheldon
   Teichmann, Marek
   Kry, Paul G.
TI Blended Linear Models for Reduced Compliant Mechanical Systems
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Character animation; physics simulation; constraints
ID CONSTRAINTS; DYNAMICS
AB We present a method for the simulation of compliant, articulated structures using a plausible approximate model that focuses on modeling endpoint interaction. We approximate the structure's behavior about a reference configuration, resulting in a first order reduced compliant system, or FORK-1S. Several levels of approximation are available depending on which parts and surfaces we would like to have interactive contact forces, allowing various levels of detail to be selected. Our approach is fast and computation of the full structure's state may be parallelized. Furthermore, we present a method for reducing error by combining multiple FORK-1S models at different linearization points, through twist blending and matrix interpolation. Our approach is suitable for stiff, articulate grippers, such as those used in robotic simulation, or physics-based characters under static proportional derivative control. We demonstrate that simulations with our method can deal with kinematic chains and loops with non-uniform stiffness across joints, and that it produces plausible effects due to stiffness, damping, and inertia.
C1 [Andrews, Sheldon; Kry, Paul G.] McGill Univ, Sch Comp Sci, Montreal, PQ, Canada.
   [Teichmann, Marek] CMLabs, Montreal, PQ, Canada.
C3 McGill University
RP Andrews, S; Kry, PG (corresponding author), McGill Univ, Sch Comp Sci, Montreal, PQ, Canada.; Teichmann, M (corresponding author), CMLabs, Montreal, PQ, Canada.
EM sheldon.andrews@gmail.com; marek@cm-labs.com; kry@cs.mcgill.ca
OI Kry, Paul/0000-0003-4176-6857
FU NSERC; CFI; MITACS; CINQ; GRAND NCE
FX The authors thank the anonymous reviewers for their suggestions for
   improving the paper. This work was supported by funding from NSERC, CFI,
   MITACS, CINQ, and GRAND NCE.
NR 39
TC 2
Z9 2
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2016
VL 22
IS 3
BP 1209
EP 1222
DI 10.1109/TVCG.2015.2453951
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE2DC
UT WOS:000370435700004
PM 26829238
DA 2025-03-07
ER

PT J
AU Ren, TX
   Yu, JB
   Guo, SH
   Ma, Y
   Ouyang, YT
   Zeng, ZJ
   Zhang, YZ
   Qin, YP
AF Ren, Tianxiang
   Yu, Jubo
   Guo, Shihui
   Ma, Ying
   Ouyang, Yutao
   Zeng, Zijiao
   Zhang, Yazhan
   Qin, Yipeng
TI Diverse Motion In-Betweening From Sparse Keyframes With Dual Posture
   Stitching
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Transformers; Interpolation; Bidirectional control; Task analysis;
   Predictive models; Discrete cosine transforms; Animation; deep learning;
   in-betweening; transition generation
ID PREDICTION
AB In-betweening is a technique for generating transitions given start and target character states. The majority of existing works require multiple (often >= 10) frames as input, which are not always available. In addition, they produce results that lack diversity, which may not fulfill artists' requirements. Addressing these gaps, our work deals with a focused yet challenging problem: generating diverse and high-quality transitions given exactly two frames (only the start and target frames). To cope with this challenging scenario, we propose a bi-directional motion generation and stitching scheme which generates forward and backward transitions from the start and target frames with two adversarial autoregressive networks, respectively, and stitches them midway between the start and target frames. In contrast to stitching at the start or target frames, where the ground truth cannot be altered, there is no strict midway ground truth. Thus, our method can capitalize on this flexibility and generate high-quality and diverse transitions simultaneously. Specifically, we employ conditional variational autoencoders (CVAEs) to implement our autoregressive networks and propose a novel stitching loss to stitch the bi-directional generated motions around the midway point. Extensive experiments demonstrate that our method achieves higher motion quality and more diverse results than existing methods on the LaFAN1, Human3.6m and AMASS datasets.
C1 [Ren, Tianxiang; Yu, Jubo; Guo, Shihui; Ma, Ying; Ouyang, Yutao] Xiamen Univ, Sch Informat, Xiamen 361005, Fujian, Peoples R China.
   [Zeng, Zijiao; Zhang, Yazhan] Tencent Technol, Shenzhen 518054, Guangdong, Peoples R China.
   [Qin, Yipeng] Cardiff Univ, Cardiff CF10 3AT, Wales.
C3 Xiamen University; Tencent; Cardiff University
RP Guo, SH (corresponding author), Xiamen Univ, Sch Informat, Xiamen 361005, Fujian, Peoples R China.
EM guoshihui@xmu.edu.cn
RI Qin, Yipeng/ACP-7391-2022
OI Qin, Yipeng/0000-0002-1551-9126
FU National Natural Science Foundation of China [62072383, 61702433];
   Fundamental Research Funds for the Central Universities [20720210044];
   Royal Society [IECNSFC211022]; CCF-Tencent Open Fund
FX This work was supported in part by National Natural Science Foundation
   of China under Grant 62072383 and Grant 61702433, in part by the
   Fundamental Research Funds for the Central Universities under Grant
   20720210044, in part by Royal Society under Grant IECNSFC211022, and in
   part by CCF-Tencent Open Fund.
NR 50
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2025
VL 31
IS 2
BP 1402
EP 1413
DI 10.1109/TVCG.2024.3363457
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA R6W2Y
UT WOS:001392823200014
PM 38324439
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Zhan, YY
   Wang, TY
   Shao, TJ
   Zhou, K
AF Zhan, Youyi
   Wang, Tuanfeng Y.
   Shao, Tianjia
   Zhou, Kun
TI Pattern Guided UV Recovery for Realistic Video Garment Texturing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Clothing; Task analysis; Estimation; Three-dimensional displays;
   Geometry; Fabrics; Electronic commerce; Garment texturing; image
   synthesis; video editing
AB The fast growth of E-Commerce creates a global market worth USD 821 billion for online fashion shopping. What unique about fashion presentation is that, the same design can usually be offered with different cloths textures. However, only real video capturing or manual per-frame editing can be used for virtual showcase on the same design with different textures, both of which are heavily labor intensive. In this paper, we present a pattern-based approach for UV and shading recovery from a captured real video so that the garment's texture can be replaced automatically. The core of our approach is a per-pixel UV regression module via blended-weight multilayer perceptrons (MLPs) driven by the detected discrete correspondences from the cloth pattern. We propose a novel loss on the Jacobian of the UV mapping to create pleasant seams around the folding areas and the boundary of occluded regions while avoiding UV distortion. We also adopts the temporal constraint to ensure consistency and accuracy in UV prediction across adjacent frames. We show that our approach is robust to a variety type of clothes, in the wild illuminations and with challenging motions. We show plausible texture replacement results in our experiment, in which the folding and overlapping of the garment can be greatly preserved. We also show clear qualitative and quantitative improvement compared to the baselines as well. With the one-click setup, we look forward to our approach contributing to the growth of fashion E-commerce.
C1 [Zhan, Youyi; Shao, Tianjia; Zhou, Kun] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
   [Wang, Tuanfeng Y.] Adobe Res, London EC1Y 8AF, England.
   [Zhou, Kun] Zhejiang Univ, FaceUnity Joint Lab Intelligent Graph, Hangzhou 310058, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Shao, TJ (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
EM 1231234zhan@gmail.com; wytf123123@gmail.com; tianjiashao@gmail.com;
   kunzhou@acm.org
OI Wang, Tuanfeng/0000-0002-8180-4988
FU National Key Research and Development Program of China [2022YFF0902302];
   NSF China [62322209, U23A20311, 62172357]; XPLORER PRIZE
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2022YFF0902302, in part by NSF
   China under Grants 62322209, U23A20311 and 62172357, and in part by the
   XPLORER PRIZE.
NR 45
TC 0
Z9 0
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2024
VL 30
IS 12
BP 7531
EP 7543
DI 10.1109/TVCG.2024.3354727
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K8D1L
UT WOS:001346124800022
PM 38227412
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Cheng, SW
   Liu, Y
   Gao, YF
   Dong, ZX
AF Cheng, Shiwei
   Liu, Yang
   Gao, Yuefan
   Dong, Zhanxun
TI "As if it were my own hand": inducing the rubber hand illusion through
   virtual reality for motor imagery enhancement
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual reality; Rubber hand illusion; Assistive technologies;
   Brain-computer interface; Virtual reality; Rubber hand illusion;
   Assistive technologies; Brain-computer interface
ID BRAIN-COMPUTER INTERFACE; DESYNCHRONIZATION; SYNCHRONIZATION;
   DISCRIMINATION; STIMULATION; PLASTICITY; SYSTEM
AB Brain-computer interfaces (BCI) are widely used in the field of disability assistance and rehabilitation, and virtual reality (VR) is increasingly used for visual guidance of BCI-MI (motor imagery). Therefore, how to improve the quality of electroencephalogram (EEG) signals for MI in VR has emerged as a critical issue. People can perform MI more easily when they visualize the hand used for visual guidance as their own, and the Rubber Hand Illusion (RHI) can increase people's ownership of the prosthetic hand. We proposed to induce RHI in VR to enhance participants' MI ability and designed five methods of inducing RHI, namely active movement, haptic stimulation, passive movement, active movement mixed with haptic stimulation, and passive movement mixed with haptic stimulation, respectively. We constructed a first-person training scenario to train participants' MI ability through the five induction methods. The experimental results showed that through the training, the participants' feeling of ownership of the virtual hand in VR was enhanced, and the MI ability was improved. Among them, the method of mixing active movement and tactile stimulation proved to have a good effect on enhancing MI. Finally, we developed a BCI system in VR utilizing the above training method, and the performance of the participants improved after the training. This also suggests that our proposed method is promising for future application in BCI rehabilitation systems.
C1 [Cheng, Shiwei; Liu, Yang] Zhejiang Univ Technol, Hangzhou, Zhejiang, Peoples R China.
   [Cheng, Shiwei] Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
   [Gao, Yuefan] Cyborg Intelligence Technol Co, Changzhou, Jiangsu, Peoples R China.
   [Dong, Zhanxun] Shanghai Jiao Tong Univ, Sch Design, Shanghai, Peoples R China.
C3 Zhejiang University of Technology; Shanghai Jiao Tong University;
   Shanghai Jiao Tong University
RP Cheng, SW (corresponding author), Zhejiang Univ Technol, Hangzhou, Zhejiang, Peoples R China.; Cheng, SW (corresponding author), Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
EM 249401866@qq.com; liuyoung@zjut.edu.cn; 913201871@qq.com;
   dongzx@sjtu.edu.cn
RI Dong, Zhanxun/JVM-8882-2024; Cheng, Shiwei/JNR-8400-2023
OI Cheng, Shiwei/0000-0003-4716-4179; Liu, Yang/0009-0005-0760-9382; Dong,
   Zhanxun/0000-0003-4855-5868
FU Zhejiang Provincial Natural Science Foundation of China [LR22F020003];
   National Natural Science Foundation of China [62172368]; Zhejiang
   Provincial Key Research and Development Program [2023C01045]
FX We thank all the volunteers who participated in the experiments. This
   research work was supported in part by the Zhejiang Provincial Natural
   Science Foundation of China under grant number LR22F020003, the National
   Natural Science Foundation of China under grant number 62172368, and the
   Zhejiang Provincial Key Research and Development Program under grant
   number 2023C01045.
NR 56
TC 0
Z9 0
U1 15
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7086
EP 7096
DI 10.1109/TVCG.2024.3456154
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300039
PM 39250394
DA 2025-03-07
ER

PT J
AU Kusuyama, H
   Kageyama, Y
   Iwai, D
   Sato, K
AF Kusuyama, Hiroki
   Kageyama, Yuta
   Iwai, Daisuke
   Sato, Kosuke
TI A Multi-aperture Coaxial Projector Balancing Shadow Suppression and
   Deblurring
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Human-centered computing; Human computer interaction (HCI); Interaction
   devices; Displays and imagers; Computing methodologies; Computer
   graphics; Graphics systems and interfaces; Mixed / augmented reality
ID REMOVAL; SHAPE
AB This paper proposes a projection system that optically removes the cast shadow in projection mapping. Specifically, we realize the large-aperture (LA) projection using a large-format Fresnel lens to suppress cast shadows by condensing the projection light from a wide viewing angle. However, the resolution and contrast of the projected results are significantly degraded by defocus blur, veiling glare, and stray light caused by the aberration of an LA Fresnel lens. To solve the technical problems, we employ two different approaches: optical and digital image processing methods. First, we introduce a residual projector with a typical aperture lens on the same optical axis as the LA projector, projecting the residual (i.e., high-frequency) components attenuated in the LA projection. These projectors play different roles in shadow suppression and blur compensation, both achieved by projecting simultaneously. Secondly, we optimize the pair of projection images that can balance the shadow suppression and deblurring performance of our projection system. We implemented a proof-of-concept prototype and validated the above-mentioned techniques through projection experiments and a user study.
C1 [Kusuyama, Hiroki; Kageyama, Yuta; Iwai, Daisuke; Sato, Kosuke] Osaka Univ, Suita, Osaka, Japan.
C3 Osaka University
RP Kusuyama, H (corresponding author), Osaka Univ, Suita, Osaka, Japan.
EM hiroki.kusuyama@sens.sys.es.osaka-u.ac.jp
RI Iwai, Daisuke/R-8174-2019
OI Kusuyama, Hiroki/0009-0007-9406-2120; Sato, Kosuke/0000-0003-1429-9990
FU JSPS KAKENHI [JP20H05958]; JST PRESTO [JPMJPR19J2]; Future Social Value
   Co-Creation Project, Osaka University
FX This work was supported by JSPS KAKENHI grant number JP20H05958, JST
   PRESTO Grant Number JPMJPR19J2, and the Future Social Value Co-Creation
   Project, Osaka University.
NR 69
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7031
EP 7041
DI 10.1109/TVCG.2024.3456183
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300035
PM 39255117
OA Green Published
DA 2025-03-07
ER

PT J
AU Cong, MT
   Lan, LA
   Fedkiw, R
AF Cong, Matthew
   Lan, Lana
   Fedkiw, Ronald
TI Local Geometric Indexing of High Resolution Data for Facial
   Reconstruction From Sparse Markers
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Faces; Geometry; Surface reconstruction; Cameras; Point cloud
   compression; Deformation; Computer graphics; image processing and
   computer vision; interpolation
AB When considering sparse motion capture marker data, one typically struggles to balance its overfitting via a high dimensional blendshape system versus underfitting caused by smoothness constraints. With the current trend towards using more and more data, our aim is not to fit the motion capture markers with a parameterized (blendshape) model or to smoothly interpolate a surface through the marker positions, but rather to find an instance in the high resolution dataset that contains local geometry to fit each marker. Just as is true for typical machine learning applications, this approach benefits from a plethora of data, and thus we also consider augmenting the dataset via specially designed physical simulations that target the high resolution dataset such that the simulation output lies on the same so-called manifold as the data targeted.
C1 [Cong, Matthew; Lan, Lana; Fedkiw, Ronald] Ind Light & Mag, San Francisco, CA 94129 USA.
   [Fedkiw, Ronald] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.
C3 Stanford University
RP Cong, MT (corresponding author), Ind Light & Mag, San Francisco, CA 94129 USA.
EM matthew.d.cong@gmail.com; lanalan@gmail.com; fedkiw@cs.stanford.edu
FU ONR [N00014-19-1-2285, N00014-21-1-2771]
FX No Statement Available
NR 48
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5289
EP 5298
DI 10.1109/TVCG.2023.3289495
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400048
PM 37363850
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Fung, KLT
   Perrault, ST
   Gastner, MT
AF Fung, Kelvin L. T.
   Perrault, Simon T.
   Gastner, Michael T.
TI Effectiveness of Area-to-Value Legends and Grid Lines in Contiguous Area
   Cartograms
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cartogram; geovisualization; interactive data exploration; quantitative
   evaluation
ID ALGORITHM
AB A contiguous area cartogram is a geographic map in which the area of each region is proportional to numerical data (e.g., population size) while keeping neighboring regions connected. In this study, we investigated whether value-to-area legends (square symbols next to the values represented by the squares' areas) and grid lines aid map readers in making better area judgments. We conducted an experiment to determine the accuracy, speed, and confidence with which readers infer numerical data values for the mapped regions. We found that, when only informed about the total numerical value represented by the whole cartogram without any legend, the distribution of estimates for individual regions was centered near the true value with substantial spread. Legends with grid lines significantly reduced the spread but led to a tendency to underestimate the values. Comparing differences between regions or between cartograms revealed that legends and grid lines slowed the estimation without improving accuracy. However, participants were more likely to complete the tasks when legends and grid lines were present, particularly when the area units represented by these features could be interactively selected. We recommend considering the cartogram's use case and purpose before deciding whether to include grid lines or an interactive legend.
C1 [Fung, Kelvin L. T.] Yale NUS Coll, Singapore 138527, Singapore.
   [Fung, Kelvin L. T.] UCL, London WC1E 6BT, England.
   [Gastner, Michael T.] Singapore Inst Technol, Singapore 138683, Singapore.
   [Perrault, Simon T.] Singapore Univ Technol & Design, Singapore 487372, Singapore.
C3 Yale NUS College; University of London; University College London;
   Singapore Institute of Technology; Singapore University of Technology &
   Design
RP Gastner, MT (corresponding author), Singapore Inst Technol, Singapore 138683, Singapore.
EM fltkelvin@u.yale-nus.edu.sg; simon_perrault@sutd.edu.sg;
   michael.gastner@singaporetech.edu.sg
RI Gastner, Michael/AAK-5088-2021
OI Perrault, Simon/0000-0002-3105-9350; Fung, Kelvin/0009-0009-1694-3234;
   Gastner, Michael/0000-0002-1097-8833
FU Ministry of Education - Singapore [MOE-T2EP20221-0007]; Academic
   Research Fund Tier 1 programme [IG18-PRB104, R-607-000-401-114];
   Yale-NUSunder its Summer Research Programme
FX No Statement Available
NR 59
TC 2
Z9 2
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4631
EP 4647
DI 10.1109/TVCG.2023.3275925
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400091
PM 37186538
OA hybrid, Green Published, Green Accepted
DA 2025-03-07
ER

PT J
AU Kumar, A
   Zhang, XY
   Xin, HL
   Yan, HF
   Huang, XJ
   Xu, W
   Müller, K
AF Kumar, Ayush
   Zhang, Xinyu
   Xin, Huolin L.
   Yan, Hanfei
   Huang, Xiaojing
   Xu, Wei
   Mueller, Klaus
TI RadVolViz: An Information Display-Inspired Transfer Function Editor for
   Multivariate Volume Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Battery; color mapping; multi channel data; multivariate data; transfer
   function; volume rendering; volume visualization
ID MULTIDIMENSIONAL TRANSFER-FUNCTIONS; COLOR
AB In volume visualization transfer functions are widely used for mapping voxel properties to color and opacity. Typically, volume density data are scalars which require simple 1D transfer functions to achieve this mapping. If the volume densities are vectors of three channels, one can straightforwardly map each channel to either red, green or blue, which requires a trivial extension of the 1D transfer function editor. We devise a new method that applies to volume data with more than three channels. These types of data often arise in scientific scanning applications, where the data are separated into spectral bands or chemical elements. Our method expands on prior work in which a multivariate information display, RadViz, was fused with a radial color map, in order to visualize multi-band 2D images. In this work, we extend this joint interface to blended volume rendering. The information display allows users to recognize the presence and value distribution of the multivariate voxels and the joint volume rendering display visualizes their spatial distribution. We design a set of operators and lenses that allow users to interactively control the mapping of the multivariate voxels to opacity and color. This enables users to isolate or emphasize volumetric structures with desired multivariate properties. Furthermore, it turns out that our method also enables more insightful displays even for RGB data. We demonstrate our method with three datasets obtained from spectral electron microscopy, high energy X-ray scanning, and atmospheric science.
C1 [Kumar, Ayush; Zhang, Xinyu; Mueller, Klaus] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Yan, Hanfei; Huang, Xiaojing; Xu, Wei] Brookhaven Natl Lab, Upton, NY 11973 USA.
   [Xin, Huolin L.] Univ Calif Irvine, Dept Phys & Astron, Irvine, CA 92697 USA.
C3 State University of New York (SUNY) System; Stony Brook University;
   United States Department of Energy (DOE); Brookhaven National
   Laboratory; University of California System; University of California
   Irvine
RP Kumar, A (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM aykumar@cs.stonybrook.edu; zhang146@cs.stonybrook.edu; huolinx@uci.edu;
   hyan@bnl.gov; xjhuang@bnl.gov; xuw@bnl.gov; mueller@cs.stonybrook.edu
RI Kumar, Ayush/KLC-5631-2024; Zhang, Xinyu/HKF-8200-2023; Huang,
   Xiaojing/K-3075-2012
OI Zhang, Xinyu/0000-0002-7475-8979; Kumar, Ayush/0000-0001-5867-5623;
   Mueller, Klaus/0000-0002-0996-8590; Xu, Wei/0000-0002-4525-4819
FU NSF [IIS 1527200, 1941613, CHE-1900401]; Brookhaven National Laboratory
   LDRD [16-041]; DOE Office of Science by Brookhaven National Laboratory
   [DE-SC0012704]
FX No Statement Available
NR 49
TC 1
Z9 1
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4464
EP 4479
DI 10.1109/TVCG.2023.3263856
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400070
PM 37030815
DA 2025-03-07
ER

PT J
AU van den Brandt, A
   Jonkheer, EM
   van Workum, DJM
   van de Wetering, H
   Smit, S
   Vilanova, A
AF van den Brandt, Astrid
   Jonkheer, Eef M.
   van Workum, Dirk-Jan M.
   van de Wetering, Huub
   Smit, Sandra
   Vilanova, Anna
TI PanVA: Pangenomic Variant Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual analytics; design study; pangenomics; comparative genomics;
   variant analysis
ID SEQUENCE; ALIGNMENT; DESIGN
AB Genomics researchers increasingly use multiple reference genomes to comprehensively explore genetic variants underlying differences in detectable characteristics between organisms. Pangenomes allow for an efficient data representation of multiple related genomes and their associated metadata. However, current visual analysis approaches for exploring these complex genotype-phenotype relationships are often based on single reference approaches or lack adequate support for interpreting the variants in the genomic context with heterogeneous (meta)data. This design study introduces PanVA, a visual analytics design for pangenomic variant analysis developed with the active participation of genomics researchers. The design uniquely combines tailored visual representations with interactions such as sorting, grouping, and aggregation, allowing users to navigate and explore different perspectives on complex genotype-phenotype relations. Through evaluation in the context of plants and pathogen research, we show that PanVA helps researchers explore variants in genes and generate hypotheses about their role in phenotypic variation.
C1 [van den Brandt, Astrid; van de Wetering, Huub; Vilanova, Anna] Eindhoven Univ Technol, Dept Math & Comp Sci, NL-5612 AZ Eindhoven, Netherlands.
   [Jonkheer, Eef M.; van Workum, Dirk-Jan M.; Smit, Sandra] Wageningen Univ & Res, Bioinformat Grp, NL-6708 PB Wageningen, Netherlands.
C3 Eindhoven University of Technology; Wageningen University & Research
RP van den Brandt, A (corresponding author), Eindhoven Univ Technol, Dept Math & Comp Sci, NL-5612 AZ Eindhoven, Netherlands.
EM a.v.d.brandt@tue.nl; eef.jonkheer@wur.nl; dirk-jan.vanworkum@wur.nl;
   h.v.d.wetering@tue.nl; sandra.smit@wur.nl; a.vilanova@tue.nl
RI Smit, Sandra/E-6787-2010
OI van de Wetering, Huub/0000-0002-0517-1322; van den Brandt,
   Astrid/0000-0002-3676-1341; van Workum, Dirk-Jan/0000-0001-6247-5499;
   Vilanova, Anna/0000-0002-1034-737X; Smit, Sandra/0000-0001-5239-5321;
   Jonkheer, Eef/0000-0002-2608-6311
FU Dutch Top Consortium for Knowledge and Innovation (TKI) Agri Food
   [TU18034]
FX This work was supported in part by the Dutch Top Consortium for
   Knowledge and Innovation (TKI) Agri & Food under Grant TU18034.
NR 64
TC 1
Z9 1
U1 3
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4895
EP 4909
DI 10.1109/TVCG.2023.3282364
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400084
PM 37267130
OA Green Published
DA 2025-03-07
ER

PT J
AU Wang, M
   Li, YJ
   Shi, JC
   Steinicke, F
AF Wang, Miao
   Li, Yi-Jun
   Shi, Jinchuan
   Steinicke, Frank
TI SceneFusion: Room-Scale Environmental Fusion for Efficient Traveling
   Between Separate Virtual Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Teleportation; Visualization; Collaboration; Task analysis; Portals;
   Legged locomotion; Three-dimensional displays; Virtual reality;
   collaborative virtual environments; scene transition
ID GROUP NAVIGATION; VISUAL COMFORT; REALITY; TRANSITION
AB Traveling between scenes has become a major requirement for navigation in numerous virtual reality (VR) social platforms and game applications, allowing users to efficiently explore multiple virtual environments (VEs). To facilitate scene transition, prevalent techniques such as instant teleportation and virtual portals have been extensively adopted. However, these techniques exhibit limitations when there is a need for frequent travel between separate VEs, particularly within indoor environments, resulting in low efficiency. In this article, we first analyze the design rationale for a novel navigation method supporting efficient travel between virtual indoor scenes. Based on the analysis, we introduce the SceneFusion technique that fuses separate virtual rooms into an integrated environment. SceneFusion enables users to perceive rich visual information from both rooms simultaneously, achieving high visual continuity and spatial awareness. While existing teleportation techniques passively transport users, SceneFusion allows users to actively access the fused environment using short-range locomotion techniques. User experiments confirmed that SceneFusion outperforms instant teleportation and virtual portal techniques in terms of efficiency, workload, and preference for both single-user exploration and multi-user collaboration tasks in separate VEs. Thus, SceneFusion presents an effective solution for seamless traveling between virtual indoor scenes.
C1 [Wang, Miao; Li, Yi-Jun; Shi, Jinchuan] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Wang, Miao] Zhongguancun Lab, Beijing 100094, Peoples R China.
   [Steinicke, Frank] Univ Hamburg, Dept Informat, D-22605 Hamburg, Germany.
C3 Beihang University; Zhongguancun Laboratory; University of Hamburg
RP Wang, M (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
EM miaowang.me@gmail.com; yaoling@buaa.edu.cn; buaashijinchuan@gmail.com;
   frank.steinicke@uni-hamburg.de
RI ; Steinicke, Frank/AAC-2976-2020
OI Li, Yi-Jun/0000-0003-2733-2913; Shi, Jin-Chuan/0009-0003-4899-2205;
   Steinicke, Frank/0000-0001-9879-7414
FU National Natural Science Foundation of China [61932003]; Fundamental
   Research Funds for the Central Universities; BMBF; BMWi; DFG; EU
FX No Statement Available
NR 102
TC 2
Z9 2
U1 7
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4615
EP 4630
DI 10.1109/TVCG.2023.3271709
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400021
PM 37126613
DA 2025-03-07
ER

PT J
AU Deng, ZK
   Chen, SF
   Xie, X
   Sun, GD
   Xu, ML
   Weng, D
   Wu, YC
AF Deng, Zikun
   Chen, Shifu
   Xie, Xiao
   Sun, Guodao
   Xu, Mingliang
   Weng, Di
   Wu, Yingcai
TI Multilevel Visual Analysis of Aggregate Geo-Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Geospatial network; multilevel analysis; information visualization;
   graph drawing
ID OF-THE-ART; MASS MOBILITY; MAP LAYOUT; VISUALIZATION; ABSTRACTION;
   EVOLUTION; DYNAMICS; PATTERNS; AWARE
AB Numerous patterns found in urban phenomena, such as air pollution and human mobility, can be characterized as many directed geospatial networks (geo-networks) that represent spreading processes in urban space. These geo-networks can be analyzed from multiple levels, ranging from the macro-level of summarizing all geo-networks, meso-level of comparing or summarizing parts of geo-networks, and micro-level of inspecting individual geo-networks. Most of the existing visualizations cannot support multilevel analysis well. These techniques work by: 1) showing geo-networks separately with multiple maps leads to heavy context switching costs between different maps; 2) summarizing all geo-networks into a single network can lead to the loss of individual information; 3) drawing all geo-networks onto one map might suffer from the visual scalability issue in distinguishing individual geo-networks. In this study, we propose GeoNetverse, a novel visualization technique for analyzing aggregate geo-networks from multiple levels. Inspired by metro maps, GeoNetverse balances the overview and details of the geo-networks by placing the edges shared between geo-networks in a stacked manner. To enhance the visual scalability, GeoNetverse incorporates a level-of-detail rendering, a progressive crossing minimization, and a coloring technique. A set of evaluations was conducted to evaluate GeoNetverse from multiple perspectives.
C1 [Deng, Zikun; Chen, Shifu; Wu, Yingcai] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [Xie, Xiao] Zhejiang Univ, Dept Sport Sci, Hangzhou 310058, Zhejiang, Peoples R China.
   [Sun, Guodao] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310014, Zhejiang, Peoples R China.
   [Xu, Mingliang] Zhengzhou Univ, Sch Comp & Artificial Intelligence, Zhengzhou 450052, Peoples R China.
   [Xu, Mingliang] MOE China Natl Supercomp Ctr Zhengzhou, Engn Res Ctr Intelligent Swarm Syst, Zhengzhou 450052, Peoples R China.
   [Weng, Di] Microsoft Res Asia, Beijing 100080, Peoples R China.
C3 Zhejiang University; Zhejiang University; Zhejiang University of
   Technology; Zhengzhou University; Microsoft; Microsoft China; Microsoft
   Research Asia
RP Wu, YC (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.; Weng, D (corresponding author), Microsoft Res Asia, Beijing 100080, Peoples R China.
EM zikun_rain@zju.edu.cn; sfchen@zju.edu.cn; xxie@zju.edu.cn;
   guodao@zjut.edu.cn; iexumingliang@zzu.edu.cn; mystery.wd@gmail.com;
   ycwu@zju.edu.cn
RI Weng, Di/ABG-7408-2020; Sun, Guodao/AAN-4428-2021; Deng,
   Zikun/IQT-3106-2023
OI Sun, Guodao/0000-0002-8383-8153; Deng, Zikun/0000-0002-4477-5292; Weng,
   Di/0000-0003-2712-7274
FU NSFC [62072400, 61972356, 62036010]; Collaborative Innovation Center of
   Artificial Intelligence by MOE; Zhejiang Provincial Government (ZJU)
FX This work was supported in part by NSFC under Grants 62072400, 61972356,
   and 62036010 and in part by the Collaborative Innovation Center of
   Artificial Intelligence by MOE and Zhejiang Provincial Government (ZJU).
   Yingcai Wu and Di Weng are the co-corresponding authors.
NR 79
TC 2
Z9 2
U1 2
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3135
EP 3150
DI 10.1109/TVCG.2022.3229953
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700049
PM 37015452
DA 2025-03-07
ER

PT J
AU Engel, D
   Hartwig, S
   Ropinski, T
AF Engel, Dominik
   Hartwig, Sebastian
   Ropinski, Timo
TI Monocular Depth Decomposition of Semi-Transparent Volume Renderings
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Volume rendering; depth compositing; monocular depth estimation
AB Neural networks have shown great success in extracting geometric information from color images. Especially, monocular depth estimation networks are increasingly reliable in real-world scenes. In this work we investigate the applicability of such monocular depth estimation networks to semi-transparent volume rendered images. As depth is notoriously difficult to define in a volumetric scene without clearly defined surfaces, we consider different depth computations that have emerged in practice, and compare state-of-the-art monocular depth estimation approaches for these different interpretations during an evaluation considering different degrees of opacity in the renderings. Additionally, we investigate how these networks can be extended to further obtain color and opacity information, in order to create a layered representation of the scene based on a single color image. This layered representation consists of spatially separated semi-transparent intervals that composite to the original input rendering. In our experiments we show that existing approaches to monocular depth estimation can be adapted to perform well on semi-transparent volume renderings, which has several applications in the area of scientific visualization, like re-composition with additional objects and labels or additional shading.
C1 [Engel, Dominik; Hartwig, Sebastian; Ropinski, Timo] Ulm Univ, Visual Comp Grp, D-89081 Ulm, Germany.
C3 Ulm University
RP Engel, D (corresponding author), Ulm Univ, Visual Comp Grp, D-89081 Ulm, Germany.
EM dominik.engel@uni-ulm.de; sebastian.hartwig@uni-ulm.de;
   timo.ropinski@uni-ulm.de
OI Hartwig, Sebastian/0000-0001-8642-2789; Ropinski,
   Timo/0000-0002-7857-5512; Engel, Dominik/0000-0002-5766-7215
FU Deutsche Forschungsgemeinschaft (DFG) [391107954]
FX This work was partially supported in part by the Deutsche
   Forschungsgemeinschaft (DFG) under Grant 391107954 (Inviwo).
NR 53
TC 1
Z9 1
U1 4
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3981
EP 3994
DI 10.1109/TVCG.2023.3245305
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700021
PM 37027532
OA hybrid, Green Submitted
DA 2025-03-07
ER

PT J
AU Gong, MJ
   Chen, XJ
AF Gong, Mingjun
   Chen, Xiaojun
TI 3D Surface-Closed Mesh Clipping Based on Polygonal Partitioning for
   Surgical Planning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Surgery; Planning; Partitioning algorithms; Computational modeling;
   Three-dimensional displays; Software algorithms; Software; Model
   clipping; surgical planning; triangular mesh; partitioning
ID MAXILLOFACIAL SURGERY; TIME; SEGMENTATION; NAVIGATION; SIMULATION
AB How to create an efficient and accurate interactive tool for triangular mesh clipping is one of the key problems to be solved in computer-assisted surgical planning. Although the existing algorithms can realize three-dimensional model clipping, problems still remain unsolved regarding the flexibility of clipping paths and the capping of clipped cross-sections. In this study, we propose a mesh clipping algorithm for surgical planning based on polygonal convex partitioning. First, two-dimensional polygonal regions are extended to three-dimensional clipping paths generated from selected reference points. Second, the convex regions are partitioned with a recursive algorithm to obtain the clipped and residual models with closed surfaces. Finally, surgical planning software with the function of mesh clipping has been developed, which is capable to create complex clipping paths by normal vector adjustment and thickness control. The robustness and efficiency of our algorithm have been demonstrated by surgical planning of craniomaxillofacial osteotomy, pelvis tumor resection and cranial vault remodeling.
C1 [Gong, Mingjun] Shanghai Jiao Tong Univ, Sch Mech Engn, Inst Biomed Mfg & Life Qual Engn, Shanghai 200240, Peoples R China.
   [Chen, Xiaojun] Shanghai Jiao Tong Univ, Sch Mech Engn, Inst Med Robot, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University; Shanghai Jiao Tong University
RP Chen, XJ (corresponding author), Shanghai Jiao Tong Univ, Sch Mech Engn, Inst Med Robot, Shanghai 200240, Peoples R China.
EM 897774009@sjtu.edu.cn; xiaojunchen@sjtu.edu.cn
FU National Key R&D Program of China [2022YFE0197900]; National Natural
   Science Foundation of China [81971709, M-0019, 82011530141]; Foundation
   of Science and Technology Commission of Shanghai Municipality
   [20490740700]; Shanghai Jiao Tong University Foundation on Medical and
   Technological Joint Science Research [YG2019ZDA06, YG2021ZD21,
   YG2021QN72, YG2022QN056]
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2022YFE0197900, in part by the National Natural Science
   Foundation of China under Grants 81971709, M-0019, and 82011530141, in
   part by the Foundation of Science and Technology Commission of Shanghai
   Municipality under Grant 20490740700, and in part by Shanghai Jiao Tong
   University Foundation on Medical and Technological Joint Science
   Research under Grants YG2019ZDA06, YG2021ZD21, YG2021QN72, and
   YG2022QN056.
NR 40
TC 0
Z9 0
U1 3
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3374
EP 3385
DI 10.1109/TVCG.2022.3230739
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700090
PM 37015426
DA 2025-03-07
ER

PT J
AU Zhang, W
   Kam-Kwai, W
   Chen, YT
   Jia, AL
   Wang, LW
   Zhang, JW
   Cheng, LC
   Qu, HM
   Chen, W
AF Zhang, Wei
   Kam-Kwai, Wong
   Chen, Yitian
   Jia, Ailing
   Wang, Luwei
   Zhang, Jian-Wei
   Cheng, Lechao
   Qu, Huamin
   Chen, Wei
TI ScrollTimes: Tracing the Provenance of Paintings as a Window Into
   History
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual analytics; digital humanities; painting analysis; traditional
   chinese painting
AB The study of cultural artifact provenance, tracing ownership and preservation, holds significant importance in archaeology and art history. Modern technology has advanced this field, yet challenges persist, including recognizing evidence from diverse sources, integrating sociocultural context, and enhancing interactive automation for comprehensive provenance analysis. In collaboration with art historians, we examined the handscroll, a traditional Chinese painting form that provides a rich source of historical data and a unique opportunity to explore history through cultural artifacts. We present a three-tiered methodology encompassing artifact, contextual, and provenance levels, designed to create a "Biography" for handscroll. Our approach incorporates the application of image processing techniques and language models to extract, validate, and augment elements within handscroll using various cultural heritage databases. To facilitate efficient analysis of non-contiguous extracted elements, we have developed a distinctive layout. Additionally, we introduce ScrollTimes, a visual analysis system tailored to support the three-tiered analysis of handscroll, allowing art historians to interactively create biographies tailored to their interests. Validated through case studies and expert interviews, our approach offers a window into history, fostering a holistic understanding of handscroll provenance and historical significance.
C1 [Zhang, Wei; Chen, Yitian; Jia, Ailing; Wang, Luwei; Zhang, Jian-Wei; Cheng, Lechao; Chen, Wei] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
   [Kam-Kwai, Wong; Qu, Huamin] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong 999077, Peoples R China.
   [Chen, Wei] Zhejiang Univ, Lab Art & Archaeol Image, Minist Educ, Hangzhou 310027, Peoples R China.
C3 Zhejiang University; Hong Kong University of Science & Technology;
   Zhejiang University
RP Chen, W (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Peoples R China.
EM zw_yixian@zju.edu.cn; kkwongar@connect.ust.hk; oscarchen@zju.edu.cn;
   jiaailing@zju.edu.cn; ppwlwpp@zju.edu.cn; zjw.cs@zju.edu.cn;
   liygcheng@zju.edu.cn; huamin@cse.ust.hk; chenvis@zju.edu.cn
RI Chen, Wei/AAR-9817-2020; Wang, Luwei/HHZ-6184-2022
OI Zhang, Jianwei/0000-0001-8358-6278; Chen, Wei/0000-0002-8365-4741;
   Cheng, Lechao/0000-0002-7546-9052; chen, yi tian/0009-0000-0590-594X;
   WONG, Kam Kwai/0000-0002-2813-1972
FU Fundamental Research Funds for the Central Universities
FX No Statement Available
NR 82
TC 0
Z9 0
U1 11
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2024
VL 30
IS 6
BP 2981
EP 2994
DI 10.1109/TVCG.2024.3388523
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WC8Z6
UT WOS:001252775500015
PM 38625782
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Pan, Y
   Tan, S
   Cheng, SR
   Lin, QF
   Zeng, ZJ
   Mitchell, K
AF Pan, Ye
   Tan, Shuai
   Cheng, Shengran
   Lin, Qunfen
   Zeng, Zijiao
   Mitchell, Kenny
TI Expressive Talking Avatars
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Human computer interaction (HCI); HCI design and evaluation methods;
   User studies; Computer graphics; Graphics systems and interfaces;
   Virtual reality; Human-centered computing
AB Stylized avatars are common virtual representations used in VR to support interaction and communication between remote collaborators. However, explicit expressions are notoriously difficult to create, mainly because most current methods rely on geometric markers and features modeled for human faces, not stylized avatar faces. To cope with the challenge of emotional and expressive generating talking avatars, we build the Emotional Talking Avatar Dataset which is a talking-face video corpus featuring 6 different stylized characters talking with 7 different emotions. Together with the dataset, we also release an emotional talking avatar generation method which enables the manipulation of emotion. We validated the effectiveness of our dataset and our method in generating audio based puppetry examples, including comparisons to state-of-the-art techniques and a user study. Finally, various applications of this method are discussed in the context of animating avatars in VR.
C1 [Pan, Ye; Tan, Shuai; Cheng, Shengran] Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
   [Lin, Qunfen; Zeng, Zijiao] Tencent Games, Shenzhen, Peoples R China.
   [Mitchell, Kenny] Edinburgh Napier Univ, Edinburgh, Scotland.
C3 Shanghai Jiao Tong University; Edinburgh Napier University
RP Pan, Y (corresponding author), Shanghai Jiao Tong Univ, Shanghai, Peoples R China.
EM whitneypanye@sjtu.edu.cn; tanshuai0219@sjtu.edu.cn;
   SR-Cheng@sjtu.edu.cn; volleylin@tencent.com; zijiaozeng@tencent.com;
   k.mitchell2@napier.ac.uk
RI Mitchell, Kenny/AAZ-3421-2020
OI Cheng, Shengran/0000-0003-4044-0185; Mitchell,
   Kenny/0000-0003-2420-7447; Tan, Shuai/0000-0003-3322-5161
FU National Natural Science Foundation of China
FX No Statement Available
NR 41
TC 1
Z9 1
U1 5
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2538
EP 2548
DI 10.1109/TVCG.2024.3372047
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400063
PM 38437076
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Tian, F
   Ni, ST
   Zhang, XY
   Chen, F
   Zhu, QL
   Xu, CY
   Li, YZ
AF Tian, Feng
   Ni, Shuting
   Zhang, Xiaoyue
   Chen, Fei
   Zhu, Qiaolian
   Xu, Chunyi
   Li, Yuzhi
TI Enhancing Tai Chi Training System: Towards Group-Based and
   Hyper-Realistic Training Experiences
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual reality; social experience; action guidance trajectories; hand
   movement trajectories analysis; hyper-realistic
AB In this article, we propose a lightweight and flexible enhanced Tai Chi training system composed of multiple standalone virtual reality (VR) devices. The system aims to enable a hyper-realistic multi-user action training platform at low cost by displaying real-time action guidance trajectories, providing real-world impossible visual effects and functions, and rapidly enhancing movement precision and communication interest for learners. We objectively evaluate participants' action quality at different levels of immersion, including traditional coach guidance (TCG), VR, and mixed reality (MR), along with subjective measures like motion sickness, quality of interaction, social meaning, presence/immersion to comprehensively explore the system's feasibility. The results indicate VR performs the best in training accuracy, but MR provides superior social experience and relatively high accuracy. Unlike TCG, MR offers hyper-realistic hand movement trajectories and Tai Chi social references. Compared with VR, MR provides more realistic avatar companions and a safer environment. In summary, MR balances accuracy and social experience.
C1 [Tian, Feng; Ni, Shuting; Zhang, Xiaoyue; Chen, Fei; Zhu, Qiaolian; Xu, Chunyi; Li, Yuzhi] Shanghai Univ, Shanghai, Peoples R China.
C3 Shanghai University
RP Tian, F (corresponding author), Shanghai Univ, Shanghai, Peoples R China.
EM ouman@shu.edu.cn; shangkenst@shu.edu.cn; zxy0716@shu.edu.cn;
   buer2001@shu.edu.cn; gjky@shu.edu.cn; xcy2008@shu.edu.cn;
   shadowmcv@shu.edu.cn
OI Zhang, Xiaoyue/0009-0004-9672-5512; Li, Yuzhi/0009-0009-7050-7775
FU Shanghai University Interdisciplinary Joint Project
FX No Statement Available
NR 8
TC 2
Z9 2
U1 5
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2713
EP 2723
DI 10.1109/TVCG.2024.3372099
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400068
PM 38457324
DA 2025-03-07
ER

PT J
AU Sisouk, K
   Delon, J
   Tierny, J
AF Sisouk, Keanu
   Delon, Julie
   Tierny, Julien
TI Wasserstein Dictionaries of Persistence Diagrams
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Ensemble data; persistence diagrams; topological data analysis
ID NONPARAMETRIC MODELS; MORSE COMPLEXES; VISUAL ANALYSIS; CRITICAL-POINTS;
   REEB GRAPHS; UNCERTAINTY; VISUALIZATION; TOPOLOGY; VARIABILITY;
   INTERPOLATION
AB This article presents a computational framework for the concise encoding of an ensemble of persistence diagrams, in the form of weighted Wasserstein barycenters Turner et al. (2014), Vidal et al. (2020) of a dictionary of atom diagrams. We introduce a multi-scale gradient descent approach for the efficient resolution of the corresponding minimization problem, which interleaves the optimization of the barycenter weights with the optimization of the atom diagrams. Our approach leverages the analytic expressions for the gradient of both sub-problems to ensure fast iterations and it additionally exploits shared-memory parallelism. Extensive experiments on public ensembles demonstrate the efficiency of our approach, with Wasserstein dictionary computations in the orders of minutes for the largest examples. We show the utility of our contributions in two applications. First, we apply Wassserstein dictionaries to data reduction and reliably compress persistence diagrams by concisely representing them with their weights in the dictionary. Second, we present a dimensionality reduction framework based on a Wasserstein dictionary defined with a small number of atoms (typically three) and encode the dictionary as a low dimensional simplex embedded in a visual space (typically in 2D). In both applications, quantitative experiments assess the relevance of our framework. Finally, we provide a C++ implementation that can be used to reproduce our results.
C1 [Sisouk, Keanu; Tierny, Julien] Sorbonne Univ, F-75005 Paris, France.
   [Delon, Julie] Univ Paris Cite, F-75006 Paris, France.
C3 Sorbonne Universite; Universite Paris Cite
RP Sisouk, K (corresponding author), Sorbonne Univ, F-75005 Paris, France.
EM Keanu.Sisouk@lip6.fr; julie.delon@u-paris.fr;
   julien.tierny@sorbonneuniversite.fr
RI Delon, Julie/U-5964-2017
OI Delon, Julie/0000-0002-7182-7537
FU European Commission
FX No Statement Available
NR 105
TC 0
Z9 0
U1 1
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2024
VL 30
IS 2
BP 1638
EP 1651
DI 10.1109/TVCG.2023.3330262
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EC6D7
UT WOS:001136746300013
PM 37930922
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Wang, CL
   Thompson, J
   Lee, B
AF Wang, Chenglong
   Thompson, John
   Lee, Bongshin
TI Data Formulator: AI-Powered Concept-Driven Visualization Authoring
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Temperature distribution; Urban areas;
   Visualization; Transforms; Histograms; Libraries; AI; visualization
   authoring; data transformation; programming by example; natural
   language; large language model
ID DESIGN; LYRA
AB With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback to help authors inspect and understand them. A user study with 10 participants shows that participants could learn and use Data Formulator to create visualizations that involve challenging data transformations, and presents interesting future research directions.
C1 [Wang, Chenglong; Thompson, John; Lee, Bongshin] Microsoft Res, Redmond, WA 98052 USA.
C3 Microsoft
RP Wang, CL (corresponding author), Microsoft Res, Redmond, WA 98052 USA.
EM chenglong.wang@microsoft.com; johnthompson@microsoft.com;
   bongshin@microsoft.com
RI Thompson, John/K-9851-2016
OI Thompson, John/0000-0002-3102-4035
NR 63
TC 4
Z9 4
U1 9
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 1128
EP 1138
DI 10.1109/TVCG.2023.3326585
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500081
PM 37871079
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Davidson, K
   Lisle, L
   Whitley, K
   Bowman, DA
   North, C
AF Davidson, Kylie
   Lisle, Lee
   Whitley, Kirsten
   Bowman, Doug A.
   North, Chris
TI Exploring the Evolution of Sensemaking Strategies in Immersive Space to
   Think
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Data visualization; Cognition; Prototypes; Visual
   analytics; Three-dimensional displays; Keyboards; Human-computer
   interaction; immersive analytics; virtual reality; information
   visualization; sensemaking
ID VISUAL ANALYTICS
AB Existing research on immersive analytics to support the sensemaking process focuses on single-session sensemaking tasks. However, in the wild, sensemaking can take days or months to complete. In order to understand the full benefits of immersive analytic systems, we need to understand how immersive analytic systems provide flexibility for the dynamic nature of the sensemaking process. In our work, we build upon an existing immersive analytic system - Immersive Space to Think, to evaluate how immersive analytic systems can support sensemaking tasks over time. We conducted a user study with eight participants with three separate analysis sessions each. We found significant differences between analysis strategies between sessions one, two, and three, which suggest that immersive space to think can benefit analysts during multiple stages in the sensemaking process.
C1 [Davidson, Kylie; Lisle, Lee; Bowman, Doug A.; North, Chris] Virginia Polytech Inst & State Univ, Dept Comp Sci, Blacksburg, VA 24060 USA.
   [Whitley, Kirsten] US Dept Def, Alexandria, VA 22350 USA.
C3 Virginia Polytechnic Institute & State University; United States
   Department of Defense
RP Davidson, K (corresponding author), Virginia Polytech Inst & State Univ, Dept Comp Sci, Blacksburg, VA 24060 USA.
EM kyliedavidson@vt.edu; llisle@vt.edu; visual.tycho@gmail.com;
   dbowman@vt.edu; north@cs.vt.edu
OI Davidson, Kylie/0000-0002-9888-5278; Bowman, Doug/0000-0003-0491-5067;
   Whitley, Kirsten/0000-0003-1356-326X
FU Laboratory for Analytic Sciences (LAS); NSF under Grant I/UCRC via the
   NSF Center for Space, High-performance, and Resilient Computing (SHREC)
   [CNS-1822080]
FX This work was supported in part by the Laboratory for Analytic Sciences
   (LAS). Any opinions, findings, conclusions, or recommendations expressed
   in this material are those of the authors and do not necessarily reflect
   the views of the LAS and/or any agency of the United States Government.
   This work was supported in part by the NSF under Grant I/UCRC
   CNS-1822080 via the NSF Center for Space, High-performance, and
   Resilient Computing (SHREC).
NR 34
TC 6
Z9 7
U1 1
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5294
EP 5307
DI 10.1109/TVCG.2022.3207357
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300033
PM 36112554
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Reyes-Aviles, F
   Fleck, P
   Schmalstieg, D
   Arth, C
AF Reyes-Aviles, Fernando
   Fleck, Philipp
   Schmalstieg, Dieter
   Arth, Clemens
TI Compact World Anchors: Registration Using Parametric Primitives as Scene
   Description
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Camera localization; correspondence problem; 3D registration;
   closed-form method; augmented reality
ID POSE ESTIMATION; POINT
AB We present a registration method relying on geometric constraints extracted from parametric primitives contained in 3D parametric models. Our method solves the registration in closed-form from three line-to-line, line-to-plane or plane-to-plane correspondences. The approach either works with semantically segmented RGB-D scans of the scene or with the output of plane detection in common frameworks like ARKit and ARCore. Based on the primitives detected in the scene, we build a list of descriptors using the normals and centroids of all the found primitives, and match them against the pre-computed list of descriptors from the model in order to find the scene-to-model primitive correspondences. Finally, we use our closed-form solver to estimate the 6DOFtransformation from three lines and one point, which we obtain from the parametric representations of the model and scene parametric primitives. Quantitative and qualitative experiments on synthetic and real-world data sets demonstrate the performance and robustness of our method. We show that it can be used to create compact world anchors for indoor localization in AR applications on mobile devices leveraging commercial SLAM capabilities.
C1 [Reyes-Aviles, Fernando] VRVis Competence Ctr Vienna, A-1220 Vienna, Austria.
   [Fleck, Philipp; Schmalstieg, Dieter; Arth, Clemens] Graz Univ Technol, A-8010 Graz, Austria.
C3 Graz University of Technology
RP Reyes-Aviles, F (corresponding author), VRVis Competence Ctr Vienna, A-1220 Vienna, Austria.
EM fernando.reyes-aviles@icg.tugraz.at; philipp.fleck@icg.tugraz.at;
   schmalstieg@tugraz.at; arth@icg.tugraz.at
OI Arth, Clemens/0000-0001-6949-4713; Reyes-Aviles,
   Fernando/0000-0002-4299-597X; Schmalstieg, Dieter/0000-0003-2813-2235
FU Competence Center VRVis; BMK; BMDW; Styria; SFG; Tyrol and Vienna
   Business Agency under the scope of COMET - Competence Centers for
   Excellent Technologies [879730]
FX This work was supported in part by Competence Center VRVis. VRVis is
   funded by BMK, BMDW, Styria, SFG, Tyrol and Vienna Business Agency under
   the scope of COMET - Competence Centers for Excellent Technologies-under
   Grant 879730 managed by FFG.
NR 51
TC 5
Z9 5
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2023
VL 29
IS 10
BP 4140
EP 4153
DI 10.1109/TVCG.2022.3183264
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8ZW3
UT WOS:001060356200008
PM 35704545
DA 2025-03-07
ER

PT J
AU Zheng, YY
   Chen, BJ
   Shen, YF
   Shen, KD
AF Zheng, Youyi
   Chen, Beijia
   Shen, Yuefan
   Shen, Kaidi
TI TeethGNN: Semantic 3D Teeth Segmentation With Graph Neural Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Teeth; Feature extraction; Three-dimensional displays; Semantics; Image
   segmentation; Deep learning; Representation learning; 3D Teeth
   segmentation; graph neural network; geometric deep learning; clustering
ID TOOTH SEGMENTATION
AB In this paper, we present TeethGNN, a novel 3D tooth segmentation method based on graph neural networks (GNNs). Given a mesh-represented 3D dental model in non-euclidean domain, our method outputs accurate and fine-grained separation of each individual tooth robust to scanning noise, foreign matters (e.g., bubbles, dental accessories, etc.), and even severe malocclusion. Unlike previous CNN-based methods that bypass handling non-euclidean mesh data by reshaping hand-crafted geometric features into regular grids, we explore the non-uniform and irregular structure of mesh itself in its dual space and exploit graph neural networks for effective geometric feature learning. To address the crowded teeth issues and incomplete segmentation that commonly exist in previous methods, we design a two-branch network, one of which predicts a segmentation label for each facet while the other regresses each facet an offset away from its tooth centroid. Clustering are later conducted on offset-shifted locations, enabling both the separation of adjoining teeth and the adjustment of incompletely segmented teeth. Exploiting GNN for directly processing mesh data frees us from extracting hand-crafted feature, and largely speeds up the inference procedure. Extensive experiments have shown that our method achieves the new state-of-the-art results for teeth segmentation and outperforms previous methods both quantitatively and qualitatively.
C1 [Zheng, Youyi; Chen, Beijia; Shen, Yuefan] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
   [Shen, Kaidi] Hangzhou Choho Tech Co Ltd, Hangzhou 310030, Peoples R China.
C3 Zhejiang University
RP Shen, KD (corresponding author), Hangzhou Choho Tech Co Ltd, Hangzhou 310030, Peoples R China.
EM youyizheng@zju.edu.cn; beibeijia@zju.edu.cn; jhonve@zju.edu.cn;
   kd_shen@outlook.com
RI Shen, Yuefan/GWN-0324-2022
OI Shen, Yuefan/0000-0002-6049-7966; Shen, Kaidi/0000-0002-9367-3911
FU National Key Research & Development Program of China [2018YFE0100900]
FX This work was supported by the National Key Research & Development
   Program of China under Grant 2018YFE0100900.
NR 36
TC 19
Z9 19
U1 5
U2 52
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2023
VL 29
IS 7
BP 3158
EP 3168
DI 10.1109/TVCG.2022.3153501
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA H4XO7
UT WOS:000996011900002
PM 35196239
DA 2025-03-07
ER

PT J
AU Xu, XC
   Zhou, Y
   Shao, BC
   Feng, GH
   Yu, C
AF Xu, Xinchi
   Zhou, Yang
   Shao, Bingchan
   Feng, Guihuan
   Yu, Chun
TI GestureSurface: VR Sketching through Assembling Scaffold Surface with
   Non-Dominant Hand
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual reality; gestural input; sketching
AB 3D sketching in virtual reality (VR) provides an immersive drawing experience for designs. However, due to the lack of depth perception cues in VR, scaffolding surfaces that constrain strokes to 2D are usually used as visual guides to reduce the difficulty of drawing accurate strokes. When the dominant hand is occupied by the pen tool, the efficiency of scaffolding-based sketching can be improved by using gesture input to reduce the idleness of the non-dominant hand. This paper presents GestureSurface, a bi-manual interface that uses non-dominant hand performing gestures to operate scaffolding and the other hand drawing with controller. We designed a set of non-dominant gestures to create and manipulate scaffolding surfaces, which are assembled by automatic combination based on five predefined primitive surfaces. We evaluated GestureSurface through a 20-person user study and found that the method of scaffolding-based sketching using non-dominant hand has the advantages of high efficiency and low fatigue.
C1 [Xu, Xinchi; Zhou, Yang; Shao, Bingchan; Feng, Guihuan] Nanjing Univ, Software Inst, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
   [Yu, Chun] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.
C3 Nanjing University; Tsinghua University
RP Feng, GH (corresponding author), Nanjing Univ, Software Inst, State Key Lab Novel Software Technol, Nanjing, Peoples R China.
EM xinchi@smail.nju.edu.cn; zhouyang1997@smail.nju.edu.cn;
   bingchanshao@smail.nju.edu.cn; fenggh@nju.edu.cn; chunyu@tsinghua.edu.cn
OI Xu, Xinchi/0000-0001-8628-0418; Yu, Chun/0000-0003-2591-7993
FU National Key RD Program [2018YFB1004900]; Dandan Wang of Nanjing
   University of Information Science and Technology
FX This work is supported by National Key R&D Program (2018YFB1004900). The
   authors thank Dandan Wang of Nanjing University of Information Science
   and Technology for providing design and support for the study.
   Participants in the experiment andanonymous reviewers also contributed
   to the publication of this paper.
NR 56
TC 7
Z9 7
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2499
EP 2507
DI 10.1109/TVCG.2023.3247059
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D0PW0
UT WOS:000965842400001
PM 37027702
DA 2025-03-07
ER

PT J
AU Wang, L
   Huang, MJ
   Yang, R
   Liang, HN
   Han, J
   Sun, Y
AF Wang, Liu
   Huang, Mengjie
   Yang, Rui
   Liang, Hai-Ning
   Han, Ji
   Sun, Ying
TI Survey of Movement Reproduction in Immersive Virtual Rehabilitation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Resists; Electromagnetic compatibility; User experience; Virtual
   environments; Modulation; Tracking; Real-time systems; Virtual reality;
   movement reproduction; rehabilitation; movement representation; user
   experience
ID UPPER-LIMB REHABILITATION; UPPER EXTREMITY FUNCTION; MOTOR IMAGERY;
   ERROR AUGMENTATION; OPTIC FLOW; REALITY; STROKE; INDIVIDUALS;
   PERFORMANCE; AVATAR
AB Virtual reality (VR) has emerged as a powerful tool for rehabilitation. Many effective VR applications have been developed to support motor rehabilitation of people affected by motor issues. Movement reproduction, which transfers users' movements from the physical world to the virtual environment, is commonly used in VR rehabilitation applications. Three major components are required for movement reproduction in VR: (1) movement input, (2) movement representation, and (3) movement modulation. Until now, movement reproduction in virtual rehabilitation has not yet been systematically studied. This article aims to provide a state-of-the-art review on this subject by focusing on existing literature on immersive motor rehabilitation using VR. In this review, we provided in-depth discussions on the rehabilitation goals and outcomes, technology issues behind virtual rehabilitation, and user experience regarding movement reproduction. Similarly, we present good practices and highlight challenges and opportunities that can form constructive suggestions for the design and development of fit-for-purpose VR rehabilitation applications and can help frame future research directions for this emerging area that combines VR and health.
C1 [Wang, Liu; Huang, Mengjie] Xian Jiaotong Liverpool Univ, Design Sch, Suzhou 215000, Peoples R China.
   [Wang, Liu; Han, Ji] Univ Liverpool, Dept Civil Engn & Ind Design, Liverpool L69 3BX, England.
   [Yang, Rui; Liang, Hai-Ning] Xian Jiaotong Liverpool Univ, Sch Adv Technol, Suzhou 215000, Peoples R China.
   [Sun, Ying] Kunshan Rehabil Hosp, Occupat Therapy Dept, Suzhou 215335, Peoples R China.
C3 Xi'an Jiaotong-Liverpool University; University of Liverpool; Xi'an
   Jiaotong-Liverpool University
RP Huang, MJ (corresponding author), Xian Jiaotong Liverpool Univ, Design Sch, Suzhou 215000, Peoples R China.
EM liu.wang19@student.xjtlu.edu.cn; mengjie.huang@xjtlu.edu.cn;
   r.yang@xjtlu.edu.cn; haining.liang@xjtlu.edu.cn; Ji.Han@liverpool.ac.uk;
   443969838@qq.com
RI Yang, Rui/AFN-1679-2022; huang, meng/LXZ-7770-2024
OI YANG, RUI/0000-0002-5634-5476; Han, Ji/0000-0003-3240-4942; Wang,
   Liu/0000-0002-0635-6054; Huang, Mengjie/0000-0001-8163-8679; Liang,
   Hai-Ning/0000-0003-3600-8955
FU Key Program Special Fund in XJTLU [KSF-E-34]; Research Development Fund
   of XJTLU [RDF-18-02-30]; Natural Science Foundation of theJiangsu Higher
   Education Institutions of China [20KJB520034]
FX This work was supported in part by Key Program Special Fund in XJTLU
   under Grant KSF-E-34, in part by Research Development Fund of XJTLU
   underGrant RDF-18-02-30 and in part by the Natural Science Foundation of
   theJiangsu Higher Education Institutions of China under Grant
   20KJB520034
NR 101
TC 26
Z9 28
U1 8
U2 29
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2023
VL 29
IS 4
BP 2184
EP 2202
DI 10.1109/TVCG.2022.3142198
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D9DT5
UT WOS:000971666900020
PM 35015645
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Wang, SD
   Wang, WC
   Zhao, H
AF Wang, Shaodong
   Wang, Wencheng
   Zhao, Hui
TI Using Foliation Leaves to Extract Reeb Graphs on Surfaces
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Harmonic analysis; Surface treatment; Shape; Level set; Task analysis;
   Skeleton; Manifolds; Reeb graph; topology; foliation
ID 3D SHAPE RETRIEVAL; TOPOLOGICAL SIMPLIFICATION; MESH
AB For Reeb graph extraction on surfaces, existing methods always use the isolines of a function defined on the surface to detect the surface components and the neighboring relationships between them. Since such detection is unstable, it is still a challenge for the extracted Reeb graphs to stably and concisely encode the topological information of the surface. In this article, we address this challenge by using foliation leaves to extract Reeb graphs. In particular, we employ a method for generating measured harmonic foliations by defining loops for foliation initialization and diffusing leaves from loops over the surface. We demonstrate that when the loops are determined, the neighboring relationships between the leaves from different loops are fixed. Thus, we can use loops to represent surface components for robustly detecting the interrelationships between surface components. As a result, we are able to extract stable and concise Reeb graphs. We developed novel measures for loop determination and improved foliation generation, and our method allows the user to manually prescribe loops for generating Reeb graphs with desired structures. Therefore, the potential of Reeb graphs for representing surfaces is enhanced, including conveniently representing the symmetries of the surface and ignoring topological noise. This is verified by our experimental results which indicate that our Reeb graphs are compact and expressive, promoting shape analysis.
C1 [Wang, Wencheng] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100045, Peoples R China.
   Univ Chinese Acad Sci, Beijing 101408, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS
RP Wang, WC (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100045, Peoples R China.
EM wangsd@ios.ac.cn; whn@ios.ac.cn; huizhao@ios.ac.cn
RI Wang, Wencheng/A-3828-2009
OI wang, wen cheng/0000-0001-5094-4606; Zhao, Hui/0000-0003-4442-043X;
   Wang, Shaodong/0000-0002-7982-6600
FU National Natural Science Foundation of China [62072446]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62072446.
NR 59
TC 0
Z9 0
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2023
VL 29
IS 4
BP 2117
EP 2131
DI 10.1109/TVCG.2022.3141764
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D9DT5
UT WOS:000971666900016
PM 35015643
DA 2025-03-07
ER

PT J
AU Matthews, BJ
   Thomas, BH
   Von Itzstein, GS
   Smith, RT
AF Matthews, Brandon J. J.
   Thomas, Bruce H. H.
   Von Itzstein, G. Stewart
   Smith, Ross T. T.
TI Adaptive Reset Techniques for Haptic Retargeted Interaction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Haptic retargeting; redirection; interaction; perception; user
   interfaces; virtual reality
ID HAND; CONTROLLER
AB This article presents a set of adaptive reset techniques for use with haptic retargeting systems focusing on interaction with hybrid virtual reality interfaces that align with a physical interface. Haptic retargeting between changing physical and virtual targets requires a reset where the physical and virtual hand positions are re-aligned. We present a modified Point technique to guide the user in the direction of their next interaction such that the remaining distance to the target is minimized upon completion of the reset. This, along with techniques drawn from existing work are further modified to consider the angular and translational gain of each redirection and identify the optimal position for the reset to take place. When the angular and translational gain is within an acceptable range, the reset can be entirely omitted. This enables continuous retargeting between targets removing interruptions from a sequence of retargeted interactions. These techniques were evaluated in a user study which showed that adaptive reset techniques can provide a significant decrease in task completion time, travel distance, and the number of user errors.
C1 [Matthews, Brandon J. J.; Thomas, Bruce H. H.; Von Itzstein, G. Stewart; Smith, Ross T. T.] Univ South Australia, Australian Res Ctr Interact & Virtual Environm, Wearable Comp Lab, Adelaide, SA 5001, Australia.
C3 University of South Australia
RP Matthews, BJ (corresponding author), Univ South Australia, Australian Res Ctr Interact & Virtual Environm, Wearable Comp Lab, Adelaide, SA 5001, Australia.
EM brandon.matthews@mymail.unisa.edu.au; bruce.thomas@unisa.edu.au;
   gsa@vonitzstein.com; ross.smith@unisa.edu.au
RI Matthews, Brandon/AAX-4910-2021; Von Itzstein, Stewart/GMW-8722-2022;
   Smith, Ross/L-4790-2016; Thomas, Bruce/A-1470-2008
OI Matthews, Brandon J./0000-0002-8673-2434; Von Itzstein, G
   Stewart/0000-0003-1173-4424; Smith, Ross/0000-0002-9044-9199; Thomas,
   Bruce/0000-0002-9148-085X
FU Australian Government Research Training Program Scholarship, Department
   of Education, Skills and Employment
FX This work was supported by an Australian Government Research Training
   Program Scholarship, Department of Education, Skills and Employment.
NR 34
TC 2
Z9 2
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2023
VL 29
IS 2
BP 1478
EP 1490
DI 10.1109/TVCG.2021.3120410
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7M2HO
UT WOS:000906475100015
PM 34653001
DA 2025-03-07
ER

PT J
AU Wang, QW
   Huang, KX
   Chandak, P
   Zitnik, M
   Gehlenborg, N
AF Wang, Qianwen
   Huang, Kexin
   Chandak, Payal
   Zitnik, Marinka
   Gehlenborg, Nils
TI Extending the Nested Model for User-Centric XAI: A Design Study on
   GNN-based Drug Repurposing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual Explanation; XAI; Graph Neural Network; Visualization Design
   Model; Drug Repurposing
AB Whether AI explanations can help users achieve specific tasks efficiently (i.e., usable explanations) is significantly influenced by their visual presentation. While many techniques exist to generate explanations, it remains unclear how to select and visually present AI explanations based on the characteristics of domain users. This paper aims to understand this question through a multidisciplinary design study for a specific problem: explaining graph neural network (GNN) predictions to domain experts in drug repurposing, i.e., reuse of existing drugs for new diseases. Building on the nested design model of visualization, we incorporate XAI design considerations from a literature review and from our collaborators' feedback into the design process. Specifically, we discuss XAI-related design considerations for usable visual explanations at each design layer: target user, usage context, domain explanation, and XAI goal at the domain layer; format, granularity, and operation of explanations at the abstraction layer; encodings and interactions at the visualization layer; and XAI and rendering algorithm at the algorithm layer. We present how the extended nested model motivates and informs the design of DrugExplorer, an XAI tool for drug repurposing. Based on our domain characterization, DrugExplorer provides path-based explanations and presents them both as individual paths and meta-paths for two key XAI operations, why and what else. DrugExplorer offers a novel visualization design called MetaMatrix with a set of interactions to help domain users organize and compare explanation paths at different levels of granularity to generate domain-meaningful insights. We demonstrate the effectiveness of the selected visual presentation and DrugExplorer as a whole via a usage scenario, a user study, and expert interviews. From these evaluations, we derive insightful observations and reflections that can inform the design of XAI visualizations for other scientific applications.
C1 [Wang, Qianwen; Zitnik, Marinka; Gehlenborg, Nils] Harvard Univ, Cambridge, MA 02138 USA.
   [Huang, Kexin] Stanford Univ, Stanford, CA USA.
   [Chandak, Payal] Harvard Hlth Sci & Technol, Cambridge, MA USA.
C3 Harvard University; Stanford University
RP Wang, QW (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.
EM qianwen_wang@hms.harvard.com; kexinh@stanford.edu;
   payal_chandak@hst.harvard.edu; marinka@hms.harvard.com;
   nils@hms.harvard.com
RI Wang, Qianwen/GRJ-9435-2022; Huang, Kexin/AAU-2699-2021
FU NSF [IIS-2030459, IIS-2033384]; Air Force Contract [FA8702-15-D-0001];
   Harvard Data Science Initiative; Amazon Research Award; Bayer Early
   Excellence in Science Award; AstraZeneca Research; Roche Alliance with
   Distinguished Scientists Award
FX The authors wish to thank all the participants in the expert interviews
   and user studies. M.Z. is supported, in part, by NSF under Nos.
   IIS-2030459 and IIS-2033384, Air Force Contract No. FA8702-15-D-0001,
   Harvard Data Science Initiative, Amazon Research Award, Bayer Early
   Excellence in Science Award, AstraZeneca Research, and Roche Alliance
   with Distinguished Scientists Award.
NR 87
TC 26
Z9 28
U1 7
U2 32
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2023
VL 29
IS 1
BP 1266
EP 1276
DI 10.1109/TVCG.2022.3209435
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7F6YZ
UT WOS:000901991800014
PM 36223348
OA hybrid, Green Submitted
DA 2025-03-07
ER

PT J
AU Corbalán-Navarro, D
   Aragón, JL
   Anglada, M
   de Lucas, E
   Parcerisa, JM
   González, A
AF Corbalan-Navarro, David
   Aragon, Juan L.
   Anglada, Marti
   de Lucas, Enrique
   Parcerisa, Joan-Manuel
   Gonzalez, Antonio
TI Omega-Test: A Predictive Early-Z Culling to Improve the Graphics
   Pipeline Energy-Efficiency
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Rendering (computer graphics); Pipelines; Graphics processing units;
   Graphics; Computer architecture; Geometry; Games; Graphics processors;
   mobile processors; portable devices; hardware architecture; processor
   architecture; energy-aware systems; low-power design; hidden line;
   surface removal; visibility determination
AB The most common task of GPUs is to render images in real time. When rendering a 3D scene, a key step is to determine which parts of every object are visible in the final image. There are different approaches to solve the visibility problem, the Z-Test being the most common. A main factor that significantly penalizes the energy efficiency of a GPU, especially in the mobile arena, is the so-called overdraw, which happens when a portion of an object is shaded and rendered but finally occluded by another object. This useless work results in a waste of energy; however, a conventional Z-Test only avoids a fraction of it. In this article we present a novel microarchitectural technique, the Omega-Test, to drastically reduce the overdraw on a Tile-Based Rendering (TBR) architecture. Graphics applications have a great degree of inter-frame coherence, which makes the output of a frame very similar to the previous one. The proposed approach leverages the frame-to-frame coherence by using the resulting information of the Z-Test for a tile (a buffer containing all the calculated pixel depths for a tile), which is discarded by nowadays GPUs, to predict the visibility of the same tile in the next frame. As a result, the Omega-Test early identifies occluded parts of the scene and avoids the rendering of non-visible surfaces eliminating costly computations and off-chip memory accesses. Our experimental evaluation shows average EDP savings in the overall GPU/Memory system of 26.4 percent and an average speedup of 16.3 percent for the evaluated benchmarks.
C1 [Corbalan-Navarro, David; Aragon, Juan L.] Univ Murcia, Dept Ingn & Tecnol Comp, E-30100 Murcia, Spain.
   [Anglada, Marti; Parcerisa, Joan-Manuel; Gonzalez, Antonio] Univ Politecn Cataluna, Dept Arquitectura Comp, Barcelona 08034, Spain.
   [de Lucas, Enrique] Imaginat Technol, Imaginat House, Kings Langley WD4 8LZ, England.
C3 University of Murcia; Universitat Politecnica de Catalunya
RP Corbalán-Navarro, D (corresponding author), Univ Murcia, Dept Ingn & Tecnol Comp, E-30100 Murcia, Spain.
EM dcorbalan@ditec.um.es; jlaragon@ditec.um.es; manglada@ac.upc.edu;
   enrique.delucas@imgtec.com; jmanel@ac.upc.edu; antonio@ac.upc.edu
RI Aragón, Juan/ABB-5489-2020; Gonzalez, Antonio/I-2961-2014; Aragon, Juan
   Luis/E-1340-2015
OI Corbalan-Navarro, David/0000-0002-7079-6687; Gonzalez,
   Antonio/0000-0002-0009-0996; Anglada, Marti/0000-0002-1204-1841; Aragon,
   Juan Luis/0000-0002-4955-7235; Parcerisa,
   Joan-Manuel/0000-0001-5771-8118
FU CoCoUnit ERC Advanced Grant through EU's Horizon 2020 Program [833057];
   Spanish State Research Agency [TIN2016-75344-R]; ICREA Academia Program;
   University of Murcia
FX This work was supported in part by the CoCoUnit ERC Advanced Grant
   through EU's Horizon 2020 Program under Grant 833057, in part by the
   Spanish State Research Agency under Grant TIN2016-75344-R (AEI/FEDER,
   EU), and in part by the ICREA Academia Program. The work of D.
   Corbalan-Navarro was supported by a PhD Research Fellowship from the
   University of Murcia.
NR 30
TC 2
Z9 2
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4375
EP 4388
DI 10.1109/TVCG.2021.3087863
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400030
PM 34106856
OA Green Published
DA 2025-03-07
ER

PT J
AU Guo, Y
   Guo, SN
   Jin, ZC
   Kaul, S
   Gotz, D
   Cao, N
AF Guo, Yi
   Guo, Shunan
   Jin, Zhuochen
   Kaul, Smiti
   Gotz, David
   Cao, Nan
TI Survey on Visual Analysis of Event Sequence Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visual analytics; Task analysis; Data mining;
   Sequences; Pipelines; Medical diagnostic imaging; Visual analysis; event
   sequences; visualization
ID MEDICAL-RECORDS; COHORT ANALYSIS; ANALYTICS; VISUALIZATION; EXPLORATION;
   PATTERNS; OPPORTUNITIES; DIFFUSION; AWARENESS; TIME
AB Event sequence data record series of discrete events in the time order of occurrence. They are commonly observed in a variety of applications ranging from electronic health records to network logs, with the characteristics of large-scale, high-dimensional and heterogeneous. This high complexity of event sequence data makes it difficult for analysts to manually explore and find patterns, resulting in ever-increasing needs for computational and perceptual aids from visual analytics techniques to extract and communicate insights from event sequence datasets. In this paper, we review the state-of-the-art visual analytics approaches, characterize them with our proposed design space, and categorize them based on analytical tasks and applications. From our review of relevant literature, we have also identified several remaining research challenges and future research opportunities.
C1 [Guo, Yi; Guo, Shunan; Jin, Zhuochen; Cao, Nan] Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai 200092, Peoples R China.
   [Kaul, Smiti; Gotz, David] Univ N Carolina, Visual Anal & Commun Lab, Chapel Hill, NC 27599 USA.
C3 Tongji University; University of North Carolina; University of North
   Carolina Chapel Hill
RP Cao, N (corresponding author), Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai 200092, Peoples R China.
EM dennis.guo.china@gmail.com; g.shunan@gmail.com; chjzcjames@gmail.com;
   smiti@unc.edu; gotz@unc.edu; nan.cao@gmail.com
RI Guo, Shunan/AAE-2616-2019; Cao, Nan/O-5397-2014
OI Gotz, David/0000-0002-6424-7374; Cao, Nan/0000-0003-1316-7515; Guo,
   Shunan/0000-0001-5355-8399
FU NSFC [62061136003]
FX This work was supported in part by the NSFC under Grant 62061136003. The
   authors would like to thank the reviewers, Prof. Catherine Plaisant,
   Prof. Ben Shneiderman, Prof. Daniel Weiskopf, and many other readers for
   their valuable feedback and suggestions.
NR 141
TC 59
Z9 60
U1 11
U2 43
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 5091
EP 5112
DI 10.1109/TVCG.2021.3100413
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400079
PM 34314358
DA 2025-03-07
ER

PT J
AU Shen, IC
   Chen, BY
AF Shen, I-Chao
   Chen, Bing-Yu
TI ClipGen: A Deep Generative Model for Clipart Vectorization and Synthesis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Three-dimensional displays; Solid modeling; Geometry; Graphics;
   Task analysis; Computational modeling; ClipGen; clipnet; clipart; vector
   graphics; deep learning; deep generative model
AB This article presents a novel deep learning-based approach for automatically vectorizing and synthesizing the clipart of man-made objects. Given a raster clipart image and its corresponding object category (e.g., airplanes), the proposed method sequentially generates new layers, each of which is composed of a new closed path filled with a single color. The final result is obtained by compositing all layers together into a vector clipart image that falls into the target category. The proposed approach is based on an iterative generative model that (i) decides whether to continue synthesizing a new layer and (ii) determines the geometry and appearance of the new layer. We formulated a joint loss function for training our generative model, including the shape similarity, symmetry, and local curve smoothness losses, as well as vector graphics rendering accuracy loss for synthesizing clipart recognizable by humans. We also introduced a collection of man-made object clipart, ClipNet, which is composed of closed-path layers, and two designed preprocessing tasks to clean up and enrich the original raw clipart. To validate the proposed approach, we conducted several experiments and demonstrated its ability to vectorize and synthesize various clipart categories. We envision that our generative model can facilitate efficient and intuitive clipart designs for novice users and graphic designers.
C1 [Shen, I-Chao; Chen, Bing-Yu] Natl Taiwan Univ, Taipei 10617, Taiwan.
C3 National Taiwan University
RP Shen, IC (corresponding author), Natl Taiwan Univ, Taipei 10617, Taiwan.
EM jdilyshen@gmail.com; robin@ntu.edu.tw
RI Shen, Yuecheng/AFJ-9304-2022; Shen, I-Chao/AHA-3605-2022; Chen,
   Bing-Yu/E-7498-2016
OI Chen, Bing-Yu/0000-0003-0169-7682
FU Ministry of Science and Technology, Taiwan [MOST109-2218-E-002-030,
   109-2634-F-002-032]; National Taiwan University; MediaTek Fellowship;
   Grants-in-Aid for Scientific Research [21F20075] Funding Source: KAKEN
FX The authors would like to thank the National Center for High-Performance
   Computing. The authors would also like to thank Tzu-mao Li, Sheng-Jie
   Luo, Yu-Ting Wu, Chi-Lan Yang, and anonymous reviewers for insightful
   suggestions and discussion. This work was supported in part by the
   Ministry of Science and Technology, Taiwan, under Grants
   MOST109-2218-E-002-030 and 109-2634-F-002-032, and in part by National
   Taiwan University. The work of I-Chao Shen was supported by the MediaTek
   Fellowship.
NR 76
TC 8
Z9 8
U1 3
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4211
EP 4224
DI 10.1109/TVCG.2021.3084944
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400019
PM 34057894
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kourtesis, P
   Vizcay, S
   Marchal, M
   Pacchierotti, C
   Argelaguet, F
AF Kourtesis, Panagiotis
   Vizcay, Sebastian
   Marchal, Maud
   Pacchierotti, Claudio
   Argelaguet, Ferran
TI Action-Specific Perception & Performance on a Fitts's Law Task in
   Virtual Reality: The Role of Haptic Feedback
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 21st IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 17-22, 2022
CL Singapore, SINGAPORE
SP IEEE
DE Spatial perception; time perception; accuracy; reaction time;
   electrotactile; vibrotactile; Fitts Law
ID OWNERSHIP; DISPLAYS; BEHAVIOR; SIZE
AB While user's perception and performance are predominantly examined independently in virtual reality, the Action-Specific Perception (ASP) theory postulates that the performance of an individual on a task modulates this individual's spatial and time perception pertinent to the task's components and procedures. This paper examines the association between performance and perception and the potential effects that tactile feedback modalities could generate. This paper reports a user study (N=24), in which participants performed a standardized Fitts's law target acquisition task by using three feedback modalities: visual, visuo-electrotactile, and visuo-vibrotactile. The users completed 3 Target Sizes x 2 Distances x 3 feedback modalities = 18 trials. The size perception, distance perception, and (movement) time perception were assessed at the end of each trial. Performance-wise, the results showed that electrotactile feedback facilitates a significantly better accuracy compared to vibrotactile and visual feedback, while vibrotactile provided the worst accuracy. Electrotactile and visual feedback enabled a comparable reaction time, while the vibrotactile offered a substantially slower reaction time than visual feedback. Although amongst feedback types the pattern of differences in perceptual aspects were comparable to performance differences, none of them was statistically significant. However, performance indeed modulated perception. Significant action-specific effects on spatial and time perception were detected. Changes in accuracy modulate both size perception and time perception, while changes in movement speed modulate distance perception. Also, the index of difficulty was found to modulate all three perceptual aspects. However, individual differences appear to affect the magnitude of action-specific effects. These outcomes highlighted the importance of haptic feedback on performance, and importantly the significance of action-specific effects on spatial and time perception in VR, which should be considered in future VR studies.
C1 [Kourtesis, Panagiotis; Vizcay, Sebastian; Pacchierotti, Claudio; Argelaguet, Ferran] Univ Rennes, INRIA, IRISA, CNRS, F-35042 Rennes, France.
   [Marchal, Maud] Univ Rennes, INSA, IRISA, INRIA,CNRS, F-35042 Rennes, France.
   [Marchal, Maud] Inst Univ France, Paris, France.
C3 Universite de Rennes; Centre National de la Recherche Scientifique
   (CNRS); Inria; Centre National de la Recherche Scientifique (CNRS);
   Universite de Rennes; Inria; Institut Universitaire de France
RP Kourtesis, P (corresponding author), Univ Rennes, INRIA, IRISA, CNRS, F-35042 Rennes, France.
EM panagiotis.kourtesis@inria.fr; sebastian.vizcay@inria.fr;
   ferran.argelaguet@inria.fr
RI Kourtesis, Panagiotis/ABA-9356-2020; Pacchierotti, Claudio/G-7304-2011
OI Kourtesis, Panagiotis/0000-0002-2914-1064; Vizcay,
   Sebastian/0000-0002-1837-5607; Pacchierotti, Claudio/0000-0002-8006-9168
FU European Union [856718]
FX This work was supported by the European Union's Horizon 2020 research
   and innovation program under grant agreement No. 856718 (TACTILITY).
NR 78
TC 10
Z9 10
U1 3
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2022
VL 28
IS 11
BP 3715
EP 3726
DI 10.1109/TVCG.2022.3203003
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 5M3ZS
UT WOS:000871038200016
PM 36048989
OA Green Published, Green Submitted
DA 2025-03-07
ER

PT J
AU Chen, YX
   Li, W
   Fan, R
   Liu, XP
AF Chen, Yixin
   Li, Wei
   Fan, Rui
   Liu, Xiaopei
TI GPU Optimization for High-Quality Kinetic Fluid Simulation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Kinetic theory; Mathematical model; Graphics processing units;
   Computational modeling; Solids; Optimization; Adaptation models; GPU
   optimization; parallel computing; fluid simulation; lattice Boltzmann
   method; immersed boundary method
ID LATTICE BOLTZMANN METHOD; NAVIER-STOKES SOLVER; IMMERSED-BOUNDARY;
   IMPLEMENTATION; ALGORITHM; EFFICIENT
AB Fluid simulations are often performed using the incompressible Navier-Stokes equations (INSE), leading to sparse linear systems which are difficult to solve efficiently in parallel. Recently, kinetic methods based on the adaptive-central-moment multiple-relaxation-time (ACM-MRT) model [1], [2] have demonstrated impressive capabilities to simulate both laminar and turbulent flows, with quality matching or surpassing that of state-of-the-art INSE solvers. Furthermore, due to its local formulation, this method presents the opportunity for highly scalable implementations on parallel systems such as GPUs. However, an efficient ACM-MRT-based kinetic solver needs to overcome a number of computational challenges, especially when dealing with complex solids inside the fluid domain. In this article, we present multiple novel GPU optimization techniques to efficiently implement high-quality ACM-MRT-based kinetic fluid simulations in domains containing complex solids. Our techniques include a new communication-efficient data layout, a load-balanced immersed-boundary method, a multi-kernel launch method using a simplified formulation of ACM-MRT calculations to enable greater parallelism, and the integration of these techniques into a parametric cost model to enable automated prameter search to achieve optimal execution performance. We also extended our method to multi-GPU systems to enable large-scale simulations. To demonstrate the state-of-the-art performance and high visual quality of our solver, we present extensive experimental results and comparisons to other solvers.
C1 [Chen, Yixin; Li, Wei; Fan, Rui; Liu, Xiaopei] ShanghaiTech Univ, Shanghai Engn Res Ctr Intelligent Vis & Imaging, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China.
C3 ShanghaiTech University
RP Liu, XP (corresponding author), ShanghaiTech Univ, Shanghai Engn Res Ctr Intelligent Vis & Imaging, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China.
EM chenyx2@shanghaitech.edu.cn; liwei@shanghaitech.edu.cn;
   fanrui@shanghaitech.edu.cn; liuxp@shanghaitech.edu.cn
OI Chen, Yixin/0000-0001-7547-9587
FU ShanghaiTech University; National Natural Science Foundation of China
   [61976138]
FX The authors would like to thank all the reviewers for their constructive
   comments. They also thank Yihui Ma, Chenqi Luo, and Chaoyang Lyu from
   the FLARE Lab at ShanghaiTech University for helping with video
   preparations. They are grateful to YOKE Intelligence for providing the
   3D reconstruction in Fig. 1 for our large scale simulation. This work
   was supported by the startup fund of ShanghaiTech University and the
   National Natural Science Foundation of China under Grant 61976138.
NR 105
TC 12
Z9 15
U1 1
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2022
VL 28
IS 9
BP 3235
EP 3251
DI 10.1109/TVCG.2021.3059753
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3K0HP
UT WOS:000833767700013
PM 33591918
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Morrical, N
   Wald, I
   Usher, W
   Pascucci, V
AF Morrical, Nate
   Wald, Ingo
   Usher, Will
   Pascucci, Valerio
TI Accelerating Unstructured Mesh Point Location With RT Cores
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Scientific ray tracing; unstructured scalar data; GPGPU; simulation;
   volume rendering
ID VOLUME; VISUALIZATION; SIMULATION; TREES
AB We present a technique that leverages ray tracing hardware available in recent Nvidia RTX GPUs to solve a problem other than classical ray tracing. Specifically, we demonstrate how to use these units to accelerate the point location of general unstructured elements consisting of both planar and bilinear faces. This unstructured mesh point location problem has previously been challenging to accelerate on GPU architectures; yet, the performance of these queries is crucial to many unstructured volume rendering and compute applications. Starting with a CUDA reference method, we describe and evaluate three approaches that reformulate these point queries to incrementally map algorithmic complexity to these new hardware ray tracing units. Each variant replaces the simpler problem of point queries with a more complex one of ray queries. Initial variants exploit ray tracing cores for accelerated BVH traversal, and subsequent variants use ray-triangle intersections and per-face metadata to detect point-in-element intersections. Although these later variants are more algorithmically complex, they are significantly faster than the reference method thanks to hardware acceleration. Using our approach, we improve the performance of an unstructured volume renderer by up to 4x for tetrahedral meshes and up to 15x for general bilinear element meshes, matching, or out-performing state-of-the-art solutions while simultaneously improving on robustness and ease-of-implementation.
C1 [Morrical, Nate; Usher, Will; Pascucci, Valerio] Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.
   [Wald, Ingo] NVIDIA, Santa Clara, CA 95051 USA.
C3 Utah System of Higher Education; University of Utah; Nvidia Corporation
RP Morrical, N (corresponding author), Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.
EM natemorrical@gmail.com; ingowald@gmail.com; will@sci.utah.edu;
   pascucci@sci.utah.edu
RI pascucci, Valerio/GXF-0616-2022
OI Morrical, Nathan/0000-0002-2262-6974; Usher, Will/0000-0001-5008-8280;
   pascucci, valerio/0000-0002-8877-2042
FU NSF [1314896, 1602127, 1649923, 1842042]; DOE [DE-SC0007446,
   DE-NA0002375]; U.S. Department of Energy (DOE) [DE-SC0007446] Funding
   Source: U.S. Department of Energy (DOE); Direct For Computer & Info Scie
   & Enginr; Office of Advanced Cyberinfrastructure (OAC) [1649923,
   1842042] Funding Source: National Science Foundation; Div Of Industrial
   Innovation & Partnersh; Directorate For Engineering [1602127] Funding
   Source: National Science Foundation; Div Of Information & Intelligent
   Systems; Direct For Computer & Info Scie & Enginr [1314896] Funding
   Source: National Science Foundation
FX The Agulhas data set is courtesy of Dr. Niklas Rober (DKRZ); the Japan
   Earthquake data set is courtesy of Carsten Burstedde, Omar Ghattas,
   James R. Martin, Georg Stadler, and Lucas C. Wilcox (ICES, the
   University of Texas at Austin) and Paul Navratil and Greg Abram (TACC).
   Hardware for development and testing was graciously provided by Nvidia
   Corp. This work was supported in part by NSF: CGV Award: 1314896,
   NSF:IIP Award: 1602127, NSF:ACI Award: 1649923, DOE/SciDAC DESC0007446,
   CCMSC DE-NA0002375, and NSF:OAC Award: 1842042.
NR 43
TC 8
Z9 10
U1 2
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2022
VL 28
IS 8
BP 2852
EP 2866
DI 10.1109/TVCG.2020.3042930
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2P6BI
UT WOS:000819823600004
PM 33290224
OA Bronze
DA 2025-03-07
ER

PT J
AU Liu, ZP
   Wang, Y
   Bernard, J
   Munzner, T
AF Liu, Zipeng
   Wang, Yang
   Bernard, Juergen
   Munzner, Tamara
TI Visualizing Graph Neural Networks With CorGIE: Corresponding a Graph to
   Its Embedding
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization for machine learning; graph neural network; graph layout
ID OF-THE-ART; LAYOUT
AB Graph neural networks (GNNs) are a class of powerful machine learning tools that model node relations for making predictions of nodes or links. GNN developers rely on quantitative metrics of the predictions to evaluate a GNN, but similar to many other neural networks, it is difficult for them to understand if the GNN truly learns characteristics of a graph as expected. We propose an approach to corresponding an input graph to its node embedding (aka latent space), a common component of GNNs that is later used for prediction. We abstract the data and tasks, and develop an interactive multi-view interface called CorGIE to instantiate the abstraction. As the key function in CorGIE, we propose the K-hop graph layout to show topological neighbors in hops and their clustering structure. To evaluate the functionality and usability of CorGIE, we present how to use CorGIE in two usage scenarios, and conduct a case study with five GNN experts.
C1 [Liu, Zipeng] Beihang Univ, Coll Software, Beijing 100190, Peoples R China.
   [Wang, Yang] Facebook Inc, Menlo Pk, CA 94025 USA.
   [Bernard, Juergen] Univ Zurich, Dept Comp Sci, CH-8006 Zurich, Switzerland.
   [Munzner, Tamara] Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1Z4, Canada.
C3 Beihang University; Facebook Inc; University of Zurich; University of
   British Columbia
RP Liu, ZP (corresponding author), Beihang Univ, Coll Software, Beijing 100190, Peoples R China.
EM zipeng@buaa.edu.cn; yvv@fb.com; bernard@ifi.uzh.ch; tmm@cs.ubc.ca
RI Munzner, Tamara/HKP-2536-2023; Bernard, Jürgen/AAK-5732-2021
NR 47
TC 9
Z9 9
U1 0
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2022
VL 28
IS 6
BP 2500
EP 2516
DI 10.1109/TVCG.2022.3148197
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0Z1CH
UT WOS:000790817100019
PM 35120005
OA Green Submitted, Bronze
DA 2025-03-07
ER

PT J
AU Arefin, MS
   Phillips, N
   Plopski, A
   Gabbard, JL
   Swan, JE 
AF Arefin, Mohammed Safayet
   Phillips, Nate
   Plopski, Alexander
   Gabbard, Joseph L.
   Swan, J. Edward, II
TI The Effect of Context Switching, Focal Switching Distance, Binocular and
   Monocular Viewing, and Transient Focal Blur on Human Performance in
   Optical See-Through Augmented Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Optical switches; Task analysis; Monitoring; Transient analysis;
   Fatigue; Augmented reality; Meters; Augmented reality; context
   switching; focal distance switching; transient focal blur; accommodation
ID VISUAL FATIGUE; ACCOMMODATION; DISPLAYS; WORK; TEXT
AB In optical see-through augmented reality (AR), information is often distributed between real and virtual contexts, and often appears at different distances from the user. To integrate information, users must repeatedly switch context and change focal distance. If the user's task is conducted under time pressure, they may attempt to integrate information while their eye is still changing focal distance, a phenomenon we term transient focal blur. Previously, Gabbard, Mehra, and Swan (2018) examined these issues, using a text-based visual search task on a one-eye optical see-through AR display. This paper reports an experiment that partially replicates and extends this task on a custom-built AR Haploscope. The experiment examined the effects of context switching, focal switching distance, binocular and monocular viewing, and transient focal blur on task performance and eye fatigue. Context switching increased eye fatigue but did not decrease performance. Increasing focal switching distance increased eye fatigue and decreased performance. Monocular viewing also increased eye fatigue and decreased performance. The transient focal blur effect resulted in additional performance decrements, and is an addition to knowledge about AR user interface design issues.
C1 [Arefin, Mohammed Safayet; Phillips, Nate; Swan, J. Edward, II] Mississippi State Univ, Mississippi State, MS 39762 USA.
   [Plopski, Alexander] Univ Otago, Dunedin, New Zealand.
   [Gabbard, Joseph L.] Virginia Tech, Blacksburg, VA USA.
C3 Mississippi State University; University of Otago; Virginia Polytechnic
   Institute & State University
RP Arefin, MS (corresponding author), Mississippi State Univ, Mississippi State, MS 39762 USA.
EM arefin@acm.org; Nathaniel.C.Phillips@ieee.org;
   alexander.plopski@otago.ac.nz; jgabbard@vt.edu
RI Arefin, Mohammed Safayet/ABF-8521-2020; Phillips, Nate/GPT-4922-2022
OI Phillips, Nate/0000-0001-9085-383X
FU National Science Foundation [IIS-1320909, IIS-1937565]
FX This material is based upon work supported by the National Science
   Foundation, under awards IIS-1320909 and IIS-1937565, to J. Edward Swan
   II. This work was conducted at the Center for Advanced Vehicular
   Systems, at Mississippi State University.
NR 54
TC 18
Z9 20
U1 3
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2022
VL 28
IS 5
BP 2014
EP 2025
DI 10.1109/TVCG.2022.3150503
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 0M8RI
UT WOS:000782415400002
PM 35167470
OA Bronze, Green Accepted
DA 2025-03-07
ER

PT J
AU Xiao, Y
   Wu, J
   Zhang, J
   Zhou, PY
   Zheng, Y
   Leung, CS
   Kavan, L
AF Xiao, Yi
   Wu, Jin
   Zhang, Jie
   Zhou, Peiyao
   Zheng, Yan
   Leung, Chi-Sing
   Kavan, Ladislav
TI Interactive Deep Colorization and its Application for Image Compression
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Deep convolutional neural network; interactive; residual learning; image
   colorization; image compression
ID COLOR; FEATURES
AB Recent methods based on deep learning have shown promise in converting grayscale images to colored ones. However, most of them only allow limited user inputs (no inputs, only global inputs, or only local inputs), to control the output colorful images. The possible difficulty lies in how to differentiate the influences of different inputs. To solve this problem, we propose a two-stage deep colorization method allowing users to control the results by flexibly setting global inputs and local inputs. The key steps include enabling color themes as global inputs by extracting K mean colors and generating K-color maps to define a global theme loss, and designing a loss function to differentiate the influences of different inputs without causing artifacts. We also propose a color theme recommendation method to help users choose color themes. Based on the colorization model, we further propose an image compression scheme, which supports variable compression ratios in a single network. Experiments on colorization show that our method can flexibly control the colorized results with only a few inputs and generate state-of-the-art results. Experiments on compression show that our method achieves much higher image quality at the same compression ratio when compared to the state-of-the-art methods.
C1 [Xiao, Yi] Hunan Univ, Sch Design, Changsha 410082, Hunan, Peoples R China.
   [Wu, Jin; Zhang, Jie; Zhou, Peiyao] Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
   [Zheng, Yan] Hunan Univ, Coll Elect & Informat Engn, Changsha 410082, Hunan, Peoples R China.
   [Leung, Chi-Sing] City Univ Hong Kong, Kowloon Tong, Hong Kong, Peoples R China.
   [Kavan, Ladislav] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
C3 Hunan University; Hunan University; Hunan University; City University of
   Hong Kong; Utah System of Higher Education; University of Utah
RP Zhang, J (corresponding author), Hunan Univ, Coll Comp Sci & Elect Engn, Changsha 410082, Hunan, Peoples R China.
EM yixiao1984@gmail.com; as850785314@qq.com; jie_zhang@hnu.edu.cn;
   dizhuyouxi@163.com; yanzheng@hnu.edu.cn; eeleungc@cityu.edu.hk;
   ladislav.kavan@gmail.com
RI Zhang, Jie/LWK-7292-2024
FU National Key R&D Program of China [2018YFB0203904]; NSFC from PRC
   [61872137, 61803150]; Hunan NSF [2020JJ4009,2018JJ3067]; China
   Scholarship Council [201806135087]
FX This work was supported by the National Key R&D Program of China
   (2018YFB0203904), NSFC from PRC (61872137, 61803150), Hunan NSF
   (2020JJ4009,2018JJ3067) and China Scholarship Council (201806135087).
NR 68
TC 4
Z9 4
U1 0
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2022
VL 28
IS 3
BP 1557
EP 1572
DI 10.1109/TVCG.2020.3021510
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP1EJ
UT WOS:000748371200008
PM 32881687
DA 2025-03-07
ER

PT J
AU Feng, TH
   Yang, J
   Eppes, MC
   Yang, ZC
   Moser, F
AF Feng, Tinghao
   Yang, Jing
   Eppes, Martha-Cary
   Yang, Zhaocong
   Moser, Faye
TI EVis: Visually Analyzing Environmentally Driven Events
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Time series analysis; Rocks; Earth; Visual analytics; Trajectory;
   Heating systems; Data visualization; Multivariate Time Series; RadViz;
   Event Data; Visual Analytics; Earth Sciences
ID TIME-SERIES DATA
AB Earth scientists are increasingly employing time series data with multiple dimensions and high temporal resolution to study the impacts of climate and environmental changes on Earth's atmosphere, biosphere, hydrosphere, and lithosphere. However, the large number of variables and varying time scales of antecedent conditions contributing to natural phenomena hinder scientists from completing more than the most basic analyses. In this paper, we present EVis (Environmental Visualization), a new visual analytics prototype to help scientists analyze and explore recurring environmental events (e.g. rock fracture, landslides, heat waves, floods) and their relationships with high dimensional time series of continuous numeric environmental variables, such as ambient temperature and precipitation. EVis provides coordinated scatterplots, heatmaps, histograms, and RadViz for foundational analyses. These features allow users to interactively examine relationships between events and one, two, three, or more environmental variables. EVis also provides a novel visual analytics approach to allowing users to discover temporally lagging relationships related to antecedent conditions between events and multiple variables, a critical task in Earth sciences. In particular, this latter approach projects multivariate time series onto trajectories in a 2D space using RadViz, and clusters the trajectories for temporal pattern discovery. Our case studies with rock cracking data and interviews with domain experts from a range of sub-disciplines within Earth sciences illustrate the extensive applicability and usefulness of EVis.
C1 [Feng, Tinghao; Yang, Jing; Yang, Zhaocong] UNC Charlotte, Dept Comp Sci, Charlotte, NC USA.
   [Eppes, Martha-Cary; Moser, Faye] UNC Charlotte, Dept Geog & Earth Sci, Charlotte, NC 28223 USA.
C3 University of North Carolina; University of North Carolina Charlotte;
   University of North Carolina; University of North Carolina Charlotte
RP Eppes, MC (corresponding author), UNC Charlotte, Dept Geog & Earth Sci, Charlotte, NC 28223 USA.
EM fvisco@uncc.edu; zyang19@uncc.edu
RI Feng, Tinghao/JWO-1400-2024; Eppes, Martha/GRF-3840-2022
OI Moser, Faye/0000-0002-8235-5117; Feng, Tinghao/0000-0003-2765-2765
FU National Science Foundation [00844335, 844401, 0805277]; University of
   North Carolina at Charlotte; Division Of Materials Research; Direct For
   Mathematical & Physical Scien [0805277] Funding Source: National Science
   Foundation; Marie Curie Actions (MSCA) [844401] Funding Source: Marie
   Curie Actions (MSCA)
FX We would like to thank the domain experts of sub-disciplines for their
   expert feedback. This publication is based upon work supported by the
   National Science Foundation under Grant Nos. EAR #00844335, #844401, and
   #0805277, and the University of North Carolina at Charlotte.
NR 46
TC 2
Z9 2
U1 2
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 912
EP 921
DI 10.1109/TVCG.2021.3114867
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XW3DW
UT WOS:000735505300011
PM 34587084
OA Bronze
DA 2025-03-07
ER

PT J
AU Horak, T
   Coenen, N
   Metzger, N
   Hahn, C
   Flemisch, T
   Mendez, J
   Dimov, D
   Finkbeiner, B
   Dachselt, R
AF Horak, Tom
   Coenen, Norine
   Metzger, Niklas
   Hahn, Christopher
   Flemisch, Tamara
   Mendez, Julian
   Dimov, Dennis
   Finkbeiner, Bernd
   Dachselt, Raimund
TI Visual Analysis of Hyperproperties for Understanding Model Checking
   Results
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Model checking; Visualization; Integrated circuit modeling; Tools;
   Computational modeling; Encoding; Process control; Analyzing
   Counterexamples; Hyperproperties; Multiple Coordinate Views; Explainable
   Formal Methods
ID FORMAL VERIFICATION; COUNTEREXAMPLES
AB Model checkers provide algorithms for proving that a mathematical model of a system satisfies a given specification. In case of a violation, a counterexample that shows the erroneous behavior is returned. Understanding these counterexamples is challenging, especially for hyperproperty specifications, i.e., specifications that relate multiple executions of a system to each other. We aim to facilitate the visual analysis of such counterexamples through our HyperVis tool, which provides interactive visualizations of the given model, specification, and counterexample. Within an iterative and interdisciplinary design process, we developed visualization solutions that can effectively communicate the core aspects of the model checking result. Specifically, we introduce graphical representations of binary values for improving pattern recognition, color encoding for better indicating related aspects, visually enhanced textual descriptions, as well as extensive cross-view highlighting mechanisms. Further, through an underlying causal analysis of the counterexample, we are also able to identify values that contributed to the violation and use this knowledge for both improved encoding and highlighting. Finally, the analyst can modify both the specification of the hyperproperty and the system directly within HyperVis and initiate the model checking of the new version. In combination, these features notably support the analyst in understanding the error leading to the counterexample as well as iterating the provided system and specification. We ran multiple case studies with HyperVis and tested it with domain experts in qualitative feedback sessions. The participants' positive feedback confirms the considerable improvement over the manual, text-based status quo and the value of the tool for explaining hyperproperties.
C1 [Horak, Tom; Flemisch, Tamara; Mendez, Julian; Dimov, Dennis] Tech Univ Dresden, Interact Media Lab, Dresden, Germany.
   [Dachselt, Raimund] Tech Univ Dresden, Interact Media Lab, Ctr Tactile Internet CeTI, Dresden, Germany.
   [Dachselt, Raimund] Tech Univ Dresden, Cluster Excellence Phys Life PoL, Dresden, Germany.
   [Coenen, Norine; Metzger, Niklas; Hahn, Christopher; Finkbeiner, Bernd] CISPA Helmholtz Ctr Informat Secur, React Syst Gmup, Saarbrucken, Germany.
C3 Technische Universitat Dresden; Technische Universitat Dresden;
   Technische Universitat Dresden
RP Horak, T (corresponding author), Tech Univ Dresden, Interact Media Lab, Dresden, Germany.
EM horakt@acm.org; norine.coenen@cispa.de; niklas.metzger@cispa.de;
   christopher.hahn@cispa.de; tamara.flemisch@tu-dresden.de;
   julian_jesus.mendez_oconitrillo@mailbox.tu-dresden.de;
   dennis.dimov@tu-dresden.de; finkbeiner@cispa.de; dachselt@acm.org
RI HAHN, CHRISTOPHER/D-8427-2018; Coenen, Norine/JHU-3150-2023; Dachselt,
   Raimund/B-2860-2017
OI Dachselt, Raimund/0000-0002-2176-876X; Mendez,
   Julian/0000-0003-1029-7656
FU DFG [EXC 2050/1, 390696704, 389792660, TRR 248 -CPE]; European Research
   Council (ERC) Grant OSARES [683300]; German Israeli Foundation (GIF)
   [I-1513-407]
FX We thank Weizhou Luo for his valuable support during the overall project
   duration. This work was funded by DFG grant 389792660 as part of TRR 248
   -CPEC, by the DFG as part of the Germany's Excellence Strategy EXC
   2050/1 -Project ID 390696704 -Cluster of Excellence "Centre for Tactile
   Internet" (CeTI) of TU Dresden, by the European Research Council (ERC)
   Grant OSARES (No. 683300), and by the German Israeli Foundation (GIF)
   Grant No. I-1513-407./2019.
NR 88
TC 7
Z9 8
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 357
EP 367
DI 10.1109/TVCG.2021.3114866
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000047
PM 34587083
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ibrahim, M
   Rautek, P
   Reina, G
   Agus, M
   Hadwiger, M
AF Ibrahim, Mohamed
   Rautek, Peter
   Reina, Guido
   Agus, Marco
   Hadwiger, Markus
TI Probabilistic Occlusion Culling using Confidence Maps for High-Quality
   Rendering of Large Particle Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Probabilistic logic; Rendering (computer graphics); Data visualization;
   Graphics processing units; Costs; Standards; Density functional theory;
   Large-scale particle data; sub-pixel occlusion culling; super-sampling;
   anti-aliasing; coverage; probabilistic methods
ID VISIBILITY; FRAMEWORK; VISUALIZATION
AB Achieving high rendering quality in the visualization of large particle data, for example from large-scale molecular dynamics simulations, requires a significant amount of sub-pixel super-sampling, due to very high numbers of particles per pixel. Although it is impossible to super-sample all particles of large-scale data at interactive rates, efficient occlusion culling can decouple the overall data size from a high effective sampling rate of visible particles. However, while the latter is essential for domain scientists to be able to see important data features, performing occlusion culling by sampling or sorting the data is usually slow or error-prone due to visibility estimates of insufficient quality. We present a novel probabilistic culling architecture for super-sampled high-quality rendering of large particle data. Occlusion is dynamically determined at the sub-pixel level, without explicit visibility sorting or data simplification. We introduce confidence maps to probabilistically estimate confidence in the visibility data gathered so far. This enables progressive, confidence-based culling, helping to avoid wrong visibility decisions. In this way, we determine particle visibility with high accuracy, although only a small part of the data set is sampled. This enables extensive super-sampling of (partially) visible particles for high rendering quality, at a fraction of the cost of sampling all particles. For real-time performance with millions of particles, we exploit novel features of recent GPU architectures to group particles into two hierarchy levels, combining fine-grained culling with high frame rates.
C1 [Ibrahim, Mohamed; Rautek, Peter; Hadwiger, Markus] King Abdullah Univ Sci & Technol KAUST, Visual Comp Ctr, Thuwal 239556900, Saudi Arabia.
   [Reina, Guido] Univ Stuttgart, Visualizat Res Ctr VISUS, Stuttgart, Germany.
   [Agus, Marco] Hamad Bin Khalifa Univ, Qatar Fdn, Coll Sci & Engn, Doha, Qatar.
C3 King Abdullah University of Science & Technology; University of
   Stuttgart; Qatar Foundation (QF); Hamad Bin Khalifa University-Qatar
RP Ibrahim, M (corresponding author), King Abdullah Univ Sci & Technol KAUST, Visual Comp Ctr, Thuwal 239556900, Saudi Arabia.
EM mohamed.ibrahim@kaust.edu.sa; peter.rautek@kaust.edu.sa;
   guido.reina@visus.uni-stuttgart.de; magus@hbku.edu.qa;
   markus.hadwiger@kaust.edu.sa
RI Agus, Marco/AAM-5898-2020
OI Rautek, Peter/0000-0003-4821-7404; Ibrahim, Mohamed/0000-0002-3559-3761;
   Reina, Guido/0000-0003-4127-1897; Hadwiger, Markus/0000-0003-1239-4871
FU Intel(R) Corporation via the Intel(R) Graphics and Visualization
   Institutes of XeLLENCE program [35512501]; King Abdullah University of
   Science and Technology (KAUST)
FX This work was partially supported by Intel (R) Corporation via the Intel
   (R) Graphics and Visualization Institutes of XeLLENCE program (CG
   #35512501). This work was supported by King Abdullah University of
   Science and Technology (KAUST).
NR 41
TC 5
Z9 5
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 573
EP 582
DI 10.1109/TVCG.2021.3114788
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000065
PM 34587033
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kehlbeck, R
   Görtler, J
   Wang, YH
   Deussen, O
AF Kehlbeck, Rebecca
   Goertler, Jochen
   Wang, Yunhai
   Deussen, Oliver
TI SPEULER: Semantics-preserving Euler Diagrams
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Task analysis; Faces; Semantics; Layout; Guidelines;
   Visualization; Euler diagrams; Venn diagrams; set visualization; layout
   algorithm
ID VENN; PERCEPTION; SETS
AB Creating comprehensible visualizations of highly overlapping set-typed data is a challenging task due to its complexity. To facilitate insights into set connectivity and to leverage semantic relations between intersections, we propose a fast two-step layout technique for Euler diagrams that are both well-matched and well-formed. Our method conforms to established form guidelines for Euler diagrams regarding semantics, aesthetics, and readability. First, we establish an initial ordering of the data, which we then use to incrementally create a planar, connected, and monotone dual graph representation. In the next step, the graph is transformed into a circular layout that maintains the semantics and yields simple Euler diagrams with smooth curves. When the data cannot be represented by simple diagrams, our algorithm always falls back to a solution that is not well-formed but still well-matched, whereas previous methods often fail to produce expected results. We show the usefulness of our method for visualizing set-typed data using examples from text analysis and infographics. Furthermore, we discuss the characteristics of our approach and evaluate our method against state-of-the-art methods.
C1 [Kehlbeck, Rebecca; Goertler, Jochen; Deussen, Oliver] Univ Konstanz, Constance, Germany.
   [Wang, Yunhai] Shandong Univ, Jinan, Peoples R China.
C3 University of Konstanz; Shandong University
RP Deussen, O (corresponding author), Univ Konstanz, Constance, Germany.; Wang, YH (corresponding author), Shandong Univ, Jinan, Peoples R China.
EM rebeccakehlbeck@uni-konstanz.de; jochen.goertler@uni-konstanz.de;
   cloudseawang@gmail.com; oliver.deussen@uni-konstanz.de
RI Deussen, Oliver/HKF-2004-2023
FU German Research Foundation (DFG) [KE 740/17-2, FOR2111, 251654672 -TRR
   161]
FX This work was supported by the German Research Foundation (DFG) within
   project KE 740/17-2 of the FOR2111 "Questions at the Interfaces" as well
   as Project-ID 251654672 -TRR 161 "Quantitative methods for visual
   computing". We would further like to thank Matthias Albrecht for help
   during the implementation and Patrick Paetzold for valuable feedback
   during the revision process.
NR 48
TC 8
Z9 8
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 433
EP 442
DI 10.1109/TVCG.2021.3114834
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000053
PM 34587064
OA Green Submitted
DA 2025-03-07
ER

PT J
AU McColeman, CM
   Yang, FM
   Brady, TF
   Franconeri, S
AF McColeman, Caitlyn M.
   Yang, Fumeng
   Brady, Timothy F.
   Franconeri, Steven
TI Rethinking the Ranks of Visual Channels
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Visualization; Bars; Data visualization; Memory
   management; Measurement uncertainty; Correlation; DataType Agnostic;
   Human-Subjects Quantitative Studies; Perception & Cognition; Charts;
   Diagrams; and Plots
ID SHORT-TERM-MEMORY; WORKING-MEMORY; VISUALIZATION; CAPACITY;
   REPRESENTATIONS; PERCEPTION; JUDGMENTS; FRAMEWORK; MODELS; BIASES
AB Data can be visually represented using visual channels like position, length or luminance. An existing ranking of these visual channels is based on how accurately participants could report the ratio between two depicted values. There is an assumption that this ranking should hold for different tasks and for different numbers of marks. However, there is surprisingly little existing work that tests this assumption, especially given that visually computing ratios is relatively unimportant in real-world visualizations, compared to seeing, remembering, and comparing trends and motifs, across displays that almost universally depict more than two values. To simulate the information extracted from a glance at a visualization, we instead asked participants to immediately reproduce a set of values from memory after they were shown the visualization. These values could be shown in a bar graph (position (bar)), line graph (position (line)), heat map (luminance), bubble chart (area), misaligned bar graph (length), or 'wind map' (angle). With a Bayesian multilevel modeling approach, we show how the rank positions of visual channels shift across different numbers of marks (2, 4 or 8) and for bias, precision, and error measures. The ranking did not hold, even for reproductions of only 2 marks, and the new probabilistic ranking was highly inconsistent for reproductions of different numbers of marks. Other factors besides channel choice had an order of magnitude more influence on performance, such as the number of values in the series (e.g., more marks led to larger errors), or the value of each mark (e.g., small values were systematically overestimated). Every visual channel was worse for displays with 8 marks than 4, consistent with established limits on visual memory. These results point to the need for a body of empirical studies that move beyond two-value ratio judgments as a baseline for reliably ranking the quality of a visual channel, including testing new tasks (detection of trends or motifs), timescales (immediate computation, or later comparison), and the number of values (from a handful, to thousands).
C1 [McColeman, Caitlyn M.; Franconeri, Steven] Northwestern Univ, Evanston, IL 60208 USA.
   [Yang, Fumeng] Brown Univ, Providence, RI 02912 USA.
   [Brady, Timothy F.] Univ San Diego, San Diego, CA 92110 USA.
C3 Northwestern University; Brown University; University of San Diego
RP McColeman, CM (corresponding author), Northwestern Univ, Evanston, IL 60208 USA.
EM caitlyn.mccoleman@gmail.com; fy@brown.edu; timbrady@ucsd.edu;
   franconeri@northwestern.edu
RI Yang, Fumeng/HME-2828-2023
FU National Science Foundation [BCS-1653457, IIS-1901485]
FX The authors would like to thank Satoru Suzuki and members of the Visual
   Thinking Laboratory at Northwestern University for their suggestions
   during the experimental design. The authors also thank the anonymous
   reviewers for their feedback. This work was supported in part by grants
   BCS-1653457 and IIS-1901485 from the National Science Foundation.
NR 87
TC 15
Z9 16
U1 0
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 707
EP 717
DI 10.1109/TVCG.2021.3114684
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA XW3DW
UT WOS:000735505300008
PM 34606455
OA Bronze, Green Submitted
DA 2025-03-07
ER

PT J
AU Yang, LN
   Xu, X
   Lan, XY
   Liu, ZY
   Guo, SN
   Shi, Y
   Qu, HM
   Cao, N
AF Yang, Leni
   Xu, Xian
   Lan, XingYu
   Liu, Ziyan
   Guo, Shunan
   Shi, Yang
   Qu, Huamin
   Cao, Nan
TI A Design Space for Applying the Freytag's Pyramid Structure to Data
   Stories
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Videos; Animation; Visual
   communication; Systematics; Data models; Freytag's Pyramid; Narrative
   Structure; Narrative Visualization; Data Storytelling; Data Video
AB Data stories integrate compelling visual content to communicate data insights in the form of narratives. The narrative structure of a data story serves as the backbone that determines its expressiveness, and it can largely influence how audiences perceive the insights. Freytag's Pyramid is a classic narrative structure that has been widely used in film and literature. While there are continuous recommendations and discussions about applying Freytag's Pyramid to data stories, little systematic and practical guidance is available on how to use Freytag's Pyramid for creating structured data stories. To bridge this gap, we examined how existing practices apply Freytag's Pyramid by analyzing stories extracted from 103 data videos. Based on our findings, we proposed a design space of narrative patterns, data flows, and visual communications to provide practical guidance on achieving narrative intents, organizing data facts, and selecting visual design techniques through story creation. We evaluated the proposed design space through a workshop with 25 participants. Results show that our design space provides a clear framework for rapid storyboarding of data stories with Freytag's Pyramid.
C1 [Yang, Leni; Lan, XingYu; Liu, Ziyan; Shi, Yang; Cao, Nan] Tongji Univ, Intelligent Big Data Visualizat Lab, Beijing, Peoples R China.
   [Yang, Leni; Xu, Xian; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Guo, Shunan] Adobe Res, San Jose, CA USA.
C3 Tongji University; Hong Kong University of Science & Technology; Adobe
   Systems Inc.
RP Cao, N (corresponding author), Tongji Univ, Intelligent Big Data Visualizat Lab, Beijing, Peoples R China.
EM lyangbb@connect.ust.hk; xxubq@connect.ust.hk; xingyulan@tongji.edu.cn;
   ziyan.liu.design@gmail.com; sguo@adobe.com; yangshi.idvx@tongji.edu.cn;
   huamin@cse.ust.hk; nan.cao@gmail.com
RI Cao, Nan/O-5397-2014; Lan, Xingyu/KYO-9537-2024; Guo,
   Shunan/AAE-2616-2019
OI Lan, Xingyu/0000-0001-7331-2433
FU National Natural Science Foundation of China [62072338]; NSF Shanghai
   [20ZR1461500]
FX Nan Cao is the corresponding author. This work was supported in part by
   the National Natural Science Foundation of China (62072338) and NSF
   Shanghai 20ZR1461500.
NR 71
TC 24
Z9 26
U1 7
U2 67
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 922
EP 932
DI 10.1109/TVCG.2021.3114774
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000093
PM 34587025
DA 2025-03-07
ER

PT J
AU Yu, K
   Gorbachev, G
   Eck, U
   Pankratz, F
   Navab, N
   Roth, D
AF Yu, Kevin
   Gorbachev, Gleb
   Eck, Ulrich
   Pankratz, Frieder
   Navab, Nassir
   Roth, Daniel
TI Avatars for Teleconsultation: Effects of Avatar Embodiment Techniques on
   User Perception in 3D Asymmetric Telepresence
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Avatars; Three-dimensional displays; Telepresence; Task analysis;
   Collaboration; Real-time systems; Faces; Telepresence; Avatars;
   Augmented Reality; Mixed Reality; Virtual Reality; Collaboration;
   Embodiment
ID SOCIAL PRESENCE; REPRESENTATION; OWNERSHIP; REALISM
AB A 3D Telepresence system allows users to interact with each other in a virtual, mixed, or augmented reality (VR, MR, AR) environment, creating a shared space for collaboration and communication. There are two main methods for representing users within these 3D environments. Users can be represented either as point cloud reconstruction-based avatars that resemble a physical user or as virtual character-based avatars controlled by tracking the users' body motion. This work compares both techniques to identify the differences between user representations and their fit in the reconstructed environments regarding the perceived presence, uncanny valley factors, and behavior impression. Our study uses an asymmetric VR/AR teleconsultation system that allows a remote user to join a local scene using VR. The local user observes the remote user with an AR head-mounted display, leading to facial occlusions in the 3D reconstruction. Participants perform a warm-up interaction task followed by a goal-directed collaborative puzzle task, pursuing a common goal. The local user was represented either as a point cloud reconstruction or as a virtual character-based avatar, in which case the point cloud reconstruction of the local user was masked. Our results show that the point cloud reconstruction-based avatar was superior to the virtual character avatar regarding perceived co-presence, social presence, behavioral impression, and humanness. Further, we found that the task type partly affected the perception. The point cloud reconstruction-based approach led to higher usability ratings, while objective performance measures showed no significant difference. We conclude that despite partly missing facial information, the point cloud-based reconstruction resulted in better conveyance of the user behavior and a more coherent fit into the simulation context.
C1 [Yu, Kevin] Tech Univ Munich, Res Grp MITI, Munich, Germany.
   [Gorbachev, Gleb; Eck, Ulrich; Navab, Nassir] Tech Univ Munich, Comp Aided Med Procedures, Munich, Germany.
   [Pankratz, Frieder] Ludwig Maximilians Univ Munchen, Inst Emergency Med, Munich, Germany.
   [Roth, Daniel] Friedrich Alexander Univ FAU Erlangen Nuremberg, Human Ctr Comp & Extended Real, Erlangen, Germany.
C3 Technical University of Munich; Technical University of Munich;
   University of Munich; University of Erlangen Nuremberg
RP Yu, K (corresponding author), Tech Univ Munich, Res Grp MITI, Munich, Germany.
EM kevin.yu@tum.de; gleb.gorbachev@tum.de; ulrich.eck@tum.de;
   frieder.pankratz@med.uni-muenchen.de; nassir.navab@tum.de; d.roth@fau.de
RI Roth, Daniel/AFK-2613-2022
FU German Federal Ministry of Education and Research (BMBF) [16SV8092]
FX The authors wish to thank Andreas Keller for his help in carrying out
   the user study. This work was supported by the German Federal Ministry
   of Education and Research (BMBF) as part of the project ArtekMed (Grant
   No. 16SV8092)
NR 77
TC 41
Z9 42
U1 4
U2 30
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2021
VL 27
IS 11
BP 4129
EP 4139
DI 10.1109/TVCG.2021.3106480
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WN2ZT
UT WOS:000711642700007
PM 34449373
DA 2025-03-07
ER

PT J
AU Dou, ZY
   Xin, SQ
   Xu, R
   Xu, J
   Zhou, YF
   Chen, SM
   Wang, WP
   Zhao, XY
   Tu, CH
AF Dou, Zhiyang
   Xin, Shiqing
   Xu, Rui
   Xu, Jian
   Zhou, Yuanfeng
   Chen, Shuangmin
   Wang, Wenping
   Zhao, Xiuyang
   Tu, Changhe
TI Top-Down Shape Abstraction Based on Greedy Pole Selection
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Three-dimensional displays; Computational modeling; Pattern
   recognition; Solid modeling; Transforms; Wires; Shape abstraction;
   medial surface; power crust; porous structure; ball-stick toy; power
   diagram
ID MEDIAL AXIS; OPTIMIZATION; TRENDS
AB Motivated by the fact that the medial axis transform is able to encode the shape completely, we propose to use as few medial balls as possible to approximate the original enclosed volume by the boundary surface. We progressively select new medial balls, in a top-down style, to enlarge the region spanned by the existing medial balls. The key spirit of the selection strategy is to encourage large medial balls while imposing given geometric constraints. We further propose a speedup technique based on a provable observation that the intersection of medial balls implies the adjacency of power cells (in the sense of the power crust). We further elaborate the selection rules in combination with two closely related applications. One application is to develop an easy-to-use ball-stick modeling system that helps non-professional users to quickly build a shape with only balls and wires, but any penetration between two medial balls must be suppressed. The other application is to generate porous structures with convex, compact (with a high isoperimetric quotient) and shape-aware pores where two adjacent spherical pores may have penetration as long as the mechanical rigidity can be well preserved.
C1 [Dou, Zhiyang; Xin, Shiqing; Xu, Rui; Zhou, Yuanfeng; Tu, Changhe] Shandong Univ, Jinan 250100, Shandong, Peoples R China.
   [Xu, Jian] Chinese Acad Sci, Ningbo Inst Mat Technol & Engn, Ningbo 315201, Zhejiang, Peoples R China.
   [Chen, Shuangmin] Qingdao Univ Sci & Technol, Qingdao 260061, Shandong, Peoples R China.
   [Wang, Wenping] Univ Hong Kong, Hong Kong, Peoples R China.
   [Zhao, Xiuyang] Jinan Univ, Jinan 250022, Shandong, Peoples R China.
C3 Shandong University; Chinese Academy of Sciences; Ningbo Institute of
   Materials Technology and Engineering, CAS; Qingdao University of Science
   & Technology; University of Hong Kong; University of Jinan
RP Xin, SQ (corresponding author), Shandong Univ, Jinan 250100, Shandong, Peoples R China.
EM sdudzy@163.com; xinshiqing@163.com; xrvitd@163.com; xujian@nimte.ac.cn;
   yfzhou@sdu.edu.cn; csmqq@163.com; Wenping@cs.hku.hk; zhaoxy@ujn.edu.cn;
   chtu@sdu.edu.cn
RI Zhou, Yuanfeng/AAT-4670-2020; Tu, Changhe/H-5162-2013; Dou,
   Zhiyang/JED-8952-2023; Xu, Rui/GWM-7132-2022
OI Xu, Jian/0000-0003-1814-3045; Xu, Rui/0000-0001-8273-1808; Chen,
   Shuangmin/0000-0002-0835-3316; Xin, Shiqing/0000-0001-8452-8723; Dou,
   Zhiyang/0000-0003-0186-8269
FU National Natural Science Foundation of China [61772016, 61772318];
   Research Grant Council of Hong Kong [GRF 17263316]
FX The authors would like to thank the anonymous reviewers for their
   valuable comments and suggestions. This work was supported in part by
   the National Natural Science Foundation of China under Grants 61772016
   and 61772318, and in part by the Research Grant Council of Hong Kong
   under Grant GRF 17263316.
NR 58
TC 1
Z9 1
U1 0
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2021
VL 27
IS 10
BP 3982
EP 3993
DI 10.1109/TVCG.2020.2995495
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL8JB
UT WOS:000692890200011
PM 32746254
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ma, RX
   Mei, HH
   Guan, HH
   Huang, W
   Zhang, F
   Xin, CY
   Dai, WZ
   Wen, X
   Chen, W
AF Ma, Ruixian
   Mei, Honghui
   Guan, Huihua
   Huang, Wei
   Zhang, Fan
   Xin, Chengye
   Dai, Wenzhuo
   Wen, Xiao
   Chen, Wei
TI LADV: Deep Learning Assisted Authoring of Dashboard Visualizations From
   Images and Sketches
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Tools; Data visualization; Image color analysis; Visualization; Layout;
   Task analysis; Rapid prototyping; Dashboard visualization; visual
   design; stylization; deep learning-based
ID INFORMATION VISUALIZATION; DESIGN; VEGA
AB Dashboard visualizations are widely used in data-intensive applications such as business intelligence, operation monitoring, and urban planning. However, existing visualization authoring tools are inefficient in the rapid prototyping of dashboards because visualization expertise and user intention need to be integrated. We propose a novel approach to rapid conceptualization that can construct dashboard templates from exemplars to mitigate the burden of designing, implementing, and evaluating dashboard visualizations. The kernel of our approach is a novel deep learning-based model that can identify and locate charts of various categories and extract colors from an input image or sketch. We design and implement a web-based authoring tool for learning, composing, and customizing dashboard visualizations in a cloud computing environment. Examples, user studies, and user feedback from real scenarios in Alibaba Cloud verify the usability and efficiency of the proposed approach.
C1 [Ma, Ruixian; Mei, Honghui; Guan, Huihua; Huang, Wei; Xin, Chengye; Dai, Wenzhuo; Wen, Xiao] Alibaba Grp, Hangzhou 311121, Peoples R China.
   [Chen, Wei] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
   [Zhang, Fan] MIT, Senseable City Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
C3 Alibaba Group; Zhejiang University; Massachusetts Institute of
   Technology (MIT)
RP Chen, W (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
EM ruixian.mrx@alibaba-inc.com; honghui.mhh@alibaba-inc.com;
   huihua.ghh@alibaba-inc.com; mujing.hw@alibaba-inc.com;
   zfancheung@gmail.com; mier.xcy@taobao.com; basi.dwz@taobao.com;
   ninglang.wx@taobao.com; chenwei@cad.zju.edu.cn
RI Zhang, Fan/N-2457-2019; Chen, Wei/AAR-9817-2020
OI Chen, Wei/0000-0002-8365-4741; Ma, Ruixian/0009-0001-8647-1131; Zhang,
   Fan/0000-0002-3643-018X
FU National Natural Science Foundation of China [61772456, 61761136020,
   U1609217]
FX Authors would like to thank all the anonymous reviewers for their
   valuable comments, and all the participants for their active
   participation. They also thank Ye Zhang from DataV Lab. This work was
   supported by the National Natural Science Foundation of China (61772456,
   61761136020, U1609217).
NR 44
TC 19
Z9 20
U1 1
U2 26
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2021
VL 27
IS 9
BP 3717
EP 3732
DI 10.1109/TVCG.2020.2980227
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TS3CP
UT WOS:000679532000009
PM 32175864
DA 2025-03-07
ER

PT J
AU Li, D
   Du, RF
   Babu, A
   Brumar, CD
   Varshney, A
AF Li, David
   Du, Ruofei
   Babu, Adharsh
   Brumar, Camelia D.
   Varshney, Amitabh
TI A Log-Rectilinear Transformation for Foveated 360-degree Video Streaming
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Streaming media; Rendering (computer graphics); Headphones; Bandwidth;
   Pipelines; Video codecs; Two dimensional displays; 360 video; foveation;
   virtual reality; video streaming; log-rectilinear; summed-area table
ID SUMMED-AREA TABLE
AB With the rapidly increasing resolutions of 360 degrees cameras, head-mounted displays, and live-streaming services, streaming high-resolution panoramic videos over limited-bandwidth networks is becoming a critical challenge. Foveated video streaming can address this rising challenge in the context of eye-tracking-equipped virtual reality head-mounted displays. However, conventional log-polar foveated rendering suffers from a number of visual artifacts such as aliasing and flickering. In this paper, we introduce a new log-rectilinear transformation that incorporates summed-area table filtering and off-the-shelf video codecs to enable foveated streaming of 360 degrees videos suitable for VR headsets with built-in eye-tracking. To validate our approach, we build a client-server system prototype for streaming 360 degrees videos which leverages parallel algorithms over real-time video transcoding. We conduct quantitative experiments on an existing 360 degrees video dataset and observe that the log-rectilinear transformation paired with summed-area table filtering heavily reduces flickering compared to log-polar subsampling while also yielding an additional 10% reduction in bandwidth usage.
C1 [Li, David; Babu, Adharsh; Brumar, Camelia D.; Varshney, Amitabh] Univ Maryland, College Pk, MD 20742 USA.
   [Du, Ruofei] Google LLC, Menlo Pk, CA USA.
C3 University System of Maryland; University of Maryland College Park;
   Google Incorporated
RP Li, D (corresponding author), Univ Maryland, College Pk, MD 20742 USA.
EM dli7319@umd.edu; me@duruofei.com; ababu@umiacs.umd.edu;
   cbrumar@terpmail.umd.edu; varshney@umd.edu
RI Du, Ruofei/AAL-4845-2020; Brumar, Camelia/KWT-3643-2024; Li,
   David/KEH-7536-2024
OI Varshney, Amitabh/0000-0002-9873-2212; Li, David/0000-0002-3187-6190
FU NSF [15-64212, 18-23321]; State of Maryland's MPower initiative
FX We would like to thank the anonymous reviewers for the valuable comments
   on the manuscript. This work has been supported in part by the NSF
   Grants 15-64212 and 18-23321 and the State of Maryland's MPower
   initiative. Any opinions, findings, conclusions, or recommendations
   expressed in this article are those of the authors and do not
   necessarily reflect the views of the research sponsors.
NR 52
TC 21
Z9 22
U1 2
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2021
VL 27
IS 5
BP 2638
EP 2647
DI 10.1109/TVCG.2021.3067762
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA RP8EF
UT WOS:000641955700001
PM 33750700
OA Bronze
DA 2025-03-07
ER

PT J
AU Ceja, CR
   McColeman, CM
   Xiong, C
   Franconeri, SL
AF Ceja, Cristina R.
   McColeman, Caitlyn M.
   Xiong, Cindy
   Franconeri, Steven L.
TI Truth or Square: Aspect Ratio Biases Recall of Position Encodings
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Memory biases; position estimation; bar charts; aspect ratio; area
ID CATEGORIES; VISUALIZATION; PARTICULARS; MEMORY; MODEL
AB Bar charts are among the most frequently used visualizations, in part because their position encoding leads them to convey data values precisely. Yet reproductions of single bars or groups of bars within a graph can be biased. Curiously, some previous work found that this bias resulted in an overestimation of reproduced data values, while other work found an underestimation. Across three empirical studies, we offer an explanation for these conflicting findings: this discrepancy is a consequence of the differing aspect ratios of the tested bar marks. Viewers are biased to remember a bar mark as being more similar to a prototypical square, leading to an overestimation of bars with a wide aspect ratio, and an underestimation of bars with a tall aspect ratio. Experiments 1 and 2 showed that the aspect ratio of the bar marks indeed influenced the direction of this bias. Experiment 3 confirmed that this pattern of misestimation bias was present for reproductions from memory, suggesting that this bias may arise when comparing values across sequential displays or views. We describe additional visualization designs that might be prone to this bias beyond bar charts (e.g., Mekko charts and treemaps), and speculate that other visual channels might hold similar biases toward prototypical values.
C1 [Ceja, Cristina R.; McColeman, Caitlyn M.; Xiong, Cindy; Franconeri, Steven L.] Northwestern Univ, Evanston, IL 60208 USA.
   [Xiong, Cindy] Univ Massachusetts, Amherst, MA 01003 USA.
C3 Northwestern University; University of Massachusetts System; University
   of Massachusetts Amherst
RP Ceja, CR (corresponding author), Northwestern Univ, Evanston, IL 60208 USA.
EM crceja@u.northwestern.edu; caitlyn.mccoleman@northwestern.edu;
   cxiong@u.northwestern.edu; franconeri@northwestern.edu
FU National Science Foundation Graduate Research Fellowship [DGE-1842165,
   IIS-1901485]; National Science Foundation
FX This work was supported in part by the National Science Foundation
   Graduate Research Fellowship under grant No. DGE-1842165, and in part by
   grant IIS-1901485 from the National Science Foundation.
NR 36
TC 10
Z9 12
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1054
EP 1062
DI 10.1109/TVCG.2020.3030422
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA WF5FO
UT WOS:000706330100089
PM 33048726
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Park, H
   Nam, Y
   Kim, JH
   Choo, J
AF Park, Heungseok
   Nam, Yoonsoo
   Kim, Ji-Hoon
   Choo, Jaegul
TI HyperTendril: Visual Analytics for User-Driven Hyperparameter
   Optimization of Deep Neural Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual analytics; deep learning; machine learning; automated machine
   learning; human-centered computing
ID MACHINE
AB To mitigate the pain of manually tuning hyperparameters of deep neural networks, automated machine learning (AutoML) methods have been developed to search for an optimal set of hyperparameters in large combinatorial search spaces. However, the search results of AutoML methods significantly depend on initial configurations, making it a non-trivial task to find a proper configuration. Therefore, human intervention via a visual analytic approach bears huge potential in this task. In response, we propose HyperTendril, a web-based visual analytics system that supports user-driven hyperparameter tuning processes in a model-agnostic environment. HyperTendril takes a novel approach to effectively steering hyperparameter optimization through an iterative, interactive tuning procedure that allows users to refine the search spaces and the configuration of the AutoML method based on their own insights from given results. Using HyperTendril, users can obtain insights into the complex behaviors of various hyperparameter search algorithms and diagnose their configurations. In addition, HyperTendril supports variable importance analysis to help the users refine their search spaces based on the analysis of relative importance of different hyperparameters and their interaction effects. We present the evaluation demonstrating how HyperTendril helps users steer their tuning processes via a longitudinal user study based on the analysis of interaction logs and in-depth interviews while we deploy our system in a professional industrial environment.
C1 [Park, Heungseok; Nam, Yoonsoo; Kim, Ji-Hoon] NAVER Corp, Clara AI Res, Seongnam Si, South Korea.
   [Choo, Jaegul] Korea Adv Inst Sci & Technol, Daejeon, South Korea.
C3 Naver; Korea Advanced Institute of Science & Technology (KAIST)
RP Kim, JH (corresponding author), NAVER Corp, Clara AI Res, Seongnam Si, South Korea.; Choo, J (corresponding author), Korea Adv Inst Sci & Technol, Daejeon, South Korea.
EM heungseok.park@navercorp.com; yoonsoo.nam@navercorp.com;
   genesis.kim@navercorp.com; jchoo@kaist.ac.kr
RI Choo, Jaegul/ABF-8315-2020
OI Kim, Ji-Hoon/0000-0002-5212-1686
FU Basic Science Research Program through the National Research Foundation
   of Korea (NRF) - Ministry of Science, ICT & Future Planning
   [2019R1A2C4070420]; Korea Electric Power Corporation [R18XA05]
FX This work was partly supported by Basic Science Research Program through
   the National Research Foundation of Korea (NRF) funded by the Ministry
   of Science, ICT & Future Planning (2019R1A2C4070420) and by Korea
   Electric Power Corporation (Grant number:R18XA05).
NR 45
TC 14
Z9 14
U1 1
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1407
EP 1416
DI 10.1109/TVCG.2020.3030380
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100120
PM 33048706
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Perovich, LJ
   Wylie, SA
   Bongiovanni, R
AF Perovich, Laura J.
   Wylie, Sara Ann
   Bongiovanni, Roseann
TI Chemicals in the Creek: Designing a Situated Data Physicalization of
   Open Government Data with the Community
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE data physicalization; Participatory Action Research; water quality;
   environmental HCl
AB Over the last decade growing amounts of government data have been made available in an attempt to increase transparency and civic participation, but it is unclear if this data serves non-expert communities due to gaps in access and the technical knowledge needed to interpret this "open" data. We conducted a two-year design study focused on the creation of a community-based data display using the United States Environmental Protection Agency data on water permit violations by oil storage facilities on the Chelsea Creek in Massachusetts to explore whether situated data physicalization and Participatory Action Research could support meaningful engagement with open data. We selected this data as it is of interest to local groups and available online, yet remains largely invisible and inaccessible to the Chelsea community. The resulting installation, Chemicals in the Creek, responds to the call for community-engaged visualization processes and provides an application of situated methods of data representation. It proposes event-centered and power-aware modes of engagement using contextual and embodied data representations. The design of Chemicals in the Creek is grounded in interactive workshops and we analyze it through event observation, interviews, and community outcomes. We reflect on the role of community engaged research in the Information Visualization community relative to recent conversations on new approaches to design studies and evaluation.
C1 [Perovich, Laura J.] Northeastern Univ, Art Design, Boston, MA 02115 USA.
   [Perovich, Laura J.] MIT, Media Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Wylie, Sara Ann] Northeastern Univ, Sociol Anthropol & Hlth Sci, Boston, MA 02115 USA.
   [Bongiovanni, Roseann] GreenRoots Inc, Chelsea, MA USA.
C3 Northeastern University; Massachusetts Institute of Technology (MIT);
   Northeastern University
RP Perovich, LJ (corresponding author), Northeastern Univ, Art Design, Boston, MA 02115 USA.; Perovich, LJ (corresponding author), MIT, Media Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM perovich@media.mit.edu; s.wylie@northeastern.edu;
   RoseannB@greenrootschelsea.org
FU CRESSH; Media Lab Elements; RIELS; Harvard TH Chan School of Public
   Health JPB fellowship
FX Thanks to ECO and Leilani Mroczkowski who were central to this work.
   Thanks to event participants, the Chelsea community, Paddle Boston, OBM,
   and the BU Cyber Law Clinic. Thanks to contributing students: Michael
   Still, Gustavo Santiago-Reyes, Jacqueline Chen, Maggie Zhang, Xavier
   Mojica, Emily Schachtele, Garance Malivel, and Shawn Sullivan; event
   volunteers: John Rao, Arushi Sood, Ed Hackett, Sharon Harlan, Olivia
   Ozkurt, Abbie Keane, Holly Coppes, Hanson Au, Lourdes Vera, Marc
   Jacobson, Dorian Stump, Angela Stewart, Laura Senier, Grace Poudrier,
   and Kaline Langley; and photographers: Rio Asch Phoenix, Will Campbell,
   Jimmy Day, and David Mussina. Thanks to the IEEE InfoViz reviewers.
   Research support from CRESSH, Media Lab Elements, RIELS, and Harvard TH
   Chan School of Public Health JPB fellowship.
NR 89
TC 21
Z9 22
U1 1
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 913
EP 923
DI 10.1109/TVCG.2020.3030472
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100076
PM 33079668
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Pister, A
   Buono, P
   Fekete, JD
   Plaisant, C
   Valdivia, P
AF Pister, Alexis
   Buono, Paolo
   Fekete, Jean-Daniel
   Plaisant, Catherine
   Valdivia, Paola
TI Integrating Prior Knowledge in Mixed-Initiative Social Network
   Clustering
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Clustering algorithms; Social networking (online); Tools; Visualization;
   Heuristic algorithms; Clustering methods; Inference algorithms; Social
   network analysis; network visualization; clustering; mixed-initiative;
   prior knowledge; user interface
ID COMMUNITY DETECTION; VISUALIZATION; ALGORITHMS
AB We propose a new approach-called PK-clustering-to help social scientists create meaningful clusters in social networks. Many clustering algorithms exist but most social scientists find them difficult to understand, and tools do not provide any guidance to choose algorithms, or to evaluate results taking into account the prior knowledge of the scientists. Our work introduces a new clustering approach and a visual analytics user interface that address this issue. It is based on a process that 1) captures the prior knowledge of the scientists as a set of incomplete clusters, 2) runs multiple clustering algorithms (similarly to clustering ensemble methods), 3) visualizes the results of all the algorithms ranked and summarized by how well each algorithm matches the prior knowledge, 4) evaluates the consensus between user-selected algorithms and 5) allows users to review details and iteratively update the acquired knowledge. We describe our approach using an initial functional prototype, then provide two examples of use and early feedback from social scientists. We believe our clustering approach offers a novel constructive method to iteratively build knowledge while avoiding being overly influenced by the results of often randomly selected black-box clustering algorithms.
C1 [Pister, Alexis; Buono, Paolo; Fekete, Jean-Daniel; Plaisant, Catherine] Univ Paris Saclay, CNRS, LRI, Inria, Gif Sur Yvette, France.
   [Valdivia, Paola] Univ Bari, Bari, Italy.
   [Plaisant, Catherine] Univ Maryland, College Pk, MD 20742 USA.
C3 Centre National de la Recherche Scientifique (CNRS); Universite Paris
   Saclay; Inria; Universita degli Studi di Bari Aldo Moro; University
   System of Maryland; University of Maryland College Park
RP Pister, A (corresponding author), Univ Paris Saclay, CNRS, LRI, Inria, Gif Sur Yvette, France.
EM alexis.pister@inria.fr; paolo.buono@di.uniba.it;
   jean-daniel.fekete@inria.fr; plaisant@umd.edu; paola.valdivia@inria.fr
RI Buono, Paolo/E-9803-2014; Valdivia, Paola/AAE-5059-2020; Fekete,
   Jean-Daniel/N-9175-2018
OI Fekete, Jean-Daniel/0000-0003-3770-8726
FU DataIA Institute (project HistorIA); European project IVAN
FX The authors wish to thank Nicole Dufournaud, Pascal Cristofoli,
   Francesco Napolitano, and Christophe Prieur. This work was supported in
   part by a grant from he DataIA Institute (project HistorIA) and the
   European project IVAN.
NR 52
TC 13
Z9 14
U1 0
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1775
EP 1785
DI 10.1109/TVCG.2020.3030347
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100153
PM 33095715
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Xie, X
   Du, F
   Wu, YC
AF Xie, Xiao
   Du, Fan
   Wu, Yingcai
TI A Visual Analytics Approach for Exploratory Causal Analysis:
   Exploration, Validation, and Applications
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Exploratory causal analysis; correlation and causation; causal graph
ID UNCERTAINTY; USER
AB Using causal relations to guide decision making has become an essential analytical task across various domains, from marketing and medicine to education and social science. While powerful statistical models have been developed for inferring causal relations from data, domain practitioners still lack effective visual interface for interpreting the causal relations and applying them in their decision-making process. Through interview studies with domain experts, we characterize their current decision-making workflows, challenges, and needs. Through an iterative design process, we developed a visualization tool that allows analysts to explore, validate, and apply causal relations in real-world decision-making scenarios. The tool provides an uncertainty-aware causal graph visualization for presenting a large set of causal relations inferred from high-dimensional data. On top of the causal graph, it supports a set of intuitive user controls for performing what-if analyses and making action plans. We report on two case studies in marketing and student advising to demonstrate that users can effectively explore causal relations and design action plans for reaching their goals.
C1 [Xie, Xiao; Wu, Yingcai] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Peoples R China.
   [Du, Fan] Adobe Res, San Jose, CA 95110 USA.
C3 Zhejiang University; Adobe Systems Inc.
RP Wu, YC (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Peoples R China.; Du, F (corresponding author), Adobe Res, San Jose, CA 95110 USA.
EM xxie.ycwu@zju.edu.cn; fdu@adobe.com; ycwu@zju.edu.cn
FU National Key R&D Program of China [2018YFB1004300]; NSFC-Zhejiang Joint
   Fund for the Integration of Industrialization and Informatization
   [U1609217]; Zhejiang Provincial Natural Science Foundation
   [LR18F020001]; 100 Talents Program of Zhejiang University; NSW
   [61761136020]
FX The authors wish to thank all the reviewers, study participants, and
   domain experts for their thoughtful feedback. The work was supported by
   National Key R&D Program of China (2018YFB1004300), NSW (61761136020),
   NSFC-Zhejiang Joint Fund for the Integration of Industrialization and
   Informatization (U1609217), Zhejiang Provincial Natural Science
   Foundation (LR18F020001) and the 100 Talents Program of Zhejiang
   University.
NR 64
TC 19
Z9 22
U1 2
U2 22
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1448
EP 1458
DI 10.1109/TVCG.2020.3028957
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA WF5FO
UT WOS:000706330100124
PM 33026999
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhang, YW
   Zhang, CM
   Wang, WP
   Chen, YZ
   Ji, ZP
   Liu, H
AF Zhang, Yu-Wei
   Zhang, Caiming
   Wang, Wenping
   Chen, Yanzhao
   Ji, Zhongping
   Liu, Hui
TI Portrait Relief Modeling from a Single Image
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Face; Three-dimensional displays; Image reconstruction; Solid modeling;
   Computational modeling; Strain; Shape; Bas-relief modeling; high relief
   modeling; 3D face reconstruction; height compression; detail recovery
ID BAS-RELIEFS; GENERATION
AB We present a novel solution to enable portrait relief modeling from a single image. The main challenges are geometry reconstruction, facial details recovery and depth structure preservation. Previous image-based methods are developed for portrait bas-relief modeling in 2.5D form, but not adequate for 3D-like high relief modeling with undercut features. In this paper, we propose a template-based framework to generate portrait reliefs of various forms. Our method benefits from Shape-from-Shading (SFS). Specifically, we use bi-Laplacian mesh deformation to guide the relief modeling. Given a portrait image, we first use a template face to fit the portrait. We then apply bi-Laplacian mesh deformation to align the facial features. Afterwards, SFS-based reconstruction with a few user interactions is used to optimize the face depth, and create a relief with similar appearance to the input. Both depth structures and geometric details can be well constructed in the final relief. Experiments and comparisons to other methods demonstrate the effectiveness of the proposed method.
C1 [Zhang, Yu-Wei; Chen, Yanzhao] Qilu Univ Technol, Shandong Acad Sci, Sch Mech & Automot Engn, Jinan 250000, Shandong, Peoples R China.
   [Zhang, Caiming] Shandong Univ, Software Coll, Jinan, Peoples R China.
   [Zhang, Caiming] Shandong Coinnovat Ctr Future Intelligent Comp, Yantai, Peoples R China.
   [Wang, Wenping] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
   [Ji, Zhongping] Hangzhou Dianzi Univ, Inst Graph & Image, Hangzhou 310005, Zhejiang, Peoples R China.
   [Liu, Hui] Shandong Univ Finance & Econ, Sch Comp Sci & Technol, Jinan, Peoples R China.
C3 Qilu University of Technology; Shandong University; University of Hong
   Kong; Hangzhou Dianzi University; Shandong University of Finance &
   Economics
RP Zhang, YW (corresponding author), Qilu Univ Technol, Shandong Acad Sci, Sch Mech & Automot Engn, Jinan 250000, Shandong, Peoples R China.
EM zhangyuwei_scott@126.com; czhang@sdu.edu.cn; wenping@cs.hku.hk;
   chyzh_ql@126.com; jzp@zju.edu.cn; liuh_lh@126.com
RI chen, yanzhao/GWC-1464-2022
OI Zhang, Yu-Wei/0000-0001-6566-5714; Chen, Yanzhao/0000-0002-5657-413X;
   zhang, caiming/0000-0003-0217-1543
FU National Natural Science Foundation of China [61772293, 61572161,
   61572286, 61602277]; NSFC-Zhejiang Joint Fund of the Integration of
   Informatization and Industrialization [U1609218]
FX We would like to thank the anonymous reviewers for their careful reviews
   and valuable suggestions. We thank Juyong Zhang from University of
   Science and Technology of China and Jing Wu from Cardiff University for
   providing comparison models. This work was supported in part by the
   National Natural Science Foundation of China (Grant No.61772293,
   No.61572161, No.61572286, No. 61602277), and the NSFC-Zhejiang Joint
   Fund of the Integration of Informatization and Industrialization
   (U1609218).
NR 33
TC 10
Z9 10
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG. 1
PY 2020
VL 26
IS 8
BP 2659
EP 2670
DI 10.1109/TVCG.2019.2892439
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MG6BL
UT WOS:000546115000009
PM 30640615
DA 2025-03-07
ER

PT J
AU Krajancich, B
   Padmanaban, N
   Wetzstein, G
AF Krajancich, Brooke
   Padmanaban, Nitish
   Wetzstein, Gordon
TI Factored Occlusion: Single Spatial Light Modulator Occlusion-capable
   Optical See-through Augmented Reality Display
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 27th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 22-26, 2020
CL Atlanta, GA
SP IEEE, IEEE Comp Soc
DE Image color analysis; Optical diffraction; Mirrors; Light emitting
   diodes; Optical imaging; Augmented reality; Modulation; Augmented
   reality; computational displays; mutual occlusion
ID MUTUAL OCCLUSION; MOUNTED DISPLAYS; COMPENSATION; PROTOTYPE; DESIGN;
   LENS
AB Occlusion is a powerful visual cue that is crucial for depth perception and realism in optical see-through augmented reality (OST-AR). However, existing OST-AR systems additively overlay physical and digital content with beam combiners - an approach that does not easily support mutual occlusion, resulting in virtual objects that appear semi-transparent and unrealistic. In this work, we propose a new type of occlusion-capable OST-AR system. Rather than additively combining the real and virtual worlds, we employ a single digital micromirror device (DMD) to merge the respective light paths in a multiplicative manner. This unique approach allows us to simultaneously block light incident from the physical scene on a pixel-by-pixel basis while also modulating the light emitted by a light-emitting diode (LED) to display digital content. Our technique builds on mixed binary/continuous factorization algorithms to optimize time-multiplexed binary DMD patterns and their corresponding LED colors to approximate a target augmented reality (AR) scene. In simulations and with a prototype benchtop display, we demonstrate hard-edge occlusions, plausible shadows, and also gaze-contingent optimization of this novel display mode, which only requires a single spatial light modulator.
C1 [Krajancich, Brooke; Padmanaban, Nitish; Wetzstein, Gordon] Stanford Univ, Stanford, CA 94305 USA.
C3 Stanford University
RP Krajancich, B (corresponding author), Stanford Univ, Stanford, CA 94305 USA.
EM brookek@stanford.edu; nit@stanford.edu; gordon.wetzstein@stanford.edu
OI Wetzstein, Gordon/0000-0002-9243-6885
FU Stanford Knight-Hennessy Fellowship; National Science Foundation (NSF)
   Graduate Research Fellowship Program; Okawa Research Grant; Sloan
   Fellowship; NSF [1553333, 1839974]; Intel
FX B.K. was supported by a Stanford Knight-Hennessy Fellowship. N.P. was
   supported by the National Science Foundation (NSF) Graduate Research
   Fellowship Program. G.W. was supported by an Okawa Research Grant and a
   Sloan Fellowship. Other funding for the project was provided by NSF
   (award numbers 1553333 and 1839974) and Intel. The authors would also
   like to thank Dr. Julien Martel for assistance with constructing the
   benchtop prototype.
NR 65
TC 31
Z9 32
U1 0
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2020
VL 26
IS 5
BP 1871
EP 1879
DI 10.1109/TVCG.2020.2973443
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA LA1WU
UT WOS:000523746000006
PM 32070978
DA 2025-03-07
ER

PT J
AU Chen, X
   Li, YW
   Luo, X
   Shao, TJ
   Yu, JY
   Zhou, K
   Zheng, YY
AF Chen, Xin
   Li, Yuwei
   Luo, Xi
   Shao, Tianjia
   Yu, Jingyi
   Zhou, Kun
   Zheng, Youyi
TI <i>AutoSweep</i>: Recovering 3D Editable Objects from a Single
   Photograph
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Solid modeling; Image segmentation; Shape;
   Trajectory; Semantics; Geometry; Editable objects; instance-aware
   segmentation; sweep surfaces
ID CALIBRATION
AB This paper presents a fully automatic framework for extracting editable 3D objects directly from a single photograph. Unlike previous methods which recover either depth maps, point clouds, or mesh surfaces, we aim to recover 3D objects with semantic parts and can be directly edited. We base our work on the assumption that most human-made objects are constituted by parts and these parts can be well represented by generalized primitives. Our work makes an attempt towards recovering two types of primitive-shaped objects, namely, generalized cuboids and generalized cylinders. To this end, we build a novel instance-aware segmentation network for accurate part separation. Our GeoNet outputs a set of smooth part-level masks labeled as profiles and bodies. Then in a key stage, we simultaneously identify profile-body relations and recover 3D parts by sweeping the recognized profile along their body contour and jointly optimize the geometry to align with the recovered masks. Qualitative and quantitative experiments show that our algorithm can recover high quality 3D models and outperforms existing methods in both instance segmentation and 3D reconstruction.
C1 [Chen, Xin; Li, Yuwei; Luo, Xi; Yu, Jingyi] ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China.
   [Shao, Tianjia] Univ Leeds, Sch Comp, Leeds LS2 9JT, W Yorkshire, England.
   [Zhou, Kun; Zheng, Youyi] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
C3 ShanghaiTech University; University of Leeds; Zhejiang University
RP Zheng, YY (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Peoples R China.
EM chenxin@shanghaitech.edu.cn; liyuwei@shanghaitech.edu.cn;
   luoxi@shanghaitech.edu.cn; T.Shao@leeds.ac.uk;
   jingyiyu1@shanghaitech.edu.cn; kunzhou@cad.zju.edu.cn;
   youyizheng@zju.edu.cn
RI Chen, Xin/AAE-5265-2020; Zhou, Kun/ITT-3967-2023
OI Li, Yuwei/0000-0001-7606-9096; Luo, Xi/0000-0001-5329-0636; Chen,
   Xin/0000-0002-9347-1367
FU National Key Research & Development Program of China [2016YFB1001403];
   National Natural Science Foundation of China [61502306, 61772462,
   U1736217, U1609215]; Microsoft Research Asia; Programs of Science and
   Technology Commission of Shanghai Municipality [17JC1403800]; Shanghai
   Academic/Technology Research Leader [17XD1402900]; China Young 1000
   Talents Program
FX This work was supported in part by the National Key Research &
   Development Program of China (2016YFB1001403), the National Natural
   Science Foundation of China No. 61502306, No. 61772462, No. U1736217,
   No. U1609215, Microsoft Research Asia, the Programs of Science and
   Technology Commission of Shanghai Municipality (17JC1403800), Shanghai
   Academic/Technology Research Leader (17XD1402900), and the China Young
   1000 Talents Program.
NR 64
TC 10
Z9 10
U1 1
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2020
VL 26
IS 3
BP 1466
EP 1475
DI 10.1109/TVCG.2018.2871190
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KH5VW
UT WOS:000510719400004
PM 30235138
OA Green Accepted, Green Submitted
DA 2025-03-07
ER

PT J
AU Meyer, M
   Dykes, J
AF Meyer, Miriah
   Dykes, Jason
TI Criteria for Rigor in Visualization Design Study
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE design study; relativism; interpretivism; knowledge construction;
   qualitative research; research through design
ID PROVENANCE; SCIENCE; TOOL; RESEARCHER
AB We develop a new perspective on research conducted through visualization design study that emphasizes design as a method of inquiry and the broad range of knowledge-contributions achieved through it as multiple, subjective, and socially constructed. From this interpretivist position we explore the nature of visualization design study and develop six criteria for rigor. We propose that rigor is established and judged according to the extent to which visualization design study research and its reporting are. This perspective and the criteria were constructed through a four-year engagement with the discourse around rigor and the nature of knowledge in social science, information systems, and design. We suggest methods from cognate disciplines that can support visualization researchers in meeting these criteria during the planning, execution, and reporting of design study. Through a series of deliberately provocative questions, we explore implications of this new perspective for design study research in visualization, concluding that as a discipline, visualization is not yet well positioned to embrace, nurture, and fully benefit from a rigorous, interpretivist approach to design study. The perspective and criteria we present are intended to stimulate dialogue and debate around the nature of visualization design study and the broader underpinnings of the discipline.
C1 [Meyer, Miriah] Univ Utah, Salt Lake City, UT 84112 USA.
   [Dykes, Jason] City Univ London, London, England.
C3 Utah System of Higher Education; University of Utah; City St Georges,
   University of London; City, University of London
RP Meyer, M (corresponding author), Univ Utah, Salt Lake City, UT 84112 USA.
EM miriah@cs.utah.edu; j.dykes@city.ac.uk
RI Dykes, Jason/AAD-6067-2021
NR 120
TC 63
Z9 74
U1 2
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 87
EP 97
DI 10.1109/TVCG.2019.2934539
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA KB0CF
UT WOS:000506166100009
PM 31442986
OA Green Accepted, Green Submitted
DA 2025-03-07
ER

PT J
AU Rathore, R
   Leggon, Z
   Lessard, L
   Schloss, KB
AF Rathore, Ragini
   Leggon, Zachary
   Lessard, Laurent
   Schloss, Karen B.
TI Estimating Color-Concept Associations from Image Statistics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual Reasoning; Visual Communication; Visual Encoding; Color
   Perception; Color Cognition; Color Categories
ID CATEGORIES; LANGUAGES
AB To interpret the meanings of colors in visualizations of categorical information, people must determine how distinct colors correspond to different concepts. This process is easier when assignments between colors and concepts in visualizations match people's expectations, making color palettes semantically interpretable. Efforts have been underway to optimize color palette design for semantic interpretablity, but this requires having good estimates of human color-concept associations. Obtaining these data from humans is costly, which motivates the need for automated methods. We developed and evaluated a new method for automatically estimating color-concept associations in a way that strongly correlates with human ratings. Building on prior studies using Google Images, our approach operates directly on Google Image search results without the need for humans in the loop. Specifically, we evaluated several methods for extracting raw pixel content of the images in order to best estimate color-concept associations obtained from human ratings. The most effective method extracted colors using a combination of cylindrical sectors and color categories in color space. We demonstrate that our approach can accurately estimate average human color-concept associations for different fruits using only a small set of images. The approach also generalizes moderately well to more complicated recycling-related concepts of objects that can appear in any color.
C1 [Rathore, Ragini] Univ Wisconsin, Comp Sci, Madison, WI 53706 USA.
   [Rathore, Ragini; Leggon, Zachary; Lessard, Laurent; Schloss, Karen B.] Univ Wisconsin, WID, Madison, WI 53706 USA.
   [Leggon, Zachary] Univ Wisconsin, Biol, Madison, WI 53706 USA.
   [Lessard, Laurent] Univ Wisconsin, Elect & Comp Engn, Madison, WI 53706 USA.
   [Schloss, Karen B.] Univ Wisconsin, Psychol, Madison, WI 53706 USA.
C3 University of Wisconsin System; University of Wisconsin Madison;
   University of Wisconsin System; University of Wisconsin Madison;
   University of Wisconsin System; University of Wisconsin Madison;
   University of Wisconsin System; University of Wisconsin Madison;
   University of Wisconsin System; University of Wisconsin Madison
RP Rathore, R (corresponding author), Univ Wisconsin, Comp Sci, Madison, WI 53706 USA.; Rathore, R (corresponding author), Univ Wisconsin, WID, Madison, WI 53706 USA.
EM rrathore3@wisc.edu; zleggon@wisc.edu; laurent.lessard@wisc.edu;
   kschloss@wisc.edu
OI Schloss, Karen/0000-0003-4833-4117; Leggon, Zachary/0000-0003-4716-1488
FU Office of the Vice Chancellor for Research and Graduate Education at
   UW-Madison; Wisconsin Alumni Research Foundation
FX The authors thank Christoph Witzel, Brian Yin, John Curtain, Joris Roos,
   Anna Bartel, and Emily Ward for their thoughtful feedback on this work,
   and Melissa Schoenlein, Shannon Sibrel, Autumn Wickman, Yuke Liang, and
   Marin Murack for their help with data collection. This work was
   supported in part by the Office of the Vice Chancellor for Research and
   Graduate Education at UW-Madison and the Wisconsin Alumni Research
   Foundation. The funding bodies played no role in designing the study,
   collecting, analyzing, or interpreting the data, or writing the
   manuscript.
NR 53
TC 11
Z9 14
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 1226
EP 1235
DI 10.1109/TVCG.2019.2934536
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA KB0CF
UT WOS:000506166100113
PM 31442984
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Vidal, J
   Budin, J
   Tierny, J
AF Vidal, Jules
   Budin, Joseph
   Tierny, Julien
TI Progressive Wasserstein Barycenters of Persistence Diagrams
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Topological data analysis; scalar data; ensemble data
AB This paper presents an efficient algorithm for the progressive approximation of Wasserstein barycenters of persistence diagrams, with applications to the visual analysis of ensemble data. Given a set of scalar fields, our approach enables the computation of a persistence diagram which is representative of the set, and which visually conveys the number, data ranges and saliences of the main features of interest found in the set. Such representative diagrams are obtained by computing explicitly the discrete Wasserstein barycenter of the set of persistence diagrams, a notoriously computationally intensive task. In particular, we revisit efficient algorithms for Wasserstein distance approximation to extend previous work on barycenter estimation. We present a new fast algorithm, which progressively approximates the barycenter by iteratively increasing the computation accuracy as well as the number of persistent features in the output diagram. Such a progressivity drastically improves convergence in practice and allows to design an interruptible algorithm, capable of respecting computation time constraints. This enables the approximation of Wasserstein barycenters within interactive times. We present an application to ensemble clustering where we revisit the-means algorithm to exploit our barycenters and compute, within execution time constraints, meaningful clusters of ensemble data along with their barycenter diagram. Extensive experiments on synthetic and real-life data sets report that our algorithm converges to barycenters that are qualitatively meaningful with regard to the applications, and quantitatively comparable to previous techniques, while offering an order of magnitude speedup when run until convergence (without time constraint). Our algorithm can be trivially parallelized to provide additional speedups in practice on standard workstations. We provide a lightweight C++ implementation of our approach that can be used to reproduce our results.
C1 [Vidal, Jules; Budin, Joseph; Tierny, Julien] Sorbonne Univ, Paris, France.
   [Vidal, Jules; Budin, Joseph; Tierny, Julien] CNRS, LIP6, Paris, France.
C3 Sorbonne Universite; Sorbonne Universite; Centre National de la
   Recherche Scientifique (CNRS)
RP Vidal, J (corresponding author), Sorbonne Univ, Paris, France.
EM jules.vidal@sorbonne-universite.fr; joseph.budin@sorbonne-universite.fr;
   julien.tierny@sorbonne-universite.fr
OI Vidal, Jules/0000-0002-1154-4391
FU European Commission grant H2020-FETHPC-2017 "VESTEC" [800904]
FX The authors would like to thank T. Lacombe, M. Cuturi and S. Oudot for
   sharing an implementation of their approach [53]. We also thank the
   reviewers for their thoughtful remarks and suggestions. This work is
   partially supported by the European Commission grant H2020-FETHPC-2017
   "VESTEC" (ref. 800904).
NR 96
TC 18
Z9 19
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 151
EP 161
DI 10.1109/TVCG.2019.2934256
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100014
PM 31403427
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Waldner, M
   Diehl, A
   Gracanin, D
   Splechtna, R
   Delrieux, C
   Matkovic, K
AF Waldner, Manuela
   Diehl, Alexandra
   Gracanin, Denis
   Splechtna, Rainer
   Delrieux, Claudio
   Matkovic, Kresimir
TI A Comparison of Radial and Linear Charts for Visualizing Daily Patterns
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Radial charts; time series series data; daily patterns; crowd-sourced
   experiment
ID INFORMATION VISUALIZATION; PERCEPTION
AB Radial charts are generally considered less effective than linear charts. Perhaps the only exception is in visualizing periodical time-dependent data, which is believed to be naturally supported by the radial layout. It has been demonstrated that the drawbacks of radial charts outweigh the benefits of this natural mapping. Visualization of daily patterns, as a special case, has not been systematically evaluated using radial charts. In contrast to yearly or weekly recurrent trends, the analysis of daily patterns on a radial chart may benefit from our trained skill on reading radial clocks that are ubiquitous in our culture. In a crowd-sourced experiment with 92 non-expert users, we evaluated the accuracy, efficiency, and subjective ratings of radial and linear charts for visualizing daily traffic accident patterns. We systematically compared juxtaposed 12-hours variants and single 24-hours variants for both layouts in four low-level tasks and one high-level interpretation task. Our results show that over all tasks, the most elementary 24-hours linear bar chart is most accurate and efficient and is also preferred by the users. This provides strong evidence for the use of linear layouts even for visualizing periodical daily patterns.
C1 [Waldner, Manuela] TU Wien, Vienna, Austria.
   [Diehl, Alexandra] Univ Zurich, Zurich, Switzerland.
   [Gracanin, Denis] Virginia Tech, Blacksburg, VA USA.
   [Splechtna, Rainer; Matkovic, Kresimir] VRVis Res Ctr, Vienna, Austria.
   [Delrieux, Claudio] Univ Nacl Sur, Elect & Comp Eng Dept, Bahia Blanca, Buenos Aires, Argentina.
C3 Technische Universitat Wien; University of Zurich; Virginia Polytechnic
   Institute & State University; National University of the South;
   Instituto de Investigaciones en Ingenieria Electrica (IIIE)
RP Waldner, M (corresponding author), TU Wien, Vienna, Austria.
EM waldner@cg.tuwien.ac.at; diehl@ifi.uzh.ch; gracanin@vt.edu;
   Splechtna@VRVis.at; cad@uns.edu.ar; Matkovic@VRVis.at
RI Diehl, Alexandra/Y-7176-2019; Delrieux, Claudio/O-8917-2019; Waldner,
   Manuela/JZC-9267-2024; Matkovic, Kresimir/AAT-8896-2021
OI Waldner, Manuela/0000-0003-1387-5132; Matkovic,
   Kresimir/0000-0001-9406-8943; Delrieux, Claudio/0000-0002-2727-8374;
   Diehl, Alexandra/0000-0002-2943-4051
FU Austrian Science Fund (FWF) [T 752-N30]; Universidad Nacional del Sur
   (Argentina) [PGI 24/K061]; VRVis [854174]; Austrian Science Fund (FWF)
   [T752] Funding Source: Austrian Science Fund (FWF)
FX This work was partially financed by the Austrian Science Fund (FWF): T
   752-N30 and by grant PGI 24/K061 of the Universidad Nacional del Sur
   (Argentina). This work was partially written in scope of the COMET
   program at VRVis (854174). The authors would like to thank Gemza Ademaj
   for open coding support.
NR 54
TC 24
Z9 28
U1 1
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 1033
EP 1042
DI 10.1109/TVCG.2019.2934784
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100096
PM 31443015
DA 2025-03-07
ER

PT J
AU Wentzel, A
   Hanula, P
   Luciani, T
   Elgohari, B
   Elhalawani, H
   Canahuate, G
   Vock, D
   Fuller, CD
   Marai, GE
AF Wentzel, A.
   Hanula, P.
   Luciani, T.
   Elgohari, B.
   Elhalawani, H.
   Canahuate, G.
   Vock, D.
   Fuller, C. D.
   Marai, G. E.
TI Cohort-based T-SSIM Visual Computing for Radiation Therapy Prediction
   and Exploration
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Biomedical and Medical Visualization; Spatial Techniques; Visual Design;
   High-Dimensional Data
ID VISUALIZATION; MODEL
AB We describe a visual computing approach to radiation therapy (RT) planning, based on spatial similarity within a patient cohort. In radiotherapy for head and neck cancer treatment, dosage to organs at risk surrounding a tumor is a large cause of treatment toxicity. Along with the availability of patient repositories, this situation has lead to clinician interest in understanding and predicting RT outcomes based on previously treated similar patients. To enable this type of analysis, we introduce a novel topology-based spatial similarity measure, T-SSIM, and a predictive algorithm based on this similarity measure. We couple the algorithm with a visual steering interface that intertwines visual encodings for the spatial data and statistical results, including a novel parallel-marker encoding that is spatially aware. We report quantitative results on a cohort of 165 patients, as well as a qualitative evaluation with domain experts in radiation oncology, data management, biostatistics, and medical imaging, who are collaborating remotely.
C1 [Wentzel, A.; Hanula, P.; Luciani, T.; Marai, G. E.] Univ Illinois, Chicago, IL 60680 USA.
   [Elgohari, B.; Elhalawani, H.; Fuller, C. D.] Univ Texas Houston, MD Anderson Canc Ctr, 1515 Holcombe Blvd, Houston, TX 77030 USA.
   [Canahuate, G.] Univ Iowa, Iowa City, IA 52242 USA.
   [Vock, D.] Univ Minnesota, Sch Publ Hlth, Minneapolis, MN 55455 USA.
C3 University of Illinois System; University of Illinois Chicago;
   University of Illinois Chicago Hospital; University of Texas System;
   University of Texas Health Science Center Houston; UTMD Anderson Cancer
   Center; University of Iowa; University of Minnesota System; University
   of Minnesota Twin Cities
RP Wentzel, A (corresponding author), Univ Illinois, Chicago, IL 60680 USA.
EM awentze2@uic.edu; gmarai@uic.edu
RI Elhalawani, Hesham/R-7172-2019; Fuller, Clifton/AAB-4012-2019
OI Elhalawani, Hesham/0000-0001-9848-2623; Fuller, Clifton
   D./0000-0002-5264-3994; Vock, David M/0000-0002-5459-9579; Elgohari,
   Baher/0000-0001-7694-6466
FU National Institutes of Health [NCI-R01-CA214825, NCI-R01CA225190];
   National Science Foundation [CNS-1625941, CNS-1828265]
FX This work was supported by the National Institutes of Health
   [NCI-R01-CA214825, NCI-R01CA225190] and the National Science Foundation
   [CNS-1625941, CNS-1828265]. We thank all members of the Electronic
   Visualization Laboratory, members of the MD Anderson Head and Neck
   Cancer Quantitative Imaging Collaborative Group, and our collaborators
   at the University of Iowa and University of Minnesota.
NR 60
TC 23
Z9 24
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 949
EP 959
DI 10.1109/TVCG.2019.2934546
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100088
PM 31442988
OA Green Accepted, Green Submitted
DA 2025-03-07
ER

PT J
AU Kruijff, E
   Orlosky, J
   Kishishita, N
   Trepkowski, C
   Kiyokawa, K
AF Kruijff, Ernst
   Orlosky, Jason
   Kishishita, Naohiro
   Trepkowski, Christina
   Kiyokawa, Kiyoshi
TI The Influence of Label Design on Search Performance and Noticeability in
   Wide Field of View Augmented Reality Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Augmented reality; head mounted display; perception; peripheral vision;
   visualization
ID VISUAL-SEARCH; OF-VIEW; MOTION; ASYMMETRIES; LEGIBILITY; OBJECTS; COLOR;
   USER
AB In Augmented Reality (AR), search performance for outdoor tasks is an important metric for evaluating the success of a large number of AR applications. Users must be able to find content quickly, labels and indicators must not be invasive but still clearly noticeable, and the user interface should maximize search performance in a variety of conditions. To address these issues, we have set up a series of experiments to test the influence of virtual characteristics such as color, size, and leader lines on the performance of search tasks and noticeability in both real and simulated environments. We evaluate two primary areas, including 1) the effects of peripheral field of view (FOV) limitations and labeling techniques on target acquisition during outdoor mobile search, and 2) the influence of local characteristics such as color, size, and motion on text labels over dynamic backgrounds. The first experiment showed that limited FOV will severely limit search performance, but that appropriate placement of labels and leaders within the periphery can alleviate this problem without interfering with walking or decreasing user comfort. In the second experiment, we found that different types of motion are more noticeable in optical versus video see-through displays, but that blue coloration is most noticeable in both. Results can aid in designing more effective view management techniques, especially for wider field of view displays.
C1 [Kruijff, Ernst] Bonn Rhein Sieg Univ Appl Sci, Inst Visual Comp, Comp Graph & Interact Syst, Grantham Allee 20, D-53757 St Augustin, Germany.
   [Kruijff, Ernst] Simon Fraser Univ, Grantham Allee 20, D-53757 St Augustin, Germany.
   [Orlosky, Jason; Kishishita, Naohiro] Osaka Univ, 1-32 Machikaneyama, Toyonaka, Osaka 5600043, Japan.
   [Trepkowski, Christina] Univ Bonn Rhein Sieg, Univ Appl Sci, Grantham Allee 20, D-53757 St Augustin, Germany.
   [Kiyokawa, Kiyoshi] Nara Inst Sci & Technol, 8916-5 Takayama Cho, Ikoma, Nara 6300192, Japan.
C3 Hochschule Bonn Rhein Sieg; Osaka University; Hochschule Bonn Rhein
   Sieg; Nara Institute of Science & Technology
RP Kruijff, E (corresponding author), Bonn Rhein Sieg Univ Appl Sci, Inst Visual Comp, Comp Graph & Interact Syst, Grantham Allee 20, D-53757 St Augustin, Germany.; Kruijff, E (corresponding author), Simon Fraser Univ, Grantham Allee 20, D-53757 St Augustin, Germany.
EM ernst.kruiff@h-brs.de; orlosky@lab.ime.cmc.osaka-u.ac.jp;
   n.kishishita@jp.fujitsu.com; christina.trepkowski@h-brs.de;
   kiyo@is.naist.jp
OI Orlosky, Jason/0000-0002-0538-6630; Kruijff, Ernst/0000-0003-1625-0955
FU Deutsche Forschungsgemeinschaft (DFG) [Kr 4531/1-1, 4531/2-1]; United
   States Department of the Navy, Office of Naval Research Grant
   [N62909-18-1-2036]; JSPS [15H02738]; Grants-in-Aid for Scientific
   Research [15H02738] Funding Source: KAKEN
FX This work was partially funded through the Deutsche
   Forschungsgemeinschaft (DFG, grants Kr 4531/1-1 and 4531/2-1), partially
   by the United States Department of the Navy, Office of Naval Research
   Grant #N62909-18-1-2036, and the JSPS Grant-in-aid for Scientific
   Research #15H02738. We also would like to thank all the participants for
   their time and effort while participating in the experiments. Finally,
   we kindly thank Katharina Stollenwerk for producing the optical flow
   density figures in MATLAB.
NR 70
TC 24
Z9 27
U1 2
U2 36
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2019
VL 25
IS 9
BP 2821
EP 2837
DI 10.1109/TVCG.2018.2854737
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA IN8OV
UT WOS:000478940300010
PM 30004877
DA 2025-03-07
ER

PT J
AU Rocha, A
   Silva, JD
   Alim, UR
   Carpendale, S
   Sousa, MC
AF Rocha, Allan
   Silva, Julio Daniel
   Alim, Usman R.
   Carpendale, Sheelagh
   Sousa, Mario Costa
TI Decal-Lenses: Interactive Lenses on Surfaces for Multivariate
   Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Focus plus context; lenses; interaction; design; multivariate;
   visualization; surfaces; decal
ID FLOW VISUALIZATION; BLOOD-FLOW; EXPLORATION; STRESS
AB We present decal-lenses, a new interaction technique that extends the concept of magic lenses to augment and manage multivariate visualizations on arbitrary surfaces. Our object-space lenses follow the surface geometry and allow the user to change the point of view during data exploration while maintaining a spatial reference to positions where one or more lenses were placed. Each lens delimits specific regions of the surface where one or more attributes can be selected or combined. Similar to 2D lenses, the user interacts with our lenses in real-time, switching between different attributes within the lens context. The user can also visualize the surface data representations from the point of view of each lens by using local cameras. To place lenses on surfaces of intricate geometry, such as the human brain, we introduce the concept of support surfaces for designing interaction techniques. Support surfaces provide a way to place and interact with the lenses while avoiding holes and occluded regions during data exploration. We further extend decal-lenses to arbitrary regions using brushing and lassoing operations. We discuss the applicability of our technique and present several examples where our lenses can be useful to create a customized exploration of multivariate data on surfaces.
C1 [Rocha, Allan; Silva, Julio Daniel; Alim, Usman R.; Carpendale, Sheelagh; Sousa, Mario Costa] Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
C3 University of Calgary
RP Rocha, A (corresponding author), Univ Calgary, Dept Comp Sci, Calgary, AB T2N 1N4, Canada.
EM acarocha@ucalgary.ca; jd.silva@ucalgary.ca; ualim@ucalgary.ca;
   sheelagh@ucalgary.ca; smcosta@ucalgary.ca
OI Rocha, Allan/0000-0002-1868-993X; Costa Sousa, Mario/0000-0002-4347-0884
FU NSERC/AITF/FCMG Industrial Research Chair Program in Scalable Reservoir
   Visualization; NSERC
FX We would like to thank the anonymous reviewers for their constructive
   comments and feedback. This research was supported in part by the
   NSERC/AITF/FCMG Industrial Research Chair Program in Scalable Reservoir
   Visualization and by Discovery Grants from NSERC.
NR 60
TC 9
Z9 9
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2019
VL 25
IS 8
BP 2568
EP 2582
DI 10.1109/TVCG.2018.2850781
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IG2AT
UT WOS:000473597800005
PM 29994679
DA 2025-03-07
ER

PT J
AU Tian, H
   Wang, CB
   Manocha, D
   Zhang, XY
AF Tian, Hao
   Wang, Changbo
   Manocha, Dinesh
   Zhang, Xinyu
TI Realtime Hand-Object Interaction Using Learned Grasp Space for Virtual
   Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Plausible hand interaction; virtual grasping; object manipulation;
   contact space
ID ALGORITHM
AB We present a realtime virtual grasping algorithm to model interactions with virtual objects. Our approach is designed for multi-fingered hands and makes no assumptions about the motion of the user's hand or the virtual objects. Given a model of the virtual hand, we use machine learning and particle swarm optimization to automatically pre-compute stable grasp configurations for that object. The learning pre-computation step is accelerated using GPU parallelization. At runtime, we rely on the pre-computed stable grasp configurations, and dynamics/non-penetration constraints along with motion planning techniques to compute plausible looking grasps. In practice, our realtime algorithm can perform virtual grasping operations in less than 20ms for complex virtual objects, including high genus objects with holes. We have integrated our grasping algorithm with Oculus Rift HMD and Leap Motion controller and evaluated its performance for different tasks corresponding to grabbing virtual objects and placing them at arbitrary locations. Our user evaluation suggests that our virtual grasping algorithm can increase the user's realism and participation in these tasks and offers considerable benefits over prior interaction algorithms, such as pinch grasping and raycast picking.
C1 [Tian, Hao; Wang, Changbo; Zhang, Xinyu] East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200062, Peoples R China.
   [Manocha, Dinesh] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
   [Manocha, Dinesh] Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.
C3 East China Normal University; University System of Maryland; University
   of Maryland College Park; University System of Maryland; University of
   Maryland College Park
RP Zhang, XY (corresponding author), East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai 200062, Peoples R China.
EM nicktian.ecnu@gmail.com; cbwang@sei.ecnu.edu.cn; dm@cs.umd.edu;
   xyzhang@sei.ecnu.edu.cn
OI Zhang, Xinyu/0000-0001-5000-2483; Tian, Hao/0000-0003-4612-9777;
   Manocha, Dinesh/0000-0001-7047-9801
FU NSFC [61631166002, 61572196, 61532002, 61672237]; National High-tech R&D
   Program of China (863 Program) [2015AA016404]
FX This research work was supported by the NSFC (No. 61631166002, No.
   61572196, No. 61532002, No. 61672237), National High-tech R&D Program of
   China (863 Program) under Grant 2015AA016404.
NR 60
TC 14
Z9 19
U1 3
U2 39
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2019
VL 25
IS 8
BP 2623
EP 2635
DI 10.1109/TVCG.2018.2849381
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IG2AT
UT WOS:000473597800009
PM 29994119
DA 2025-03-07
ER

PT J
AU Boukhayma, A
   Boyer, E
AF Boukhayma, Adnane
   Boyer, Edmond
TI Surface Motion Capture Animation Synthesis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Character animation; 3D video; multiview reconstruction; video-based
   animation; 4D modeling; 4D performance capture
ID ALIGNMENT; MODEL
AB We propose to generate novel animations from a set of elementary examples of video-based surface motion capture, under user-specified constraints. 4D surface capture animation is motivated by the increasing demand from media production for highly realistic 3D content. To this aim, data driven strategies that consider video-based information can produce animation with real shapes, kinematics and appearances. Our animations rely on the combination and the interpolation of textured 3D mesh data, which requires examining two aspects: (1) Shape geometry and (2) appearance. First, we propose an animation synthesis structure for the shape geometry, the Essential graph, that outperforms standard Motion graphs in optimality with respect to quantitative criteria, and we extend optimized interpolated transition algorithms to mesh data. Second, we propose a compact view-independent representation for the shape appearance. This representation encodes subject appearance changes due to viewpoint and illumination, and due to inaccuracies in geometric modelling independently. Besides providing compact representations, such decompositions allow for additional applications such as interpolation for animation.
C1 [Boukhayma, Adnane; Boyer, Edmond] Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France.
C3 Communaute Universite Grenoble Alpes; Universite Grenoble Alpes (UGA);
   Institut National Polytechnique de Grenoble; Inria; Centre National de
   la Recherche Scientifique (CNRS)
RP Boukhayma, A (corresponding author), Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France.
EM adnane.boukhayma@gmail.com; edmond.boyer@inria.fr
NR 55
TC 9
Z9 10
U1 0
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2019
VL 25
IS 6
BP 2270
EP 2283
DI 10.1109/TVCG.2018.2831233
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HW8AC
UT WOS:000466910200011
PM 29993745
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Berger, M
   Li, JX
   Levine, JA
AF Berger, Matthew
   Li, Jixian
   Levine, Joshua A.
TI A Generative Model for Volume Rendering
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Volume rendering; generative models; deep learning; generative
   adversarial networks
ID OF-THE-ART; VISUALIZATION; EXPLORATION; HISTOGRAMS; FRAMEWORK; DESIGN;
   TEXT
AB We present a technique to synthesize and analyze volume-rendered images using generative models. We use the Generative Adversarial Network (GAN) framework to compute a model from a large collection of volume renderings, conditioned on (1) viewpoint and (2) transfer functions for opacity and color. Our approach facilitates tasks for volume analysis that are challenging to achieve using existing rendering techniques such as ray casting or texture-based methods. We show how to guide the user in transfer function editing by quantifying expected change in the output image. Additionally, the generative model transforms transfer functions into a view-invariant latent space specifically designed to synthesize volume-rendered images. We use this space directly for rendering, enabling the user to explore the space of volume-rendered images. As our model is independent of the choice of volume rendering process, we show how to analyze volume-rendered images produced by direct and global illumination lighting, for a variety of volume datasets.
C1 [Berger, Matthew; Li, Jixian; Levine, Joshua A.] Univ Arizona, Dept Comp Sci, Tucson, AZ 85721 USA.
C3 University of Arizona
RP Berger, M (corresponding author), Univ Arizona, Dept Comp Sci, Tucson, AZ 85721 USA.
EM matthewberger@email.arizona.edu; jixianli@email.arizona.edu;
   josh@email.arizona.edu
RI Li, Jixian/HKV-6024-2023
OI Li, Jixian/0000-0002-6613-2773
FU US National Science Foundation [IIS-1654221]; Direct For Computer & Info
   Scie & Enginr [1314813] Funding Source: National Science Foundation; Div
   Of Information & Intelligent Systems [1314813] Funding Source: National
   Science Foundation; Div Of Information & Intelligent Systems; Direct For
   Computer & Info Scie & Enginr [1654221] Funding Source: National Science
   Foundation; Div Of Information & Intelligent Systems; Direct For
   Computer & Info Scie & Enginr [1314896] Funding Source: National Science
   Foundation
FX We thank Peer-Timo Bremer for stimulating discussions. This work was
   partially supported by the US National Science Foundation IIS-1654221.
NR 53
TC 53
Z9 67
U1 5
U2 50
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2019
VL 25
IS 4
BP 1636
EP 1650
DI 10.1109/TVCG.2018.2816059
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HN6TU
UT WOS:000460319500001
PM 29993811
OA Bronze, Green Submitted
DA 2025-03-07
ER

PT J
AU Chen, W
   Guo, FZ
   Han, DM
   Pan, JC
   Nie, XT
   Xia, JZ
   Zhang, XL
AF Chen, Wei
   Guo, Fangzhou
   Han, Dongming
   Pan, Jacheng
   Nie, Xiaotao
   Xia, Jiazhi
   Zhang, Xiaolong
TI Structure-Based Suggestive Exploration: A New Approach for Effective
   Exploration of Large Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Large Network Exploration; Structure-Based Exploration; Suggestive
   Exploration
ID SCALE GRAPH VISUALIZATION; VISUAL ANALYSIS; INTERACTIVE EXPLORATION;
   LAYOUT
AB When analyzing a visualized network, users need to explore different sections of the network to gain insight. However, effective exploration of large networks is often a challenge. While various tools are available for users to explore the global and local features of a network, these tools usually require significant interaction activities, such as repetitive navigation actions to follow network nodes and edges. In this paper, we propose a structure-based suggestive exploration approach to support effective exploration of large networks by suggesting appropriate structures upon user request. Encoding nodes with vectorized representations by transforming information of surrounding structures of nodes into a high dimensional space, our approach can identify similar structures within a large network, enable user interaction with multiple similar structures simultaneously, and guide the exploration of unexplored structures. We develop a web-based visual exploration system to incorporate this suggestive exploration approach and compare performances of our approach under different vectorizing methods and networks. We also present the usability and effectiveness of our approach through a controlled user study with two datasets.
C1 [Chen, Wei; Guo, Fangzhou; Han, Dongming; Pan, Jacheng; Nie, Xiaotao] Zhejiang Univ, State Key Labo CAD & CG, Hangzhou, Zhejiang, Peoples R China.
   [Xia, Jiazhi] Cent S Univ, Changsha, Hunan, Peoples R China.
   [Zhang, Xiaolong] Penn State Univ, University Pk, PA 16802 USA.
C3 Zhejiang University; Central South University; Pennsylvania Commonwealth
   System of Higher Education (PCSHE); Pennsylvania State University;
   Pennsylvania State University - University Park
RP Chen, W (corresponding author), Zhejiang Univ, State Key Labo CAD & CG, Hangzhou, Zhejiang, Peoples R China.
EM chenwei@cad.zju.edu.cn; guofangzhou@zju.edu.cn; dongminghan@zju.edu.cn;
   anxis@zju.edu.cn; net@zju.edu.cn; xiajiazhi@csu.edu.cn;
   lzhang@ist.psu.edu
RI Chen, Wei/AAR-9817-2020; ZHANG, XIAOLONG/IZQ-4553-2023
OI Zhang, Xiaolong/0000-0002-6828-4930
FU National Key Research and Development Program [2018YFB0904503]; National
   Natural Science Foundation of China [61772456, 61761136020, U1736109]
FX This research has been sponsored supported by National Key Research and
   Development Program (2018YFB0904503), National Natural Science
   Foundation of China (61772456, 61761136020, U1736109).
NR 78
TC 35
Z9 40
U1 0
U2 27
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 555
EP 565
DI 10.1109/TVCG.2018.2865139
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000053
PM 30136986
DA 2025-03-07
ER

PT J
AU Goodall, JR
   Ragan, ED
   Steed, CA
   Reed, JW
   Richardson, GD
   Huffer, KMT
   Bridges, RA
   Laska, JA
AF Goodall, John R.
   Ragan, Eric D.
   Steed, Chad A.
   Reed, Joel W.
   Richardson, G. David
   Huffer, Kelly M. T.
   Bridges, Robert A.
   Laska, Jason A.
TI Situ: Identifying and Explaining Suspicious Behavior in Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Network security; situational awareness; privacy and security; streaming
   data; machine learning; visualization
AB Despite the best efforts of cyber security analysts, networked computing assets are routinely compromised, resulting in the loss of intellectual property, the disclosure of state secrets, and major financial damages. Anomaly detection methods are beneficial for detecting new types of attacks and abnormal network activity, but such algorithms can be difficult to understand and trust. Network operators and cyber analysts need fast and scalable tools to help identify suspicious behavior that bypasses automated security systems, but operators do not want another automated tool with algorithms they do not trust. Experts need tools to augment their own domain expertise and to provide a contextual understanding of suspicious behavior to help them make decisions. In this paper we present Situ, a visual analytics system for discovering suspicious behavior in streaming network data. Situ provides a scalable solution that combines anomaly detection with information visualization. The system's visualizations enable operators to identify and investigate the most anomalous events and IP addresses, and the tool provides context to help operators understand why they are anomalous. Finally; operators need tools that can be integrated into their workflow and with their existing tools. This paper describes the Situ platform and its deployment in an operational network setting. We discuss how operators are currently using the tool in a large organization's security operations center and present the results of expert reviews with professionals.
C1 [Goodall, John R.; Steed, Chad A.; Reed, Joel W.; Richardson, G. David; Huffer, Kelly M. T.; Bridges, Robert A.; Laska, Jason A.] Oak Ridge Natl Lab, Oak Ridge, TN 37830 USA.
   [Ragan, Eric D.] Univ Florida, Gainesville, FL 32611 USA.
C3 United States Department of Energy (DOE); Oak Ridge National Laboratory;
   State University System of Florida; University of Florida
RP Goodall, JR (corresponding author), Oak Ridge Natl Lab, Oak Ridge, TN 37830 USA.
EM jgoodall@ornl.gov; eragan@ufl.edu; steedca@ornl.gov; reedjw@ornl.gov;
   richardsongd@ornl.gov; hufferkm@ornl.gov; bridgesra@ornl.gov;
   laskaja@ornl.gov
RI Goodall, John/A-4250-2010; Steed, Chad/AAA-7502-2021
OI Richardson, Gregory/0000-0002-2200-9386
FU US Department of Energy [DE-AC05-00OR22725]; Laboratory Directed
   Research and Development Program of ORNL; Dept. of Homeland Security
   Science & Technology Directorate, HSARPA, Cyber Security Division under
   the Transition to Practice program; DARPA XAI program
   [N66001-17-2-4031]; IARPA
FX This manuscript has been authored by UT-Battelle, LLC under Contract No.
   DE-AC05-00OR22725 with the US Department of Energy. The US Government
   retains and the publisher, by accepting the article for publication,
   acknowledges that the US Government retains a non-exclusive, paid-up,
   irrevocable, worldwide license to publish or reproduce the published
   form of this manuscript, or allow others to do so, for US Government
   purposes. The Department of Energy will provide public access to these
   results of federally sponsored research in accordance with the DOE
   Public Access Plan (http://energy.gov/downloads/doe-public-access-plan).
   Research sponsored by the Laboratory Directed Research and Development
   Program of ORNL, managed by UT-Battelle, LLC, for the U.S. DOE. This
   work was supported by the Dept. of Homeland Security Science &
   Technology Directorate, HSARPA, Cyber Security Division under the
   Transition to Practice program. This research is also supported in part
   by the DARPA XAI program under Grant N66001-17-2-4031. The data
   referenced in this paper was created by Skaion Corporation with funding
   from IARPA.
NR 53
TC 39
Z9 48
U1 1
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 204
EP 214
DI 10.1109/TVCG.2018.2865029
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000020
PM 30136975
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Lindow, N
   Baum, D
   Leborgne, M
   Hege, HC
AF Lindow, Norbert
   Baum, Daniel
   Leborgne, Morgan
   Hege, Hans-Christian
TI Interactive Visualization of RNA and DNA Structures
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Ribonucleic acids; DNA; RNA; secondary & tertiary structures;
   interactive rendering; ray casting; brushing & linking
ID NUCLEIC-ACIDS; BASE-STACKING; CLASSIFICATION; DISPLAY; MOTIFS; SYSTEM;
   TOOL
AB The analysis and visualization of nucleic acids (RNA and DNA) is playing an increasingly important role due to their fundamental importance for all forms of life and the growing number of known 3D structures of such molecules. The great complexity of these structures, in particular, those of RNA, demands interactive visualization to get deeper insights into the relationship between the 2D secondary structure motifs and their 3D tertiary structures. Over the last decades, a lot of research in molecular visualization has focused on the visual exploration of protein structures while nucleic acids have only been marginally addressed. In contrast to proteins, which are composed of amino acids, the ingredients of nucleic acids are nucleotides. They form structuring patterns that differ from those of proteins and, hence, also require different visualization and exploration techniques. In order to support interactive exploration of nucleic acids, the computation of secondary structure motifs as well as their visualization in 2D and 3D must be fast. Therefore, in this paper, we focus on the performance of both the computation and visualization of nucleic acid structure. We present a ray casting-based visualization of RNA and DNA secondary and tertiary structures, which enables for the first time real-time visualization of even large molecular dynamics trajectories. Furthermore, we provide a detailed description of all important aspects to visualize nucleic acid secondary and tertiary structures. With this, we close an important gap in molecular visualization.
C1 [Lindow, Norbert; Baum, Daniel; Leborgne, Morgan; Hege, Hans-Christian] Zuse Inst Berlin, Berlin, Germany.
C3 Zuse Institute Berlin
RP Lindow, N (corresponding author), Zuse Inst Berlin, Berlin, Germany.
EM norbert.lindow@zib.de; baum@zib.de; leborgne@zib.de; hege@zib.de
OI Baum, Daniel/0000-0003-1550-7245; Hege,
   Hans-Christian/0000-0002-6574-0988
NR 48
TC 11
Z9 15
U1 3
U2 27
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 967
EP 976
DI 10.1109/TVCG.2018.2864507
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000092
PM 30334794
DA 2025-03-07
ER

PT J
AU Patnaik, B
   Batch, A
   Elmqvist, N
AF Patnaik, Biswaksen
   Batch, Andrea
   Elmqvist, Niklas
TI Information Olfactation: Harnessing Scent to Convey Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Olfaction; smell; scent; olfactory display; immersive analytics;
   immersion
ID FUNCTIONAL NEUROANATOMY; OLFACTORY SYSTEM; AIR-FLOW; PERCEPTION; ODORS;
   SNIFF; SMELL; ACTIVATION; MAGNITUDE; HUMIDITY
AB Olfactory feedback for analytical tasks is a virtually unexplored area in spite of the advantages it offers for information recall, feature identification. and location detection. Here we introduce the concept of information olfactation as the fragrant sibling of information visualization. and discuss how scent can be used to convey data. Building on a review of the human olfactory system and mirroring common visualization practice. we propose olfactory marks, the substrate in which they exist. and their olfactory channels that are available to designers. To exemplify this idea, we present vlScENT: A six-scent stereo olfactory display capable of conveying olfactory glyphs of varying temperature and direction. as well as a corresponding software system that integrates the display with a traditional visualization display. Finally, we present three applications that make use of the viScent system: A 2D graph visualization, a 2D line and point chart. and an immersive analytics graph visualization in 3D virtual reality. We close the paper with a review of possible extensions of viScent and applications of information olfactation for general visualization beyond the examples in this paper.
C1 [Patnaik, Biswaksen; Batch, Andrea; Elmqvist, Niklas] Univ Maryland, College Pk, MD 20742 USA.
C3 University System of Maryland; University of Maryland College Park
RP Patnaik, B (corresponding author), Univ Maryland, College Pk, MD 20742 USA.
EM bpatnaik@umd.edu; ajulca@umd.edu; elm@umd.edu
OI PATNAIK, BISWAKSEN/0000-0001-9081-032X; Elmqvist,
   Niklas/0000-0001-5805-5301
FU U.S. National Science Foundation [IIS-1539534]
FX The authors would like to thank Chloe Batch for designing the icons used
   in this paper. This work was partially supported by the U.S. National
   Science Foundation award IIS-1539534. Any opinions, findings, and
   conclusions expressed in this material are those of the authors and do
   not necessarily reflect the views of the funding agency.
NR 104
TC 42
Z9 53
U1 1
U2 24
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 726
EP 736
DI 10.1109/TVCG.2018.2865237
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA HD6IJ
UT WOS:000452640000069
PM 30137003
DA 2025-03-07
ER

PT J
AU Ibrahim, A
   Huynh, B
   Downey, J
   Höllerer, T
   Chun, D
   O'Donovan, J
AF Ibrahim, Adam
   Huynh, Brandon
   Downey, Jonathan
   Hollerer, Tobias
   Chun, Dorothy
   O'Donovan, John
TI ARbis Pictus: A Study of Vocabulary Learning with Augmented Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 17th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 16-20, 2018
CL Munich, GERMANY
SP IEEE, IEEE Comp Soc, IEEE VGTC, ACM SIGCHI, Mozilla, Apple, Intel, DAQRI, PTC, Amazon, Facebook, Qualcomm, Umajin, Disney Res, Univ S Australia Ventures Pty Ltd, REFLEKT, Occipital, Envisage AR, KHRONOS Grp, TUM, ETH Zurich
DE Language learning; education; augmented reality; HCl; experimentation
ID MULTIMEDIA; GLOSSES
AB We conducted a fundamental user study to assess potential benefits of AR technology for immersive vocabulary learning. With the idea that AR systems will soon be able to label real-world objects in any language in real time, our within-subjects (N=52) lab-based study explores the effect of such an AR vocabulary prompter on participants learning nouns in an unfamiliar foreign language, compared to a traditional flashcard-based learning approach. Our results show that the immersive AR experience of learning with virtual labels on real-world objects is both more effective and more enjoyable for the majority of participants, compared to flashcards. Specifically, when participants learned through augmented reality, they scored significantly better on both same-day and 4-day delayed productive recall tests than when they learned using the flashcard method. We believe this result is an indication of the strong potential for language learning in augmented reality, particularly because of the improvement shown in sustained recall compared to the traditional approach.
C1 [Ibrahim, Adam; Huynh, Brandon; Hollerer, Tobias; O'Donovan, John] Univ Calif Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.
   [Downey, Jonathan; Chun, Dorothy] Univ Calif Santa Barbara, Dept Educ, Santa Barbara, CA 93106 USA.
C3 University of California System; University of California Santa Barbara;
   University of California System; University of California Santa Barbara
RP Ibrahim, A (corresponding author), Univ Calif Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.
EM ai@cs.ucsb.edu; bhuynh@cs.ucsb.edu; jonathandowney@ucsb.edu;
   holl@cs.ucsb.edu; dcluen@education.ucsb.edu; jod@cs.ucsb.edu
FU ONR [N00014-16-1-3002]; NSF DRL planning grant [1427729]; Direct For
   Education and Human Resources [1427729] Funding Source: National Science
   Foundation; Division Of Research On Learning [1427729] Funding Source:
   National Science Foundation
FX Parts of this research were supported by ONR grant #N00014-16-1-3002 and
   NSF DRL planning grant #1427729. Thanks to Matthew Turk, Yun Suk Chang,
   and JB Lanier for their contributions and feedback on the project.
NR 29
TC 76
Z9 81
U1 3
U2 107
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2018
VL 24
IS 11
BP 2867
EP 2874
DI 10.1109/TVCG.2018.2868568
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GZ0TM
UT WOS:000449077900005
PM 30207959
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ohl, S
AF Ohl, Stephan
TI Tele-Immersion Concepts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Tele-immersion; telepresence; immersive tele-collaboration; virtual
   reality; augmented reality; mixed reality; communication notation;
   immersive extent; joint social attention
ID TELECONFERENCING SYSTEM; VIRTUAL-REALITY; VIDEO; EMBODIMENT; DISPLAY
AB Tele-immersive systems development is always driven as well as restricted by the available immersive technology. Hence, existing such systems are described mainly from a technological point of view; their conceptual description is usually limited to the description of a scenario that is implementable with or circumvents the limitations of the chosen technology. This focus on technology makes it difficult to compare systems' concepts; moreover, it has led to different views on tele-immersion in different fields, such as remotely controlled robots, immersive video conferencing, and tele-collaboration. In this work, we give a general, structured principle to describe the conceptual part of any tele-immersion system. This principle naturally unifies the different views on tele-immersion. Our idea is based on the insight that, in order to be general, immersion must be described separately for each direction of communication. We characterize communication between locations using a graph; for each directed edge of this graph, we describe immersion as operations on volumes. Using this principle, we define a typology, which enables the comparison and enumeration of tele-immersion concepts. We apply this typology to survey the concepts of existing tele-immersion systems and thereby demonstrate how three well-known tele-immersive scenarios-Marvin Minsky's tele-operated robot, the Office of the Future, and the asymmetric Beaming scenario-integrate naturally. We show how the general principle can be utilized conveniently to grasp conceptual ideas in tele-immersion, such as direct interaction, locational presence, spatial consistency, symmetries, and self-inclusion.
C1 [Ohl, Stephan] Univ Rostock, Comp Sci, D-18051 Rostock, Germany.
C3 University of Rostock
RP Ohl, S (corresponding author), Univ Rostock, Comp Sci, D-18051 Rostock, Germany.
EM stephan.ohl@posteo.net
NR 56
TC 9
Z9 10
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2018
VL 24
IS 10
BP 2827
EP 2842
DI 10.1109/TVCG.2017.2767590
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GS7PN
UT WOS:000443894900013
PM 29990081
DA 2025-03-07
ER

PT J
AU Harper, J
   Agrawala, M
AF Harper, Jonathan
   Agrawala, Maneesh
TI Converting Basic D3 Charts into Reusable Style Templates
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Chart restyling; reusable style templates; declarative representation;
   D3 deconstruction; vega-lite
ID VISUALIZATION
AB We present a technique for converting a basic D3 chart into a reusable style template. Then, given a new data source we can apply the style template to generate a chart that depicts the new data, but in the style of the template. To construct the style template we first deconstruct the input D3 chart to recover its underlying structure: the data, the marks and the mappings that describe how the marks encode the data. We then rank the perceptual effectiveness of the deconstructed mappings. To apply the resulting style template to a new data source we first obtain importance ranks for each new data field. We then adjust the template mappings to depict the source data by matching the most important data fields to the most perceptually effective mappings. We show how the style templates can be applied to source data in the form of either a data table or another D3 chart. While our implementation focuses on generating templates for basic chart types (e.g., variants of bar charts, line charts, dot plots, scatterplots, etc.), these are the most commonly used chart types today. Users can easily find such basic D3 charts on the Web, turn them into templates, and immediately see how their own data would look in the visual style (e.g., colors, shapes, fonts, etc.) of the templates. We demonstrate the effectiveness of our approach by applying a diverse set of style templates to a variety of source datasets.
C1 [Harper, Jonathan] Univ Calif Berkeley, Berkeley, CA 95064 USA.
   [Agrawala, Maneesh] Stanford Univ, Comp Sci, Stanford, CA 94305 USA.
   [Agrawala, Maneesh] Stanford Univ, Brown Inst Media Innovat, Stanford, CA 94305 USA.
C3 University of California System; University of California Berkeley;
   Stanford University; Stanford University
RP Harper, J (corresponding author), Univ Calif Berkeley, Berkeley, CA 95064 USA.
EM jharper@berkeley.edu; maneesh@cs.stanford.edu
OI Agrawala, Maneesh/0000-0002-8996-7327
FU Allen Distinguished Investigator Award
FX This work was supported by an Allen Distinguished Investigator Award.
NR 37
TC 38
Z9 41
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2018
VL 24
IS 3
BP 1274
EP 1286
DI 10.1109/TVCG.2017.2659744
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FU0LA
UT WOS:000423541200005
PM 28186898
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Wu, J
   Aage, N
   Westermann, R
   Sigmund, O
AF Wu, Jun
   Aage, Niels
   Westermann, Ruediger
   Sigmund, Ole
TI Infill Optimization for Additive Manufacturing-Approaching Bone-Like
   Porous Structures
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Infill; additive manufacturing; trabecular bone; porous structures;
   topology optimization
ID MINIMUM LENGTH SCALE; TOPOLOGY OPTIMIZATION; TRABECULAR BONE; DESIGN;
   ADAPTATION; DENSITY
AB Porous structures such as trabecular bone are widely seen in nature. These structures are lightweight and exhibit strong mechanical properties. In this paper, we present amethod to generate bone-like porous structures as lightweight infill for additive manufacturing. Ourmethod builds upon and extends voxel-wise topology optimization. In particular, for the purpose of generating sparse yet stable structures distributed in the interior of a given shape, we propose upper bounds on the localizedmaterial volume in the proximity of each voxel in the design domain. We then aggregate the local per-voxel constraints by their p-norm into an equivalent global constraint, in order to facilitate an efficient optimization process. Implemented on a high-resolution topology optimization framework, our results demonstrate mechanically optimized, detailed porous structures which mimic those found in nature. We further show variants of the optimized structures subject to different design specifications, and we analyze the optimality and robustness of the obtained structures.
C1 [Wu, Jun; Aage, Niels; Sigmund, Ole] Tech Univ Denmark, Dept Mech Engn, DK-2800 Lyngby, Denmark.
   [Westermann, Ruediger] Tech Univ Munich, Dept Comp Sci, D-85748 Garching, Germany.
C3 Technical University of Denmark; Technical University of Munich
RP Wu, J (corresponding author), Tech Univ Denmark, Dept Mech Engn, DK-2800 Lyngby, Denmark.
EM junwu@mek.dtu.dk; naage@mek.dtu.dk; westermann@tum.de;
   sigmund@mek.dtu.dk
RI Aage, Niels/J-1747-2016; Wu, Jun/L-2487-2017; Sigmund, Ole/A-5354-2008
OI Aage, Niels/0000-0002-3042-0036; Wu, Jun/0000-0003-4237-1806; Sigmund,
   Ole/0000-0003-0344-7249
FU H.C. Orsted Postdoc Programme at the Technical University of Denmark
   from the People Programme (Marie Curie Actions) of the European Union's
   Seventh Framework Programme (FP7) under REA [609405]; Villum foundation
   through the NextTop project
FX The authors gratefully acknowledge the support from the H.C. Orsted
   Postdoc Programme at the Technical University of Denmark, which has
   received funding from the People Programme (Marie Curie Actions) of the
   European Union's Seventh Framework Programme (FP7/2007-2013) under REA
   grant agreement no. 609405 (COFUNDPostdocDTU), and the support from the
   Villum foundation through the NextTop project. Jun Wu is the
   corresponding author.
NR 53
TC 375
Z9 410
U1 19
U2 200
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2018
VL 24
IS 2
BP 1127
EP 1140
DI 10.1109/TVCG.2017.2655523
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR8BW
UT WOS:000419299900009
PM 28129160
OA Green Submitted
HC Y
HP N
DA 2025-03-07
ER

PT J
AU Bujack, R
   Turton, TL
   Sannsel, F
   Ware, C
   Rogers, DH
   Ahrens, J
AF Bujack, Roxana
   Turton, Terece L.
   Sannsel, Francesca
   Ware, Colin
   Rogers, David H.
   Ahrens, James
TI The Good, the Bad, and the Ugly: A Theoretical Framework for the
   Assessment of Continuous Colormaps
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE colormap; survey; taxonomy; order; uniformity; discriminative power;
   smoothness; monotonicity; linearity; speed
ID DIFFERENCE FORMULA; VISUALIZATION; UNIVARIATE; SEQUENCES; DISPLAY;
   SCALES; HUE
AB A myriad of design rules for what constitutes a "good" colormap can be found in the literature. Some common rules include order, uniformity, and high discriminative power. However, the meaning of many of these terms is often ambiguous or open to interpretation. At times, different authors may use the same term to describe different concepts or the same rule is described by varying nomenclature. These ambiguities stand in the way of collaborative work, the design of experiments to assess the characteristics of colormaps. and automated colormap generation. In this paper, we review current and historical guidelines for colormap design. We propose a specified taxonomy and provide unambiguous mathematical definitions for the most common design rules.
C1 [Bujack, Roxana; Rogers, David H.; Ahrens, James] Los Alamos Natl Lab, Los Alamos, NM 87545 USA.
   [Turton, Terece L.; Sannsel, Francesca] Univ Texas Austin, Austin, TX 78712 USA.
   [Ware, Colin] Univ New Hampshire, Durham, NH 03824 USA.
C3 United States Department of Energy (DOE); Los Alamos National
   Laboratory; University of Texas System; University of Texas Austin;
   University System Of New Hampshire; University of New Hampshire
RP Bujack, R (corresponding author), Los Alamos Natl Lab, Los Alamos, NM 87545 USA.
EM bujack@lanl.gov; tlturton@cat.utexas.edu; figs@cat.utexas.edu;
   cware@ccum.unh.edu; dhr@lanl.gov; ahrens@lanl.gov
OI Ahrens, James/0000-0001-9378-282X; Samsel,
   Francesca/0000-0002-8596-6159; Turton, Terece/0000-0003-4345-7783;
   Bujack, Roxana/0000-0002-5479-3726
FU U.S. Department of Energy Office of Science, Advanced Scientific
   Computing Research [DE-AS52-06NA25396, DE-SC-0012516]
FX We would like to thank Prof. Hans Hagen, Dr. Kenneth Moreland, and Dr.
   Bernice Rogowitz for inspiring discussions. We thank Curt Canada, Jonas
   Lukasczyk, and Dr. Jon Woodring for their help. This material is based
   upon work supported by Dr. Lucy Nowell of the U.S. Department of Energy
   Office of Science, Advanced Scientific Computing Research under,Award
   Numbers DE-AS52-06NA25396 and DE-SC-0012516.
NR 88
TC 60
Z9 72
U1 1
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 923
EP 933
DI 10.1109/TVCG.2017.2743978
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400091
PM 28866507
DA 2025-03-07
ER

PT J
AU Ivson, P
   Nascimento, D
   Celes, W
   Barbosa, SDJ
AF Ivson, Paulo
   Nascimento, Daniel
   Celes, Waldemar
   Barbosa, Simone D. J.
TI CasCADe: A Novel 4D Visualization System for Virtual Construction
   Planning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Visualization in physical sciences and engineering; design studies;
   integrating spatial and non-spatial data visualization; task and
   requirements analysis
ID SPACE-TIME CUBE; COLLISION DETECTION; INFORMATION VISUALIZATION; 3D;
   DESIGN; CAD; BIM; REFLECTIONS; ALGORITHMS; MANAGEMENT
AB Building Information Modeling (BIM) provides an integrated 3D environment to manage large-scale engineering projects. The Architecture, Engineering and Construction (AEC) industry explores 4D visualizations over these datasets for virtual construction planning. However, existing solutions lack adequate visual mechanisms to inspect the underlying schedule and make inconsistencies readily apparent. The goal of this paper is to apply best practices of information visualization to improve 4D analysis of construction plans. We first present a review of previous work that identifies common use cases and limitations. We then consulted with AEC professionals to specify the main design requirements for such applications. These guided the development of CasCADe, a novel 4D visualization system where task sequencing and spatio-temporal simultaneity are immediately apparent. This unique framework enables the combination of diverse analytical features to create an information-rich analysis environment. We also describe how engineering collaborators used CasCADe to review the real-world construction plans of an Oil & Gas process plant. The system made evident schedule uncertainties, identified work-space conflicts and helped analyze other constructability issues. The results and contributions of this paper suggest new avenues for future research in information visualization for the AEC industry.
C1 [Ivson, Paulo; Nascimento, Daniel; Celes, Waldemar] Pontificia Univ Catolica Rio de Janeiro, Tecgraf Inst, Rio de Janeiro, Brazil.
   [Barbosa, Simone D. J.] Pontificia Univ Catolica Rio de Janeiro, Informat Dept, Rio de Janeiro, Brazil.
C3 Pontificia Universidade Catolica do Rio de Janeiro; Pontificia
   Universidade Catolica do Rio de Janeiro
RP Ivson, P (corresponding author), Pontificia Univ Catolica Rio de Janeiro, Tecgraf Inst, Rio de Janeiro, Brazil.
EM psantos@tecgraf.puc-rio.br; daniehmn@tecgraf.puc-rio.br;
   celes@tecgraf.pnc-rio.br; simone@inf.pur-rio.br
RI Ivson, Paulo/GNM-7800-2022; Nascimento, Daniel/ABE-6193-2020; Diniz
   Junqueira Barbosa, Simone/F-8012-2014; de Mattos Nascimento, Dr. Daniel
   Luiz/AAX-8461-2021
OI Diniz Junqueira Barbosa, Simone/0000-0002-0044-503X; de Mattos
   Nascimento, Dr. Daniel Luiz/0000-0002-4977-8652
FU CNPq, Conselho Nacional de Desenvolvimento Cientifico e Tecnologico -
   Brasil [140933/2014-0, 311620/2014-0, 309828/2015-5, 453996/2014-0]
FX The present work was done with support from CNPq, Conselho Nacional de
   Desenvolvimento Cientifico e Tecnologico - Brasil (processes
   140933/2014-0, 311620/2014-0, 309828/2015-5 and 453996/2014-0). The
   authors wish to thank the reviewers for their valuable comments and
   suggestions. The authors also thank Andre Souza and in Francisco Queiroz
   for their help with document formatting, and editing of the
   supplementary video.
NR 119
TC 32
Z9 34
U1 2
U2 38
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 687
EP 697
DI 10.1109/TVCG.2017.2745105
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400068
PM 28866569
DA 2025-03-07
ER

PT J
AU Kern, M
   Hewson, T
   Sadlo, F
   Westermann, R
   Rautenhaus, M
AF Kern, Michael
   Hewson, Tim
   Sadlo, Filip
   Westermann, Ruediger
   Rautenhaus, Marc
TI Robust Detection and Visualization of Jet-stream Core Lines in
   Atmospheric Flow
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Meteorology; weather forecast; jet-stream; feature detection
ID WEATHER FORECASTS; CLIMATOLOGY; VARIABILITY; POSITION; STRENGTH; VORTEX;
   TRENDS
AB Jet-streams. their core lines and their role in atmospheric dynamics have been subject to considerable meteorological research since the first half of the twentieth century. Yet, until today no consistent automated feature detection approach has been proposed to identify jet-stream core lines from 3D wind fields. Such 3D core lines can facilitate meteorological analyses previously not possible. Although jet-stream cores can be manually analyzed by meteorologists in 2D as height ridges in the wind speed field, to the best of our knowledge no automated ridge detection approach has been applied to jet-stream core detection. In this work, we-a team of visualization scientists and meteorologists- propose a method that exploits directional information in the wind field to extract core lines in a robust and numerically less involved manner than traditional 3D ridge detection. For the first time, we apply the extracted 3D core lines to meteorological analysis. considering real-world case studies and demonstrating our method's benefits for weather forecasting and meteorological research.
C1 [Kern, Michael; Westermann, Ruediger; Rautenhaus, Marc] Tech Univ Munich, Comp Graph & Visualizat Grp, Munich, Germany.
   [Hewson, Tim] European Ctr Medium Range Weather Forecasts, Reading, Berks, England.
   [Sadlo, Filip] Heidelberg Univ, Visual Comp Grp, Heidelberg, Germany.
C3 Technical University of Munich; European Centre for Medium-Range Weather
   Forecasts (ECMWF); Ruprecht Karls University Heidelberg
RP Kern, M (corresponding author), Tech Univ Munich, Comp Graph & Visualizat Grp, Munich, Germany.
EM michi.kern@tum.de; tim.hewson@ecmwf.int; sadlo@uni-heidelberg.de;
   westermann@tum.de; marc.rautenhaus@tum.de
RI Sadlo, Filip/B-1624-2019
OI Rautenhaus, Marc/0000-0002-2715-2165; Hewson,
   Timothy/0000-0002-3266-8828
FU German Research Foundation (DFG) [SFB/TRR 165]; European Union under the
   ERC [291372]
FX The research leading to these results has been done within the
   subproject "Visualization of coherence and variation in meteorological
   dynamics" of the Transregional Collaborative Research Center SFB/TRR 165
   "Waves to Weather" funded by the German Research Foundation (DFG). The
   work was partly funded by the European Union under the ERC Advanced
   Grant 291372 SaferVis: Uncertainty Visualization for Reliable Data
   Discovery. Access to ECMWF prediction data has been kindly provided in
   the context of the ECMWF special project "Support Tool for HALO
   Missions".
NR 54
TC 23
Z9 24
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 893
EP 902
DI 10.1109/TVCG.2017.2743989
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400088
PM 28866511
DA 2025-03-07
ER

PT J
AU Klein, T
   Autin, L
   Kozlíková, B
   Goodsell, DS
   Olson, A
   Gröller, ME
   Viola, I
AF Klein, Tobias
   Autin, Ludovic
   Kozlikova, Barbora
   Goodsell, David S.
   Olson, Arthur
   Groeller, M. Eduard
   Viola, Ivan
TI Instant Construction and Visualization of Crowded Biological
   Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Interactive modeling; population; biological data; interactive
   visualization
ID GENERATION; MODEL
AB We present the first approach to integrative structural modeling of the biological mesoscale within an interactive visual environment. These complex models can comprise up to millions of molecules with defined atomic structures, locations, and interactions. Their construction has previously been attempted only within a non-visual and non-interactive environment. Our solution unites the modeling and visualization aspect, enabling interactive construction of atomic resolution mesoscale models of large portions of a cell. We present a novel set of GPU algorithms that build the basis for the rapid construction of complex biological structures. These structures consist of multiple membrane-enclosed compartments including both soluble molecules and fibrous structures. The compartments are defined using volume voxelization of triangulated meshes. For membranes, we present an extension of the Wang Tile concept that populates the bilayer with individual lipids. Soluble molecules are populated within compartments distributed according to a Halton sequence. Fibrous structures, such as RNA or actin filaments, are created by self-avoiding random walks. Resulting overlaps of molecules are resolved by a forced-based system. Our approach opens new possibilities to the world of interactive construction of cellular compartments. We demonstrate its effectiveness by showcasing scenes of different scale and complexity that comprise blood plasma, mycoplasma, and HIV.
C1 [Klein, Tobias; Groeller, M. Eduard; Viola, Ivan] TU Wien, Vienna, Austria.
   [Groeller, M. Eduard] VRVis Res Ctr, Vienna, Austria.
   [Autin, Ludovic; Goodsell, David S.; Olson, Arthur] Scripps Res Inst, San Diego, CA USA.
   [Kozlikova, Barbora] Maswyk Univ, Brno, Czech Republic.
C3 Technische Universitat Wien; Scripps Research Institute; Masaryk
   University Brno
RP Klein, T (corresponding author), TU Wien, Vienna, Austria.
EM tklein@cg.tuwien.ac.at; ludovic.autin@gmail.com; kozlikova@fi.muni.cz;
   goodsell@scripps.edu; olson@scripps.edu; groeller@cg.tuwien.ac.at;
   viola@cg.tuwien.ac.at
RI Gröller, Eduard/AAH-2111-2020; Kozlikova, Barbora/G-3890-2014; Viola,
   Ivan/O-8944-2014
OI Kozlikova, Barbora/0000-0003-0045-0872; Autin,
   Ludovic/0000-0002-2197-191X; Viola, Ivan/0000-0003-4248-6574
FU Vienna Science and Technology Fund (WWTF) [VRG11-010]; EC Marie Curie
   Career Integration Grant [PCIG13-GA-2013-618680]; NIH [P41-GM103426,
   R01-GM120604]; TSRI [29494]
FX This project has been funded by the Vienna Science and Technology Fund
   (WWTF) through project VRG11-010 and also supported by EC Marie Curie
   Career Integration Grant through project PCIG13-GA-2013-618680. The
   Scripps Research Institute researchers acknowledge support from NIH
   P41-GM103426 and R01-GM120604, and this manuscript has TSRI reference
   number 29494.
NR 49
TC 31
Z9 32
U1 0
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 862
EP 872
DI 10.1109/TVCG.2017.2744258
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400085
PM 28866533
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Wang, YH
   Chu, XW
   Bao, C
   Zhu, LF
   Deussen, O
   Chen, BQ
   Sedlmair, M
AF Wang, Yunhai
   Chu, Xiaowei
   Bao, Chen
   Zhu, Lifeng
   Deussen, Oliver
   Chen, Baoquan
   Sedlmair, Michael
TI EdWordle: Consistency-preserving Word Cloud Editing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Wordle; consistency; text visualization
AB We present EdWordle, a method for consistently editing word clouds. At its heart, EdWordle allows users to move and edit words while preserving the neighborhoods of other words. To do so, we combine a constrained rigid body simulation with a neighborhood-aware local Wordle algorithm to update the cloud and to create very compact layouts. The consistent and stable behavior of EdWordle enables users to create new forms of word clouds such as storytelling clouds in which the position of words is carefully edited. We compare our approach with state-of-the-art methods and show that we can improve user performance; user satisfaction, as well as the layout itself.
C1 [Wang, Yunhai; Chu, Xiaowei; Bao, Chen; Chen, Baoquan] Shandong Univ, Jinan, Shandong, Peoples R China.
   [Zhu, Lifeng] Southeast Univ, Nanjing, Jiangsu, Peoples R China.
   [Deussen, Oliver] Konstanz Univ, Constance, Germany.
   [Deussen, Oliver] VCC SIAT, Hong Kong, Hong Kong, Peoples R China.
   [Sedlmair, Michael] Univ Vienna, Vienna, Austria.
C3 Shandong University; Southeast University - China; University of
   Konstanz; University of Vienna
RP Wang, YH (corresponding author), Shandong Univ, Jinan, Shandong, Peoples R China.
EM wang.yh@sdu.edu.cn; caxiaoxie@gmail.com; baochen95@gmail.com;
   lfzhulf@gmail.com; oliver.deussen@uni-konstanz.de; baoquan@sdu.edu.cn;
   michael.sedlmair@univie.ac.at
RI zhu, lifeng/IST-2069-2023; Deussen, Oliver/HKF-2004-2023
OI Sedlmair, Michael/0000-0001-7048-9292
FU NSFC-Guangdong Joint Fund [U1501255]; NSFC [61379091, 91630204];
   National Key Research & Development Plan of China [2016YFB1001404];
   Shandong Provincial Natural Science Foundation [2016ZRE27617]; NSF of
   Jiangsu Province [BK20150634]; National Foreign 1000 Talent Plan
   [WQ201344000169]; Leading Talents of Guangdong Program [00201509];
   Fundamental Research Funds of Shandong University; FFG project [845898]
FX The authors would like to thank Haifeng Zhang for making the evaluation.
   This work is supported by the grants of NSFC-Guangdong Joint Fund
   (U1501255), NSFC (61379091, 91630204), the National Key Research &
   Development Plan of China (2016YFB1001404), Shandong Provincial Natural
   Science Foundation (2016ZRE27617), NSF of Jiangsu Province (BK20150634),
   National Foreign 1000 Talent Plan (WQ201344000169), Leading Talents of
   Guangdong Program (00201509), the Fundamental Research Funds of Shandong
   University, and the FFG project 845898 (VALID).
NR 39
TC 27
Z9 35
U1 3
U2 30
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 647
EP 656
DI 10.1109/TVCG.2017.2745859
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400064
PM 28866587
DA 2025-03-07
ER

PT J
AU Kawai, N
   Sato, T
   Nakashima, Y
   Yokoya, N
AF Kawai, Norihiko
   Sato, Tomokazu
   Nakashima, Yuta
   Yokoya, Naokazu
TI Augmented Reality Marker Hiding with Texture Deformation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Marker hiding; diminished reality; texture deformation
AB Augmented reality (AR) marker hiding is a technique to visually remove AR markers in a real-time video stream. A conventional approach transforms a background image with a homography matrix calculated on the basis of a camera pose and overlays the transformed image on an AR marker region in a real-time frame, assuming that the AR marker is on a planar surface. However, this approach may cause discontinuities in textures around the boundary between the marker and its surrounding area when the planar surface assumption is not satisfied. This paper proposes a method for AR marker hiding without discontinuities around texture boundaries even under nonplanar background geometry without measuring it. For doing this, our method estimates the dense motion in the marker's background by analyzing the motion of sparse feature points around it, together with a smooth motion assumption, and deforms the background image according to it. Our experiments demonstrate the effectiveness of the proposed method in various environments with different background geometries and textures.
C1 [Kawai, Norihiko; Nakashima, Yuta] Nara Inst Sci & Technol, Grad Sch Informat Sci, Ikoma, Nara 6300192, Japan.
   [Sato, Tomokazu; Yokoya, Naokazu] Nara Inst Sci & Technol, Ikoma, Nara 6300192, Japan.
C3 Nara Institute of Science & Technology; Nara Institute of Science &
   Technology
RP Kawai, N (corresponding author), Nara Inst Sci & Technol, Grad Sch Informat Sci, Ikoma, Nara 6300192, Japan.
EM norihi-k@is.naist.jp; tomoka-s@is.naist.jp; n-yuta@is.naist.jp;
   yokoya@is.naist.jp
RI Yokoya, Naoto/AAC-1530-2022; Nakashima, Yuta/Y-6218-2019
FU Japan Society for the Promotion of Science (JSPS) [15K16039];
   Grants-in-Aid for Scientific Research [15K16039] Funding Source: KAKEN
FX This work was partially supported by Grants-in-Aid for Scientific
   Research No. 15K16039 from the Japan Society for the Promotion of
   Science (JSPS).
NR 34
TC 10
Z9 12
U1 1
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2017
VL 23
IS 10
BP 2288
EP 2300
DI 10.1109/TVCG.2016.2617325
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FG0VB
UT WOS:000409496700008
PM 27775521
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Beck, F
   Weiskopf, D
AF Beck, Fabian
   Weiskopf, Daniel
TI Word-Sized Graphics for Scientific Texts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE Pacific Visualization Symposium (IEEE PacificVis)
CY APR 18-21, 2017
CL Seoul Natl Univ, Seoul, SOUTH KOREA
SP IEEE, IEEE Comp Soc Visualizat & Graph Tech Comm, IEEE Comp Soc
HO Seoul Natl Univ
DE Sparklines; word-sized graphics; literature survey; text and
   visualization; interactive documents; scientific publishing
ID VISUALIZATION; DISPLAY; DESIGN; RANGE
AB Generating visualizations at the size of a word creates dense information representations often called sparklines. The integration of word-sized graphics into text could avoid additional cognitive load caused by splitting the readers' attention between figures and text. In scientific publications, these graphics make statements easier to understand and verify because additional quantitative information is available where needed. In this work, we perform a literature review to find out how researchers have already applied such word-sized representations. Illustrating the versatility of the approach, we leverage these representations for reporting empirical and bibliographic data in three application examples. For interactive Web-based publications, we explore levels of interactivity and discuss interaction patterns to link visualization and text. We finally call the visualization community to be a pioneer in exploring new visualization-enriched and interactive publication formats.
C1 [Beck, Fabian] Univ Duisburg Essen, D-47057 Duisburg, Germany.
   [Weiskopf, Daniel] Univ Stuttgart, VISUS, D-70174 Stuttgart, Germany.
C3 University of Duisburg Essen; University of Stuttgart
RP Beck, F (corresponding author), Univ Duisburg Essen, D-47057 Duisburg, Germany.
EM fabian.beck@wiwinf.uni-due.de; weiskopf@visus.uni-stuttgart.de
RI Weiskopf, Daniel/KWT-7459-2024
OI Beck, Fabian/0000-0003-4042-3043; Weiskopf, Daniel/0000-0003-1174-1026
FU Baden-Wurttemberg Stiftung
FX We thank Yasett Acurana, Tanja Blascheck, Corinna Vehlow, and Yuliya
   Volga for helping design and implementing the word-sized graphics listed
   in Section 4. Fabian Beck is indebted to the Baden-Wurttemberg Stiftung
   for the financial support of this research project within the
   Postdoctoral Fellowship for Leading Early Career Researchers.
NR 67
TC 29
Z9 32
U1 0
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2017
VL 23
IS 6
BP 1576
EP 1587
DI 10.1109/TVCG.2017.2674958
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA ET8DC
UT WOS:000400527500002
PM 28252406
DA 2025-03-07
ER

PT J
AU Zhao, HN
   Bryant, GW
   Griffin, W
   Terrill, JE
   Chen, J
AF Zhao, Henan
   Bryant, Garnett W.
   Griffin, Wesley
   Terrill, Judith E.
   Chen, Jian
TI Validation of SplitVectors Encoding for Quantitative Visualization of
   Large-Magnitude-Range Vector Fields
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Vector field; scientific visualization in virtual environments; quantum
   physics; visual encoding; large-range data
ID INFORMATION; STEREO; SIZE; DESIGN; FLOW; CUES
AB We designed and evaluated SplitVectors, a new vector field display approach to help scientists perform new discrimination tasks on large-magnitude-range scientific data shown in three-dimensional (3D) visualization environments. SplitVectors uses scientific notation to display vector magnitude, thus improving legibility. We present an empirical study comparing the SplitVectors approach with three other approaches - direct linear representation, logarithmic, and text display commonly used in scientific visualizations. Twenty participants performed three domain analysis tasks: reading numerical values (a discrimination task), finding the ratio between values (a discrimination task), and finding the larger of two vectors (a pattern detection task). Participants used both mono and stereo conditions. Our results suggest the following: (1) SplitVectors improve accuracy by about 10 times compared to linear mapping and by four times to logarithmic in discrimination tasks; (2) SplitVectors have no significant differences from the textual display approach, but reduce cluttering in the scene; (3) SplitVectors and textual display are less sensitive to data scale than linear and logarithmic approaches; (4) using logarithmic can be problematic as participants' confidence was as high as directly reading from the textual display, but their accuracy was poor; and (5) Stereoscopy improved performance, especially in more challenging discrimination tasks.
C1 [Zhao, Henan; Griffin, Wesley; Chen, Jian] Univ Maryland, Dept Comp Sci & Elect Engn, Baltimore, MD 21250 USA.
   [Bryant, Garnett W.; Griffin, Wesley; Terrill, Judith E.] NIST, Gaithersburg, MD 20899 USA.
C3 University System of Maryland; University of Maryland Baltimore;
   National Institute of Standards & Technology (NIST) - USA
RP Zhao, HN (corresponding author), Univ Maryland, Dept Comp Sci & Elect Engn, Baltimore, MD 21250 USA.
EM henan1@umbc.edu; garnett.bryant@nist.gov; wesley.griffin@nist.gov;
   judith.terrill@nist.gov; jichen@umbc.edu
RI jian, chan/ABG-5546-2021
OI Chen, Jian/0000-0002-1599-0831
FU NIST [70NANB13H181]; NSF [IIS-1302755]
FX The authors would like to thank all participants and Katrina Avery for
   their contributions. We thank the anonymous reviewers for their careful
   reading of our manuscript and their many insightful comments and
   suggestions. This work was supported in part by grants from NIST
   70NANB13H181 and NSF IIS-1302755. Any opinions, findings, and
   conclusions or recommendations expressed in this material are those of
   the author(s) and do not necessarily reflect the views of the National
   Science Foundation. Certain commercial products are identified in this
   paper in order to specify the experimental procedure adequately. Such
   identification is not intended to imply recommendation or endorsement by
   the National Institute of Standards and Technology, nor is it intended
   to imply that the products identified are necessarily the best available
   for the purpose.
NR 45
TC 5
Z9 7
U1 0
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2017
VL 23
IS 6
BP 1691
EP 1705
DI 10.1109/TVCG.2016.2539949
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA ET8DC
UT WOS:000400527500011
PM 28113469
OA Green Accepted, Bronze
DA 2025-03-07
ER

PT J
AU Regan, M
   Miller, GSP
AF Regan, Matthew
   Miller, Gavin S. P.
TI The Problem of Persistence with Rotating Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 19th IEEE Virtual Reality Conference (VR)
CY MAR 18-22, 2017
CL Los Angeles, CA
SP IEEE, IEEE Comp Soc, IEEE Comp Soc, Visualizat & Graph Tech Comm
DE Terms Virtual reality; latency; persistence; display modulation
ID DYNAMIC VISUAL-ACUITY
AB Motion-to-photon latency causes images to sway from side to side in a VR/AR system, while display persistence causes smearing; both of these are undesirable artifacts. We show that once latency is reduced or eliminated, smearing due to display persistence becomes the dominant visual artifact, even with accurate tracker prediction. We investigate the human perceptual mechanisms responsible for this and we demonstrate a modified 3D rotation display controller architecture for driving a high speed digital display which minimizes latency and persistence. We simulate it in software and we built a testbench based on a very high frame rate (2880 fps 1-bit images) display system mounted on a mechanical rotation gantry which emulates display rotation during head rotation in an HMD.
C1 [Regan, Matthew] Monash Hlth, Melbourne, Vic, Australia.
   [Miller, Gavin S. P.] Adobe Syst Inc, San Jose, CA USA.
C3 Monash Health; Adobe Systems Inc.
RP Regan, M (corresponding author), Monash Hlth, Melbourne, Vic, Australia.
EM Matthew.Regan@MonashHealth.org; gmiller@adobe.com
NR 19
TC 14
Z9 15
U1 0
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2017
VL 23
IS 4
BP 1295
EP 1301
DI 10.1109/TVCG.2017.2656979
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EO0QU
UT WOS:000396403800005
PM 28129165
DA 2025-03-07
ER

PT J
AU Kim, NW
   Schweickart, E
   Liu, ZC
   Dontcheva, M
   Li, W
   Popovic, J
   Pfister, H
AF Kim, Nam Wook
   Schweickart, Eston
   Liu, Zhicheng
   Dontcheva, Mira
   Li, Wilmot
   Popovic, Jovan
   Pfister, Hanspeter
TI Data-Driven Guides: Supporting Expressive Design for Information
   Graphics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Information graphics; visualization; design tools; 2D graphics
ID VISUAL EMBELLISHMENTS; VISUALIZATION
AB In recent years, there is a growing need for communicating complex data in an accessible graphical form. Existing visualization creation tools support automatic visual encoding, but lack flexibility for creating custom design; on the other hand, freeform illustration tools require manual visual encoding, making the design process time-consuming and error-prone. In this paper, we present Data-Driven Guides (DDG), a technique for designing expressive information graphics in a graphic design environment. Instead of being confined by predefined templates or marks, designers can generate guides from data and use the guides to draw, place and measure custom shapes. We provide guides to encode data using three fundamental visual encoding channels: length, area, and position. Users can combine more than one guide to construct complex visual structures and map these structures to data. When underlying data is changed, we use a deformation technique to transform custom shapes using the guides as the backbone of the shapes. Our evaluation shows that data-driven guides allow users to create expressive and more accurate custom data-driven graphics.
C1 [Kim, Nam Wook; Pfister, Hanspeter] Harvard Univ, John Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
   [Schweickart, Eston] Cornell Univ, Dept Comp Sci, Ithaca, NY USA.
   [Liu, Zhicheng; Dontcheva, Mira; Li, Wilmot; Popovic, Jovan] Adobe Res, South Lake Tahoe, NV USA.
C3 Harvard University; Cornell University
RP Kim, NW (corresponding author), Harvard Univ, John Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
EM namwkim@seas.harvard.edu; ers273@cornell.edu; leoli@adobe.com;
   mirad@adobe.com; wilmotli@adobe.com; jovan@adobe.com;
   pfister@seas.harvard.edu
OI Kim, Nam Wook/0000-0003-4899-6671; Pfister,
   Hanspeter/0000-0002-3620-2582
FU Kwanjeong Educational Foundation
FX The authors wish to thank Jean-Daniel Fekete, Jeremy Boy, Johanna Beyer,
   Kasper Dinkla, Hendrik Strobelt, and James Tompkin for valuable feedback
   on this project. This work was supported in part by a grant from the
   Kwanjeong Educational Foundation.
NR 55
TC 83
Z9 98
U1 0
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 491
EP 500
DI 10.1109/TVCG.2016.2598620
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600052
PM 27875165
DA 2025-03-07
ER

PT J
AU Liu, ZG
   Zhou, LY
   Leung, H
   Shum, HPH
AF Liu, Zhiguang
   Zhou, Liuyang
   Leung, Howard
   Shum, Hubert P. H.
TI Kinect Posture Reconstruction Based on a Local Mixture of Gaussian
   Process Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Gaussian process; incremental learning; kinect; posture reconstruction
AB Depth sensor based 3D human motion estimation hardware such as Kinect has made interactive applications more popular recently. However, it is still challenging to accurately recognize postures from a single depth camera due to the inherently noisy data derived from depth images and self-occluding action performed by the user. In this paper, we propose a new real-time probabilistic framework to enhance the accuracy of live captured postures that belong to one of the action classes in the database. We adopt the Gaussian Process model as a prior to leverage the position data obtained from Kinect and marker-based motion capture system. We also incorporate a temporal consistency term into the optimization framework to constrain the velocity variations between successive frames. To ensure that the reconstructed posture resembles the accurate parts of the observed posture, we embed a set of joint reliability measurements into the optimization framework. A major drawback of Gaussian Process is its cubic learning complexity when dealing with a large database due to the inverse of a covariance matrix. To solve the problem, we propose a new method based on a local mixture of Gaussian Processes, in which Gaussian Processes are defined in local regions of the state space. Due to the significantly decreased sample size in each local Gaussian Process, the learning time is greatly reduced. At the same time, the prediction speed is enhanced as the weighted mean prediction for a given sample is determined by the nearby local models only. Our system also allows incrementally updating a specific local Gaussian Process in real time, which enhances the likelihood of adapting to run-time postures that are different from those in the database. Experimental results demonstrate that our system can generate high quality postures even under severe self-occlusion situations, which is beneficial for real-time applications such as motion-based gaming and sport training.
C1 [Liu, Zhiguang; Leung, Howard] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
   [Zhou, Liuyang] Wiscrs Informat Ltd, Wisers Res, Hong Kong, Hong Kong, Peoples R China.
   [Shum, Hubert P. H.] Northumbria Univ, Fac Engn & Environm, Newcastle Upon Tyne NE1 8ST, Tyne & Wear, England.
C3 City University of Hong Kong; Northumbria University
RP Liu, ZG (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China.
EM zhigualiu2-c@my.cityu.edu.hk; leozhou@wisers.com; howard@cityu.edu.hk;
   hubert.shum@northumbria.ac.uk
RI Shum, Hubert P. H./E-8060-2015
OI Shum, Hubert P. H./0000-0001-5651-6039
FU City University of Hong Kong [7004548]; Engineering and Physical
   Sciences Research Council (EPSRC) [EP/M002632/1]; EPSRC [EP/M002632/1]
   Funding Source: UKRI
FX The work described in this paper was partially supported by a grant from
   City University of Hong Kong (Project No. 7004548) and the Engineering
   and Physical Sciences Research Council (EPSRC) (Ref: EP/M002632/1).
NR 43
TC 30
Z9 33
U1 0
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2016
VL 22
IS 11
BP 2437
EP 2450
DI 10.1109/TVCG.2015.2510000
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DX7OV
UT WOS:000384578800009
PM 26701789
OA Green Accepted, hybrid
DA 2025-03-07
ER

PT J
AU Vehlow, C
   Beck, F
   Weiskopf, D
AF Vehlow, Corinna
   Beck, Fabian
   Weiskopf, Daniel
TI Visualizing Dynamic Hierarchies in Graph Sequences
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Dynamic graph; hierarchical graph; graph visualization
ID TASK TAXONOMY; EVOLUTION; TIME
AB Graphs are used to model relations between objects, where these objects can be grouped hierarchically based on their connectivity. In many applications, the relations change over time and so does the hierarchical group structure. We developed a visualization technique that supports the analysis of the topology and the hierarchical group structure of a dynamic graph and the tracking of changes over time. Each graph of a sequence is visualized by an adjacency matrix, where the hierarchical group structure is encoded within the matrix using indentation and nested contours, complemented by icicle plots attached to the matrices. The density within and between subgroups of the hierarchy is represented within the matrices using a gray scale. To visualize changes, transitions and dissimilarities between the hierarchically structured graphs are shown using a flow metaphor and color coding. The design of our visualization technique allows us to show more than one hierarchical group structure of the same graph by stacking the sequences, where hierarchy comparison is supported not only within but also between sequences. To improve the readability, we minimize the number of crossing curves within and between sequences based on a sorting algorithm that sweeps through the sequences of hierarchies.
C1 [Vehlow, Corinna; Beck, Fabian; Weiskopf, Daniel] Univ Stuttgart, VISUS, Stuttgart, Germany.
C3 University of Stuttgart
RP Vehlow, C (corresponding author), Univ Stuttgart, VISUS, Stuttgart, Germany.
EM corinna.vehlow@visus.uni-stuttgart.de;
   fabian.beck@visus.uni-stuttgart.de; weiskopf@visus.uni-stuttgart.de
RI Weiskopf, Daniel/KWT-7459-2024
OI Weiskopf, Daniel/0000-0003-1174-1026; Beck, Fabian/0000-0003-4042-3043
FU DFG [SFB 716/D.5, DFG WE 2836/6-1]
FX This work was supported by DFG within SFB 716/D.5 and within project DFG
   WE 2836/6-1 "Visual analytics of static and dynamic networks taking into
   account uncertainty and fuzzy clustering".
NR 55
TC 19
Z9 21
U1 1
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2016
VL 22
IS 10
BP 2343
EP 2357
DI 10.1109/TVCG.2015.2507595
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DV1IM
UT WOS:000382674500012
PM 26685254
DA 2025-03-07
ER

PT J
AU Schulz, HJ
   Angelini, M
   Santucci, G
   Schumann, H
AF Schulz, Hans-Joerg
   Angelini, Marco
   Santucci, Giuseppe
   Schumann, Heidrun
TI An Enhanced Visualization Process Model for Incremental Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization pipeline; data state reference model; progressive
   visualization; proactive visualization
AB With today's technical possibilities, a stable visualization scenario can no longer be assumed as a matter of course, as underlying data and targeted display setup are much more in flux than in traditional scenarios. Incremental visualization approaches are a means to address this challenge, as they permit the user to interact with, steer, and change the visualization at intermediate time points and not just after it has been completed. In this paper, we put forward a model for incremental visualizations that is based on the established Data State Reference Model, but extends it in ways to also represent partitioned data and visualization operators to facilitate intermediate visualization updates. In combination, partitioned data and operators can be used independently and in combination to strike tailored compromises between output quality, shown data quantity, and responsiveness-i.e., frame rates. We showcase the new expressive power of this model by discussing the opportunities and challenges of incremental visualization in general and its usage in a real world scenario in particular.
C1 [Schulz, Hans-Joerg] Fraunhofer IGD Rostock, Rostock, Germany.
   [Angelini, Marco; Santucci, Giuseppe] Univ Roma La Sapienza, Dept Comp Sci, Rome, Italy.
   [Schumann, Heidrun] Univ Rostock, Comp Graph Res Grp, D-18055 Rostock, Germany.
C3 Sapienza University Rome; University of Rostock
RP Schulz, HJ (corresponding author), Fraunhofer IGD Rostock, Rostock, Germany.
EM hans-joerg.schulz@igd-r.fraunhofer.de; angelini@dis.uniroma1.it;
   santucci@dis.uniroma1.it; schumann@informatik.uni-rostock.de
RI Santucci, Giuseppe/F-3907-2011; Schulz, Hans-Jorg/G-1788-2013
OI Santucci, Giuseppe/0000-0003-4350-1123; Schulz,
   Hans-Jorg/0000-0001-9974-535X; Angelini, Marco/0000-0001-9051-6972
FU state of Mecklenburg-Vorpommern; EFRE within the project "Basic and
   Applied Research in Interactive Document Engineering and Maritime
   Graphics"
FX The authors thank Dieter Schmalstieg for fruitful discussions on the
   architectural aspects of incremental visualization, as well as the
   reviewers for their feedback. Partial funding by the state of
   Mecklenburg-Vorpommern and EFRE within the project "Basic and Applied
   Research in Interactive Document Engineering and Maritime Graphics" is
   gratefully acknowledged.
NR 39
TC 30
Z9 35
U1 0
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2016
VL 22
IS 7
BP 1830
EP 1842
DI 10.1109/TVCG.2015.2462356
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DO0NI
UT WOS:000377475200005
PM 27244708
DA 2025-03-07
ER

PT J
AU Chen, SM
   Yuan, XR
   Wang, ZH
   Guo, C
   Liang, J
   Wang, ZC
   Zhang, XL
   Zhang, JW
AF Chen, Siming
   Yuan, Xiaoru
   Wang, Zhenhuang
   Guo, Cong
   Liang, Jie
   Wang, Zuchao
   Zhang, Xiaolong (Luke)
   Zhang, Jiawan
TI Interactive Visual Discovering of Movement Patterns from Sparsely
   Sampled Geo-tagged Social Media Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Spatial temporal visual analytics; Geo-tagged social media; Sparsely
   sampling; Uncertainty. Movement
ID VISUALIZATION; AGGREGATION; EXPLORATION
AB Social media data with geotags can be used to track people's movements in their daily lives. By providing both rich text and movement information, visual analysis on social media data can be both interesting and challenging. In contrast to traditional movement data, the sparseness and irregularity of social media data increase the difficulty of extracting movement patterns. To facilitate the understanding of people's movements, we present an interactive visual analytics system to support the exploration of sparsely sampled trajectory data from social media. We propose a heuristic model to reduce the uncertainty caused by the nature of social media data. In the proposed system, users can filter and select reliable data from each derived movement category, based on the guidance of uncertainty model and interactive selection tools. By iteratively analyzing filtered movements, users can explore the semantics of movements, including the transportation methods, frequent visiting sequences and keyword descriptions. We provide two cases to demonstrate how our system can help users to explore the movement patterns.
C1 [Chen, Siming; Yuan, Xiaoru; Wang, Zhenhuang; Guo, Cong; Liang, Jie; Wang, Zuchao] Peking Univ, Sch EECS, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.
   [Zhang, Xiaolong (Luke)] Penn State Univ, Coll Informat Sci & Technol, University Pk, PA 16802 USA.
   [Zhang, Jiawan] Tianjin Univ, Sch Comp Sci & Technol, Tianjin, Peoples R China.
   [Zhang, Jiawan] Tianjin Univ, Sch Comp Software, Tianjin, Peoples R China.
C3 Peking University; Pennsylvania Commonwealth System of Higher Education
   (PCSHE); Pennsylvania State University; Pennsylvania State University -
   University Park; Tianjin University; Tianjin University
RP Chen, SM (corresponding author), Peking Univ, Sch EECS, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.
EM siming.chen@pku.edu.cn; xiaoru.yuan@pku.edu.cn;
   zhenhuang.wang@pku.edu.cn; cong.guo@pku.edu.cn; jie.liang@pku.edu.cn;
   zuchao.wang@pku.edu.cn; lzhang@ist.psu.edu; jwzhang@tju.edu.cn
RI Chen, Siming/AAK-1874-2020; Liang, Jie/R-6331-2017; Yuan,
   Xiaoru/E-1798-2013
OI Yuan, Xiaoru/0000-0002-7233-980X; Zhang, Xiaolong/0000-0002-6828-4930;
   Zhang, Jiawan/0000-0002-0667-6744
FU NSFC [61170204, 61232012]; National Program on Key Basic Research
   Project (973 Program) [2015CB352500]; PKU-Qihu Joint Data Visual
   Analytics Research Center
FX The authors wish to thank Ziteng Wang and Siqi Tu for the discussions
   and evaluations, and the anonymous reviewers for their valuable
   comments. This work is supported by NSFC No. 61170204, and partially
   funded by NSFC Key Project No. 61232012 and the National Program on Key
   Basic Research Project (973 Program) No. 2015CB352500. This work is also
   supported by PKU-Qihu Joint Data Visual Analytics Research Center.
NR 46
TC 92
Z9 108
U1 3
U2 48
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 270
EP 279
DI 10.1109/TVCG.2015.2467619
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400032
PM 26340781
DA 2025-03-07
ER

PT J
AU Liu, SJ
   Wang, HB
   Yan, DM
   Li, QS
   Luo, FF
   Teng, Z
   Liu, XR
AF Liu, Shengjun
   Wang, Haibo
   Yan, Dong-Ming
   Li, Qinsong
   Luo, Feifan
   Teng, Zi
   Liu, Xinru
TI Spectral Descriptors for 3D Deformable Shape Matching: A Comparative
   Survey
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Three-dimensional displays; Robustness; Eigenvalues and
   eigenfunctions; Surveys; Kernel; Computational modeling; 3D deformable
   shape matching; comparative survey; spectral descriptor
ID OBJECT RECOGNITION; SURFACE; LAPLACIAN; REGISTRATION; REDUCTION;
   RETRIEVAL; FRAMEWORK; IMAGES
AB A large number of 3D spectral descriptors have been proposed in the literature, which act as an essential component for 3D deformable shape matching and related applications. An outstanding descriptor should have desirable natures including high-level descriptive capacity, cheap storage, and robustness to a set of nuisances. It is, however, unclear which descriptors are more suitable for a particular application. This paper fills the gap by comprehensively evaluating nine state-of-the-art spectral descriptors on ten popular deformable shape datasets as well as perturbations such as mesh discretization, geometric noise, scale transformation, non-isometric setting, partiality, and topological noise. Our evaluated terms for a spectral descriptor cover four major concerns, i.e., distinctiveness, robustness, compactness, and computational efficiency. In the end, we present a summary of the overall performance and several interesting findings that can serve as guidance for the following researchers to construct a new spectral descriptor and choose an appropriate spectral feature in a particular application.
C1 [Liu, Shengjun; Wang, Haibo; Luo, Feifan; Teng, Zi; Liu, Xinru] Cent South Univ, Inst Engn Modeling & Sci Comp, Sch Math & Stat, Changsha 410083, Peoples R China.
   [Yan, Dong-Ming] Univ Chinese Acad Sci, Inst Automat, Sch Artificial Intelligence, State Key Lab Multimodal Artificial Intelligence S, Beijing 100190, Peoples R China.
   [Li, Qinsong] Cent South Univ, Big Data Inst, Changsha 410083, Peoples R China.
C3 Central South University; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS; Central South University
RP Li, QS (corresponding author), Cent South Univ, Big Data Inst, Changsha 410083, Peoples R China.
EM shjliu.cg@csu.edu.cn; wang_haibo2017@163.com; yandongming@gmail.com;
   qinsli.cg@csu.edu.cn; 1976685499@qq.com; tengzi@csu.edu.cn;
   liuxinru@csu.edu.cn
RI Luo, Feifan/LSK-4223-2024; Wang, Haibo/ABF-0773-2022
OI Yan, Dong-Ming/0000-0003-2209-2404; Teng, Zi/0009-0004-4094-9066; Wang,
   Haibo/0000-0002-6612-7528; Li, Qinsong/0000-0002-6795-8956
FU Natural Science Foundation of China [62172447, 62302530, 62172415];
   Hunan Provincial Natural Science Foundation of China [2023JJ40769]
FX This work was supported in part by the Natural Science Foundation of
   China under Grant 62172447, Grant 62302530, and Grant 62172415 and in
   part by the Hunan Provincial Natural Science Foundation of China under
   Grant 2023JJ40769. Recommended for acceptance by M. Ovsjanikov.
   (Corresponding author: Qinsong Li)
NR 154
TC 1
Z9 1
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2025
VL 31
IS 3
BP 1677
EP 1697
DI 10.1109/TVCG.2024.3368083
PG 21
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U7J0S
UT WOS:001413499200005
PM 38381625
DA 2025-03-07
ER

PT J
AU Podo, L
   Prenkaj, B
   Velardi, P
AF Podo, Luca
   Prenkaj, Bardh
   Velardi, Paola
TI Agnostic Visual Recommendation Systems: Open Challenges and Future
   Directions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Automated plot generation; machine learning for visualization;
   visualization recommendation systems; Automated plot generation; machine
   learning for visualization; visualization recommendation systems
ID VISUALIZATION; INFOGRAPHICS; DESIGN
AB Visualization Recommendation Systems (VRSs) are a novel and challenging field of study aiming to help generate insightful visualizations from data and support non-expert users in information discovery. Among the many contributions proposed in this area, some systems embrace the ambitious objective of imitating human analysts to identify relevant relationships in data and make appropriate design choices to represent these relationships with insightful charts. We denote these systems as "agnostic" VRSs since they do not rely on human-provided constraints and rules but try to learn the task autonomously. Despite the high application potential of agnostic VRSs, their progress is hindered by several obstacles, including the absence of standardized datasets to train recommendation algorithms, the difficulty of learning design rules, and defining quantitative criteria for evaluating the perceptual effectiveness of generated plots. This article summarizes the literature on agnostic VRSs and outlines promising future research directions.
C1 [Podo, Luca; Prenkaj, Bardh; Velardi, Paola] Sapienza Univ Rome, Comp Sci Dept, I-00185 Rome, Italy.
C3 Sapienza University Rome
RP Velardi, P (corresponding author), Sapienza Univ Rome, Comp Sci Dept, I-00185 Rome, Italy.
EM podo@di.uniroma1.it; prenkaj@di.uniroma1.it; velardi@di.uniroma1.it
RI Prenkaj, Bardh/AAL-6461-2020
OI Prenkaj, Bardh/0000-0002-2991-2279; Velardi, Paola/0000-0003-0884-1499
FU Project "@HOME : AI and IoT based solutions for HOme care Monitoring of
   the Elderly"; Riposizionamento Competitivo RSI Programma Regionale -FESR
   Lazio 2021-2027 [F89J23001050007]
FX This work was supported in part by the Project "@HOME : AI and IoT based
   solutions for HOme care Monitoring of the Elderly" and in part by
   Riposizionamento Competitivo RSI Programma Regionale -FESR Lazio
   2021-2027 under Grant CUP: F89J23001050007.
NR 95
TC 0
Z9 0
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2025
VL 31
IS 3
BP 1902
EP 1917
DI 10.1109/TVCG.2024.3374571
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U7J0S
UT WOS:001413499200021
PM 38466597
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Montambault, B
   Appleby, G
   Rogers, J
   Brumar, CD
   Li, MW
   Chang, R
AF Montambault, Brian
   Appleby, Gabriel
   Rogers, Jen
   Brumar, Camelia D.
   Li, Mingwei
   Chang, Remco
TI DimBridge: Interactive Explanation of Visual Patterns in Dimensionality
   Reductions with Predicate Logic
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Dimensionality reduction; Shape;
   Principal component analysis; Logic; Distortion; Predicates;
   Dimensionality Reduction; Explainable Machine Learning
ID EXPLORATION; PROJECTION; SCATTERPLOT; QUALITY
AB Dimensionality reduction techniques are widely used for visualizing high-dimensional data. However, support for interpreting patterns of dimension reduction results in the context of the original data space is often insufficient. Consequently, users may struggle to extract insights from the projections. In this paper, we introduce DimBridge, a visual analytics tool that allows users to interact with visual patterns in a projection and retrieve corresponding data patterns. DimBridge supports several interactions, allowing users to perform various analyses, from contrasting multiple clusters to explaining complex latent structures. Leveraging first-order predicate logic, DimBridge identifies subspaces in the original dimensions relevant to a queried pattern and provides an interface for users to visualize and interact with them. We demonstrate how DimBridge can help users overcome the challenges associated with interpreting visual patterns in projections.
C1 [Montambault, Brian; Appleby, Gabriel; Rogers, Jen; Brumar, Camelia D.; Li, Mingwei; Chang, Remco] Tufts Univ, Medford, MA 02155 USA.
C3 Tufts University
RP Montambault, B (corresponding author), Tufts Univ, Medford, MA 02155 USA.
EM brianmontambault@gmail.com; gabriel.appleby@tufts.edu;
   jennifer.rogers@tufts.edu; camelia_daniela.brumar@tufts.edu;
   mingwei.li@tufts.edu; remco.chang@tufts.edu
RI Brumar, Camelia/KWT-3643-2024
OI Chang, Remco/0000-0002-6484-6430
FU National Science Foundation [OAC-2118201, IIS-1452977]; Department of
   Defense [HQ014719C7056]; Merck's Exploratory Science Center
FX We sincerely thank Vanessa Meschke and Eric Toberer from the NSF
   Institute for Data Driven Dynamical Design (ID4) for their valuable
   support and contribution to this work, and Matthew Berger for his
   contribution to an earlier draft of this paper. This work was supported
   by National Science Foundation grants (OAC-2118201, IIS-1452977), the
   Department of Defense (HQ014719C7056) and Merck's Exploratory Science
   Center.
NR 95
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2025
VL 31
IS 1
BP 207
EP 217
DI 10.1109/TVCG.2024.3456391
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N9Y7G
UT WOS:001367808800003
PM 39312423
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ang, S
   Quarles, J
AF Ang, Samuel
   Quarles, John
TI SmoothRide: A Versatile Solution to Combat Cybersickness in
   Elevation-Altering Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cybersickness; Navigation; Virtual environments; Meters; Frequency
   modulation; Solid modeling; Skin; Human-centered computing-Human
   computer interaction (HCI)-Empirical studies in HCI; Human-centered
   computing-Human computer interaction (HCI)-Interaction paradigms-Virtual
   reality
ID TESTS
AB Cybersickness continues to bar many individuals from taking full advantage of virtual reality (VR) technology. Previous work has established that navigating virtual terrain with elevation changes poses a significant risk in this regard. In this paper, we investigate the effectiveness of three cybersickness reduction strategies on users performing a navigation task across virtual elevation-altering terrain. These strategies include static field of view (FOV) reduction, a flat surface approach that disables terrain collision and maintains constant elevation for users, and SmoothRide, a novel technique designed to dampen a user's perception of vertical motion as they travel. To assess the impact of these strategies, we conducted a within-subjects study involving 61 participants. Each strategy was compared against a control condition, where users navigated across terrain without any cybersickness reduction measures in place. Cybersickness data were collected using the Fast Motion Sickness Scale (FMS) and Simulator Sickness Questionnaire (SSQ), along with galvanic skin response (GSR) data. We measured user presence using the IGroup Presence questionnaire (IPQ) and a Single-Item Presence Scale (SIP). Our findings reveal that users experienced significantly lower levels of cybersickness using SmoothRide or FOV reduction. Presence scores reported on the IPQ were statistically similar between SmoothRide and the control condition. Conversely, terrain flattening had adverse effects on user presence scores, and we could not identify a significant effect on cybersickness compared to the control. We demonstrate that SmoothRide is an effective, lightweight, configurable, and easy-to-integrate tool for reducing cybersickness in simulations featuring elevation-altering terrain.
C1 [Ang, Samuel; Quarles, John] Univ Texas San Antonio, San Antonio, TX 78249 USA.
C3 University of Texas System; University of Texas at San Antonio (UTSA)
RP Ang, S (corresponding author), Univ Texas San Antonio, San Antonio, TX 78249 USA.
EM samuel.ang@utsa.edu; john.quarles@utsa.edu
FU National Science Foundation [IIS 2007041, IIS 2211785]
FX This work was funded through grants from the National Science Foundation
   (IIS 2007041, IIS 2211785). This organization had no input regarding the
   planning or details of the study.
NR 57
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7152
EP 7161
DI 10.1109/TVCG.2024.3456201
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300025
PM 39255132
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Preim, B
   Meuschke, M
   Weiss, V
AF Preim, Bernhard
   Meuschke, Monique
   Weiss, Veronika
TI A Survey of Medical Visualization Through the Lens of Metaphors
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Human-Computer interaction; interaction design; medical visualization;
   metaphors; Human-Computer interaction; interaction design; medical
   visualization; metaphors
ID VIRTUAL LIVER RESECTION; REALITY; GENERATION; RECONSTRUCTION;
   INFORMATION; EXPLORATION; SIMULATION; ALGORITHM; SYSTEM; VIDEOS
AB We provide an overview of metaphors that were used in medical visualization and related user interfaces. Metaphors are employed to translate concepts from a source domain to a target domain. The survey is grounded in a discussion of metaphor-based design involving the identification and reflection of candidate metaphors. We consider metaphors that have a source domain in one branch of medicine, e.g., the virtual mirror that solves problems in orthopedics and laparoscopy with a mirror that resembles the dentist's mirror. Other metaphors employ the physical world as the source domain, such as crepuscular rays that inspire a solution for access planning in tumor therapy. Aviation is another source of inspiration, leading to metaphors, such as surgical cockpits, surgical control towers, and surgery navigation according to an instrument flight. This paper should raise awareness for metaphors and their potential to focus the design of computer-assisted systems on useful features and a positive user experience. Limitations and potential drawbacks of a metaphor-based user interface design for medical applications are also considered.
C1 [Preim, Bernhard; Meuschke, Monique] Univ Magdeburg, D-39106 Magdeburg, Germany.
   [Weiss, Veronika] Hsch RheinMain, D-65197 Wiesbaden, Germany.
C3 Otto von Guericke University
RP Preim, B (corresponding author), Univ Magdeburg, D-39106 Magdeburg, Germany.
EM bernhard.preim@ovgu.de; meuschke@isg.cs.uni-magdeburg.de;
   veronika.weiss@hs-rm.de
RI Preim, Bernhard/AAF-6565-2021; Preim, Bernhard/E-8037-2015
OI Preim, Bernhard/0000-0001-9826-9478; Weiss, Veronika/0000-0003-4313-9259
NR 158
TC 0
Z9 0
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6639
EP 6664
DI 10.1109/TVCG.2023.3330546
PG 26
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600002
PM 37934633
OA hybrid
DA 2025-03-07
ER

PT J
AU Zhang, YJ
   Yang, Q
   Zhou, YF
   Xu, XZ
   Yang, L
   Xu, YL
AF Zhang, Yujie
   Yang, Qi
   Zhou, Yifei
   Xu, Xiaozhong
   Yang, Le
   Xu, Yiling
TI TCDM: Transformational Complexity Based Distortion Metric for Perceptual
   Point Cloud Quality Assessment
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image quality assessment; point cloud quality assessment (PCQA);
   predictive coding; vector autoregressive model; Image quality
   assessment; point cloud quality assessment (PCQA); predictive coding;
   vector autoregressive model
ID COLOR; MODEL
AB The goal of objective point cloud quality assessment (PCQA) research is to develop quantitative metrics that measure point cloud quality in a perceptually consistent manner. Merging the research of cognitive science and intuition of the human visual system (HVS), in this article, we evaluate the point cloud quality by measuring the complexity of transforming the distorted point cloud back to its reference, which in practice can be approximated by the code length of one point cloud when the other is given. For this purpose, we first make space segmentation for the reference and distorted point clouds based on a 3D Voronoi diagram to obtain a series of local patch pairs. Next, inspired by the predictive coding theory, we utilize a space-aware vector autoregressive (SA-VAR) model to encode the geometry and color channels of each reference patch with and without the distorted patch, respectively. Assuming that the residual errors follow the multi-variate Gaussian distributions, the self-complexity of the reference and transformational complexity between the reference and distorted samples are computed using covariance matrices. Additionally, the prediction terms generated by SA-VAR are introduced as one auxiliary feature to promote the final quality prediction. The effectiveness of the proposed transformational complexity based distortion metric (TCDM) is evaluated through extensive experiments conducted on five public point cloud quality assessment databases. The results demonstrate that TCDM achieves state-of-the-art (SOTA) performance, and further analysis confirms its robustness in various scenarios.
C1 [Zhang, Yujie; Xu, Yiling] Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
   [Yang, Qi] Tencent MediaLab, Shanghai 200433, Peoples R China.
   [Zhou, Yifei] Shanghai Maritime Univ, Shanghai 201306, Peoples R China.
   [Xu, Xiaozhong] Tencent MediaLab, Palo Alto, CA 94306 USA.
   [Yang, Le] Univ Canterbury, Dept Elect & Comp Engn, Christchurch 4800, New Zealand.
C3 Shanghai Jiao Tong University; Shanghai Maritime University; University
   of Canterbury
RP Xu, YL (corresponding author), Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai 200240, Peoples R China.
EM yujie19981026@sjtu.edu.cn; chinoyang@tencent.com;
   202110310163@stu.shmtu.edu.cn; xiaozhongxu@tencent.com;
   le.yang@canterbury.ac.nz; yl.xu@sjtu.edu.cn
RI Yang, Le/GZG-6603-2022
OI Yang, Qi/0000-0002-4274-3457; Zhou, Yifei/0009-0004-6960-8281; Yang,
   Le/0000-0001-7945-6323; Zhang, Yujie/0000-0002-5534-0198
FU National Natural Science Foundation of China [61971282, U20A20185]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 61971282, U20A20185.
NR 12
TC 4
Z9 4
U1 4
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6707
EP 6724
DI 10.1109/TVCG.2023.3338359
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600003
PM 38039169
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhong, HL
   Zhang, JB
   Liao, J
AF Zhong, Hongliang
   Zhang, Jingbo
   Liao, Jing
TI VQ-NeRF: Neural Reflectance Decomposition and Editing With Vector
   Quantization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Reflectivity; Lighting; Three-dimensional displays; Rendering (computer
   graphics); Geometry; Computational modeling; Image color analysis;
   Neural implicit fields; physically based rendering; vector quantization
AB We propose VQ-NeRF, a two-branch neural network model that incorporates Vector Quantization (VQ) to decompose and edit reflectance fields in 3D scenes. Conventional neural reflectance fields use only continuous representations to model 3D scenes, despite the fact that objects are typically composed of discrete materials in reality. This lack of discretization can result in noisy material decomposition and complicated material editing. To address these limitations, our model consists of a continuous branch and a discrete branch. The continuous branch follows the conventional pipeline to predict decomposed materials, while the discrete branch uses the VQ mechanism to quantize continuous materials into individual ones. By discretizing the materials, our model can reduce noise in the decomposition process and generate a segmentation map of discrete materials. Specific materials can be easily selected for further editing by clicking on the corresponding area of the segmentation outcomes. Additionally, we propose a dropout-based VQ codeword ranking strategy to predict the number of materials in a scene, which reduces redundancy in the material segmentation process. To improve usability, we also develop an interactive interface to further assist material editing. We evaluate our model on both computer-generated and real-world scenes, demonstrating its superior performance. To the best of our knowledge, our model is the first to enable discrete material editing in 3D scenes.
C1 [Zhong, Hongliang; Zhang, Jingbo; Liao, Jing] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 City University of Hong Kong
RP Liao, J (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
EM hlzhong2-c@my.cityu.edu.hk; jbzhang6-c@my.cityu.edu.hk;
   jingliao@cityu.edu.hk
RI Zhong, Hongliang/LDE-6340-2024; Zhang, Jingbo/GRX-3761-2022; Zhong,
   Hongliang/KTI-6761-2024
OI LIAO, Jing/0000-0001-7014-5377; Zhong, Hongliang/0009-0002-0840-8812;
   Zhang, Jingbo/0000-0003-0009-2315
FU GRF Research Grants Council (RGC) of the Hong Kong Special
   Administrative Region, China [CityU 11208123]
FX This work was supported by a GRF grant from the Research Grants Council
   (RGC) of the Hong Kong Special Administrative Region,China under Grant
   CityU 11208123.
NR 36
TC 0
Z9 0
U1 1
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6247
EP 6260
DI 10.1109/TVCG.2023.3330518
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000018
PM 37956018
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ge, T
   Luo, X
   Wang, YH
   Sedlmair, M
   Cheng, ZL
   Zhao, Y
   Liu, X
   Deussen, O
   Chen, BQ
AF Ge, Tong
   Luo, Xu
   Wang, Yunhai
   Sedlmair, Michael
   Cheng, Zhanglin
   Zhao, Ying
   Liu, Xin
   Deussen, Oliver
   Chen, Baoquan
TI Optimally Ordered Orthogonal Neighbor Joining Trees for Hierarchical
   Cluster Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Couplings; Clustering algorithms; Layout; Visualization; Biology;
   Germanium; Sorting; Neighbor joining; leaf ordering; orthogonal layout
ID VISUALIZATION; ALGORITHMS; MODEL
AB We propose to use optimally ordered orthogonal neighbor-joining (O-3 NJ) trees as a new way to visually explore cluster structures and outliers in multi-dimensional data. Neighbor-joining (NJ) trees are widely used in biology, and their visual representation is similar to that of dendrograms. The core difference to dendrograms, however, is that NJ trees correctly encode distances between data points, resulting in trees with varying edge lengths. We optimize NJ trees for their use in visual analysis in two ways. First, we propose to use a novel leaf sorting algorithm that helps users to better interpret adjacencies and proximities within such a tree. Second, we provide a new method to visually distill the cluster tree from an ordered NJ tree. Numerical evaluation and three case studies illustrate the benefits of this approach for exploring multi-dimensional data in areas such as biology or image analysis.
C1 [Ge, Tong; Luo, Xu; Wang, Yunhai] Shandong Univ, Dept Comp Sci, Jinan 250100, Shandong, Peoples R China.
   [Sedlmair, Michael] Univ Stuttgart, D-70174 Stuttgart, Germany.
   [Cheng, Zhanglin] SIAT, Shenzhen VisuCA Key Lab, Shenzhen 518055, Guangdong, Peoples R China.
   [Zhao, Ying] Cent South Univ, Changsha 410017, Hunan, Peoples R China.
   [Liu, Xin] Beijing Genom Inst BGI Shenzhen, Shenzhen 518083, Guangdong, Peoples R China.
   [Deussen, Oliver] Univ Konstanz, D-78464 Constance, Germany.
   [Chen, Baoquan] Peking Univ, Beijing 100871, Peoples R China.
C3 Shandong University; University of Stuttgart; Chinese Academy of
   Sciences; Shenzhen Institute of Advanced Technology, CAS; Central South
   University; Beijing Genomics Institute (BGI); University of Konstanz;
   Peking University
RP Wang, YH (corresponding author), Shandong Univ, Dept Comp Sci, Jinan 250100, Shandong, Peoples R China.
EM tgeconf@gmail.com; luoxu9days@gmail.com; cloudseawang@gmail.com;
   Michael.Sedlmair@visus.uni-stuttgart.de; zl.cheng@siat.ac.cn;
   zhaoying@csu.edu.cn; liuxin@genomics.cn; Oliver.Deussen@uni-konstanz.de;
   baoquan@pku.edu.cn
RI Deussen, Oliver/HKF-2004-2023; Liu, Xin/ABJ-9485-2022; Zhao,
   Liangyu/IAO-7294-2023; Cheng, Zhanglin/AAP-1760-2021
OI Luo, Xu/0000-0003-1501-7385; Cheng, Zhanglin/0000-0002-3360-2679; Chen,
   Baoquan/0000-0003-4702-036X; Liu, Xin/0000-0003-3256-2940
FU National Key R&D Program of China [2022ZD0160805]; NSFC [62132017,
   62141217]; Shandong Provincial Natural Science Foundation [ZQ2022JQ32];
   DFG [EXC2117-422037984, 251654672 - TRR 161]; Shenzhen Science and
   Technology Program [GJHZ20210705141402008]
FX This work was supported by the grants of the National Key R&D Program of
   China under Grant 2022ZD0160805, in part by NSFC under Grants 62132017
   and 62141217, in part by Shandong Provincial Natural Science Foundation
   under Grant ZQ2022JQ32 as well as in part by by the DFG (German Research
   Foundation) under Germany's Excellence under Grant
   Strategy-EXC2117-422037984,in part by the Project-ID 251654672 - TRR
   161, and in part by Shenzhen Science and Technology Program under Grant
   GJHZ20210705141402008.
NR 53
TC 2
Z9 2
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5034
EP 5046
DI 10.1109/TVCG.2023.3284499
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400062
PM 37294655
DA 2025-03-07
ER

PT J
AU Hu, L
   Zhang, ZH
   Zhong, CY
   Jiang, BY
   Xia, SH
AF Hu, Lei
   Zhang, Zihao
   Zhong, Chongyang
   Jiang, Boyuan
   Xia, Shihong
TI Pose-Aware Attention Network for Flexible Motion Retargeting by Body
   Part
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Deep learning; motion processing; motion retargeting
AB Motion retargeting is a fundamental problem in computer graphics and computer vision. Existing approaches usually have many strict requirements, such as the source-target skeletons needing to have the same number of joints or share the same topology. To tackle this problem, we note that skeletons with different structure may have some common body parts despite the differences in joint numbers. Following this observation, we propose a novel, flexible motion retargeting framework. The key idea of our method is to regard the body part as the basic retargeting unit rather than directly retargeting the whole body motion. To enhance the spatial modeling capability of the motion encoder, we introduce a pose-aware attention network (PAN) in the motion encoding phase. The PAN is pose-aware since it can dynamically predict the joint weights within each body part based on the input pose, and then construct a shared latent space for each body part by feature pooling. Extensive experiments show that our approach can generate better motion retargeting results both qualitatively and quantitatively than state-of-the-art methods. Moreover, we also show that our framework can generate reasonable results even for a more challenging retargeting scenario, like retargeting between bipedal and quadrupedal skeletons because of the body part retargeting strategy and PAN.
C1 [Hu, Lei; Zhang, Zihao; Zhong, Chongyang; Jiang, Boyuan; Xia, Shihong] Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Hu, Lei; Zhong, Chongyang; Jiang, Boyuan; Xia, Shihong] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS
RP Xia, SH (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing 100190, Peoples R China.; Xia, SH (corresponding author), Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
EM hulei19z@ict.ac.cn; zhangzihao@ict.ac.cn; zhongchongyang@ict.ac.cn;
   jiangboyuan20s@ict.ac.cn; xsh@ict.ac.cn
OI Hu, Lei/0000-0001-8938-5071; Zhong, Chongyang/0000-0003-0020-1892;
   Zhang, Zihao/0000-0001-6859-7518
FU National Key R#x0026;D Program #x201C;Industrial Software#x201D; Special
   Project of China [2022YFB3303202]
FX No Statement Available
NR 52
TC 0
Z9 0
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4792
EP 4808
DI 10.1109/TVCG.2023.3277918
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400082
PM 37204962
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Sevastjanova, R
   Hauptmann, H
   Deterding, S
   El-Assady, M
AF Sevastjanova, Rita
   Hauptmann, Hanna
   Deterding, Sebastian
   El-Assady, Mennatallah
TI Personalized Language Model Selection Through Gamified Elicitation of
   Contrastive Concept Preferences
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Adaptation models; Task analysis; Games; Data models; Computational
   modeling; Cognitive science; Analytical models; Language model
   personalization; gamification; visual analytics
ID GAMIFICATION
AB Language models are widely used for different Natural Language Processing tasks while suffering from a lack of personalization. Personalization can be achieved by, e.g., fine-tuning the model on training data that is created by the user (e.g., social media posts). Previous work shows that the acquisition of such data can be challenging. Instead of adapting the model's parameters, we thus suggest selecting a model that matches the user's mental model of different thematic concepts in language. In this article, we attempt to capture such individual language understanding of users. In this process, two challenges have to be considered. First, we need to counteract disengagement since the task of communicating one's language understanding typically encompasses repetitive and time-consuming actions. Second, we need to enable users to externalize their mental models in different contexts, considering that language use changes depending on the environment. In this article, we integrate methods of gamification into a visual analytics (VA) workflow to engage users in sharing their knowledge within various contexts. In particular, we contribute the design of a gameful VA playground called Concept Universe. During the four-phased game, the users build personalized concept descriptions by explaining given concept names through representative keywords. Based on their performance, the system reacts with constant visual, verbal, and auditory feedback. We evaluate the system in a user study with six participants, showing that users are engaged and provide more specific input when facing a virtual opponent. We use the generated concepts to make personalized language model suggestions.
C1 [Sevastjanova, Rita] Univ Konstanz, D-78457 Constance, Germany.
   [Hauptmann, Hanna] Univ Utrecht, NL-3584 CS Utrecht, Netherlands.
   [Deterding, Sebastian] Imperial Coll London, London SW7 2BX, England.
   [El-Assady, Mennatallah] ETH, Res Ctr Energy Networks, CH-8092 Zurich, Switzerland.
C3 University of Konstanz; Utrecht University; Imperial College London;
   Swiss Federal Institutes of Technology Domain; ETH Zurich
RP Sevastjanova, R (corresponding author), Univ Konstanz, D-78457 Constance, Germany.
EM rita.sevastjanova@uni-konstanz.de; h.j.hauptmann@uu.nl;
   s.deterding@imperial.ac.uk; melassady@ai.ethz.ch
RI ; Deterding, Sebastian/L-4649-2017; Hauptmann, Hanna/R-3492-2016
OI SEVASTJANOVA, RITA/0000-0002-2629-9579; El-Assady,
   Mennatallah/0000-0001-8526-2613; Deterding,
   Sebastian/0000-0003-0033-2104; Hauptmann, Hanna/0000-0002-6840-5341
FU Deutsche Forschungsgemein-schaft [BU1806/10-2]; ETH AI Center
FX No Statement Available
NR 41
TC 1
Z9 1
U1 1
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5449
EP 5465
DI 10.1109/TVCG.2023.3296905
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400073
PM 37494152
DA 2025-03-07
ER

PT J
AU Ghaffari, B
   Gatti, D
   Westermann, R
AF Ghaffari, Behdad
   Gatti, Davide
   Westermann, Rudiger
TI Spatio-Temporal Visual Analysis of Turbulent Superstructures in Unsteady
   Flow
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Visualization; Graphics processing units;
   Periodic structures; Data visualization; Feature extraction; Bandwidth;
   Flow visualization; large-scale data techniques; animation and
   motion-related techniques
ID SCALE STRUCTURES; WALL; VISUALIZATION; THERAPIES; FRAMEWORK
AB The large-scale motions in 3D turbulent channel flows, known as Turbulent Superstructures (TSS), play an essential role in the dynamics of small-scale structures within the turbulent boundary layer. However, as of today, there is no common agreement on the spatial and temporal relationships between these multiscale structures. We propose a novel space-time visualization technique for analyzing the temporal evolution of these multiscale structures in their spatial context and, thus, to further shed light on the conceptually different explanations of their dynamics. Since the temporal dynamics of TSS are believed to influence the structures in the turbulent boundary layer, we propose a combination of a 2D space-time velocity plot with an orthogonal 2D plot of projected 3D flow structures, which can interactively span the time and the space axis. Besides flow structures indicating the fluid motion, we propose showing the variations in derived fields as an additional source of explanation. The relationships between the structures in different spatial and temporal scales can be more effectively resolved by using various filtering operations and image registration algorithms. To reduce the information loss due to the non-injective nature of projection, spatial information is encoded into transparency or color. Since the proposed visualization is heavily demanding computational resources and memory bandwidth to stream unsteady flow fields and instantly compute derived 3D flow structures, the implementation exploits data compression, parallel computation capabilities, and high memory bandwidth on recent GPUs via the CUDA compute library.
C1 [Ghaffari, Behdad; Westermann, Rudiger] Tech Univ Munich, D-80333 Munich, Germany.
   [Gatti, Davide] Karlsruhe Inst Technol, D-76131 Karlsruhe, Germany.
C3 Technical University of Munich; Helmholtz Association; Karlsruhe
   Institute of Technology
RP Ghaffari, B (corresponding author), Tech Univ Munich, D-80333 Munich, Germany.
EM behdad.ghaffari@tum.de; davide.gatti@kit.edu; westermann@tum.de
RI Gatti, Davide/AAY-1842-2021
OI Westermann, Rudiger/0000-0002-3394-0731
FU German Research Foundation (DFG) [SPP 1881]
FX This work was supported by the German Research Foundation (DFG) within
   the Priority Programme Turbulent Superstructures under Grant SPP 1881.
NR 44
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3346
EP 3358
DI 10.1109/TVCG.2022.3232367
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700052
PM 37015508
DA 2025-03-07
ER

PT J
AU Ma, ZH
   Li, CZ
   Liu, XT
   Wu, HS
   Wen, ZK
AF Ma, Ziheng
   Li, Chengze
   Liu, Xueting
   Wu, Huisi
   Wen, Zhenkun
TI Separating Shading and Reflectance From Cartoon Illustrations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cartoon; deep learning; layer decomposition; shading extraction; shading
   removal
ID INTRINSIC IMAGE DECOMPOSITION
AB Shading plays an important role in cartoon drawings to present the 3D lighting and depth information in a 2D image to improve the visual information and pleasantness. But it also introduces apparent challenges in analyzing and processing the cartoon drawings for different computer graphics and vision applications, such as segmentation, depth estimation, and relighting. Extensive research has been made in removing or separating the shading information to facilitate these applications. Unfortunately, the existing researches only focused on natural images, which are natively different from cartoons since the shading in natural images is physically correct and can be modeled based on physical priors. However, shading in cartoons is manually created by artists, which may be imprecise, abstract, and stylized. This makes it extremely difficult to model the shading in cartoon drawings. Without modeling the shading prior, in the paper, we propose a learning-based solution to separate the shading from the original colors using a two-branch system consisting of two subnetworks. To the best of our knowledge, our method is the first attempt in separating shading information from cartoon drawings. Our method significantly outperforms the methods tailored for natural images. Extensive evaluations have been performed with convincing results in all cases.
C1 [Ma, Ziheng; Liu, Xueting; Wu, Huisi; Wen, Zhenkun] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Guangdong, Peoples R China.
   [Li, Chengze] Caritas Inst Higher Educ, Sch Comp & Informat Sci, Hong Kong 999077, Peoples R China.
C3 Shenzhen University; Saint Francis University Hong Kong
RP Wu, HS (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Guangdong, Peoples R China.
EM 2070276193@email.szu.edu.cn; ljsabc@gmail.com; xtliu@szu.edu.cn;
   hswu@szu.edu.cn; wenzk@szu.edu.cn
RI ; Li, Chengze/AAU-7168-2021
OI Ma, Ziheng/0000-0002-2628-553X; Li, Chengze/0000-0002-1519-750X; Wu,
   Huisi/0000-0002-0399-9089
FU National Natural Science Foundation of China [61973221, 62002232,
   62273241]; Natural Science Foundation of Guangdong Province, China
   [2019A1515011165]; Major Project of the New Generation of Artificial
   Intelligence [2018AAA0102900]; Research Grants Council of the Hong Kong
   Special Administrative Region, China [UGC/FDS11/E02/21]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61973221, 62002232 and 62273241, inpart
   by the Natural Science Foundation of Guangdong Province, China under
   Grant 2019A1515011165, in part by the Major Project of the New
   Generation of Artificial Intelligence under Grant 2018AAA0102900, and in
   part by the Research Grants Council of the Hong Kong Special
   Administrative Region, China under Grant UGC/FDS11/E02/21.
NR 29
TC 0
Z9 0
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3664
EP 3679
DI 10.1109/TVCG.2023.3239364
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700039
PM 37021997
DA 2025-03-07
ER

PT J
AU Ruan, SL
   Liang, ZD
   Guan, Q
   Griffin, P
   Wen, XL
   Lin, YN
   Wang, Y
AF Ruan, Shaolun
   Liang, Zhiding
   Guan, Qiang
   Griffin, Paul
   Wen, Xiaolin
   Lin, Yanna
   Wang, Yong
TI <i>VIOLET</i>: <underline>V</underline>isual
   Analyt<underline>i</underline>cs f<underline>o</underline>r
   Exp<underline>l</underline>ainable Quantum N<underline>e</underline>ural
   Ne<underline>t</underline>works
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; explainable artificial intelligence (XAI); quantum
   machine learning; qunatum neural networks
ID VISUAL ANALYTICS; DESIGN; VISUALIZATION; INFORMATION
AB With the rapid development of Quantum Machine Learning, quantum neural networks (QNN) have experienced great advancement in the past few years, harnessing the advantages of quantum computing to significantly speed up classical machine learning tasks. Despite their increasing popularity, the quantum neural network is quite counter-intuitive and difficult to understand, due to their unique quantum-specific layers (e.g., data encoding and measurement) in their architecture. It prevents QNN users and researchers from effectively understanding its inner workings and exploring the model training status. To fill the research gap, we propose VIOLET, a novel visual analytics approach to improve the explainability of quantum neural networks. Guided by the design requirements distilled from the interviews with domain experts and the literature survey, we developed three visualization views: the Encoder View unveils the process of converting classical input data into quantum states, the Ansatz View reveals the temporal evolution of quantum states in the training process, and the Feature View displays the features a QNN has learned after the training process. Two novel visual designs, i.e., satellite chart and augmented heatmap, are proposed to visually explain the variational parameters and quantum circuit measurements respectively. We evaluate VIOLET through two case studies and in-depth interviews with 12 domain experts. The results demonstrate the effectiveness and usability of VIOLET in helping QNN users and developers intuitively understand and explore quantum neural networks.
C1 [Ruan, Shaolun; Griffin, Paul; Wen, Xiaolin; Wang, Yong] Singapore Management Univ, Singapore 188065, Singapore.
   [Liang, Zhiding] Univ Notre Dame, Notre Dame, IN 46556 USA.
   [Guan, Qiang] Kent State Univ, Kent, OH 44240 USA.
   [Lin, Yanna] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
C3 Singapore Management University; University of Notre Dame; University
   System of Ohio; Kent State University; Kent State University Salem; Kent
   State University Kent; Hong Kong University of Science & Technology
RP Wang, Y (corresponding author), Singapore Management Univ, Singapore 188065, Singapore.
EM slruan.2021@phdcs.smu.edu.sg; zliang5@nd.edu; qguan@kent.edu;
   paulgriffin@smu.edu.sg; xiaolinwen@smu.edu.sg; ylindg@connect.ust.hk;
   yongwang@smu.edu.sg
RI Liang, Zhiding/GYD-7280-2022; GRIFFIN, Paul/L-9582-2016; Griffin,
   Paul/L-4696-2014
OI Griffin, Paul/0000-0002-1656-421X; Ruan, Shaolun/0000-0002-6163-9786;
   Griffin, Paul/0000-0003-2294-5980; Liang, Zhiding/0000-0002-7568-0165;
   Wen, Xiaolin/0000-0002-8562-7640; Lin, Yanna/0000-0003-3730-0827; Wang,
   Yong/0000-0002-0092-0793
FU Lee Kong Chian Fellowship
FX No Statement Available
NR 86
TC 0
Z9 0
U1 6
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2024
VL 30
IS 6
BP 2862
EP 2874
DI 10.1109/TVCG.2024.3388557
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WC8Z6
UT WOS:001252775500007
PM 38652613
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Yao, LJ
   Vuillemot, R
   Bezerianos, A
   Isenberg, P
AF Yao, Lijie
   Vuillemot, Romain
   Bezerianos, Anastasia
   Isenberg, Petra
TI Designing for Visualization in Motion: Embedding Visualizations in
   Swimming Videos
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Embedded visualization; sports analytics; design framework;
   visualization in motion
AB We report on challenges and considerations for supporting design processes for visualizations in motion embedded in sports videos. We derive our insights from analyzing swimming race visualizations and motion-related data, building a technology probe, as well as a study with designers. Understanding how to design situated visualizations in motion is important for a variety of contexts. Competitive sports coverage, in particular, increasingly includes information on athlete or team statistics and records. Although moving visual representations attached to athletes or other targets are starting to appear, systematic investigations on how to best support their design process in the context of sports videos are still missing. Our work makes several contributions in identifying opportunities for visualizations to be added to swimming competition coverage but, most importantly, in identifying requirements and challenges for designing situated visualizations in motion. Our investigations include the analysis of a survey with swimming enthusiasts on their motion-related information needs, an ideation workshop to collect designs and elicit design challenges, the design of a technology probe that allows to create embedded visualizations in motion based on real data (Fig. 1), and an evaluation with visualization designers that aimed to understand the benefits of designing directly on videos.
C1 [Yao, Lijie; Isenberg, Petra] Univ Paris Saclay, CNRS, Inria, F-91190 Gif Sur Yvette, France.
   [Bezerianos, Anastasia] Univ Paris Saclay, CNRS, Inria, LISN, F-91190 Gif Sur Yvette, France.
   [Vuillemot, Romain] Univ Lyon, Ecole Cent Lyon, CNRS, UMR5205,LIRIS, F-69134 Lyon, France.
C3 Centre National de la Recherche Scientifique (CNRS); Universite Paris
   Saclay; Inria; Centre National de la Recherche Scientifique (CNRS);
   Inria; Universite Paris Saclay; Ecole Centrale de Lyon; Institut
   National des Sciences Appliquees de Lyon - INSA Lyon; Centre National de
   la Recherche Scientifique (CNRS)
RP Yao, LJ (corresponding author), Univ Paris Saclay, CNRS, Inria, F-91190 Gif Sur Yvette, France.
EM yaolijie0219@gmail.com; romain.vuillemot@ec-lyon.fr;
   anastasiab@gmail.com; petra.isenberg@inria.fr
RI Yao, Lijie/ISS-7925-2023; Vuillemot, Romain/ABE-5719-2020
OI Vuillemot, Romain/0000-0003-1447-6926; Yao, Lijie/0000-0002-4208-5140;
   Isenberg, Petra/0000-0002-2948-6417
FU Agence Nationale de la Recherche
FX No Statement Available
NR 56
TC 2
Z9 2
U1 6
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2024
VL 30
IS 3
BP 1821
EP 1836
DI 10.1109/TVCG.2023.3341990
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IN0A9
UT WOS:001166876500005
PM 38090861
OA Green Published
DA 2025-03-07
ER

PT J
AU Khan, D
   Bohak, C
   Viola, I
AF Khan, Dawar
   Bohak, Ciril
   Viola, Ivan
TI Dr. KID: Direct Remeshing and K-Set Isometric Decomposition for Scalable
   Physicalization of Organic Shapes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Physicalization; Physical visualization; 3D printing; Isometric
   decomposition; Direct remeshing; Biological structures; Intracellular
   compartments
AB Dr. KID is an algorithm that uses isometric decomposition for the physicalization of potato-shaped organic models in a puzzle fashion. The algorithm begins with creating a simple, regular triangular surface mesh of organic shapes, followed by iterative K-means clustering and remeshing. For clustering, we need similarity between triangles (segments) which is defined as a distance function. The distance function maps each triangle's shape to a single point in the virtual 3D space. Thus, the distance between the triangles indicates their degree of dissimilarity. K-means clustering uses this distance and sorts segments into k classes. After this, remeshing is applied to minimize the distance between triangles within the same cluster by making their shapes identical. Clustering and remeshing are repeated until the distance between triangles in the same cluster reaches an acceptable threshold. We adopt a curvature-aware strategy to determine the surface thickness and finalize puzzle pieces for 3D printing. Identical hinges and holes are created for assembling the puzzle components. For smoother outcomes, we use triangle subdivision along with curvature-aware clustering, generating curved triangular patches for 3D printing. Our algorithm was evaluated using various models, and the 3D-printed results were analyzed. Findings indicate that our algorithm performs reliably on target organic shapes with minimal loss of input geometry.
C1 [Khan, Dawar; Bohak, Ciril; Viola, Ivan] King Abdullah Univ Sci & Technol, Visual Comp Ctr, Thuwal, Saudi Arabia.
C3 King Abdullah University of Science & Technology
RP Khan, D (corresponding author), King Abdullah Univ Sci & Technol, Visual Comp Ctr, Thuwal, Saudi Arabia.
EM dawar.khan@kaust.edu.sa; ciril.bohak@kaust.edu.sa;
   ivan.viola@kaust.edu.sa
RI Khan, Dawar/Q-7730-2019; Viola, Ivan/O-8944-2014
OI Viola, Ivan/0000-0003-4248-6574; Khan, Dawar/0000-0001-5864-1888
FU King Abdullah University of Science and Technology (KAUST)
FX No Statement Available
NR 53
TC 0
Z9 0
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 705
EP 715
DI 10.1109/TVCG.2023.3326595
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500115
PM 37871062
OA Green Submitted, hybrid, Green Published
DA 2025-03-07
ER

PT J
AU Miftari, E
   Durstewitz, D
   Sadlo, F
AF Miftari, Egzon
   Durstewitz, Daniel
   Sadlo, Filip
TI Visualization of Discontinuous Vector Field Topology
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Topology; Manifolds; Eigenvalues and eigenfunctions; Dynamical systems;
   Switches; Orbits; Behavioral sciences; Discontinuous vector field
   topology; equivalence in non-unique flow; non-smooth dynamical systems
ID ATTRACTORS; SEMIFLOWS
AB This paper extends the concept and the visualization of vector field topology to vector fields with discontinuities. We address the non-uniqueness of flow in such fields by introduction of a time-reversible concept of equivalence. This concept generalizes streamlines to streamsets and thus vector field topology to discontinuous vector fields in terms of invariant streamsets. We identify respective novel critical structures as well as their manifolds, investigate their interplay with traditional vector field topology, and detail the application and interpretation of our approach using specifically designed synthetic cases and a simulated case from physics.
C1 [Miftari, Egzon; Durstewitz, Daniel; Sadlo, Filip] Heidelberg Univ, Heidelberg, Germany.
C3 Ruprecht Karls University Heidelberg
RP Miftari, E (corresponding author), Heidelberg Univ, Heidelberg, Germany.
EM egzon.miftari@iwr.uni-heidelberg.de; daniel.durstewitz@zi-mannheim.de;
   sadlo@uni-heidelberg.de
FU Deutsche Forschungsgemeinschaft (DFG)
FX No Statement Available
NR 23
TC 0
Z9 0
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 45
EP 54
DI 10.1109/TVCG.2023.3326519
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500075
PM 37878439
DA 2025-03-07
ER

PT J
AU Jiao, B
   Lu, X
   Xia, JB
   Gupta, BB
   Bao, L
   Zhou, QS
AF Jiao, Bo
   Lu, Xin
   Xia, Jingbo
   Gupta, Brij Bhooshan
   Bao, Lei
   Zhou, Qingshan
TI Hierarchical Sampling for the Visualization of Large Scale-Free Graphs
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Partitioning algorithms; Scalability; Generators; Shape
   measurement; Clustering algorithms; Analytical models; Graph sampling;
   large scale-free graph; graph visualization
ID NETWORKS
AB Graph sampling frequently compresses a large graph into a limited screen space. This paper proposes a hierarchical structure model that partitions scale-free graphs into three blocks: the core, which captures the underlying community structure, the vertical graph, which represents minority structures that are important in visual analysis, and the periphery, which describes the connection structure between low-degree nodes. A new algorithm named hierarchical structure sampling (HSS) was then designed to preserve the characteristics of the three blocks, including complete replication of the connection relationship between high-degree nodes in the core, joint node/degree distribution between high- and low-degree nodes in the vertical graph, and proportional replication of the connection relationship between low-degree nodes in the periphery. Finally, the importance of some global statistical properties in visualization was analyzed. Both the global statistical properties and local visual features were used to evaluate the proposed algorithm, which verify that the algorithm can be applied to sample scale-free graphs with hundreds to one million nodes from a visualization perspective.
C1 [Jiao, Bo; Xia, Jingbo; Bao, Lei] Xiamen Univ, Sch Informat Sci & Technol, Tan Kah Kee Coll, Zhangzhou 361005, Fujian, Peoples R China.
   [Lu, Xin; Zhou, Qingshan] Foshan Univ, Sch Math & Big Data, Foshan 528011, Guangdong, Peoples R China.
   [Gupta, Brij Bhooshan] Asia Univ, Dept Comp Sci & Informat Engn, Taichung 413, Taiwan.
   [Gupta, Brij Bhooshan] Lebanese Amer Univ, Beirut 1102, Lebanon.
   [Gupta, Brij Bhooshan] King Abdulaziz Univ, Jeddah 21589, Saudi Arabia.
C3 Xiamen University; Foshan University; Asia University Taiwan; Lebanese
   American University; King Abdulaziz University
RP Jiao, B (corresponding author), Xiamen Univ, Sch Informat Sci & Technol, Tan Kah Kee Coll, Zhangzhou 361005, Fujian, Peoples R China.
EM bjluoyang@hotmail.com; luxin1024@126.com; jbxiad@xujc.com;
   gupta.brij@ieee.org; blnj2000@xujc.com; q476308142@qq.com
RI Gupta, Brij/E-9813-2011; Xia, Jingbo/MGB-5929-2025; Jiao,
   Bo/GOG-8250-2022
OI Jiao, Bo/0000-0002-8173-8060
FU Guangdong Basic and Applied Basic Research Foundation [2021A1515012289,
   2019A1515110279]; National Natural Science Foundation of China
   [61402485, 12001104, 61802063, 61901116]; Natural Science Foundation of
   Fujian Province of China [2022J01047]
FX Thisworkwas supported in part by Guangdong Basic and Applied Basic
   Research Foundation under Grants 2021A1515012289 and 2019A1515110279, in
   part by the National Natural Science Foundation of China under Grants
   61402485, 12001104, 61802063, and 61901116, and in part by the Natural
   Science Foundation of Fujian Province of China underGrant 2022J01047.
NR 45
TC 1
Z9 1
U1 2
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5111
EP 5123
DI 10.1109/TVCG.2022.3201567
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300020
PM 36006887
DA 2025-03-07
ER

PT J
AU Song, ZX
   Wang, X
   Zhu, H
   Zhou, GQ
   Wang, Q
AF Song, Zhengxi
   Wang, Xue
   Zhu, Hao
   Zhou, Guoqing
   Wang, Qing
TI Learning Reliable Gradients From Undersampled Circular Light Field for
   3D Reconstruction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Trajectory; Light fields; Reliability;
   Cameras; Image reconstruction; Estimation; 3d reconstruction; circular
   light field; CNN plus LSTM; circular epipolar plane volume (CEPV)
ID NETWORK; REGISTRATION; STEREO; DEPTH
AB The paper presents a 3D reconstruction algorithm from an undersampled circular light field (LF). With an ultra-dense angular sampling rate, every scene point captured by a circular LF corresponds to a smooth trajectory in the circular epipolar plane volume (CEPV). Thus per-pixel disparities can be calculated by retrieving the local gradients of the CEPV-trajectories. However, the continuous curve will be broken up into discrete segments in an undersampled circular LF, which leads to a noticeable deterioration of the 3D reconstruction accuracy. We observe that the coherent structure is still embedded in the discrete segments. With less noise and ambiguity, the scene points can be reconstructed using gradients from reliable epipolar plane image (EPI) regions. By analyzing the geometric characteristics of the coherent structure in the CEPV, both the trajectory itself and its gradients could be modeled as 3D predictable series. Thus a mask-guided CNN+LSTM network is proposed to learn the mapping from the CEPV with a lower angular sampling rate to the gradients under a higher angular sampling rate. To segment the reliable regions, the reliable-mask-based loss that assesses the difference between learned gradients and ground truth gradients is added to the loss function. We construct a synthetic circular LF dataset with ground truth for depth and foreground/background segmentation to train the network. Moreover, a real-scene circular LF dataset is collected for performance evaluation. Experimental results on both public and self-constructed datasets demonstrate the superiority of the proposed method over existing state-of-the-art methods.
C1 [Song, Zhengxi; Wang, Xue; Zhou, Guoqing; Wang, Qing] Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China.
   [Zhu, Hao] Nanjing Univ, Sch Elect Sci & Engn, Nanjing 210023, Jiangsu, Peoples R China.
C3 Northwestern Polytechnical University; Nanjing University
RP Wang, Q (corresponding author), Northwestern Polytech Univ, Sch Comp Sci, Xian 710072, Shaanxi, Peoples R China.
EM songzx@nwpu.edu.cn; xwang@nwpu.edu.cn; zhuhao_photo@nju.edu.cn;
   zhouguoqing@nwpu.edu.cn; qwang@nwpu.edu.cn
RI Zhou, Guoqing/AAY-2007-2020
OI Wang, Qing/0000-0003-3439-0644; Zhu, Hao/0000-0002-6756-9571; Wang,
   Xue/0009-0003-5224-906X
FU NSFC [62031023, 61801396]
FX This work was supported by NSFC under Grants 62031023 and 61801396.
NR 47
TC 4
Z9 4
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5194
EP 5207
DI 10.1109/TVCG.2022.3206207
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300026
PM 36099223
DA 2025-03-07
ER

PT J
AU Shen, JX
   Dudley, J
   Kristensson, PO
AF Shen, Junxiao
   Dudley, John
   Kristensson, Per Ola
TI Fast and Robust Mid-Air Gesture Typing for AR Headsets using 3D
   Trajectory Decoding
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 17th International Conference on High Speed Machining (HSM)
CY OCT 25-28, 2023
CL Nanjing, PEOPLES R CHINA
DE Text Entry; Machine Learning; Augmented Reality
AB We present a fast mid-air gesture keyboard for head-mounted optical see-through augmented reality (OST AR) that supports users in articulating word patterns by merely moving their own physical index finger in relation to a virtual keyboard plane without a need to indirectly control a visual 2D cursor on a keyboard plane. To realize this, we introduce a novel decoding method that directly translates users' three-dimensional fingertip gestural trajectories into their intended text. We evaluate the efficacy of the system in three studies that investigate various design aspects, such as immediate efficacy, accelerated learning, and whether it is possible to maintain performance without providing visual feedback. We find that the new 3D trajectory decoding design results in significant improvements in entry rates while maintaining low error rates. In addition, we demonstrate that users can maintain their performance even without fingertip and gesture trace visualization.
C1 [Shen, Junxiao; Dudley, John; Kristensson, Per Ola] Univ Cambridge, Cambridge, England.
C3 University of Cambridge
RP Shen, JX (corresponding author), Univ Cambridge, Cambridge, England.
EM js2283@cam.ac.uk; jjd50@cam.ac.uk; pok21@cam.ac.uk
OI Kristensson, Per Ola/0000-0002-7139-871X; Shen,
   Junxiao/0000-0002-1552-4689
NR 44
TC 9
Z9 9
U1 3
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2023
VL 29
IS 11
BP 4622
EP 4632
DI 10.1109/TVCG.2023.3320218
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA X6ZW5
UT WOS:001099919100027
PM 37782613
DA 2025-03-07
ER

PT J
AU Zheng, CW
   Lin, WB
   Xu, F
AF Zheng, Chengwei
   Lin, Wenbin
   Xu, Feng
TI A Self-Occlusion Aware Lighting Model for Real-Time Dynamic
   Reconstruction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Lighting; Surface reconstruction; Real-time systems; Geometry; Image
   reconstruction; Computational modeling; Shape; Albedo reconstruction; 3D
   dynamic reconstruction; spatially varying lighting; real-time
   reconstruction
ID INTRINSIC IMAGE DECOMPOSITION; AMBIENT OCCLUSION; SHAPE; REPRESENTATION;
   VIDEO
AB In real-time dynamic reconstruction, geometry and motion are the major focuses while appearance is not fully explored, leading to the low-quality appearance of the reconstructed surfaces. In this article, we propose a lightweight lighting model that considers spatially varying lighting conditions caused by self-occlusion. This model estimates per-vertex masks on top of a single Spherical Harmonic (SH) lighting to represent spatially varying lighting conditions without adding too much computation cost. The mask is estimated based on the local geometry of a vertex to model the self-occlusion effect, which is the major reason leading to the spatial variation of lighting. Furthermore, to use this model in dynamic reconstruction, we also improve the motion estimation quality by adding a real-time per-vertex displacement estimation step. Experiments demonstrate that both the reconstructed appearance and the motion are largely improved compared with the current state-of-the-art techniques.
C1 [Zheng, Chengwei; Lin, Wenbin; Xu, Feng] Tsinghua Univ, Sch Software & BNRist, Beijing 100190, Peoples R China.
C3 Tsinghua University
RP Zheng, CW (corresponding author), Tsinghua Univ, Sch Software & BNRist, Beijing 100190, Peoples R China.
EM zhengcw18@gmail.com; lwb20@mails.tsinghua.edu.cn; xufeng2003@gmail.com
RI Zheng, Chengwei/HPC-8073-2023; Lin, Wenbin/LOS-1323-2024
OI Zheng, Chengwei/0000-0002-3657-0297; Lin, Wenbin/0000-0001-9175-6003
FU Beijing Natural Science Foundation [JQ19015]; NSFC [62021002, 61727808];
   National Key Ramp;D Program of China [2018YFA0704000]; Institute for
   Brain and Cognitive Science; Tsinghua University (THUIBCS); Beijing
   Laboratory of Brain and Cognitive Intelligence, Beijing Municipal
   Education Commission (BLBCI)
FX This work was supported in part by the Beijing Natural Science
   Foundation under Grant JQ19015, the NSFC under Grants 62021002, and
   61727808, the National Key R & D Program of China under Grant
   2018YFA0704000. This work was supported by the Institute for Brain and
   Cognitive Science, Tsinghua University (THUIBCS) and Beijing Laboratory
   of Brain and Cognitive Intelligence, Beijing Municipal Education
   Commission (BLBCI).
NR 79
TC 0
Z9 0
U1 3
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2023
VL 29
IS 10
BP 4062
EP 4073
DI 10.1109/TVCG.2022.3178237
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8ZW3
UT WOS:001060356200003
PM 35622791
DA 2025-03-07
ER

PT J
AU Qiao, XT
   Cao, Y
   Lau, RWH
AF Qiao, Xiaotian
   Cao, Ying
   Lau, Rynson W. H.
TI Design Order Guided Visual Note Layout Optimization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual note; design order; layout optimization
ID OVERLAP; VISUALIZATION
AB With the goal of making contents easy to understand, memorize and share, a clear and easy-to-follow layout is important for visual notes. Unfortunately, since visual notes are often taken by the designers in real time while watching a video or listening to a presentation, the contents are usually not carefully structured, resulting in layouts that may be difficult for others to follow. In this article, we address this problem by proposing a novel approach to automatically optimize the layouts of visual notes. Our approach predicts the design order of a visual note and then warps the contents along the predicted design order such that the visual note can be easier to follow and understand. At the core of our approach is a learning-based framework to reason about the element-wise design orders of visual notes. In particular, we first propose a hierarchical LSTM-based architecture to predict a grid-based design order of the visual note, based on the graphical and textual information. We then derive the element-wise order from the grid-based prediction. Such an idea allows our network to be weakly-supervised, i.e., making it possible to predict dense grid-based orders from visual notes with only coarse annotations. We evaluate the effectiveness of our approach on visual notes with diverse content densities and layouts. The results show that our network can predict plausible design orders for various types of visual notes and our approach can effectively optimize their layouts in order for them to be easier to follow.
C1 [Qiao, Xiaotian] Xidian Univ, Sch Comp Sci & Technol, Xian 710071, Peoples R China.
   [Cao, Ying; Lau, Rynson W. H.] City Univ Hong Kong, Hong Kong, Peoples R China.
C3 Xidian University; City University of Hong Kong
RP Cao, Y; Lau, RWH (corresponding author), City Univ Hong Kong, Hong Kong, Peoples R China.
EM qiaoxt1992@gmail.com; caoying59@gmail.com; rynson.lau@cityu.edu.hk
RI Qiao, Xiaotian/ABB-7324-2022
OI LAU, Rynson W H/0000-0002-8957-8129; Qiao, Xiaotian/0000-0002-5351-8335
FU RGC of Hong Kong [11205620]
FX & nbsp;This work was supported in part by a General Research Fund from
   RGC of Hong Kong under Grant 11205620.
NR 66
TC 1
Z9 1
U1 0
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2023
VL 29
IS 9
BP 3922
EP 3936
DI 10.1109/TVCG.2022.3171839
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA O2BA8
UT WOS:001041912300014
PM 35503828
DA 2025-03-07
ER

PT J
AU Fleck, P
   Calepso, AS
   Hubenschmid, S
   Sedlmair, M
   Schmalstieg, D
AF Fleck, Philipp
   Calepso, Aimee Sousa
   Hubenschmid, Sebastian
   Sedlmair, Michael
   Schmalstieg, Dieter
TI RagRug: A Toolkit for Situated Analytics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Solid modeling; Encoding; Data
   models; Computational modeling; Three-dimensional displays; Augmented
   reality; immersive analytics; situated analytics; visual analytics;
   visualization
ID AUGMENTED REALITY; INFORMATION VISUALIZATION; DESIGN CONSIDERATIONS;
   MIXED REALITY
AB We present RagRug, an open-source toolkit for situated analytics. The abilities of RagRug go beyond previous immersive analytics toolkits by focusing on specific requirements emerging when using augmented reality (AR) rather than virtual reality. RagRug combines state of the art visual encoding capabilities with a comprehensive physical-virtual model, which lets application developers systematically describe the physical objects in the real world and their role in AR. We connect AR visualizations with data streams from the Internet of Things using distributed dataflow. To this end, we use reactive programming patterns so that visualizations become context-aware, i.e., they adapt to events coming in from the environment. The resulting authoring system is low-code; it emphasises describing the physical and the virtual world and the dataflow between the elements contained therein. We describe the technical design and implementation of RagRug, and report on five example applications illustrating the toolkit's abilities.
C1 [Fleck, Philipp; Schmalstieg, Dieter] Graz Univ Technol, Inst Comp G & Vis, A-8010 Graz, Austria.
   [Calepso, Aimee Sousa; Sedlmair, Michael] Univ Stuttgart, D-70174 Stuttgart, Germany.
   [Hubenschmid, Sebastian] Univ Konstanz, D-78464 Constance, Germany.
C3 Graz University of Technology; University of Stuttgart; University of
   Konstanz
RP Fleck, P (corresponding author), Graz Univ Technol, Inst Comp G & Vis, A-8010 Graz, Austria.
EM philipp.fleck@icg.tugraz.at; Aimee.Sousa-Calepso@visus.uni-stuttgart.de;
   sebastian.hubenschmid@uni-konstanz.de;
   michael.sedlmair@visus.uni-stuttgart.de; schmalstieg@tugraz.at
OI Sousa Calepso, Aimee/0000-0002-6625-0585; Hubenschmid,
   Sebastian/0000-0002-8704-8503; Schmalstieg, Dieter/0000-0003-2813-2235
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
   [390831618]; DFG [251654672 TRR 161]
FX This research was supported by the Deutsche Forschungsgemeinschaft (DFG,
   German Research Foundation) under Germany's Excellence Strategy
   underGrant EXC 2120/1 - 390831618 and the DFG under Grant 251654672 -TRR
   161
NR 80
TC 30
Z9 32
U1 1
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2023
VL 29
IS 7
BP 3281
EP 3297
DI 10.1109/TVCG.2022.3157058
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA H4XO7
UT WOS:000996011900010
PM 35254986
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhou, JH
   Wang, XM
   Wang, J
   Ye, H
   Wang, HL
   Zhou, ZH
   Han, DM
   Ying, HC
   Wu, J
   Chen, W
AF Zhou, Jiehui
   Wang, Xumeng
   Wang, Jie
   Ye, Hui
   Wang, Huanliang
   Zhou, Zihan
   Han, Dongming
   Ying, Haochao
   Wu, Jian
   Chen, Wei
TI FraudAuditor: A Visual Analytics Approach for Collusive Fraud in Health
   Insurance
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Fraud; Insurance; Drugs; Behavioral sciences; Diseases; Feature
   extraction; Visual analytics; collusive fraud; fraud detection; health
   insurance
AB Collusive fraud, in which multiple fraudsters collude to defraud health insurance funds, threatens the operation of the healthcare system. However, existing statistical and machine learning-based methods have limited ability to detect fraud in the scenario of health insurance due to the high similarity of fraudulent behaviors to normal medical visits and the lack of labeled data. To ensure the accuracy of the detection results, expert knowledge needs to be integrated with the fraud detection process. By working closely with health insurance audit experts, we propose FraudAuditor, a three-stage visual analytics approach to collusive fraud detection in health insurance. Specifically, we first allow users to interactively construct a co-visit network to holistically model the visit relationships of different patients. Second, an improved community detection algorithm that considers the strength of fraud likelihood is designed to detect suspicious fraudulent groups. Finally, through our visual interface, users can compare, investigate, and verify suspicious patient behavior with tailored visualizations that support different time scales. We conducted case studies in a real-world healthcare scenario, i.e., to help locate the actual fraud group and exclude the false positive group. The results and expert feedback proved the effectiveness and usability of the approach.
C1 [Zhou, Jiehui; Zhou, Zihan; Han, Dongming; Ying, Haochao; Chen, Wei] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
   [Wang, Xumeng] Nankai Univ, TMCC, CS, Tianjin 300071, Peoples R China.
   [Wang, Jie] Alibaba Grp, Hangzhou 311121, Zhejiang, Peoples R China.
   [Ye, Hui] Tencent, Shenzhen 518054, Guangdong, Peoples R China.
   [Ying, Haochao] Zhejiang Univ, Sch Publ Hlth, Key Lab Intelligent Prevent Med Zhejiang Prov, Hangzhou 310027, Zhejiang, Peoples R China.
   [Wu, Jian] Zhejiang Univ, Inst Wenzhou, Affiliated Hosp 2, Sch Med,Sch Publ Hlth, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University; Nankai University; Alibaba Group; Tencent; Zhejiang
   University; Zhejiang University
RP Ying, HC; Chen, W (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.; Ying, HC (corresponding author), Zhejiang Univ, Sch Publ Hlth, Key Lab Intelligent Prevent Med Zhejiang Prov, Hangzhou 310027, Zhejiang, Peoples R China.
EM zhoujiehui@zju.edu.cn; wangxumeng@nankai.edu.cn;
   siwei.wj@alibabainc.com; hazelye@tencent.com; wanghuanliang@zju.edu.cn;
   zhouzihan@zju.edu.cn; dongminghan@zju.edu.cn; haochaoying@zju.edu.cn;
   wujian2000@zju.edu.cn; chenvis@zju.edu.cn
RI Wang, Xumeng/JBJ-0416-2023; Chen, Wei/AAR-9817-2020; Zhou,
   Jiehui/KBC-2015-2024
OI Ying, Haochao/0000-0001-7832-2518; Chen, Wei/0000-0002-8365-4741; Zhou,
   Jiehui/0000-0003-0709-775X
FU NSFC [62132017, 62202244]
FX This work was supported by NSFC under Grants 62132017 and 62202244.
NR 30
TC 3
Z9 3
U1 9
U2 53
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2023
VL 29
IS 6
BP 2849
EP 2861
DI 10.1109/TVCG.2023.3261910
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F4DZ2
UT WOS:000981880500002
PM 37030774
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Fidalgo, CG
   Yan, YK
   Cho, HYS
   Sousa, M
   Lindlbauer, D
   Jorge, J
AF Fidalgo, Catarina G. G.
   Yan, Yukang
   Cho, Hyunsung
   Sousa, Mauricio
   Lindlbauer, David
   Jorge, Joaquim
TI A Survey on Remote Assistance and Training in Mixed Reality Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Training; Virtual reality; Collaboration; Mixed reality; Task analysis;
   Maintenance engineering; Visualization; Mixed Reality; Virtual Reality;
   Augmented Reality; Extended Reality; Remote; Assistance
ID AUGMENTED REALITY; COLLABORATION; DESIGN; SYSTEM; USER
AB The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a taxonomy based on degree of collaboration, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. All supplemental materials are available at https://augmented-perception.org/publications/2023-training-survey.html.
C1 [Fidalgo, Catarina G. G.] Univ Lisbon, INESC ID, Inst Super Tecn, Lisbon, Portugal.
   [Fidalgo, Catarina G. G.; Yan, Yukang; Cho, Hyunsung; Lindlbauer, David] Carnegie Mellon Univ, Pittsburgh, PA USA.
   [Sousa, Mauricio] Univ Toronto, Toronto, ON, Canada.
   [Jorge, Joaquim] INESC ID, Lisbon, Portugal.
   [Jorge, Joaquim] Univ Lisbon, Inst Super Tecn, Lisbon, Portugal.
C3 INESC-ID; Universidade de Lisboa; Carnegie Mellon University; University
   of Toronto; INESC-ID; Universidade de Lisboa; Universidade de Lisboa
RP Fidalgo, CG (corresponding author), Univ Lisbon, INESC ID, Inst Super Tecn, Lisbon, Portugal.
EM cfidalgo@andrew.cmu.edu; yukangy@andrew.cmu.edu;
   hyunsung@andrew.cmu.edu; mauricio.sousa@utoronto.ca;
   dlindlba@andrew.cmu.edu; jorgej@tecnico.ulisboa.pt
RI ; Jorge, Joaquim/C-5596-2008
OI Fidalgo, Catarina/0000-0003-1621-1999; Cho,
   Hyunsung/0000-0002-4521-2766; Lindlbauer, David/0000-0002-0809-9696;
   Yan, Yukang/0000-0001-7515-3755; Jorge, Joaquim/0000-0001-5441-4637;
   Sousa, Antonio/0000-0003-1438-2882
FU Fundacao para a Ciencia e a Tecnologia (Portuguese Foundation for
   Science and Technology) [2022.09212.PTDC, UIDB/50021/2020]; Carnegie
   Mellon/Portugal Program fellowship under the UNESCO Chair on AIXR
   [SFRH/BD/151465/2021]
FX This work is co-financed by Fundacao para a Ciencia e a Tecnologia
   (Portuguese Foundation for Science and Technology) partially through
   grants 2022.09212.PTDC (XAVIER), UIDB/50021/2020 and the Carnegie
   Mellon/Portugal Program fellowship SFRH/BD/151465/2021, under the UNESCO
   Chair on AI&XR.
NR 96
TC 14
Z9 15
U1 5
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2291
EP 2303
DI 10.1109/TVCG.2023.3247081
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D0KT0
UT WOS:000965708600001
PM 37027742
DA 2025-03-07
ER

PT J
AU Song, LC
   Chen, AP
   Li, Z
   Chen, Z
   Chen, LL
   Yuan, JS
   Xu, Y
   Geiger, A
AF Song, Liangchen
   Chen, Anpei
   Li, Zhong
   Chen, Zhang
   Chen, Lele
   Yuan, Junsong
   Xu, Yi
   Geiger, Andreas
TI NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed
   Neural Radiance Fields
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Neural rendering; free-viewpoint video; immersive video; NeRF
ID VIDEO
AB Visually exploring in a real-world 4D spatiotemporal space freely in VR has been a long-term quest. The task is especially appealing when only a few or even single RGB cameras are used for capturing the dynamic scene. To this end, we present an efficient framework capable of fast reconstruction, compact modeling, and streamable rendering. First, we propose to decompose the 4D spatiotemporal space according to temporal characteristics. Points in the 4D space are associated with probabilities of belonging to three categories: static, deforming, and new areas. Each area is represented and regularized by a separate neural field. Second, we propose a hybrid representations based feature streaming scheme for efficiently modeling the neural fields. Our approach, coined NeRFPlayer, is evaluated on dynamic scenes captured by single hand-held cameras and multi-camera arrays, achieving comparable or superior rendering performance in terms of quality and speed comparable to recent state-of-the-art methods, achieving reconstruction in 10 seconds per frame and interactive rendering. Project website: https://bit.ly/nerfplayer.
C1 [Song, Liangchen; Yuan, Junsong] SUNY Buffalo, Buffalo, NY USA.
   [Song, Liangchen; Li, Zhong; Chen, Zhang; Chen, Lele; Xu, Yi] InnoPeak Technol, OPPO US Res Ctr, Palo Alto, CA 94303 USA.
   [Chen, Anpei] Swiss Fed Inst Technol, Zurich, Switzerland.
   [Chen, Anpei; Geiger, Andreas] Univ Tubingen, Tubingen, Germany.
C3 State University of New York (SUNY) System; University at Buffalo, SUNY;
   Swiss Federal Institutes of Technology Domain; ETH Zurich; Eberhard
   Karls University of Tubingen
RP Li, Z (corresponding author), InnoPeak Technol, OPPO US Res Ctr, Palo Alto, CA 94303 USA.
EM lsong8@buffalo.edu; zhonglee323@gmail.com
RI Li, Zhong/GYU-9049-2022; Yuan, Junsong/A-5171-2011; Chen,
   Lele/AAW-1209-2021; Song, Liangchen/AAZ-9431-2021
OI Chen, Lele/0000-0002-7073-0450; Chen, Anpei/0000-0003-2150-2176; Yuan,
   Junsong/0000-0002-7901-8793; Chen, Zhang/0000-0001-8582-1024; Li,
   Zhong/0000-0002-7416-1216; Geiger, Andreas/0000-0002-8151-3726
NR 84
TC 66
Z9 70
U1 3
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2732
EP 2742
DI 10.1109/TVCG.2023.3247082
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D0JV2
UT WOS:000965684600001
PM 37027699
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Vermast, A
   Hürst, W
AF Vermast, Alissa
   Hurst, Wolfgang
TI Introducing 3D Thumbnails to Access 360-Degree Videos in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE 360-degree video; video search; 360-degree video interaction; interfaces
   for video collections
AB 360 degrees videos provide an immersive experience, especially when watched in virtual reality (VR). Yet, even though the video data is inherently three-dimensional, interfaces to access datasets of such videos in VR almost always use two-dimensional thumbnails shown in a grid on a flat or curved plane. We claim that using spherical and cube-shaped 3D thumbnails may provide a better user experience and be more effective at conveying the high-level subject matter of a video or when searching for a specific item in it. A comparative study against the most used existing representation, that is, 2D equirectangular projections, showed that the spherical 3D thumbnails did indeed provide the best user experience, whereas traditional 2D equirectangular projections still performed better for high-level classification tasks. Yet, they were outperformed by spherical thumbnails when participants had to search for details within the videos. Our results thus confirm a potential benefit of 3D thumbnail representations for 360-degree videos in VR, especially with respect to user experience and detailed content search and suggest a mixed interface design providing both options to the users. Supplemental materials about the user study and used data are available at https://osf.io/5vk49/.
C1 [Vermast, Alissa; Hurst, Wolfgang] Univ Utrecht, Utrecht, Netherlands.
C3 Utrecht University
RP Hürst, W (corresponding author), Univ Utrecht, Utrecht, Netherlands.
NR 36
TC 3
Z9 3
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2547
EP 2556
DI 10.1109/TVCG.2023.3247462
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D0UT0
UT WOS:000965969600001
PM 37027582
OA Green Published
DA 2025-03-07
ER

PT J
AU Seebacher, D
   Polk, T
   Janetzko, H
   Keim, DA
   Schreck, T
   Stein, M
AF Seebacher, Daniel
   Polk, Tom
   Janetzko, Halldor
   Keim, Daniel A.
   Schreck, Tobias
   Stein, Manuel
TI Investigating the Sketchplan: A Novel Way of Identifying Tactical
   Behavior in Massive Soccer Datasets
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Sports; Visualization; Data visualization; Magnetic domains; Trajectory;
   Europe; Data analysis; Sport analytics; soccer analytics; visual
   analytics
ID VISUALIZATION; DESIGN
AB Coaches and analysts prepare for upcoming matches by identifying common patterns in the positioning and movement of the competing teams in specific situations. Existing approaches in this domain typically rely on manual video analysis and formation discussion using whiteboards; or expert systems that rely on state-of-the-art video and trajectory visualization techniques and advanced user interaction. We bridge the gap between these approaches by contributing a light-weight, simplified interaction and visualization system, which we conceptualized in an iterative design study with the coaching team of a European first league soccer team. Our approach is walk-up usable by all domain stakeholders, and at the same time, can leverage advanced data retrieval and analysis techniques: a virtual magnetic tactic-board. Users place and move digital magnets on a virtual tactic-board, and these interactions get translated to spatio-temporal queries, used to retrieve relevant situations from massive team movement data. Despite such seemingly imprecise query input, our approach is highly usable, supports quick user exploration, and retrieval of relevant results via query relaxation. Appropriate simplified result visualization supports in-depth analyses to explore team behavior, such as formation detection, movement analysis, and what-if analysis. We evaluated our approach with several experts from European first league soccer clubs. The results show that our approach makes the complex analytical processes needed for the identification of tactical behavior directly accessible to domain experts for the first time, demonstrating our support of coaches in preparation for future encounters.
C1 [Seebacher, Daniel; Polk, Tom; Keim, Daniel A.] Univ Konstanz, D-78464 Constance, Germany.
   [Janetzko, Halldor] Lucerne Univ Appl Sci & Arts, CH-6002 Luzern, Switzerland.
   [Schreck, Tobias] Graz Univ Technol, A-8010 Graz, Austria.
   [Stein, Manuel] Subsequent GmbH, Constance, Germany.
C3 University of Konstanz; Graz University of Technology
RP Seebacher, D (corresponding author), Univ Konstanz, D-78464 Constance, Germany.
EM Daniel.Seebacher@uni-konstanz.de; thomas.polk@uni-konstanz.de;
   halldor.janetzko@hslu.ch; keim@uni-konstanz.de;
   tobias.schreck@cgv.tugraz.at; manuel.stein@subsequent.ai
OI Stein, Manuel/0000-0002-7198-1438; Schreck, Tobias/0000-0003-0778-8665;
   Seebacher, Daniel/0000-0003-0097-5855
FU Subsequent GmbH; German Research Foundation
FX This work was partially supported by the Subsequent GmbH and partially
   supported by the German Research Foundation as part of the priority
   programme 1894 "Volunteered Geographic Information: Interpretation,
   Visu-alisation and Social Computing".
NR 79
TC 4
Z9 4
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2023
VL 29
IS 4
BP 1920
EP 1936
DI 10.1109/TVCG.2021.3134814
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D9DT5
UT WOS:000971666900002
PM 34898435
DA 2025-03-07
ER

PT J
AU Macedo, MCF
   Apolinario, AL
AF Macedo, Marcio C. F.
   Apolinario, Antonio L.
TI Occlusion Handling in Augmented Reality: Past, Present and Future
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computer graphics; augmented reality; mutual occlusion; X-ray vision;
   computational displays; depth maps
ID TIME-OF-FLIGHT; RESOLVING OCCLUSION; VISUALIZATION TECHNIQUES; MUTUAL
   OCCLUSION; GHOSTED VIEWS; DISPLAY; TAXONOMY; OBJECTS
AB One of the main goals of many augmented reality applications is to provide a seamless integration of a real scene with additional virtual data. To fully achieve that goal, such applications must typically provide high-quality real-world tracking, support real-time performance and handle the mutual occlusion problem, estimating the position of the virtual data into the real scene and rendering the virtual content accordingly. In this survey, we focus on the occlusion handling problem in augmented reality applications and provide a detailed review of 161 articles published in this field between January 1992 and August 2020. To do so, we present a historical overview of the most common strategies employed to determine the depth order between real and virtual objects, to visualize hidden objects in a real scene, and to build occlusion-capable visual displays. Moreover, we look at the state-of-the-art techniques, highlight the recent research trends, discuss the current open problems of occlusion handling in augmented reality, and suggest future directions for research.
C1 [Macedo, Marcio C. F.; Apolinario, Antonio L.] Univ Fed Bahia, Dept Comp Sci, BR-40170110 Salvador, BA, Brazil.
C3 Universidade Federal da Bahia
RP Macedo, MCF (corresponding author), Univ Fed Bahia, Dept Comp Sci, BR-40170110 Salvador, BA, Brazil.
EM marciocfmacedo@gmail.com; antonio.apolinario@ufba.br
RI Apolinário, Antonio/R-2106-2019
OI Macedo, Marcio/0000-0003-2729-7193; Apolinario Jr., Antonio
   Lopes/0000-0002-2592-5048
FU Postdoctoral National Program of the Coordination for the Improvement of
   Higher Education Personnel (PNPD/CAPES) [88882.306277/2018-01]
FX The work of Marcio C. F. Macedo was supported in part by the
   Postdoctoral National Program of the Coordination for the Improvement of
   Higher Education Personnel (PNPD/CAPES) under Grant 88882.306277/2018-01
NR 190
TC 22
Z9 24
U1 2
U2 37
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2023
VL 29
IS 2
BP 1590
EP 1609
DI 10.1109/TVCG.2021.3117866
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7M2HO
UT WOS:000906475100023
PM 34613916
DA 2025-03-07
ER

PT J
AU Xiong, WD
   Cheung, CM
   Sander, P
   Joneja, A
AF Xiong, Weidan
   Cheung, Chong Mo
   Sander, Pedro, V
   Joneja, Ajay
TI Rationalizing Architectural Surfaces Based on Clustering of Joints
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Faces; Shape; Clustering algorithms; Geometry; Measurement; Steel;
   Indexing; Joints; discrete equivalence joint classes; conformal
   geometry; architectural geometry
ID ALGORITHM
AB We introduce the problem of clustering the set of vertices in a given 3D mesh. The problem is motivated by the need for value engineering in architectural projects. We first derive a max-norm based metric to estimate the geometric disparity between a given pair of vertices, and characterize the problem in terms of this measure. We show that this distance can be computed by using Sequential Quadratic Programming (SQP). Next we introduce two different algorithms for clustering the set of vertices on a given mesh, respectively based on two disparity measurements: max-norm and L2-norm based metric. An equivalence is established between mesh vertices and physical joints in an architectural mesh. By replacing individual joints by their equivalent cluster representative, the number of unique joints in the facade mesh, and therefore the fabrication cost, is dramatically reduced. Finally, we present an algorithm for remeshing a given surface in order to further reduce the number of joint clusters. The framework is tested for a set of real-world architectural surfaces to illustrate the effectiveness and utility of our approach. Overall, this approach tackles the important problem reducing fabrication cost of joints without modifying the underlying connectivity that was specified by the architect.
C1 [Xiong, Weidan] Nanyang Technol Univ, Singapore 639798, Singapore.
   [Cheung, Chong Mo; Sander, Pedro, V; Joneja, Ajay] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
C3 Nanyang Technological University; Hong Kong University of Science &
   Technology
RP Sander, P; Joneja, A (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
EM wxiongab@connect.ust.hk; ieterence@ust.hk; joneja@ust.hk
RI Xiong, Weidan/KCL-5461-2024
OI Xiong, Weidan/0000-0001-5490-3676
FU UGC GRF [613312, 16213519]
FX The authors would like to thank the Departments of CSE, IEDA, and ECE in
   HKUST. The authors also would like to thank our reviewers for their
   helpful comments. Part of the research was supported by UGC GRF Grants
   #613312 and #16213519.
NR 35
TC 1
Z9 1
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4274
EP 4288
DI 10.1109/TVCG.2021.3085685
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400024
PM 34061746
DA 2025-03-07
ER

PT J
AU Fereydooni, N
   Tenenboim, E
   Walker, BN
   Peeta, S
AF Fereydooni, Nadia
   Tenenboim, Einat
   Walker, Bruce N.
   Peeta, Srinivas
TI Incorporating Situation Awareness Cues in Virtual Reality for Users in
   Dynamic in-Vehicle Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 21st IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 17-22, 2022
CL Singapore, SINGAPORE
SP IEEE, IEEE Comp Soc, IEEE VGTC, ACM SIGGRAPH, Zoom, Qualcomm, Nvidia, Oppo, Advent2 Labs Consultat, Serl Io, Hiverlab
DE Virtual Reality; Situation Awareness; Perceived Risk; Fully Automated
   Vehicles
ID RISK; PERFORMANCE; ENGAGEMENT; COMMUTERS; AGE
AB The increasing ubiquity and mobility of virtual reality (VR) devices has introduced novel use cases, one of which is using VR in vehicles, both human-driven and fully automated. However, the effects of the adoption of VR-in-the-car on user task performance, safety, trust, and perceived risk are still largely unknown or not fully understood. Blocking out the physical world and substituting it with a virtual environment has many potential benefits including fewer distractions and greater productivity. However, one shortcoming of this seclusion is losing situation awareness which becomes critical in dynamic, in-vehicle environments, even when the user is not in the driver's seat. Hence, this study aims to understand the effects of providing VR users with situation awareness cues about the real world, when riding in a human-driven or a fully automated car. The results of this driving simulator experiment provide valuable insights into passengers' experience and their information needs while immersed in VR environments. Identifying passengers' unique challenges and needs, as well as developing solutions for them, is expected to improve users' travel experience towards a wider adoption of VR devices.
C1 [Fereydooni, Nadia] Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA.
   [Tenenboim, Einat; Peeta, Srinivas] Georgia Inst Technol, Sch Civil & Environm Engn, Atlanta, GA 30332 USA.
   [Walker, Bruce N.] Georgia Inst Technol, Dept Psychol, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology;
   University System of Georgia; Georgia Institute of Technology;
   University System of Georgia; Georgia Institute of Technology
RP Fereydooni, N (corresponding author), Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA.
EM nadia.fereydooni@gatech.edu; einat.tenenboim@gatech.edu;
   bruce.walker@psych.gatech.edu; peeta@gatech.edu
OI Peeta, Srinivas/0000-0002-4146-6793
NR 94
TC 6
Z9 6
U1 3
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2022
VL 28
IS 11
BP 3865
EP 3873
DI 10.1109/TVCG.2022.3203086
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 5M3ZS
UT WOS:000871038200029
PM 36048985
DA 2025-03-07
ER

PT J
AU Li, YS
   Baciu, G
AF Li, Yushi
   Baciu, George
TI SG-GAN: Adversarial Self-Attention GCN for Point Cloud Topological Parts
   Generation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Gallium nitride; Training; Shape; Generative
   adversarial networks; Solid modeling; Convolution; Generative
   adversarial network; graph convolution network; binary tree;
   self-attention; 3D shape generation; point cloud learning; gradient
   penalty
AB Point clouds are fundamental in the representation of 3D objects. However, they can also be highly unstructured and irregular. This makes it difficult to directly extend 2D generative models to three-dimensional space. In this article, we cast the problem of point cloud generation as a topological representation learning problem. In order to capture the representative features of 3D shapes in the latent space, we propose a hierarchical mixture model that integrates self-attention with an inference tree structure for constructing a point cloud generator. Based on this, we design a novel Generative Adversarial Network (GAN) architecture that is capable of generating recognizable point clouds in an unsupervised manner. The proposed adversarial framework (SG-GAN) relies on self-attention mechanism and Graph Convolution Network (GCN) to hierarchically infer the latent topology of 3D shapes. Embedding and transferring the global topology information in a tree framework allows our model to capture and enhance the structural connectivity. Furthermore, the proposed architecture endows our model with partially generating 3D structures. Finally, we propose two gradient penalty methods to stabilize the training of SG-GAN and overcome the possible mode collapse of GAN networks. To demonstrate the performance of our model, we present both quantitative and qualitative evaluations and show that SG-GAN is more efficient in training and it exceeds the state-of-the-art in 3D point cloud generation.
C1 [Li, Yushi; Baciu, George] Hong Kong Polytech Univ, Dept Comp, Hung Hom, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University
RP Li, YS (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hung Hom, Hong Kong, Peoples R China.
EM csysli@comp.polyu.edu.hk; csgeorge@polyu.edu.hk
RI Baciu, George/AAU-7143-2021
OI BACIU, George/0000-0002-1766-6357; li, yushi/0000-0001-7164-5605
NR 43
TC 14
Z9 14
U1 3
U2 46
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2022
VL 28
IS 10
BP 3499
EP 3512
DI 10.1109/TVCG.2021.3069195
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4G5UL
UT WOS:000849261100014
PM 33769934
DA 2025-03-07
ER

PT J
AU Yang, WK
   Ye, X
   Zhang, XX
   Xiao, LX
   Xia, JZ
   Wang, ZY
   Zhu, J
   Pfister, H
   Liu, SX
AF Yang, Weikai
   Ye, Xi
   Zhang, Xingxing
   Xiao, Lanxi
   Xia, Jiazhi
   Wang, Zhongyuan
   Zhu, Jun
   Pfister, Hanspeter
   Liu, Shixia
TI Diagnosing Ensemble Few-Shot Classifiers
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Feature extraction; Predictive models; Training;
   Analytical models; Data models; Behavioral sciences; Few-shot learning;
   ensemble model; subset selection; matrix visualization; scatterplot
AB The base learners and labeled samples (shots) in an ensemble few-shot classifier greatly affect the model performance. When the performance is not satisfactory, it is usually difficult to understand the underlying causes and make improvements. To tackle this issue, we propose a visual analysis method, FSLDiagnotor. Given a set of base learners and a collection of samples with a few shots, we consider two problems: 1) finding a subset of base learners that well predict the sample collections; and 2) replacing the low-quality shots with more representative ones to adequately represent the sample collections. We formulate both problems as sparse subset selection and develop two selection algorithms to recommend appropriate learners and shots, respectively. A matrix visualization and a scatterplot are combined to explain the recommended learners and shots in context and facilitate users in adjusting them. Based on the adjustment, the algorithm updates the recommendation results for another round of improvement. Two case studies are conducted to demonstrate that FSLDiagnotor helps build a few-shot classifier efficiently and increases the accuracy by 12% and 21%, respectively.
C1 [Yang, Weikai; Liu, Shixia] Tsinghua Univ, Sch Software, BNRist, Beijing 100084, Peoples R China.
   [Ye, Xi] Univ Texas Austin, Austin, TX 78712 USA.
   [Zhang, Xingxing; Zhu, Jun] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
   [Xiao, Lanxi] Tsinghua Univ, Acad Arts & Design, Beijing 100084, Peoples R China.
   [Xia, Jiazhi] Cent South Univ, Changsha 410017, Hunan, Peoples R China.
   [Wang, Zhongyuan] Kuaishou Technol Co Ltd, Beijing 100085, Peoples R China.
   [Pfister, Hanspeter] Harvard Univ, Cambridge, MA 02138 USA.
C3 Tsinghua University; University of Texas System; University of Texas
   Austin; Tsinghua University; Tsinghua University; Central South
   University; Harvard University
RP Liu, SX (corresponding author), Tsinghua Univ, Sch Software, BNRist, Beijing 100084, Peoples R China.
EM ywk19@mails.tsinghua.edu.cn; xiye@cs.utexas.edu;
   xxzhang2020@mail.tsinghua.edu.cn; xlq20@mails.tsinghua.edu.cn;
   xiajiazhi@csu.edu.cn; wzhy@outlook.com; dcszj@tsinghua.edu.cn;
   pfister@g.harvard.edu; shixia@tsinghua.edu.cn
RI Zhang, Xingxing/HGE-4445-2022; Liu, Shi-Xia/C-5574-2016; ye,
   xi/KTH-8756-2024
OI Pfister, Hanspeter/0000-0002-3620-2582; yang, wei
   kai/0000-0002-6520-1642; Xiao, Lanxi/0009-0001-5385-1453
FU National Key R&D Program of China [2020YFB2104100]; National Natural
   Science Foundation of China [U21A20469, 61936002]; Institute Guo Qiang;
   THUIBCS; BLBCI; Tsinghua-Kuaishou Institute of Future Media Data
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2020YFB2104100, in part by the National Natural Science
   Foundation of China under Grants U21A20469, 61936002, in part by
   Institute Guo Qiang, THUIBCS, and BLBCI, and in part by the
   Tsinghua-Kuaishou Institute of Future Media Data.
NR 64
TC 13
Z9 15
U1 1
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2022
VL 28
IS 9
BP 3292
EP 3306
DI 10.1109/TVCG.2022.3182488
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3K0HP
UT WOS:000833767700017
PM 35696465
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Bhatia, H
   Hoang, D
   Morrical, N
   Pascucci, V
   Bremer, PT
   Lindstrom, P
AF Bhatia, Harsh
   Hoang, Duong
   Morrical, Nate
   Pascucci, Valerio
   Bremer, Peer-Timo
   Lindstrom, Peter
TI AMM: Adaptive Multilinear Meshes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Adaptive Meshes; Wavelets; Compression Techniques; Multiresolution
   Techniques; Streaming Data; Scalar Field Data
ID COMPRESSION; EFFICIENT; VISUALIZATION; INSTABILITY; REFINEMENT;
   SIMULATION; FRAMEWORK; WAVELETS
AB Adaptive representations are increasingly indispensable for reducing the in-memory and on-disk footprints of large-scale data. Usual solutions are designed broadly along two themes: reducing data precision, e.g., through compression, or adapting data resolution, e.g., using spatial hierarchies. Recent research suggests that combining the two approaches, i.e., adapting both resolution and precision simultaneously, can offer significant gains over using them individually. However, there currently exist no practical solutions to creating and evaluating such representations at scale. In this work, we present a new resolution-precision-adaptive representation to support hybrid data reduction schemes and offer an interface to existing tools and algorithms. Through novelties in spatial hierarchy, our representation, Adaptive Multilinear Meshes (AMM), provides considerable reduction in the mesh size. AMM creates a piecewise multilinear representation of uniformly sampled scalar data and can selectively relax or enforce constraints on conformity, continuity, and coverage, delivering a flexible adaptive representation. AMM also supports representing the function using mixed-precision values to further the achievable gains in data reduction. We describe a practical approach to creating AMM incrementally using arbitrary orderings of data and demonstrate AMM on six types of resolution and precision datastreams. By interfacing with state-of-the-art rendering tools through VTK, we demonstrate the practical and computational advantages of our representation for visualization techniques. With an open-source release of our tool to create AMM, we make such evaluation of data reduction accessible to the community, which we hope will foster new opportunities and future data reduction schemes.
C1 [Bhatia, Harsh; Bremer, Peer-Timo; Lindstrom, Peter] Lawrence Livermore Natl Lab, Ctr Appl Sci Comp, Livermore, CA 94550 USA.
   [Hoang, Duong; Morrical, Nate; Pascucci, Valerio] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
C3 United States Department of Energy (DOE); Lawrence Livermore National
   Laboratory; Utah System of Higher Education; University of Utah
RP Bhatia, H (corresponding author), Lawrence Livermore Natl Lab, Ctr Appl Sci Comp, Livermore, CA 94550 USA.
RI Hoang, Thanh/ABZ-4320-2022; pascucci, Valerio/GXF-0616-2022
OI Hoang, DT/0000-0002-4463-4270; pascucci, valerio/0000-0002-8877-2042;
   Lindstrom, Peter/0000-0003-3817-4199
FU U.S. Department of Energy (DOE) by Lawrence Livermore National
   Laboratory [DE-AC52-07NA27344]; LLNL-LDRD Program [17-SI-004]; NSF OAC
   [2127548, 1941085, 2138811]; DOE award [DE-FE0031880]; Intel Graphics
   and Visualization Institute of XeLLENCE; oneAPI Center of Excellence
   [LLNL-JRNL-771697]; NSF CMMI awards [1629660]; Div Of Civil, Mechanical,
   & Manufact Inn; Directorate For Engineering [1629660] Funding Source:
   National Science Foundation
FX This work was performed under the auspices of the U.S. Department of
   Energy (DOE) by Lawrence Livermore National Laboratory under Contract
   DE-AC52-07NA27344 and supported by the LLNL-LDRD Program under Project
   No. 17-SI-004. We thank Jeffrey Hittinger for insightful discussions
   during this project. This work was funded in part by NSF OAC awards
   2127548, 1941085, 2138811 NSF CMMI awards 1629660, DOE award
   DE-FE0031880, and the Intel Graphics and Visualization Institute of
   XeLLENCE, and oneAPI Center of Excellence. LLNL-JRNL-771697.
NR 59
TC 3
Z9 4
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2022
VL 28
IS 6
BP 2350
EP 2363
DI 10.1109/TVCG.2022.3165392
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0Z1CH
UT WOS:000790817100008
PM 35394910
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Guan, YR
   Liu, H
   Liu, K
   Yin, KX
   Hu, RZ
   van Kaick, O
   Zhang, Y
   Yumer, E
   Carr, N
   Mech, R
   Zhang, H
AF Guan, Yanran
   Liu, Han
   Liu, Kun
   Yin, Kangxue
   Hu, Ruizhen
   van Kaick, Oliver
   Zhang, Yan
   Yumer, Ersin
   Carr, Nathan
   Mech, Radomir
   Zhang, Hao
TI FAME: 3D Shape Generation via Functionality-Aware Model Evolution
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Three-dimensional displays; Tools; Solid modeling; Computational
   modeling; Sociology; Statistics; Cross-category hybrids;
   functionality-aware shape modeling; functionality partial matching; set
   evolution
AB We introduce a modeling tool which can evolve a set of 3D objects in a functionality-aware manner. Our goal is for the evolution to generate large and diverse sets of plausible 3D objects for data augmentation, constrained modeling, as well as open-ended exploration to possibly inspire new designs. Starting with an initial population of 3D objects belonging to one or more functional categories, we evolve the shapes through part recombination to produce generations of hybrids or crossbreeds between parents from the heterogeneous shape collection. Evolutionary selection of offsprings is guided both by a functional plausibility score derived from functionality analysis of shapes in the initial population and user preference, as in a design gallery. Since cross-category hybridization may result in offsprings not belonging to any of the known functional categories, we develop a means for functionality partial matching to evaluate functional plausibility on partial shapes. We show a variety of plausible hybrid shapes generated by our functionality-aware model evolution, which can complement existing datasets as training data and boost the performance of contemporary data-driven segmentation schemes, especially in challenging cases. Our tool supports constrained modeling, allowing users to restrict or steer the model evolution with functionality labels. At the same time, unexpected yet functional object prototypes can emerge during open-ended exploration owing to structure breaking when evolving a heterogeneous collection.
C1 [Guan, Yanran; van Kaick, Oliver] Carleton Univ, Sch Comp Sci, Ottawa, ON K1S 5B6, Canada.
   [Liu, Han] Elect Arts, Ashburn, VA 20147 USA.
   [Liu, Kun; Zhang, Yan] Nanjing Univ, Dept Comp Sci & Technol, Nanjing 210093, Peoples R China.
   [Yin, Kangxue] NVIDIA, Toronto, ON M5V 3M4, Canada.
   [Hu, Ruizhen] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Yumer, Ersin] Uber ATG, Pittsburgh, PA 15201 USA.
   [Carr, Nathan; Mech, Radomir] Adobe, San Jose, CA 95110 USA.
   [Zhang, Hao] Simon Fraser Univ, Sch Comp Sci, Burnaby, BC V5A 1S6, Canada.
C3 Carleton University; Nanjing University; Shenzhen University; Adobe
   Systems Inc.; Simon Fraser University
RP Hu, RZ (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
EM yanran.guan@carleton.ca; han.liu@kaust.edu.sa;
   mf1733036@smail.nju.edu.cn; kangxue.yin@gmail.com; ruizhen.hu@gmail.com;
   Oliver.vanKaick@carleton.ca; zhangyannju@nju.edu.cn; meyumer@gmail.ccmi;
   ncarr@adobe.com; rmech@adobe.com; haoz@cs.sfu.ca
RI Zhang, Hao/HHM-1940-2022; Zhang, Yan/AAO-6966-2021; Guan,
   Yanran/JZT-6135-2024
OI Zhang, Hao/0000-0003-1991-119X; van Kaick, Oliver/0000-0001-9869-6832;
   Guan, Yanran/0000-0002-7348-366X; Carr, Nathan/0000-0003-2324-6428;
   Mech, Radomir/0000-0002-5558-0327
FU NSERC Canada [611370, 611649, 2015-05407]; NSFC [61872250, 62032011]
FX The authors would like to thank the reviewers for their comments and
   suggestions. This work was supported in part by NSERC Canada (under
   Grant Nos. 611370, 611649, and 2015-05407), NSFC (under Grant Nos.
   61872250 and 62032011), and in part by a gift funding from Adobe.
NR 34
TC 5
Z9 7
U1 2
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2022
VL 28
IS 4
BP 1758
EP 1772
DI 10.1109/TVCG.2020.3029759
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZH9CR
UT WOS:000761227900005
PM 33044933
OA Green Published, Green Submitted
DA 2025-03-07
ER

PT J
AU Henkin, R
   Turkay, C
AF Henkin, Rafael
   Turkay, Cagatay
TI Words of Estimative Correlation: Studying Verbalizations of Scatterplots
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Correlation; Task analysis; Data analysis; Taxonomy;
   Natural language processing; Information visualization; natural language
   generation; natural language processing; human-computer interaction
ID RANKING VISUALIZATIONS; LANGUAGE; PERCEPTION
AB Natural language and visualization are being increasingly deployed together for supporting data analysis in different ways, from multimodal interaction to enriched data summaries and insights. Yet, researchers still lack systematic knowledge on how viewers verbalize their interpretations of visualizations, and how they interpret verbalizations of visualizations in such contexts. We describe two studies aimed at identifying characteristics of data and charts that are relevant in such tasks. The first study asks participants to verbalize what they see in scatterplots that depict various levels of correlations. The second study then asks participants to choose visualizations that match a given verbal description of correlation. We extract key concepts from responses, organize them in a taxonomy and analyze the categorized responses. We observe that participants use a wide range of vocabulary across all scatterplots, but particular concepts are preferred for higher levels of correlation. A comparison between the studies reveals the ambiguity of some of the concepts. We discuss how the results could inform the design of multimodal representations aligned with the data and analytical tasks, and present a research roadmap to deepen the understanding about visualizations and natural language.
C1 [Henkin, Rafael] Queen Mary Univ London, Ctr Translat Bioinformat, London E1 4NS, England.
   [Turkay, Cagatay] Univ Warwick, Ctr Interdisciplinary Methodol, Coventry CV4 7AL, W Midlands, England.
C3 University of London; Queen Mary University London; University of
   Warwick
RP Henkin, R (corresponding author), Queen Mary Univ London, Ctr Translat Bioinformat, London E1 4NS, England.
EM r.henkin@qmul.ac.uk; cagatay.turkay@warwick.ac.uk
RI Turkay, Cagatay/AAA-3810-2020
OI Turkay, Cagatay/0000-0001-6788-251X; Henkin, Rafael/0000-0002-5511-5230
FU UK Engineering and Physical Sciences Research Council (EPSRC)
   [EP/P025501/1]; EPSRC [EP/P025501/1] Funding Source: UKRI
FX RH performed this worked while with the giCentre, City, University of
   London. The authors would like to thank Johannes Liem for helping to set
   up the online studies and discussions and Radu Jianu for helpful
   comments. This work was supported by the UK Engineering and Physical
   Sciences Research Council (EPSRC) with Grant number EP/P025501/1.
NR 50
TC 2
Z9 2
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2022
VL 28
IS 4
BP 1967
EP 1981
DI 10.1109/TVCG.2020.3023537
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZH9CR
UT WOS:000761227900020
PM 32915742
OA Green Accepted, Green Submitted
DA 2025-03-07
ER

PT J
AU Zhuang, MD
   Concannon, D
   Manley, E
AF Zhuang, Mengdie
   Concannon, David
   Manley, Ed
TI A Framework for Evaluating Dashboards in Healthcare
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Refining; Medical services; Market research; Software;
   Usability; Task analysis; Visualization; dashboard; evaluation;
   healthcare
ID INFORMATION VISUALIZATION; DIABETES DASHBOARD; QUALITY; DESIGN; SYSTEM;
   IMPLEMENTATION; SATISFACTION; TECHNOLOGY; IMPROVE; MODEL
AB In the era of 'information overload', effective information provision is essential for enabling rapid response and critical decision making. In making sense of diverse information sources, dashboards have become an indispensable tool, providing fast, effective, adaptable, and personalized access to information for professionals and the general public alike. However, these objectives place heavy requirements on dashboards as information systems in usability and effective design. Understanding these issues is challenging given the absence of consistent and comprehensive approaches to dashboard evaluation. In this article we systematically review literature on dashboard implementation in healthcare, where dashboards have been employed widely, and where there is widespread interest for improving the current state of the art, and subsequently analyse approaches taken towards evaluation. We draw upon consolidated dashboard literature and our own observations to introduce a general definition of dashboards which is more relevant to current trends, together with seven evaluation scenarios - task performance, behaviour change, interaction workflow, perceived engagement, potential utility, algorithm performance and system implementation. These scenarios distinguish different evaluation purposes which we illustrate through measurements, example studies, and common challenges in evaluation study design. We provide a breakdown of each evaluation scenario, and highlight some of the more subtle questions. We demonstrate the use of the proposed framework by a design study guided by this framework. We conclude by comparing this framework with existing literature, outlining a number of active discussion points and a set of dashboard evaluation best practices for the academic, clinical and software development communities alike.
C1 [Zhuang, Mengdie] Univ Sheffield, Informat Sch, Sheffield S10 2TN, S Yorkshire, England.
   [Zhuang, Mengdie; Concannon, David] UCL, Ctr Adv Spatial Anal, London WC1E 6BT, England.
   [Manley, Ed] Univ Leeds, Sch Geog, Leeds LS2 9JT, W Yorkshire, England.
   [Manley, Ed] Alan Turing Inst Data Sci & Artificial Intelligen, London NW1 2DB, England.
C3 University of Sheffield; University of London; University College
   London; University of Leeds
RP Zhuang, MD (corresponding author), Univ Sheffield, Informat Sch, Sheffield S10 2TN, S Yorkshire, England.
EM m.zhuang@sheffield.ac.uk; david.concannon.14@ucl.ac.uk;
   E.J.Manley@leeds.ac.uk
OI Manley, Ed/0000-0002-8904-0513; Zhuang, Mengdie/0000-0002-4546-1033
FU i-sense EPSRC IRC in Agile Early Warning Sensing Systems for Infectious
   Diseases and Antimicrobial Resistance [EP/R00529X/1.]; EPSRC
   [EP/K031953/1, EP/R00529X/1] Funding Source: UKRI
FX This work was supported by i-sense EPSRC IRC in Agile Early Warning
   Sensing Systems for Infectious Diseases and Antimicrobial Resistance
   under Grant EP/R00529X/1.
NR 90
TC 16
Z9 17
U1 9
U2 31
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2022
VL 28
IS 4
BP 1715
EP 1731
DI 10.1109/TVCG.2022.3147154
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA ZH9CR
UT WOS:000761227900002
PM 35213306
OA Green Accepted, Green Submitted
DA 2025-03-07
ER

PT J
AU Lundgard, A
   Satyanarayan, A
AF Lundgard, Alan
   Satyanarayan, Arvind
TI Accessible Visualization via Natural Language Descriptions: A Four-Level
   Model of Semantic Content
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Semantics; Natural languages; Visualization; Guidelines; Data
   visualization; Media; Graphics; Visualization; natural language;
   description; caption; semantic; model; theory; alt text; blind;
   disability; accessibility
AB Natural language descriptions sometimes accompany visualizations to better communicate and contextualize their insights, and to improve their accessibility for readers with disabilities. However, it is difficult to evaluate the usefulness of these descriptions, and how effectively they improve access to meaningful information, because we have little understanding of the semantic content they convey, and how different readers receive this content. In response, we introduce a conceptual model for the semantic content conveyed by natural language descriptions of visualizations. Developed through a grounded theory analysis of 2,147 sentences, our model spans four levels of semantic content: enumerating visualization construction properties (e.g., marks and encodings); reporting statistical concepts and relations (e.g., extrema and correlations); identifying perceptual and cognitive phenomena (e.g., complex trends and patterns); and elucidating domain-specific insights (e.g., social and political context). To demonstrate how our model can be applied to evaluate the effectiveness of visualization descriptions, we conduct a mixed-methods evaluation with 30 blind and 90 sighted readers, and find that these reader groups differ significantly on which semantic content they rank as most useful. Together, our model and findings suggest that access to meaningful information is strongly reader-specific, and that research in automatic visualization captioning should orient toward descriptions that more richly communicate overall trends and statistics, sensitive to reader preferences. Our work further opens a space of research on natural language as a data interface coequal with visualization.
C1 [Lundgard, Alan; Satyanarayan, Arvind] MIT CSAIL, Cambridge, MA 02139 USA.
C3 Massachusetts Institute of Technology (MIT)
RP Lundgard, A (corresponding author), MIT CSAIL, Cambridge, MA 02139 USA.
EM lundgard@mit.edu; arvindsalya@mit.edu
FU National Science Foundation [GRFP-1122374, III-1900991]
FX For their valuable feedback, we thank Emilie Gossiaux, Chancey Fleet,
   Michael Correll, Frank Elavsky, Beth Semel, Stephanie Tuerk, Crystal
   Lee, and the MIT Visualization Group. This work was supported by
   National Science Foundation GRFP-1122374 and III-1900991.
NR 106
TC 58
Z9 64
U1 3
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 1073
EP 1083
DI 10.1109/TVCG.2021.3114770
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA XW3DW
UT WOS:000735505300015
PM 34591762
OA Green Submitted, Bronze
DA 2025-03-07
ER

PT J
AU Fan, CR
   Matkovic, K
   Hauser, H
AF Fan, Chaoran
   Matkovic, Kresimir
   Hauser, Helwig
TI Sketch-Based Fast and Accurate Querying of Time Series Using
   Parameter-Sharing LSTM Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Time series analysis; Visualization; Deep learning; Shape; Data
   visualization; Data models; Heuristic algorithms; Machine learning;
   sketch-based interaction; visual analytics; time series data
AB Sketching is one common approach to query time series data for patterns of interest. Most existing solutions for matching the data with the interaction are based on an empirically modeled similarity function between the user's sketch and the time series data with limited efficiency and accuracy. In this article, we introduce a machine learning based solution for fast and accurate querying of time series data based on a swift sketching interaction. We build on existing LSTM technology (long short-term memory) to encode both the sketch and the time series data in a network with shared parameters. We use data from a user study to let the network learn a proper similarity function. We focus our approach on perceived similarities and achieve that the learned model also includes a user-side aspect. To the best of our knowledge, this is the first data-driven solution for querying time series data in visual analytics. Besides evaluating the accuracy and efficiency directly in a quantitative way, we also compare our solution to the recently published Qetch algorithm as well as the commonly used dynamic time warping (DTW) algorithm.
C1 [Fan, Chaoran; Hauser, Helwig] Univ Bergen, N-5007 Bergen, Norway.
   [Matkovic, Kresimir] VRVis Res Ctr, A-1220 Vienna, Austria.
C3 University of Bergen
RP Fan, CR (corresponding author), Univ Bergen, N-5007 Bergen, Norway.
EM fantasyfans2012@gmail.com; matkovic@vrvis.at; Helwig.Hauser@UiB.no
RI Matkovic, Kresimir/AAT-8896-2021
FU BMK; BMDW; Styria; SFG; Vienna Business Agency [854174]
FX lThe author would like to thank all participants of the three user
   studies for taking their valuable time to support this research. They
   also thank the reviewers of an earlier version of this paper for their
   valuable input. Parts of this work were done in the context of CEDAS,
   UiB's Center for Data Science. VRVis is funded by BMK, BMDW, Styria,
   SFG, and Vienna Business Agency in the scope of COMET -Competence
   Centers for Excellent Technologies (854174) which is managed by FFG.
NR 46
TC 13
Z9 13
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2021
VL 27
IS 12
BP 4495
EP 4506
DI 10.1109/TVCG.2020.3002950
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WN2ZU
UT WOS:000711642800013
PM 32746264
DA 2025-03-07
ER

PT J
AU Fennedy, K
   Hartmann, J
   Roy, Q
   Perrault, ST
   Vogel, D
AF Fennedy, Katherine
   Hartmann, Jeremy
   Roy, Quentin
   Perrault, Simon Tangi
   Vogel, Daniel
TI OctoPocus in VR: Using a Dynamic Guide for 3D Mid-Air Gestures in
   Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Feedforward systems; Visualization;
   Protocols; Virtual reality; Shape; Rendering (computer graphics); User
   interfaces; evaluation; methodology; artificial; augmented; and virtual
   realities
ID INTERACTION FIDELITY; REGISTRATION
AB Bau and Mackays OctoPocus dynamic guide helps novices learn, execute, and remember 2D surface gestures. We adapt OctoPocus to 3D mid-air gestures in Virtual Reality (VR) using an optimization-based recognizer, and by introducing an optional exploration mode to help visualize the spatial complexity of guides in a 3D gesture set. A replication of the original experiment protocol is used to compare OctoPocus in VR with a VR implementation of a crib-sheet. Results show that despite requiring 0.9s more reaction time than crib-sheet, OctoPocus enables participants to execute gestures 1.8s faster with 13.8 percent more accuracy during training, while remembering a comparable number of gestures. Subjective ratings support these results, 75 percent of participants found OctoPocus easier to learn and 83 percent found it more accurate. We contribute an implementation and empirical evidence demonstrating that an adaptation of the OctoPocus guide to VR is feasible and beneficial.
C1 [Fennedy, Katherine; Perrault, Simon Tangi] Singapore Univ Technol, Informat Syst Technol & Design Pillar, Singapore 487372, Singapore.
   [Hartmann, Jeremy; Roy, Quentin; Vogel, Daniel] Univ Waterloo, Sch Comp Sci, Waterloo, ON N2L 3G1, Canada.
C3 University of Waterloo
RP Fennedy, K (corresponding author), Singapore Univ Technol, Informat Syst Technol & Design Pillar, Singapore 487372, Singapore.
EM katherine.fennedy@gmail.com; jeremy.hartmann@uwaterloo.ca;
   quentin.roy@uwaterloo.ca; perrault.simon@gmail.com; dvogel@uwaterloo.ca
RI ; Fennedy, Katherine/O-8288-2016
OI Perrault, Simon/0000-0002-3105-9350; Fennedy,
   Katherine/0000-0002-9643-2072; Roy, Quentin/0000-0002-9150-0996; Vogel,
   Daniel/0000-0001-7620-0541
FU NSERC [2018-05187]; Canada Foundation for Innovation Infrastructure
   [33151]; Ontario Early Researcher [ER16-12-184]; Singapore Ministry of
   Education and Singapore University of Technology and Design (SUTD)
   Start-up Research [T1SRIS18141]
FX This work was supported in part by NSERC Discovery under Grant
   2018-05187, in part by the Canada Foundation for Innovation
   Infrastructure under Grant 33151 "Facility for Fully Interactive
   Physio-digital Spaces", in part by Ontario Early Researcher under Award
   ER16-12-184, and in part by the Singapore Ministry of Education and
   Singapore University of Technology and Design (SUTD) Start-up Research
   under Grant T1SRIS18141.
NR 61
TC 14
Z9 16
U1 0
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2021
VL 27
IS 12
BP 4425
EP 4438
DI 10.1109/TVCG.2021.3101854
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WN2ZU
UT WOS:000711642800008
PM 34347600
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Fribourg, R
   Ogawa, N
   Hoyet, L
   Argelaguet, F
   Narumi, T
   Hirose, M
   Lécuyer, A
AF Fribourg, Rebecca
   Ogawa, Nami
   Hoyet, Ludovic
   Argelaguet, Ferran
   Narumi, Takuji
   Hirose, Michitaka
   Lecuyer, Anatole
TI Virtual Co-Embodiment: Evaluation of the Sense of Agency While Sharing
   the Control of a Virtual Body Among Two Individuals
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Avatars; Atmospheric measurements; Particle measurements; Robots;
   Training; Task analysis; Collaboration; Virtual embodiment; sense of
   agency; avatars; virtual reality; user experimentation
ID PERCEIVED OWNERSHIP; NEURAL SIGNATURES; VICARIOUS AGENCY; SELF;
   EXPERIENCE; MOVEMENT; FEEDBACK; ILLUSION; GENDER; FEEL
AB In this article, we introduce a concept called "virtual co-embodiment", which enables a user to share their virtual avatar with another entity (e.g., another user, robot, or autonomous agent). We describe a proof-of-concept in which two users can be immersed from a first-person perspective in a virtual environment and can have complementary levels of control (total, partial, or none) over a shared avatar. In addition, we conducted an experiment to investigate the influence of users' level of control over the shared avatar and prior knowledge of their actions on the users' sense of agency and motor actions. The results showed that participants are good at estimating their real level of control but significantly overestimate their sense of agency when they can anticipate the motion of the avatar. Moreover, participants performed similar body motions regardless of their real control over the avatar. The results also revealed that the internal dimension of the locus of control, which is a personality trait, is negatively correlated with the user's perceived level of control. The combined results unfold a new range of applications in the fields of virtual-reality-based training and collaborative teleoperation, where users would be able to share their virtual body.
C1 [Fribourg, Rebecca; Hoyet, Ludovic; Argelaguet, Ferran; Narumi, Takuji] Univ Rennes, CNRS, IRISA, INRIA, F-35000 Rennes, France.
   [Ogawa, Nami; Narumi, Takuji; Hirose, Michitaka] Univ Tokyo, Tokyo 1138654, Japan.
   [Narumi, Takuji] JST PREST, Tokyo 1020076, Japan.
C3 Inria; Universite de Rennes; Centre National de la Recherche
   Scientifique (CNRS); University of Tokyo; Japan Science & Technology
   Agency (JST)
RP Fribourg, R (corresponding author), Univ Rennes, CNRS, IRISA, INRIA, F-35000 Rennes, France.
EM rebecca.fribourg@hotmail.fr; ogawa@cyber.t.u-tokyo.ac.jp;
   ludovic.hoyet@inria.fr; ferran.argelaguet@inria.fr;
   narumi@cyber.t.u-tokyo.ac.jp; hirose@cyber.t.u-tokyo.ac.jp;
   anatole.lecuyer@inria.fr
RI Hoyet, Ludovic/IWU-9100-2023; Narumi, Takuji/K-3925-2014
OI Fribourg, Rebecca/0000-0002-4572-477X; Hoyet,
   Ludovic/0000-0002-7373-6049; Narumi, Takuji/0000-0002-9010-1491
FU Region Bretagne; Inria IPL Avatar project; JSPS Overseas Challenge
   Program for Young Researchers; Grants-in-Aid for Scientific Research
   [20K21801] Funding Source: KAKEN
FX The author would like to thank the participants who took part in our
   experiment. This work was sponsored by the Region Bretagne, the Inria
   IPL Avatar project, and the JSPS Overseas Challenge Program for Young
   Researchers. Nami Ogawa and Rebecca Fribourg contributed equally to this
   work.
NR 75
TC 51
Z9 51
U1 1
U2 49
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2021
VL 27
IS 10
BP 4023
EP 4038
DI 10.1109/TVCG.2020.2999197
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA UL8JB
UT WOS:000692890200014
PM 32746257
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Lee, M
   Norouzi, N
   Bruder, G
   Wisniewski, PJ
   Welch, GF
AF Lee, Myungho
   Norouzi, Nahal
   Bruder, Gerd
   Wisniewski, Pamela J.
   Welch, Gregory F.
TI Mixed Reality Tabletop Gameplay: Social Interaction With a Virtual Human
   Capable of Physical Influence
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Robots; Actuators; Virtual reality; Glass; Games; Aerospace electronics;
   Shape; Augmented reality; virtual humans; physical-virtual interaction;
   latency
AB In this article, we investigate the effects of the physical influence of a virtual human (VH) in the context of face-to-face interaction in a mixed reality environment. In Experiment 1, participants played a tabletop game with a VH, in which each player takes a turn and moves their own token along the designated spots on the shared table. We compared two conditions as follows: the VH in the virtual condition moves a virtual token that can only be seen through augmented reality (AR) glasses, while the VH in the physical condition moves a physical token as the participants do; therefore the VH's token can be seen even in the periphery of the AR glasses. For the physical condition, we designed an actuator system underneath the table. The actuator moves a magnet under the table which then moves the VH's physical token over the surface of the table. Our results indicate that participants felt higher co-presence with the VH in the physical condition, and participants assessed the VH as a more physical entity compared to the VH in the virtual condition. We further observed transference effects when participants attributed the VH's ability to move physical objects to other elements in the real world. Also, the VH's physical influence improved participants' overall experience with the VH. In Experiment 2, we further looked into the question how the physical-virtual latency in movements affected the perceived plausibility of the VH's interaction with the real world. Our results indicate that a slight temporal difference between the physical token reacting to the virtual hand's movement increased the perceived realism and causality of the mixed reality interaction. We discuss potential explanations for the findings and implications for future shared mixed reality tabletop setups.
C1 [Lee, Myungho] Pusan Natl Univ, Sch Comp Sci & Engn, Busan 46241, South Korea.
   [Norouzi, Nahal; Bruder, Gerd; Wisniewski, Pamela J.; Welch, Gregory F.] Univ Cent Florida, Comp Sci, 3100 Technol Pkwy, Orlando, FL 32816 USA.
C3 Pusan National University; State University System of Florida;
   University of Central Florida
RP Lee, M (corresponding author), Pusan Natl Univ, Sch Comp Sci & Engn, Busan 46241, South Korea.
EM myungho.lee@pusan.ac.kr; nahal.norouzi@knights.ucf.edu; bruder@ucf.edu;
   Pamela.Wisniewski@ucf.edu; welch@ucf.edu
RI Lee, Myungho/AAA-4103-2022
OI Welch, Gregory/0000-0002-8243-646X; Wisniewski,
   Pamela/0000-0002-6223-1029
FU Office of Naval Research (ONR) [N00014-14-10248, N00014-12-1-1003,
   N00014-17-1-2927]; US National Science Foundation (NSF) [1800961,
   1564065]; AdventHealth
FX The work presented in this publication is supported in part by the
   Office of Naval Research (ONR) Code 34 under Dr. Peter Squire, Program
   Officer (ONR awards N00014-14-10248, N00014-12-1-1003, and
   N00014-17-1-2927). The work is also supported in part by the US National
   Science Foundation (NSF) under Grant Number 1800961 (Dr. Tonya
   Smith-Jackson) and 1564065 (Dr. Ephraim P. Glinert). We also acknowledge
   AdventHealth for their support of Prof. Welch via their Endowed Chair in
   Simulation.
NR 43
TC 24
Z9 25
U1 1
U2 28
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2021
VL 27
IS 8
BP 3534
EP 3545
DI 10.1109/TVCG.2019.2959575
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TC7PK
UT WOS:000668831500014
PM 31869794
OA Bronze
DA 2025-03-07
ER

PT J
AU Neuhauser, C
   Wang, JP
   Westermann, R
AF Neuhauser, Christoph
   Wang, Junpeng
   Westermann, Rudiger
TI Interactive Focus plus Context Rendering for Hexahedral Mesh Inspection
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Rendering (computer graphics); Strain; Visualization; Faces; Graphics
   processing units; Inspection; Sorting; Visualization of hex-meshes;
   real-time rendering; GPUs
ID SPACE
AB The visual inspection of a hexahedral mesh with respect to element quality is difficult due to clutter and occlusions that are produced when rendering all element faces or their edges simultaneously. Current approaches overcome this problem by using focus on specific elements that are then rendered opaque, and carving away all elements occluding their view. In this work, we make use of advanced GPU shader functionality to generate a focus+context rendering that highlights the elements in a selected region and simultaneously conveys the global mesh structure and deformation field. To achieve this, we propose a gradual transition from edge-based focus rendering to volumetric context rendering, by combining fragment shader-based edge and face rendering with per-pixel fragment lists. A fragment shader smoothly transitions between wireframe and face-based rendering, including focus-dependent rendering style and depth-dependent edge thickness and halos, and per-pixel fragment lists are used to blend fragments in correct visibility order. To maintain the global mesh structure in the context regions, we propose a new method to construct a sheet-based level-of-detail hierarchy and smoothly blend it with volumetric information. The user guides the exploration process by moving a lens-like hotspot. Since all operations are performed on the GPU, interactive frame rates are achieved even for large meshes.
C1 [Neuhauser, Christoph; Wang, Junpeng; Westermann, Rudiger] Tech Univ Munich, Comp Graph & Visualizat Grp, D-80333 Garching, Germany.
C3 Technical University of Munich
RP Wang, JP (corresponding author), Tech Univ Munich, Comp Graph & Visualizat Grp, D-80333 Garching, Germany.
EM neuhauser@tum.de; junpeng.wang@tum.de; westermann@tum.de
RI Wang, Junpeng/LEM-3617-2024
OI Neuhauser, Christoph/0000-0002-0290-1991; Wang,
   Junpeng/0000-0002-4607-844X; Westermann, Rudiger/0000-0002-3394-0731
FU German Research Foundation (DFG) [WE 2754/101]
FX The authors would like to thank Maximilian Bandle, Technical University
   of Munich, for his support concerning the efficient implementation of
   per-pixel fragment sorting on GPUs, and the various authors for
   providing the hexmeshes we have used. This work was supported by the
   German Research Foundation (DFG) under Grant WE 2754/101 "Stress
   Visualization via Force-Induced Material Growth."
NR 42
TC 5
Z9 5
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2021
VL 27
IS 8
BP 3505
EP 3518
DI 10.1109/TVCG.2021.3074607
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TC7PK
UT WOS:000668831500012
PM 33877981
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Coeurjolly, D
   Lachaud, JO
   Gueth, P
AF Coeurjolly, David
   Lachaud, Jacques-Olivier
   Gueth, Pierre
TI Digital Surface Regularization With Guarantees
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Surface reconstruction; Image reconstruction; Surface treatment;
   Geometry; Sea surface; Art; Three-dimensional displays; Digital
   geometry; surface reconstruction; voxel geometry processing
ID MESH GENERATION; ROBUST; ESTIMATORS; CURVATURE
AB Voxel based modeling is a very attractive way to represent complex multi-material objects. Beside artistic choices of pixel/voxel arts, representing objects as voxels allows efficient and dynamic interactions with the scene. For geometry processing purposes, many applications in material sciences, medical imaging or numerical simulation rely on a regular partitioning of the space with labeled voxels. In this article, we consider a variational approach to reconstruct interfaces in multi-labeled digital images. This approach efficiently produces piecewise smooth quadrangulated surfaces with some theoretical stability guarantee. Non-manifold parts at intersecting interfaces are handled naturally by our model. We illustrate the strength of our tool for digital surface regularization, as well as voxel art regularization by transferring colorimetric information to regularized quads and computing isotropic geodesic on digital surfaces.
C1 [Coeurjolly, David] Univ Lyon, Lab LIRIS, CNRS, F-69007 Lyon, France.
   [Lachaud, Jacques-Olivier] Univ Savoy Mt Blanc Chambery, Lab LAMA, F-73000 Chambery, France.
   [Gueth, Pierre] Adobe Syst Inc, F-75016 Paris, France.
C3 Centre National de la Recherche Scientifique (CNRS); Institut National
   des Sciences Appliquees de Lyon - INSA Lyon
RP Coeurjolly, D (corresponding author), Univ Lyon, Lab LIRIS, CNRS, F-69007 Lyon, France.
EM david.coeurjolly@liris.cnrs.fr; jacques-olivier.lachaud@univ-savoie.fr;
   pierre.gueth@adobe.com
RI Lachaud, Jacques-Olivier/JCU-8247-2023
OI Coeurjolly, David/0000-0003-3164-8697; Lachaud,
   Jacques-Olivier/0000-0003-4236-2133
FU  [CoMeDiC ANR-15-CE40-0006]
FX This work was supported in part by the CoMeDiC ANR-15-CE40-0006 research
   grant.
NR 55
TC 5
Z9 5
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2021
VL 27
IS 6
BP 2896
EP 2907
DI 10.1109/TVCG.2021.3055242
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SA9KC
UT WOS:000649620700010
PM 33507870
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kim, YM
   Ryu, S
   Kim, IJ
AF Kim, Young Min
   Ryu, Sangwoo
   Kim, Ig-Jae
TI Planar Abstraction and Inverse Rendering of 3D Indoor Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Geometry; Rendering (computer graphics); Indoor environment;
   Three-dimensional displays; Lighting; Image reconstruction; Semantics;
   3D content creation; indoor modeling; texture generation; inverse
   rendering
AB Scanning and acquiring a 3D indoor environment suffers from complex occlusions and misalignment errors. The reconstruction obtained from an RGB-D scanner contains holes in geometry and ghosting in texture. These are easily noticeable and cannot be considered as visually compelling VR content without further processing. On the other hand, the well-known Manhattan World priors successfully recreate relatively simple structures. In this article, we would like to push the limit of planar representation in indoor environments. Given an initial 3D reconstruction captured by an RGB-D sensor, we use planes not only to represent the environment geometrically but also to solve an inverse rendering problem considering texture and light. The complex process of shape inference and intrinsic imaging is greatly simplified with the help of detected planes and yet produces a realistic 3D indoor environment. The generated content can adequately represent the spatial arrangements for various AR/VR applications and can be readily composited with virtual objects possessing plausible lighting and texture.
C1 [Kim, Young Min] Seoul Natl Univ, Seoul, South Korea.
   [Ryu, Sangwoo] Pohang Univ Sci & Technol POSTECH, Pohang Si, Gyeongsangbuk D, South Korea.
   [Kim, Ig-Jae] Korea Inst Sci & Technol KIST, Seoul, South Korea.
C3 Seoul National University (SNU); Pohang University of Science &
   Technology (POSTECH); Korea Institute of Science & Technology (KIST)
RP Kim, YM (corresponding author), Seoul Natl Univ, Seoul, South Korea.
EM youngmin.kim@snu.ac.kr; rswoo@postech.ac.kr; drjay@kist.re.kr
RI Kim, Young-Min/CAI-2456-2022
OI Ryu, Sangwoo/0000-0001-8594-3961; Kim, Young Min/0000-0002-6735-8539;
   KIM, IG-JAE/0000-0002-2741-7047
FU New Faculty Startup Fund from SeoulNational University; ICT R&D program
   of MSIP/IITP [2017-0-00162]; KIST institutional program [2E29450]
FX This work was supported in part by the New Faculty Startup Fund from
   SeoulNational University, the ICT R&D program of MSIP/IITP
   [2017-0-00162, Development of Human-care Robot Technology for Aging
   Society], and the KIST institutional program [Project No. 2E29450].
NR 58
TC 4
Z9 6
U1 0
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2021
VL 27
IS 6
BP 2992
EP 3006
DI 10.1109/TVCG.2019.2960776
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SA9KC
UT WOS:000649620700017
PM 31869795
OA hybrid
DA 2025-03-07
ER

PT J
AU Itoh, Y
   Kaminokado, T
   Aksit, K
AF Itoh, Yuta
   Kaminokado, Takumi
   Aksit, Kaan
TI Beaming Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Lenses; Headphones; Retina; Proposals; Optical imaging; Optical
   distortion; Mirrors; Augmented reality; Near-eye displays; Projectors;
   Ergonomics; Power; Thermal concerns; Performance
AB Existing near-eye display designs struggle to balance between multiple trade-offs such as form factor, weight, computational requirements, and battery life. These design trade-offs are major obstacles on the path towards an all-day usable near-eye display. In this work, we address these trade-offs by, paradoxically, removing the display from near-eye displays. We present the beaming displays, a new type of near-eye display system that uses a projector and an all passive wearable headset. We modify an off-the-shelf projector with additional lenses. We install such a projector to the environment to beam images from a distance to a passive wearable headset. The beaming projection system tracks the current position of a wearable headset to project distortion-free images with correct perspectives. In our system, a wearable headset guides the beamed images to a user's retina, which are then perceived as an augmented scene within a user's field of view. In addition to providing the system design of the beaming display, we provide a physical prototype and show that the beaming display can provide resolutions as high as consumer-level near-eye displays. We also discuss the different aspects of the design space for our proposal.
C1 [Itoh, Yuta; Kaminokado, Takumi] Tokyo Inst Technol, Tokyo, Japan.
   [Aksit, Kaan] UCL, London, England.
C3 Institute of Science Tokyo; Tokyo Institute of Technology; University of
   London; University College London
RP Itoh, Y (corresponding author), Tokyo Inst Technol, Tokyo, Japan.
EM yuta.itoh@c.titech.ac.jp; kaminokado@ar.c.titech.ac.jp;
   kaksit@cs.ucl.ac.uk
RI Aksit, Kaan/AAY-6704-2020
OI AKSIT, KAAN/0000-0002-5934-5500; Itoh, Yuta/0000-0002-5901-797X
FU JST PRESTO [JPMJPR17J2]; JSPS KAKENHI [JP20H04222]; Grants-in-Aid for
   Scientific Research [20H04222] Funding Source: KAKEN
FX The authors are thankful and grateful to Duygu Ceylan, Daisuke Iwai,
   Toshiyuki Amano, and Kiyoshi Kiyokawa for the fruitful discussions. Yuta
   Itoh are partially supported by JST PRESTO Grant Number JPMJPR17J2 and
   JSPS KAKENHI Grant Number and JP20H04222, Japan.
NR 37
TC 18
Z9 19
U1 2
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2021
VL 27
IS 5
BP 2659
EP 2668
DI 10.1109/TVCG.2021.3067764
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR5YU
UT WOS:000661120200015
PM 33750701
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Peck, TC
   Good, JJ
   Seitz, K
AF Peck, Tabitha C.
   Good, Jessica J.
   Seitz, Katharina
TI Evidence of Racial Bias Using Immersive Virtual Reality: Analysis of
   Head and Hand Motions During Shooting Decisions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shooter bias; virtual reality; body -swap illusions; user studies;
   racism; bias; implicit bias; tracker data
ID IMPLICIT ASSOCIATION TEST; SUSPECT RACE; SELF-AVATAR; POLICE; RESPONSES;
   PREJUDICE; DISCRIMINATION; PERFORMANCE; EMBODIMENT; PERCEPTION
AB Shooter bias is the tendency to more quickly shoot at unarmed Black suspects compared to unarmed White suspects. The primary goal of this research was to investigate the efficacy of shooter bias simulation studies in a more realistic immersive virtual scenario instead of the traditional methodologies using desktop computers. In this paper we present results from a user study (N=99) investigating shooter and racial bias in an immersive virtual environment. Our results highlight how racial bias was observed differently in an immersive virtual environment compared to previous desktop-based simulation studies. Latency to shoot, the standard shooter bias measure, was not found to be significantly different between race or socioeconomic status in our more realistic scenarios where participants chose to raise a weapon and pull a trigger. However, more nuanced head and hand motion analysis was able to predict participants' racial shooting accuracy and implicit racism scores. Discussion of how these nuanced measures can be used for detecting behavior changes for body-swap illusions. and implications of this work related to racial justice and police brutality are discussed.
C1 [Peck, Tabitha C.; Good, Jessica J.; Seitz, Katharina] Davidson Coll, Davidson, NC 28036 USA.
C3 Davidson College
RP Peck, TC (corresponding author), Davidson Coll, Davidson, NC 28036 USA.
EM tapeck@davidson.edu; jegood@davidson.edu; kaseitz@davidson.edu
RI Peck, Tabitha/AAH-2032-2021
FU Davidson Research Initiative (DRI); Davidson Faculty, Study, and
   Research (FSR)
FX The authors wish to thank the Davidson Research Initiative (DRI), and
   the Davidson Faculty, Study, and Research (FS&R) grant for supporting
   this work. We further thank student contributor Evan Blanpied and the
   reviewers and Evan Suma Rosenburg for helpful and thoughtful feedback.
NR 62
TC 23
Z9 24
U1 4
U2 26
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2021
VL 27
IS 5
BP 2502
EP 2512
DI 10.1109/TVCG.2021.3067767
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA SR5YU
UT WOS:000661120200001
PM 33750702
DA 2025-03-07
ER

PT J
AU Akiyama, R
   Yamamoto, G
   Amano, T
   Taketomi, T
   Plopski, A
   Sandor, C
   Kato, H
AF Akiyama, Ryo
   Yamamoto, Goshiro
   Amano, Toshiyuki
   Taketomi, Takafumi
   Plopski, Alexander
   Sandor, Christian
   Kato, Hirokazu
TI Robust Reflectance Estimation for Projection-Based Appearance Control in
   a Dynamic Light Environment
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Appearance manipulation; environmental light; projector-camera system;
   reflectance estimation; spatial augmented reality
AB We present a novel method that robustly estimates the reflectance, even in an environment with dynamically changing light. To control the appearance of an object by using a projector-camera system, an appropriate estimate of the object's reflectance is vital to the creation of an appropriate projection image. Most conventional estimation methods assume static light conditions; however, in practice, the appearance is affected by both the reflectance and environmental light. In an environment with dynamically changing light, conventional reflectance estimation methods require calibration every time the conditions change. In contrast, our method requires no additional calibration because it simultaneously estimates both the reflectance and environmental light. Our method is based on the concept of creating two different light conditions by switching the projection at a rate higher than that perceived by the human eye and captures the images of a target object separately under each condition. The reflectance and environmental light are then simultaneously estimated by using the pair of images acquired under these two conditions. We implemented a projector-camera system that switches the projection on and off at 120 Hz. Experiments confirm the robustness of our method when changing the environmental light. Further, our method can robustly estimate the reflectance under practical indoor lighting conditions.
C1 [Akiyama, Ryo; Taketomi, Takafumi; Plopski, Alexander; Kato, Hirokazu] Nara Inst Sci & Technol, Ikoma, Nara 6300192, Japan.
   [Yamamoto, Goshiro] Kyoto Univ, Kyoto 6068501, Japan.
   [Amano, Toshiyuki] Wakayama Univ, Wakayama 6408510, Japan.
   [Sandor, Christian] City Univ Hong Kong, Kowloon Tong, Hong Kong, Peoples R China.
C3 Nara Institute of Science & Technology; Kyoto University; Wakayama
   University; City University of Hong Kong
RP Akiyama, R (corresponding author), Nara Inst Sci & Technol, Ikoma, Nara 6300192, Japan.
EM akiyama.ryo.ah7@is.naist.jp; goshiro@kuhp.kyoto-u.ac.jp;
   amano@sys.wakayama-u.ac.jp; takafumi-t@is.naist.jp; plopski@is.naist.jp;
   christian@sandor.com; kato@is.naist.jp
RI Taketomi, Takafumi/AAE-7546-2021; Taketomi, Takafumi/T-4236-2017
OI Plopski, Alexander/0000-0003-1354-0279; Yamamoto,
   Goshiro/0000-0002-2014-7195; Akiyama, Ryo/0000-0002-6185-3811; SANDOR,
   Christian/0000-0002-3990-2728; Amano, Toshiyuki/0000-0003-4146-1375;
   Taketomi, Takafumi/0000-0002-5353-0895; Kato,
   Hirokazu/0000-0003-3921-2871
FU JST ACT-I grant [JPMJPR17U1]; JSPS KAKENHI [16H04384]; Grants-in-Aid for
   Scientific Research [20H04230] Funding Source: KAKEN
FX This work was supported by JST ACT-I grant number JPMJPR17U1 and
   partially supported by JSPS KAKENHI grant number 16H04384.
NR 44
TC 3
Z9 3
U1 0
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2021
VL 27
IS 3
BP 2041
EP 2055
DI 10.1109/TVCG.2019.2940453
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QA9EF
UT WOS:000613744500013
PM 31514141
OA Green Published
DA 2025-03-07
ER

PT J
AU Cheng, FR
   Ming, Y
   Qu, HM
AF Cheng, Furui
   Ming, Yao
   Qu, Huamin
TI DECE: Decision Explorer with Counterfactual Explanations for Machine
   Learning Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Tabular Data; Explainable Machine Learning; Counterfactual Explanation;
   Decision Making
AB With machine learning models being increasingly applied to various decision-making scenarios, people have spent growing efforts to make machine learning models more transparent and explainable. Among various explanation techniques, counterfactual explanations have the advantages of being human-friendly and actionable-a counterfactual explanation tells the user how to gain the desired prediction with minimal changes to the input. Besides, counterfactual explanations can also serve as efficient probes to the models' decisions. In this work, we exploit the potential of counterfactual explanations to understand and explore the behavior of machine learning models. We design DECE, an interactive visualization system that helps understand and explore a model's decisions on individual instances and data subsets, supporting users ranging from decision-subjects to model developers. DECE supports exploratory analysis of model decisions by combining the strengths of counterfactual explanations at instance- and subgroup-levels. We also introduce a set of interactions that enable users to customize the generation of counterfactual explanations to find more actionable ones that can suit their needs. Through three use cases and an expert interview, we demonstrate the effectiveness of DECE in supporting decision exploration tasks and instance explanations.
C1 [Cheng, Furui; Ming, Yao; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Ming, Yao] Bloomberg LP, New York, NY USA.
C3 Hong Kong University of Science & Technology; Bloomberg L.P.
RP Cheng, FR (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
EM fchengaa@ust.hk; yming7@bloomberg.net; huamin@ust.hk
FU Hong Kong Theme-based Research Scheme [T41-709/17N]; MSRA
FX This research was supported in part by Hong Kong Theme-based Research
   Scheme grant T41-709/17N and also a grant from MSRA.
NR 44
TC 36
Z9 36
U1 1
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1438
EP 1447
DI 10.1109/TVCG.2020.3030342
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100123
PM 33074811
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhu, MF
   Chen, W
   Hu, YZ
   Hou, YX
   Liu, LJ
   Zhang, KY
AF Zhu, Minfeng
   Chen, Wei
   Hu, Yuanzhe
   Hou, Yuxuan
   Liu, Liangjun
   Zhang, Kaiyuan
TI DRGraph: An Efficient Graph Layout Algorithm for Large-scale Graphs by
   Dimensionality Reduction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Layout; Dimensionality reduction; Computational complexity;
   Optimization; Acceleration; Memory management; Principal component
   analysis; graph visualization; graph layout; dimensionality reduction;
   force-directed layout
ID VISUALIZATION
AB Efficient layout of large-scale graphs remains a challenging problem: the force-directed and dimensionality reduction-based methods suffer from high overhead for graph distance and gradient computation. In this paper, we present a new graph layout algorithm, called DRGraph, that enhances the nonlinear dimensionality reduction process with three schemes: approximating graph distances by means of a sparse distance matrix, estimating the gradient by using the negative sampling technique, and accelerating the optimization process through a multi-level layout scheme. DRGraph achieves a linear complexity for the computation and memory consumption, and scales up to large-scale graphs with millions of nodes. Experimental results and comparisons with state-of-the-art graph layout methods demonstrate that DRGraph can generate visually comparable layouts with a faster running time and a lower memory requirement.
C1 [Zhu, Minfeng; Chen, Wei; Hu, Yuanzhe; Hou, Yuxuan; Liu, Liangjun; Zhang, Kaiyuan] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Peoples R China.
C3 Zhejiang University
RP Chen, W (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Peoples R China.
EM minfeng_zhu@zju.edu.cn; chenwei@cad.zju.edu.cn; cadhyz@zju.edu.cn;
   houyuxuan@zju.edu.cn; liuliangjun@zju.edu.cn; zhangkaiyuan20@gmail.com
RI Hu, Yuanzhe/LEN-2379-2024; Chen, Wei/AAR-9817-2020; Zhu,
   Minfeng/R-6788-2019
OI Zhu, Minfeng/0000-0002-6711-3099; Zhang, Kaiyuan/0000-0001-6023-363X
FU National Natural Science Foundation of China [61772456, 61761136020]
FX This work is supported by National Natural Science Foundation of China
   (61772456, 61761136020).
NR 68
TC 23
Z9 27
U1 0
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1666
EP 1676
DI 10.1109/TVCG.2020.3030447
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100143
PM 33275582
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Biener, V
   Schneider, D
   Gesslein, T
   Otte, A
   Kuth, B
   Kristensson, PO
   Ofek, E
   Pahud, M
   Grubert, J
AF Biener, Verena
   Schneider, Daniel
   Gesslein, Travis
   Otte, Alexander
   Kuth, Bastian
   Kristensson, Per Ola
   Ofek, Eyal
   Pahud, Michel
   Grubert, Jens
TI Breaking the Screen: Interaction Across Touchscreen Boundaries in
   Virtual Reality for Mobile Knowledge Workers
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 19th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY NOV 09-13, 2020
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, IEEE VGTC, ACM SIGGRAPH, VirBELA, Immers Learning Res Network, Liferay, React Real, Qualcomm
DE Terms virtual reality; knowledge work; mobile office; window management;
   eye tracking; multimodal interaction
ID VISUALIZATION; LENSES
AB Virtual Reality (VR) has the potential to transform knowledge work. One advantage of VR knowledge work is that it allows extending 2D displays into the third dimension, enabling new operations. such as selecting overlapping objects or displaying additional layers of information. On the other hand, mobile knowledge workers often work on established mobile devices. such as tablets. limiting interaction with those devices to a small input space. This challenge of a constrained input space is intensified in situations when VR knowledge work is situated in cramped environments, such as airplanes and touchdown spaces.
   In this paper, we investigate the feasibility of interacting jointly between an immersive VR head-mounted display and a tablet within the context of knowledge work. Specifically, we 1) design, implement and study how to interact with information that reaches beyond a single physical touchscreen in VR: 2) design and evaluate a set of interaction concepts: and 3) build example applications and gather user feedback on those applications.
C1 [Biener, Verena; Schneider, Daniel; Gesslein, Travis; Otte, Alexander; Kuth, Bastian; Grubert, Jens] Coburg Univ Appl Sci, Coburg, Germany.
   [Ofek, Eyal; Pahud, Michel] Microsoft Res, Redmond, WA USA.
   [Kristensson, Per Ola] Univ Cambridge, Cambridge, England.
C3 Hochschule Coburg; Microsoft; University of Cambridge
RP Grubert, J (corresponding author), Coburg Univ Appl Sci, Coburg, Germany.
EM jens.grubert@hs-coburg.de
RI Ofek, Eyal/LPP-8746-2024; Grubert, Jens/B-1012-2018
OI Biener, Verena/0000-0002-2856-5076; Ofek, Eyal/0000-0003-4750-1569;
   Kuth, Bastian/0000-0001-9473-8847
NR 132
TC 35
Z9 39
U1 0
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2020
VL 26
IS 12
BP 3490
EP 3502
DI 10.1109/TVCG.2020.3023567
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA OR1DW
UT WOS:000589217900012
PM 33017290
OA Green Published, hybrid, Green Submitted
DA 2025-03-07
ER

PT J
AU Marquardt, A
   Trepkowski, C
   Eibich, TD
   Maiero, J
   Kruijff, E
   Schöning, J
AF Marquardt, Alexander
   Trepkowski, Christina
   Eibich, Tom David
   Maiero, Jens
   Kruijff, Ernst
   Schoening, Johannes
TI Comparing Non-Visual and Visual Guidance Methods for Narrow Field of
   View Augmented Reality Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 19th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY NOV 09-13, 2020
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, IEEE VGTC, ACM SIGGRAPH, VirBELA, Immers Learning Res Network, Liferay, React Real, Qualcomm
DE Augmented Reality; view-management; guidance; audio -tactile cues;
   performance; situation awareness
ID SITUATION AWARENESS; AUDITORY CUES; SEARCH PERFORMANCE; LOCALIZATION;
   INFORMATION; COST
AB Current augmented reality displays still have a very limited field of view compared to the human vision. In order to localize out-of-view objects, researchers have predominantly explored visual guidance approaches to visualize information in the limited (in-view) screen space. Unfortunately, visual conflicts like cluttering or occlusion of information often arise, which can lead to search performance issues and a decreased awareness about the physical environment. In this paper, we compare an innovative non-visual guidance approach based on audio-tactile cues with the state-of-the-art visual guidance technique EyeSee360 for localizing out-of-view objects in augmented reality displays with limited field of view. In our user study, we evaluate both guidance methods in terms of search performance and situation awareness. We show that although audio-tactile guidance is generally slower than the well-performing EyeSee360 in terms of search times, it is on a par regarding the hit rate. Even more so, the audio-tactile method provides a significant improvement in situation awareness compared to the visual approach.
C1 [Marquardt, Alexander; Trepkowski, Christina; Eibich, Tom David; Maiero, Jens; Kruijff, Ernst] Bonn Rhein Sieg Univ Appl Sci, St Augustin, Germany.
   [Kruijff, Ernst] Simon Fraser Univ, Burnaby, BC, Canada.
   [Schoening, Johannes] Univ Bremen, Bremen, Germany.
C3 Hochschule Bonn Rhein Sieg; Simon Fraser University; University of
   Bremen
RP Marquardt, A (corresponding author), Bonn Rhein Sieg Univ Appl Sci, St Augustin, Germany.
EM alexander.marquardt@h-brs.de; christina.trepkowski@h-brs.de;
   tom.eibich@h-brs.de; jens.maiero@h-brs.de; ernst.kruijff@h-brs.de;
   schoening@uni-bremen.de
RI Schöning, Johannes/AAE-9585-2020
OI Schoning, Johannes/0000-0002-8823-4607; Eibich, Tom
   David/0000-0003-4309-9904
FU Deutsche Forschungsgemeinschaft [KR 4521/2-1]
FX We would like to thank Uwe Gruenefeld for providing the source code and
   useful comments for the EyeSee360 technique. This work was partially
   supported by the Deutsche Forschungsgemeinschaft (grant KR 4521/2-1).
NR 89
TC 32
Z9 34
U1 2
U2 33
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2020
VL 26
IS 12
BP 3389
EP 3401
DI 10.1109/TVCG.2020.3023605
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA OR1DW
UT WOS:000589217900003
PM 32941150
DA 2025-03-07
ER

PT J
AU Strnad, D
   Kohek, S
   Nerat, A
   Zalik, B
AF Strnad, Damjan
   Kohek, Stefan
   Nerat, Andrej
   Zalik, Borut
TI Efficient Representation of Geometric Tree Models with Level-of-Detail
   Using Compressed 3D Chain Code
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Vegetation; Encoding; Three-dimensional displays; Skeleton; Geometry;
   Data models; Solid modeling; Geometric tree model; differential chain
   code; interpolative compression; level-of-detail
ID OBJECTS
AB In the paper, we present a method for space-efficient representation of geometric tree models, which are provided as skeletons with radii attached to individual branch segments. The proposed approach uses a new differential 3D chain code to encode orientation changes of consecutive branch segments, which allows optimizing chain code generation for increased compressibility while maintaining control over the model reconstruction error. The presented method is the first to encode the complete branching geometry including the branch radii and provides level-of-detail construction directly from the chain code. It is demonstrated that by using interpolative encoding of the resulting tree descriptors and radii sequences, the storage requirements for geometric description of a mixed all-aged forest can be reduced to less than 15 percent of its raw size while preserving the structural fidelity of tree models.
C1 [Strnad, Damjan] Univ Maribor, Fac Elect Engn & Comp Sci, Comp Sci, Maribor 2000, Slovenia.
   [Kohek, Stefan; Zalik, Borut] Univ Maribor, Comp Sci, Maribor 2000, Slovenia.
   [Nerat, Andrej; Zalik, Borut] Univ Maribor, Fac Elect Engn & Comp Sci, Lab Geometr Modelling & Multimedia Algorithms, Maribor 2000, Slovenia.
C3 University of Maribor; University of Maribor; University of Maribor
RP Strnad, D (corresponding author), Univ Maribor, Fac Elect Engn & Comp Sci, Comp Sci, Maribor 2000, Slovenia.
EM damjan.strnad@um.si; stefan.kohek@um.si; andrej.nerat@um.si;
   borut.zalik@um.si
RI Kohek, Štefan/J-8143-2019
OI Kohek, Stefan/0000-0002-6210-0889; Nerat, Andrej/0000-0003-1559-9776
FU Slovenian Research Agency [P2-0041, J2-8176]
FX This work was supported by Slovenian Research Agency (research program
   P2-0041 and project J2-8176).
NR 64
TC 4
Z9 4
U1 0
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV 1
PY 2020
VL 26
IS 11
BP 3177
EP 3188
DI 10.1109/TVCG.2019.2924430
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CK
UT WOS:000574745100004
PM 31247555
DA 2025-03-07
ER

PT J
AU Liang, YL
   Wang, BB
   Wang, L
   Holzschuch, N
AF Liang, Yulin
   Wang, Beibei
   Wang, Lu
   Holzschuch, Nicolas
TI Fast Computation of Single Scattering in Participating Media with
   Refractive Boundaries Using Frequency Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Covariance matrices; Scattering; Media; Lighting; Kernel; Cameras;
   Rendering (computer graphics); Single scattering; participating media;
   frequency analysis; covariance tracing
AB Many materials combine a refractive boundary and a participating media on the interior. If the material has a low opacity, single scattering effects dominate in its appearance. Refraction at the boundary concentrates the incoming light, resulting in an important phenomenon called volume caustics. This phenomenon is hard to simulate. Previous methods used point-based light transport, but attributed point samples inefficiently, resulting in long computation time. In this paper, we use frequency analysis of light transport to allocate point samples efficiently. Our method works in two steps: in the first step, we compute volume samples along with their covariance matrices, encoding the illumination frequency content in a compact way. In the rendering step, we use the covariance matrices to compute the kernel size for each volume sample: small kernel for high-frequency single scattering, large kernel for lower frequencies. Our algorithm computes volume caustics with fewer volume samples, with no loss of quality. Our method is both faster and uses less memory than the original method. It is roughly twice as fast and uses one fifth of the memory. The extra cost of computing covariance matrices for frequency information is negligible.
C1 [Liang, Yulin; Wang, Lu] Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
   [Wang, Beibei] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Nanjing 210094, Peoples R China.
   [Holzschuch, Nicolas] Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France.
C3 Shandong University; Nanjing University of Science & Technology; Inria;
   Communaute Universite Grenoble Alpes; Institut National Polytechnique de
   Grenoble; Centre National de la Recherche Scientifique (CNRS);
   Universite Grenoble Alpes (UGA)
RP Wang, L (corresponding author), Shandong Univ, Sch Software, Jinan 250101, Peoples R China.
EM liangyulin1994@gmail.com; bebei.wang@gmail.com; luwang_hcivr@sdu.edu.cn;
   nicolas.holzschuch@inria.fr
RI Holzschuch, Nicolas/E-8861-2014
FU National Key R&D Program of China [2017YFB0203000]; National Natural
   Science Foundation of China [61802187, 61872223]; Natural Science
   Foundation of Jiangsu [BK20170857]; fundamental research funds for the
   central universities [30918011320]; ANR project [ANR-15-CE38-0005];
   Young Scholars Program of Shandong University [2015WLJH41]; Agence
   Nationale de la Recherche (ANR) [ANR-15-CE38-0005] Funding Source:
   Agence Nationale de la Recherche (ANR)
FX We thank the reviewers for the valuable comments. This work has been
   partially supported by the National Key R&D Programof China under
   grantNo. 2017YFB0203000, theNational Natural Science Foundation of China
   under grant No. 61802187, 61872223, the Natural Science Foundation of
   Jiangsu under grant No. BK20170857, the fundamental research funds for
   the central universities No. 30918011320, ANR project ANR-15-CE38-0005
   "Materials" and the Young Scholars Program of Shandong University under
   grant No. 2015WLJH41. Yulin Liang and BeibeiWang share the first
   authorship and contributed equally.
NR 17
TC 3
Z9 3
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2020
VL 26
IS 10
BP 2961
EP 2969
DI 10.1109/TVCG.2019.2909875
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NJ9AP
UT WOS:000566336800003
PM 30969925
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Duchowski, AT
   Krejtz, K
   Zurawska, J
   House, DH
AF Duchowski, Andrew T.
   Krejtz, Krzysztof
   Zurawska, Justyna
   House, Donald H.
TI Using Microsaccades to Estimate Task Difficulty During Visual Search of
   Layered Surfaces
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual search; eye tracking; microsaccades; cognitive load; textured
   surfaces
ID PUPIL-DILATION; EYE FIXATIONS; DIFFERENTIATION; PERCEPTION; MECHANISMS;
   ATTENTION; SACCADE; VISION; LOAD
AB We develop an approach to using microsaccade dynamics for the measurement of task difficulty/cognitive load imposed by a visual search task of a layered surface. Previous studies provide converging evidence that task difficulty/cognitive load can influence microsaccade activity. We corroborate this notion. Specifically, we explore this relationship during visual search for features embedded in a terrain-like surface, with the eyes allowed to move freely during the task. We make two relevant contributions. First, we validate an approach to distinguishing between the ambient and focal phases of visual search. We show that this spectrum of visual behavior can be quantified by a single previously reported estimator, known as Krejtz's kappa coefficient. Second, we use ambient/focal segments based on kappa as a moderating factor for microsaccade analysis in response to task difficulty. We find that during the focal phase of visual search (a) microsaccade magnitude increases significantly, and (b) microsaccade rate decreases significantly, with increased task difficulty. We conclude that the combined use of kappa and microsaccade analysis may be helpful in building effective tools that provide an indication of the level of cognitive activity within a task while the task is being performed.
C1 [Duchowski, Andrew T.; House, Donald H.] Clemson Univ, Sch Comp, Clemson, SC 29634 USA.
   [Krejtz, Krzysztof; Zurawska, Justyna] SWPS Univ Social Sci & Humanities, PL-03815 Warsaw, Poland.
C3 Clemson University; SWPS University of Social Sciences & Humanities
RP Duchowski, AT (corresponding author), Clemson Univ, Sch Comp, Clemson, SC 29634 USA.
EM duchowski@clemson.edu; kkrejtz@swps.edu.pl; jzurawska@st.swps.edu.pl;
   dhouse@clemson.edu
RI Krejtz, Krzysztof/AAP-6165-2021; Duchowski, Andrew/P-2068-2015
OI Duchowski, Andrew/0000-0003-1681-7878; Krejtz,
   Krzysztof/0000-0002-9558-3039; Garnier (Zurawska),
   Justyna/0000-0002-4800-1171
NR 75
TC 17
Z9 19
U1 1
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2020
VL 26
IS 9
BP 2904
EP 2918
DI 10.1109/TVCG.2019.2901881
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA MS7LW
UT WOS:000554457900013
PM 30835226
DA 2025-03-07
ER

PT J
AU Wang, ZJ
   Lu, F
AF Wang, Zongji
   Lu, Feng
TI VoxSegNet: Volumetric CNNs for Semantic Part Segmentation of 3D Shapes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Feature extraction; Shape; Semantics;
   Convolution; Data mining; Task analysis; Shape analysis; semantic
   segmentation; convolutional neural networks; volumetric models
AB Volumetric representation has been widely used for 3D deep learning in shape analysis due to its generalization ability and regular data format. However, for fine-grained tasks like part segmentation, volumetric data has not been widely adopted compared to other representations. Aiming at delivering an effective volumetric method for 3D shape part segmentation, this paper proposes a novel volumetric convolutional neural network. Our method can extract discriminative features encoding detailed information from voxelized 3D data under limited resolution. To this purpose, a spatial dense extraction (SDE) module is designed to preserve spatial resolution during feature extraction procedure, alleviating the loss of details caused by sub-sampling operations such as max pooling. An attention feature aggregation (AFA) module is also introduced to adaptively select informative features from different abstraction levels, leading to segmentation with both semantic consistency and high accuracy of details. Experimental results demonstrate that promising results can be achieved by using volumetric data, with part segmentation accuracy comparable or superior to state-of-the-art non-volumetric methods.
C1 [Wang, Zongji; Lu, Feng] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Lu, Feng] Beihang Univ, Beijing Adv Innovat Ctr Big Databased Precis Med, Beijing 100191, Peoples R China.
   [Lu, Feng] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
C3 Beihang University; Beihang University; Peng Cheng Laboratory
RP Lu, F (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Lu, F (corresponding author), Beihang Univ, Beijing Adv Innovat Ctr Big Databased Precis Med, Beijing 100191, Peoples R China.; Lu, F (corresponding author), Peng Cheng Lab, Shenzhen 518000, Peoples R China.
EM wzjgintoki@buaa.edu.cn; lufeng@buaa.edu.cn
OI Lu, Feng/0000-0001-9064-7964
FU National Natural Science Foundation of China (NSFC) [61602020, 61732016]
FX This work was supported by National Natural Science Foundation of China
   (NSFC) under Grant 61602020, and Grant 61732016.
NR 48
TC 61
Z9 73
U1 4
U2 39
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2020
VL 26
IS 9
BP 2919
EP 2930
DI 10.1109/TVCG.2019.2896310
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MS7LW
UT WOS:000554457900014
PM 30714926
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhang, BX
   Sheng, B
   Li, P
   Lee, TY
AF Zhang, Benxuan
   Sheng, Bin
   Li, Ping
   Lee, Tong-Yee
TI Depth of Field Rendering Using Multilayer-Neighborhood Optimization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Rendering (computer graphics); Image color analysis; Cameras;
   Optimization; Three-dimensional displays; Scattering; Ray tracing; Depth
   of field; multilayer; neighborhood optimization; rendering
ID REAL-TIME DEPTH; SIMULATION; ALGORITHM
AB Depth of field (DOF) is utilized widely to deliver artistic effects in photography. However, existing post-processing techniques for rendering DOF effects introduce visual artifacts such as color leakage, blurring discontinuity, and the partial occlusion problems which limit the application of DOF. Traditionally, occluded pixels are ignored or not well estimated although they might make key contributions to images. In this paper, we propose a new filtering approach which takes approximated occluded pixels into account to synthesize the DOF effects for images. In our approach, images are separated into different layers based on depth. Besides, we utilize adaptive PatchMatch method to estimate the intensities of occluded pixels, especially in the background region. We again propose a new multilayer-neighborhood optimization to estimate occluded pixels contributions and render the images. Finally, we apply gathering filter to achieve the rendered images with elite DOF effects. Multiple experiments have shown that our approach can handle color leakage, blurring discontinuity and partial occlusion problem while providing high-quality DOF rendering effects.
C1 [Zhang, Benxuan; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Li, Ping] Macau Univ Sci & Technol, Fac Informat Technol, Macau 999078, Peoples R China.
   [Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
C3 Shanghai Jiao Tong University; Macau University of Science & Technology;
   National Cheng Kung University
RP Sheng, B (corresponding author), Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
EM 150576806@qq.com; shengbin@sjtu.edu.cn; pli@must.edu.mo;
   tonylee@mail.ncku.edu.tw
RI Sheng, Bin/LMO-9532-2024; Li, Ping/AAO-2019-2020
OI Sheng, Bin/0000-0001-8510-2556; Li, Ping/0000-0002-1503-0240
FU National Natural Science Foundation of China [61872241, 61572316];
   National Key Research and Development Program of China [2017YFE0104000,
   2016YFC1300302]; Macau Science and Technology Development Fund
   [0027/2018/A1]; Science and Technology Commission of Shanghai
   Municipality [18410750700, 17411952600, 16DZ0501100]; Ministry of
   Science and Technology, Taiwan [107-2221-E-006-196-MY3]
FX We thank the reviewers for their insightful comments and suggestions.
   This work was supported in part by the National Natural Science
   Foundation of China (61872241, 61572316), the National Key Research and
   Development Program of China (2017YFE0104000, 2016YFC1300302), the Macau
   Science and Technology Development Fund (0027/2018/A1), the Science and
   Technology Commission of Shanghai Municipality (18410750700,
   17411952600, 16DZ0501100), and the Ministry of Science and Technology
   (107-2221-E-006-196-MY3), Taiwan.
NR 45
TC 27
Z9 27
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG. 1
PY 2020
VL 26
IS 8
BP 2546
EP 2559
DI 10.1109/TVCG.2019.2894627
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MG6BL
UT WOS:000546115000002
PM 30676963
DA 2025-03-07
ER

PT J
AU Hikaru, I
   Wojtan, C
   Thuerey, N
   Igarashi, T
   Ando, R
AF Hikaru, Ibayashi
   Wojtan, Chris
   Thuerey, Nils
   Igarashi, Takeo
   Ando, Ryoichi
TI Simulating Liquids on Dynamically Warping Grids
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Adaptation models; Strain; Liquids; Computational modeling; Streaming
   media; Computer graphics; Animation; Computer graphics; physics-based
   animation; fluid simulation; liquid; adaptivity; curvilinear grids
ID ACCURATE; FLUID; SMOKE; WATER; ANIMATION; FLOW
AB We introduce dynamically warping grids for adaptive liquid simulation. Our primary contributions are a strategy for dynamically deforming regular grids over the course of a simulation and a method for efficiently utilizing these deforming grids for liquid simulation. Prior work has shown that unstructured grids are very effective for adaptive fluid simulations. However, unstructured grids often lead to complicated implementations and a poor cache hit rate due to inconsistent memory access. Regular grids, on the other hand, provide a fast, fixed memory access pattern and straightforward implementation. Our method combines the advantages of both: we leverage the simplicity of regular grids while still achieving practical and controllable spatial adaptivity. We demonstrate that our method enables adaptive simulations that are fast, flexible, and robust to null-space issues. At the same time, our method is simple to implement and takes advantage of existing highly-tuned algorithms.
C1 [Hikaru, Ibayashi] Univ Southern Calif, Dept Comp Sci, Los Angels, CA 90007 USA.
   [Wojtan, Chris] IST Austria, Visual Comp Grp, Campus 1, A-3400 Klosterneuburg, Austria.
   [Thuerey, Nils] Tech Univ Munich, Arcisstr 21, D-80333 Munich, Germany.
   [Igarashi, Takeo] Univ Tokyo, Dept Comp Sci, Bunkyo Ku, Tokyo 1138654, Japan.
   [Ando, Ryoichi] Natl Inst Informat, Chiyoda Ku, Tokyo 1018430, Japan.
C3 University of Southern California; Institute of Science & Technology -
   Austria; Technical University of Munich; University of Tokyo; Research
   Organization of Information & Systems (ROIS); National Institute of
   Informatics (NII) - Japan
RP Ando, R (corresponding author), Natl Inst Informat, Chiyoda Ku, Tokyo 1018430, Japan.
EM ibayashi@usc.edu; wojtan@ist.ac.at; nils.thuerey@tum.de; takeo@acm.org;
   rand@nii.ac.jp
RI Igarashi, Takeo/ITT-5921-2023
OI Thuerey, Nils/0000-0001-6647-8910; Ibayashi, Hikaru/0000-0001-6983-1927;
   Ando, Ryoichi/0000-0002-7899-9091
FU JSPS [16H07410]; ERC [StG-2015-637014, StG-2014638176]; Scientific
   Service Units (SSU) of IST Austria; Grants-in-Aid for Scientific
   Research [16H07410] Funding Source: KAKEN
FX This work was partially supported by JSPS Grant-in-Aid for Young
   Scientists (Start-up) 16H07410, the ERC Starting Grants realFlow
   (StG-2015-637014) and BigSplash (StG-2014638176). This research was
   supported by the Scientific Service Units (SSU) of IST Austria through
   resources provided by Scientific Computing. We would like to express my
   gratitude to Nobuyuki Umetani and Tomas Skrivan for insightful
   discussion.
NR 72
TC 6
Z9 6
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2020
VL 26
IS 6
BP 2288
EP 2302
DI 10.1109/TVCG.2018.2883628
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LM5NO
UT WOS:000532295600014
PM 30507534
DA 2025-03-07
ER

PT J
AU Tong, WH
   Yang, XK
   Pan, MD
   Chen, FL
AF Tong, Weihua
   Yang, Xiankang
   Pan, Maodong
   Chen, Falai
TI Spectral Mesh Segmentation via <i>l</i><sub>0</sub> Gradient
   Minimization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Mesh segmentation; spectral analysis; l(0) gradient minimization; ADMM
ID SPARSITY
AB Mesh segmentation is a process of partitioning a mesh model into meaningful parts - a fundamental problem in various disciplines. This paper introduces a novel mesh segmentation method inspired by sparsity pursuit. Based on the local geometric and topological information of a given mesh, we build a Laplacian matrix whose Fiedler vector is used to characterize the uniformity among elements of the same segment. By analyzing the Fiedler vector, we reformulate the mesh segmentation problem as a l(0) gradient minimization problem. To solve this problem efficiently, we adopt a coarse-to-fine strategy. A fast heuristic algorithm is first devised to find a rational coarse segmentation, and then an optimization algorithm based on the alternating direction method of multiplier (ADMM) is proposed to refine the segment boundaries within their local regions. To extract the inherent hierarchical structure of the given mesh, our method performs segmentation in a recursive way. Experimental results demonstrate that the presented method outperforms the state-of-the-art segmentation methods when evaluated on the Princeton Segmentation Benchmark, the LIFL/LIRIS Segmentation Benchmark and a number of other complex meshes.
C1 [Tong, Weihua; Yang, Xiankang; Pan, Maodong; Chen, Falai] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Tong, WH (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
EM tongwh@ustc.edu.cn; yangxk@mail.ustc.edu.cn; mdpan@mail.ustc.edu.cn;
   chenfl@ustc.edu.cn
OI Pan, Maodong/0000-0002-4975-6808
FU NSF of China [11571338, 61877056]; Fundamental Research Funds for the
   Central Universities [WK0010000051]; China Postdoctoral Science
   Foundation [2018M632548]; Open Project Program of the State Key Lab of
   CADCG [A1819]
FX This work is supported by the NSF of China (No. 11571338, 61877056), the
   Fundamental Research Funds for the Central Universities (WK0010000051),
   the China Postdoctoral Science Foundation (No. 2018M632548) and the Open
   Project Program of the State Key Lab of CAD&CG (No. A1819). Weihua Tong,
   Xiankang Yang Joint first authors.
NR 43
TC 17
Z9 19
U1 0
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2020
VL 26
IS 4
BP 1807
EP 1820
DI 10.1109/TVCG.2018.2882212
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KU2OG
UT WOS:000519547200014
PM 30452373
DA 2025-03-07
ER

PT J
AU El-Assady, M
   Kehlbeck, R
   Collins, C
   Keim, D
   Deussen, O
AF El-Assady, Mennatallah
   Kehlbeck, Rebecca
   Collins, Christopher
   Keim, Daniel
   Deussen, Oliver
TI Semantic Concept Spaces: Guided Topic Model Refinement using
   Word-Embedding Projections
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Topic Model Optimization; Word Embedding; Mixed-Initiative Refinement;
   Guided Visual Analytics; Semantic Mapping
ID VISUAL ANALYTICS
AB We present a framework that allows users to incorporate the semantics of their domain knowledge for topic model refinement while remaining model-agnostic. Our approach enables users to (1) understand the semantic space of the model, (2) identify regions of potential conflicts and problems, and (3) readjust the semantic relation of concepts based on their understanding, directly influencing the topic modeling. These tasks are supported by an interactive visual analytics workspace that uses word-embedding projections to define concept regions which can then be refined. The user-refined concepts are independent of a particular document collection and can be transferred to related corpora. All user interactions within the concept space directly affect the semantic relations of the underlying vector space model, which, in turn, change the topic modeling. In addition to direct manipulation, our system guides the users decision-making process through recommended interactions that point out potential improvements. This targeted refinement aims at minimizing the feedback required for an efficient human-in-the-loop process. We confirm the improvements achieved through our approach in two user studies that show topic model quality improvements through our visual knowledge externalization and learning process.
C1 [El-Assady, Mennatallah; Kehlbeck, Rebecca; Keim, Daniel; Deussen, Oliver] Univ Konstanz, Constance, Germany.
   [El-Assady, Mennatallah; Collins, Christopher] Ontario Tech Univ, Oshawa, ON, Canada.
C3 University of Konstanz
RP El-Assady, M (corresponding author), Univ Konstanz, Constance, Germany.; El-Assady, M (corresponding author), Ontario Tech Univ, Oshawa, ON, Canada.
EM mennatallah.el-assady@uni.kn; rebecca.kehlbeck@uni.kn;
   christopher.collins@uoit.ca; daniel.keim@uni.kn; oliver.deussen@uni.kn
RI Collins, Christopher/AAJ-6345-2020; Keim, Daniel/X-7749-2019; Deussen,
   Oliver/HKF-2004-2023
OI El-Assady, Mennatallah/0000-0001-8526-2613
FU DFG [376714276, SPP-1999, FOR2111]; NSERC Canada Research Chairs; 
   [SFB/Transregio 161];  [251654672]
FX This work has received funding from the DFG/SPP-1999 VALIDA project
   (number 376714276) and the DFG Research Unit FOR2111/QI project 8. It
   was further supported by the SFB/Transregio 161 (number 251654672),
   projects A03 and A04, as well as NSERC Canada Research Chairs.
NR 59
TC 27
Z9 29
U1 1
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 1001
EP 1011
DI 10.1109/TVCG.2019.2934654
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100093
PM 31443000
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Johnson, S
   Samsel, F
   Abram, G
   Olson, D
   Solis, AJ
   Herman, B
   Wolfram, PJ
   Lenglet, C
   Keefe, DF
AF Johnson, Seth
   Samsel, Francesca
   Abram, Gregory
   Olson, Daniel
   Solis, Andrew J.
   Herman, Bridger
   Wolfram, Phillip J.
   Lenglet, Christophe
   Keefe, Daniel F.
TI Artifact-Based Rendering: Harnessing Natural and Traditional Visual
   Media for More Expressive and Engaging 3D Visualizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization Design; Art and Visualization; Data Physicalization;
   Multivariate Visualization
ID TEXTURES; WYSIWYG; MAPS
AB We introduce Artifact-Based Rendering (ABR), a framework of tools, algorithms, and processes that makes it possible to produce real, data-driven 3D scientific visualizations with a visual language derived entirely from colors, lines, textures, and forms created using traditional physical media or found in nature. A theory and process for ABR is presented to address three current needs: (i) designing better visualizations by making it possible for non-programmers to rapidly design and critique many alternative data-to-visual mappings; (ii) expanding the visual vocabulary used in scientific visualizations to depict increasingly complex multivariate data; (iii) bringing a more engaging, natural, and human-relatable handcrafted aesthetic to data visualization. New tools and algorithms to support ABR include front-end applets for constructing artifact-based colormaps, optimizing 3D scanned meshes for use in data visualization, and synthesizing textures from artifacts. These are complemented by an interactive rendering engine with custom algorithms and interfaces that demonstrate multiple new visual styles for depicting point, line, surface, and volume data. A within-the-research-team design study provides early evidence of the shift in visualization design processes that ABR is believed to enable when compared to traditional scientific visualization systems. Qualitative user feedback on applications to climate science and brain imaging support the utility of ABR for scientific discovery and public communication.
C1 [Johnson, Seth; Olson, Daniel; Herman, Bridger; Lenglet, Christophe; Keefe, Daniel F.] Univ Minnesota, Minneapolis, MN 55455 USA.
   [Samsel, Francesca; Abram, Gregory; Solis, Andrew J.] Univ Texas Austin, Austin, TX 78712 USA.
   [Wolfram, Phillip J.] Los Alamos Natl Lab, Fluid Dynam & Solid Mech Theoret Div, Los Alamos, NM USA.
C3 University of Minnesota System; University of Minnesota Twin Cities;
   University of Texas System; University of Texas Austin; United States
   Department of Energy (DOE); Los Alamos National Laboratory
RP Johnson, S (corresponding author), Univ Minnesota, Minneapolis, MN 55455 USA.
EM joh08230@umn.edu; fsamsel@tacc.utexas.edu; gda@tacc.utexas.edu;
   olso7118@umn.edu; herma582@umn.edu; pwolfram@lanl.gov; clenglet@umn.edu;
   dfk@umn.edu
RI Lenglet, Christophe/IWU-5683-2023; Adams, Greg/HLQ-2582-2023
OI Lenglet, Christophe/0000-0003-4646-3185; Samsel,
   Francesca/0000-0002-8596-6159; Solis, Andrew/0000-0002-8917-2874
FU National Science Foundation [IIS-1704604, IIS-1704904]; National
   Institutes of Health [P41 EB015894, P30 NS076408]; U.S. Department of
   Energy (DOE), Office of Science, Office of Biological and Environmental
   Research [DE-FOA-0001726, 17/CJ000/09/01]
FX This research was supported in part by the National Science Foundation
   (IIS-1704604 & IIS-1704904). Brain microstructure applications were
   supported in part by the National Institutes of Health (P41 EB015894,
   P30 NS076408). MPAS-O simulations were conducted by Mathew E. Maltrud
   and Riley X. Brady as part of the Energy Exascale Earth System Model
   (E3SM) project, funded by the U.S. Department of Energy (DOE), Office of
   Science, Office of Biological and Environmental Research with analyses
   conducted by PJW, MEM, and RXB under ARPA-E Funding Opportunity No.
   DE-FOA-0001726, MARINER Award 17/CJ000/09/01, Pacific Northwest National
   Laboratory, prime recipient.
NR 83
TC 15
Z9 17
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 492
EP 502
DI 10.1109/TVCG.2019.2934260
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100046
PM 31403430
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Nguyen, PH
   Henkin, R
   Chen, SM
   Andrienko, N
   Andrienko, G
   Thonnard, O
   Turkay, C
AF Nguyen, Phong H.
   Henkin, Rafael
   Chen, Siming
   Andrienko, Natalia
   Andrienko, Gennady
   Thonnard, Olivier
   Turkay, Cagatay
TI VASABI: Hierarchical User Profiles for Interactive Visual User Behaviour
   Analytics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE hierarchical user profiles; user behaviour analytics; visual analytics;
   cybersecurity
AB User behaviour analytics (UBA) systems offer sophisticated models that capture users; behaviour over time with an aim to identify fraudulent activities that do not match their profiles. Motivated by the challenges in the interpretation of UBA models, this paper presents a visual analytics approach to help analysts gain a comprehensive understanding of user behaviour at multiple levels, namely individual and group level. We take a user-centred approach to design a visual analytics framework supporting the analysis of collections of users and the numerous sessions of activities they conduct within digital applications. The framework is centred around the concept of hierarchical user profiles that are built based on features derived from sessions, as well as on user tasks extracted using a topic modelling approach to summarise and stratify user behaviour. We externalise a series of analysis goals and tasks, and evaluate our methods through use cases conducted with experts. We observe that with the aid of interactive visual hierarchical user profiles, analysts are able to conduct exploratory and investigative analysis effectively, and able to understand the characteristics of user behaviour to make informed decisions whilst evaluating suspicious users and activities.
C1 [Nguyen, Phong H.; Henkin, Rafael; Turkay, Cagatay] City Univ London, London, England.
   [Chen, Siming; Andrienko, Natalia; Andrienko, Gennady] Fraunhofer IAIS, St Augustin, Germany.
   [Thonnard, Olivier] Amadeus, Issy Les Moulineaux, France.
C3 City St Georges, University of London; City, University of London
RP Nguyen, PH (corresponding author), City Univ London, London, England.
EM p.nguyen@city.ac.uk; rafael.henkin@city.ac.uk;
   siming.chen@iais.fraunhofer.de; natalia.andrienko@iais.fraunhofer.de;
   gennady.andrienko@iais.fraunhofer.de; olivier.thonnard@amadeus.com;
   cagatay.turkay@city.ac.uk
RI Andrienko, Gennady/B-6486-2014; Turkay, Cagatay/AAA-3810-2020; Chen,
   Siming/AAK-1874-2020; Andrienko, Natalia/KHV-4755-2024; Nguyen, Phong
   H/HHN-2723-2022
FU European Commission through the H2020 programme [700692]; H2020 Societal
   Challenges Programme [700692] Funding Source: H2020 Societal Challenges
   Programme
FX This work is supported by the European Commission through the H2020
   programme under grant agreement 700692 (DiSIEM). We also would like to
   thank Zayani Dabbabi and Miruna-Mihaela Mironescu for their help in the
   evaluation of VASABI.
NR 42
TC 20
Z9 20
U1 3
U2 52
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 77
EP 86
DI 10.1109/TVCG.2019.2934609
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100008
PM 31442992
DA 2025-03-07
ER

PT J
AU Saket, B
   Huron, S
   Perin, C
   Endert, A
AF Saket, Bahador
   Huron, Samuel
   Perin, Charles
   Endert, Alex
TI Investigating Direct Manipulation of Graphical Encodings as a Method for
   User Interaction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Direct Manipulation; Data Visualization
ID VISUALIZATION; DESIGN
AB We investigate direct manipulation of graphical encodings as a method for interacting with visualizations. There is an increasing interest in developing visualization tools that enable users to perform operations by directly manipulating graphical encodings rather than external widgets such as checkboxes and sliders. Designers of such tools must decide which direct manipulation operations should be supported, and identify how each operation can be invoked. However, we lack empirical guidelines for how people convey their intended operations using direct manipulation of graphical encodings. We address this issue by conducting a qualitative study that examines how participants perform 15 operations using direct manipulation of standard graphical encodings. From this study, we 1) identify a list of strategies people employ to perform each operation, 2) observe commonalities in strategies across operations, and 3) derive implications to help designers leverage direct manipulation of graphical encoding as a method for user interaction.
C1 [Saket, Bahador; Endert, Alex] Georgia Tech, Atlanta, GA 30332 USA.
   [Huron, Samuel] Univ Paris Saclay, Paris, France.
   [Perin, Charles] Univ Victoria, Victoria, BC, Canada.
C3 University System of Georgia; Georgia Institute of Technology;
   Universite Paris Saclay; University of Victoria
RP Saket, B (corresponding author), Georgia Tech, Atlanta, GA 30332 USA.
EM saket@gatech.edu; samuel.huron@telecom-paristech.fr; cperin@uvic.ca;
   endert@gatech.edu
FU NSF [IIS-1750474]; Natural Sciences and Engineering Research Council of
   Canada (NSERC)
FX We are grateful to the study participants, reviewers, and members of the
   Georgia Tech Visualization Lab for their helpful insights and comments.
   This project is supported by NSF IIS-1750474 and the Natural Sciences
   and Engineering Research Council of Canada (NSERC).
NR 47
TC 9
Z9 11
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 482
EP 491
DI 10.1109/TVCG.2019.2934534
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100045
PM 31442983
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Snyder, LS
   Lin, YS
   Karimzadeh, M
   Goldwasser, D
   Ebert, DS
AF Snyder, Luke S.
   Lin, Yi-Shan
   Karimzadeh, Morteza
   Goldwasser, Dan
   Ebert, David S.
TI Interactive Learning for Identifying Relevant Tweets to Support
   Real-time Situational Awareness
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Interactive machine learning; human-computer interaction; social media
   analytics; emergency; disaster management; situational awareness
ID MICROBLOG
AB Various domain users are increasingly leveraging real-time social media data to gain rapid situational awareness. However, due to the high noise in the deluge of data, effectively determining semantically relevant information can be difficult, further complicated by the changing definition of relevancy by each end user for different events. The majority of existing methods for short text relevance classification fail to incorporate users knowledge into the classification process. Existing methods that incorporate interactive user feedback focus on historical datasets. Therefore, classifiers cannot be interactively retrained for specific events or user-dependent needs in real-time. This limits real-time situational awareness, as streaming data that is incorrectly classified cannot be corrected immediately, permitting the possibility for important incoming data to be incorrectly classified as well. We present a novel interactive learning framework to improve the classification process in which the user iteratively corrects the relevancy of tweets in real-time to train the classification model on-the-fly for immediate predictive improvements. We computationally evaluate our classification model adapted to learn at interactive rates. Our results show that our approach outperforms state-of-the-art machine learning models. In addition, we integrate our framework with the extended Social Media Analytics and Reporting Toolkit (SMART) 2.0 system, allowing the use of our interactive learning framework within a visual analytics system tailored for real-time situational awareness. To demonstrate our frameworks effectiveness, we provide domain expert feedback from first responders who used the extended SMART 2.0 system.
C1 [Snyder, Luke S.; Lin, Yi-Shan; Karimzadeh, Morteza; Goldwasser, Dan; Ebert, David S.] Purdue Univ, W Lafayette, IN 47907 USA.
   [Karimzadeh, Morteza] Univ Colorado, Boulder, CO 80309 USA.
C3 Purdue University System; Purdue University; University of Colorado
   System; University of Colorado Boulder
RP Snyder, LS (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.
EM snyde238@purdue.edu; lin670@purdue.edu; karimzadeh@colotado.edu;
   dgoldwas@purdue.edu; ebertd@purdue.edu
RI Lin, Yishan/I-8615-2014; Karimzadeh, Morteza/AAE-8300-2020
OI Karimzadeh, Morteza/0000-0002-6498-1763; Ebert,
   David/0000-0001-6177-1296
FU U.S. Department of Homeland Security (DHS) VACCINE Center
   [2009-ST-061-CI0003]; DHS Science and Technology Directorate Award
   [70RSAT18CB0000004]; DHS [2014-ST-061-ML0001]
FX This material is based upon work funded by the U.S. Department of
   Homeland Security (DHS) VACCINE Center under Award No.
   2009-ST-061-CI0003, DHS Cooperative Agreement No. 2014-ST-061-ML0001,
   and DHS Science and Technology Directorate Award No. 70RSAT18CB0000004.
NR 51
TC 28
Z9 35
U1 1
U2 29
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 558
EP 568
DI 10.1109/TVCG.2019.2934614
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA KB0CF
UT WOS:000506166100052
PM 31442995
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Rolin, RA
   Fooken, J
   Spering, M
   Pai, DK
AF Rolin, Robert A.
   Fooken, Jolande
   Spering, Miriam
   Pai, Dinesh K.
TI Perception of Looming Motion in Virtual Reality Egocentric Interception
   Tasks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual reality; motion perception; time to contact; augmented reality;
   games
ID DISTANCE PERCEPTION; CONTACT; DEPTH; ENVIRONMENTS; DISPARITY; COLLISION;
   GRAVITY; USERS; SIZE; CUES
AB Motion in depth is commonly misperceived in Virtual Reality (VR), making it difficult to intercept moving objects, for example, in games. We investigate whether motion cues could be modified to improve these interactions in VR. We developed a time-to-contact estimation task, in which observers (n = 18) had to indicate by button press when a looming virtual object would collide with their head. We show that users consistently underestimate speed. We construct a user-specific model of motion-in-depth perception, and use this model to propose a novel method to modify monocular depth cues tailored to the specific user, correcting individual response errors in speed estimation. A user study was conducted in a simulated baseball environment and observers were asked to hit a looming baseball back in the direction of the pitcher. The study was conducted with and without intervention and demonstrates the effectiveness of the method in reducing interception errors following cue modifications. The intervention was particularly effective at fast ball speeds where performance is most limited by the user's sensorimotor constraints. The proposed approach is easy to implement and could improve the user experience of interacting with dynamic virtual environments.
C1 [Rolin, Robert A.; Pai, Dinesh K.] Univ British Columbia, Comp Sci, Vancouver, BC V6T 1Z4, Canada.
   [Fooken, Jolande; Spering, Miriam] Univ British Columbia, Ophthalmol & Visual Sci, Vancouver, BC V6T 1Z4, Canada.
C3 University of British Columbia; University of British Columbia
RP Rolin, RA (corresponding author), Univ British Columbia, Comp Sci, Vancouver, BC V6T 1Z4, Canada.
EM robbie.rolin@gmail.com; fooken@cs.ubc.ca; miriam.spering@ubc.ca;
   pai@cs.ubc.ca
OI Pai, Dinesh K/0000-0002-5115-7230
FU Natural Sciences and Engineering Research Council of Canada [RGPIN
   153236, RGPIN 418493]
FX Natural Sciences and Engineering Research Council of Canada Discovery
   Grants to DKP (RGPIN 153236) and MS (RGPIN 418493).
NR 59
TC 14
Z9 16
U1 0
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2019
VL 25
IS 10
BP 3042
EP 3048
DI 10.1109/TVCG.2018.2859987
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA IV5CC
UT WOS:000484287800013
PM 30072330
DA 2025-03-07
ER

PT J
AU Schmidt, J
   Fleischmann, D
   Preim, B
   Brändle, N
   Mistelbauer, G
AF Schmidt, Johanna
   Fleischmann, Dominik
   Preim, Bernhard
   Braendle, Norbert
   Mistelbauer, Gabriel
TI Popup-Plots: Warping Temporal Data Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Temporal data; time-dependent visualization; 3D plots; ellipsoidal
   coordinate system
ID VISUAL ANALYSIS; TIME; EVOLUTION; DISTANCE; GRAPHS
AB Temporal data visualization is used to analyze dependent variables that vary over time, with time being an independent variable. Visualizing temporal data is inherently difficult, due to the many aspects that need to be communicated to the users (e.g., time and variable changes). This is an important topic in visualization, and a wide range of visualization techniques dealing with different tasks have already been designed. In this paper we propose popup-plots, a novel concept where the common interaction of 3D rotation is used to navigate through the data. This allows the users to view the data from different perspectives without having to learn and adapt to new interaction concepts. Popup-plots are therefore a novel method for visualizing and interacting with dependent variables over time. We extend 2D plots with the temporal information by bending the space according to the time. The bending is calculated based on a spherical coordinates approach, which is continuously influenced by the viewing direction towards the plot. Hence, the plot can be viewed from various angles with seamless transitions in between, offering the possibility to analyze different aspects of the represented data. As the current viewing direction is inherently depicted by the shape of the data, the users are able to deduce which part of the data is currently viewed. The temporal information is encoded into the visualization itself, resembling annual rings of a tree. We demonstrate our method by applying it to data from two different domains, comprising measurements at spatial positions over time, and we also evaluated the usability of our solution.
C1 [Schmidt, Johanna; Braendle, Norbert] AIT, A-1210 Vienna, Austria.
   [Fleischmann, Dominik] Stanford Univ, Sch Med, Stanford, CA 94305 USA.
   [Preim, Bernhard; Mistelbauer, Gabriel] Otto Von Guericke Univ, D-39106 Magdeburg, Germany.
C3 Austrian Institute of Technology (AIT); Stanford University; Otto von
   Guericke University
RP Schmidt, J (corresponding author), AIT, A-1210 Vienna, Austria.
EM johanna.schmidt@ait.ac.at; d.fleischmann@stanford.edu;
   bernhard@isg.cs.uni-magdeburg.de; norbert.braendle@ait.ac.at;
   gmistelbauer@isg.cs.uni-magdeburg.de
RI Mistelbauer, Gabriel/JVY-7873-2024; Preim, Bernhard/AAF-6565-2021
OI Mistelbauer, Gabriel/0000-0003-2333-5404; Fleischmann,
   Dominik/0000-0003-0715-0952; Schmidt, Johanna/0000-0002-9638-6344;
   Brandle, Norbert/0000-0002-2976-3138
NR 52
TC 3
Z9 4
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2019
VL 25
IS 7
BP 2443
EP 2457
DI 10.1109/TVCG.2018.2841385
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IA8WG
UT WOS:000469838700009
PM 29993580
DA 2025-03-07
ER

PT J
AU Lee, TM
   Yoon, JC
   Lee, IK
AF Lee, Tae Min
   Yoon, Jong-Chul
   Lee, In-Kwon
TI Motion Sickness Prediction in Stereoscopic Videos using 3D Convolutional
   Neural Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE 360 degrees Stereoscopic video; motion sickness; virtual reality;
   saliency; machine learning; 3D CNN
ID VECTION; SALIENCY; ROLL
AB In this paper. we propose a three-dimensional (3D) convolutional neural network (CNN)-based method for predicting the degree of motion sickness induced by a 360 degrees stereoscopic video. We consider the user's eye movement as a new feature, in addition to the motion velocity and depth features of a video used in previous work. For this purpose, we use saliency. optical flow, and disparity maps of an input video, which represent eye movement. velocity, and depth, respectively, as the input of the 3D CNN. To train our machine-learning model, we extend the dataset established in the previous work using two data augmentation techniques: frame shifting and pixel shifting. Consequently, our model can predict the degree of motion sickness more precisely than the previous method, and the results have a more similar correlation to the distribution of ground-truth sickness.
C1 [Lee, Tae Min; Lee, In-Kwon] Yonsei Univ, Seoul, South Korea.
   [Yoon, Jong-Chul] Kangwon Natl Univ, Chunchon, South Korea.
C3 Yonsei University; Kangwon National University
RP Lee, TM (corresponding author), Yonsei Univ, Seoul, South Korea.
EM dnflxoals@gmail.com; media19@kangwon.ac.kr; iklee@yonsei.ac.kr
RI Lee, In-Kwon/AGP-6124-2022
OI Lee, In-Kwon/0000-0002-1534-1882
FU National Research Foundation of Korea(NRF) - Korea government(MSIT)
   [NRF-2017R1A2B4005469]; MSIT(Ministry of Science and ICT). Korea. under
   the ITRC(Information Technology Research Center) support program
   [IITP-2018-0-01419]
FX This work has supported by the :National Research Foundation of
   Korea(NRF) grant funded by the Korea government(MSIT)(No.
   NRF-2017R1A2B4005469), and by the MSIT(Ministry of Science and ICT).
   Korea. under the ITRC(Information Technology Research Center) support
   program(IITP-2018-0-01419) supervised by the IITP(Institute for
   Information communications Technology Promotion.
NR 56
TC 50
Z9 55
U1 0
U2 30
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2019
VL 25
IS 5
BP 1919
EP 1927
DI 10.1109/TVCG.2019.2899186
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HR3EI
UT WOS:000463019100011
PM 30794181
DA 2025-03-07
ER

PT J
AU Schollmeyer, A
   Froehlich, B
AF Schollmeyer, Andre
   Froehlich, Bernd
TI Efficient and Anti-Aliased Trimming for Rendering Large NURBS Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Trimming; NURBS; anti-aliasing; adaptive tessellation
ID TESSELLATION
AB In Computer-Aided Design (CAD), Non-Uniform Rational B-Splines (NURBS) are a common model representation for export, simulation and visualization. In this paper, we present a direct rendering method for trimmed NURBS models based on their parametric description. Our approach builds on a novel trimming method and a three-pass pipeline which both allow for a sub-pixel precise visualization. The rendering pipeline bypasses tessellation limitations of current hardware using a feedback mechanism. In contrast to existing work, our trimming method scales well with a large number of trim curves and estimates the trimmed surface's footprint in screen-space which allows for an anti-aliasing with minimal performance overhead. Fragments with trimmed edges are routed into a designated off-screen buffer for subsequent blending with background faces. The evaluation of the presented algorithms shows that our rendering system can handle CAD models with ten thousands of trimmed NURBS surfaces. The suggested two-level data structure used for trimming outperforms state-of-the-art methods while being more precise and memory efficient. Our curve coverage estimation used for anti-aliasing provides an efficient trade-off between quality and performance compared to multisampling or screen-space anti-aliasing approaches.
C1 [Schollmeyer, Andre; Froehlich, Bernd] Bauhaus Univ Weimar, Fac Media, D-99423 Weimar, Thuringia, Germany.
C3 Bauhaus-Universitat Weimar
RP Schollmeyer, A (corresponding author), Bauhaus Univ Weimar, Fac Media, D-99423 Weimar, Thuringia, Germany.
EM andre.schollmeyer@uni-weimar.de; bernd.froehlich@uni-weimar.de
OI Schollmeyer, Andre/0000-0002-7333-4592; Froehlich,
   Bernd/0000-0002-9439-1959
FU German Federal Ministry of Education and Research (BMBF) [03IPT704X]
FX This work was supported by the German Federal Ministry of Education and
   Research (BMBF) under grant 03IPT704X (project Big Data Analytics). The
   VW New Beetle model is courtesy of Volkswagen AG.
NR 34
TC 5
Z9 6
U1 0
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2019
VL 25
IS 3
BP 1489
EP 1498
DI 10.1109/TVCG.2018.2814987
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HK3NZ
UT WOS:000457824500005
PM 29993810
DA 2025-03-07
ER

PT J
AU Lavoué, G
   Langer, M
   Peytavie, A
   Poulin, P
AF Lavoue, Guillaume
   Langer, Michael
   Peytavie, Adrien
   Poulin, Pierre
TI A Psychophysical Evaluation of Texture Compression Masking Effects
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Texture compression; psychophysical experiment; image quality
   assessment; diffuse map; normal map
ID IMAGE QUALITY ASSESSMENT; VISUAL QUALITY; ERROR; SIMPLIFICATION
AB Lossy texture compression is increasingly used to reduce GPU memory and bandwidth consumption. However, as raised by recent studies, evaluating the quality of compressed textures is a difficult problem. Indeed using Peak Signal-to-Noise Ratio (PSNR) on texture images, like done in most applications, may not be a correct way to proceed. In particular, there is evidence that masking effects apply when the texture image is mapped on a surface and combined with other textures (e.g., affecting geometry or normal). These masking effects have to be taken into account when compressing a set of texture maps, in order to have a real understanding of the visual impact of the compression artifacts on the rendered images. In this work, we present the first psychophysical experiment investigating the perceptual impact of texture compression on rendered images. We explore the influence of compression bit rate, light direction, and diffuse and normal map content on the visual impact of artifacts. The collected data reveal huge masking effects from normal map to diffuse map artifacts and vice versa, and reveal the weakness of PSNR applied on individual textures for evaluating compression quality. The results allow us to also analyze the performance and failures of image quality metrics for predicting the visibility of these artifacts. We finally provide some recommendations for evaluating the quality of texture compression and show a practical application to approximating the distortion measured on a rendered 3D shape.
C1 [Lavoue, Guillaume; Peytavie, Adrien] Univ Lyon, LIRIS, CNRS, F-69621 Villeurbanne, France.
   [Langer, Michael] McGill Univ, Montreal, PQ H3A 0G4, Canada.
   [Poulin, Pierre] Univ Montreal, Montreal, PQ H3T 1J4, Canada.
C3 Institut National des Sciences Appliquees de Lyon - INSA Lyon; Centre
   National de la Recherche Scientifique (CNRS); Universite de Montreal
RP Lavoué, G (corresponding author), Univ Lyon, LIRIS, CNRS, F-69621 Villeurbanne, France.
EM glavoue@liris.cnrs.fr; langer@cim.mcgill.ca;
   adrien.peytavie@univ-lyon1.fr; poulin@iro.umontreal.ca
RI Peytavie, Adrien/X-2253-2019
OI Lavoue, Guillaume/0000-0003-3988-6702; Peytavie,
   Adrien/0000-0002-6994-9164
FU Auvergne-Rhone-Alpes region under the COOPERA grant "ComplexLoD"; French
   National Research Agency, ANR-PISCo project [ANR-17-CE33-0005-01];
   Natural Sciences and Engineering Research Council of Canada (NSERC);
   Agence Nationale de la Recherche (ANR) [ANR-17-CE33-0005] Funding
   Source: Agence Nationale de la Recherche (ANR)
FX This work was partly supported by Auvergne-Rhone-Alpes region under the
   COOPERA grant "ComplexLoD", by French National Research Agency as part
   of ANR-PISCo project (ANR-17-CE33-0005-01) and by the Natural Sciences
   and Engineering Research Council of Canada (NSERC).
NR 47
TC 2
Z9 2
U1 0
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2019
VL 25
IS 2
BP 1336
EP 1346
DI 10.1109/TVCG.2018.2805355
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG5ZY
UT WOS:000455062000008
PM 29994636
OA Green Published
DA 2025-03-07
ER

PT J
AU Angelini, M
   Blasilli, G
   Catarci, T
   Lenti, S
   Santucci, G
AF Angelini, Marco
   Blasilli, Graziano
   Catarci, Tiziana
   Lenti, Simone
   Santucci, Giuseppe
TI Vulnus: Visual Vulnerability Analysis for Network Security
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual Analytics; Network security; Vulnerability analysis; CVE; CVSS;
   Attack Graph; Vulnerability triage and management
AB Vulnerabilities represent one of the main weaknesses of IT systems and the availability of consolidated official data, like CVE (Common Vulnerabilities and Exposures), allows for using them to compute the paths an attacker is likely to follow. However, even it patches are available, business constraints or lack of resources create obstacles to their straightforward application. As a consequence, the security manager of a network needs to deal with a large number of vulnerabilities, making decisions on how to cope with them. This paper presents VULNUS (VULNerabilities visUal aSsessment), a visual analytics solution for dynamically inspecting the vulnerabilities spread on networks, allowing for a quick understanding of the network status and visually classifying nodes according to their vulnerabilities. Moreover, VULNUS computes the approximated optimal sequence of patches able to eliminate all the attack paths and allows for exploring sub-optimal patching strategies, simulating the effect of removing one or more vulnerabilities. VULNUS has been evaluated by domain experts using a lab-test experiment, investigating the effectiveness and efficiency of the proposed solution.
C1 [Angelini, Marco; Blasilli, Graziano; Catarci, Tiziana; Lenti, Simone; Santucci, Giuseppe] Univ Roma La Sapienza, Rome, Italy.
C3 Sapienza University Rome
RP Angelini, M (corresponding author), Univ Roma La Sapienza, Rome, Italy.
EM angelini@diag.uniroma1.it; blasilli@diag.uniroma1.it;
   catarci@diag.uniroma1.it; lenti@diag.uniroma1.it;
   santucci@diag.uniroma1.it
RI Blasilli, Graziano/HLQ-6056-2023; CATARCI, Tiziana/AAH-3157-2020;
   Santucci, Giuseppe/F-3907-2011; Lenti, Simone/ABA-3229-2020
OI Lenti, Simone/0000-0001-8281-3723; Blasilli,
   Graziano/0000-0003-3339-6403; CATARCI, Tiziana/0000-0002-3578-1121;
   Angelini, Marco/0000-0001-9051-6972
NR 39
TC 27
Z9 29
U1 3
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 183
EP 192
DI 10.1109/TVCG.2018.2865028
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000018
PM 30136974
DA 2025-03-07
ER

PT J
AU Badam, SK
   Liu, ZC
   Elmqvist, N
AF Badam, Sriram Karthik
   Liu, Zhicheng
   Elmqvist, Niklas
TI Elastic Documents: Coupling Text and Tables through Contextual
   Visualizations for Enhanced Document Reading
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Document reading; contextual visualizations; visual aids; comprehension;
   summarization
AB Today's data-rich documents are often complex datasets in themselves, consisting of information in different formats such as text, figures, and data tables. These additional media augment the textual narrative in the document. However, the static layout of a traditional for-print document often impedes deep understanding of its content because of the need to navigate to access content scattered throughout the text. In this paper, we seek to facilitate enhanced comprehension of such documents through a contextual visualization technique that couples text content with data tables contained in the document. We parse the text content and data tables, cross-link the components using a keyword-based matching algorithm, and generate on-demand visualizations based on the reader's current focus within a document. We evaluate this technique in a user study comparing our approach to a traditional reading experience. Results from our study show that (1) participants comprehend the content better with tighter coupling of text and data, (2) the contextual visualizations enable participants to develop better summaries that capture the main data-rich insights within the document, and (3) overall, our method enables participants to develop a more detailed understanding of the document content.
C1 [Badam, Sriram Karthik; Elmqvist, Niklas] Univ Maryland, College Pk, MD 20742 USA.
   [Liu, Zhicheng] Adobe Res, Seattle, WA USA.
C3 University System of Maryland; University of Maryland College Park;
   Adobe Systems Inc.
RP Badam, SK (corresponding author), Univ Maryland, College Pk, MD 20742 USA.
EM sbadam@umd.edu; leoli@adobe.com; elm@umd.edu
RI Badam, Sriram/AAK-7256-2021
OI Elmqvist, Niklas/0000-0001-5805-5301
FU U.S. National Science Foundation [IIS-1539534]
FX We thank the anonymous reviewers, Catherine Plaisant, and Clemens N.
   Klokmose for their valuable feedback that substantially improved this
   manuscript. Majority of the ideation and technical development for this
   research work was done during the first author's summer internship at
   Adobe Research in Seattle. This work was partially supported by the U.S.
   National Science Foundation award IIS-1539534. Any opinions, findings,
   and conclusions or recommendations expressed in this material are those
   of the authors and do not necessarily reflect the views of the funding
   agency.
NR 62
TC 35
Z9 39
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 661
EP 671
DI 10.1109/TVCG.2018.2865119
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA HD6IJ
UT WOS:000452640000063
PM 30136984
DA 2025-03-07
ER

PT J
AU El-Assady, M
   Sperrle, F
   Deussen, O
   Keim, D
   Collins, C
AF El-Assady, Mennatallah
   Sperrle, Fabian
   Deussen, Oliver
   Keim, Daniel
   Collins, Christopher
TI Visual Analytics for Topic Model Optimization based on User-Steerable
   Speculative Execution
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE User-Steerable Topic Modeling; Speculative Execution; Mixed-Initiative
   Visual Analytics; Explainable Machine Learning
ID UNCERTAINTY
AB To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-loop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.
C1 [El-Assady, Mennatallah; Sperrle, Fabian; Deussen, Oliver; Keim, Daniel] Univ Konstanz, Constance, Germany.
   [El-Assady, Mennatallah; Collins, Christopher] Univ Ontario Inst Technol, Oshawa, ON, Canada.
C3 University of Konstanz; Ontario Tech University
RP El-Assady, M (corresponding author), Univ Konstanz, Constance, Germany.; El-Assady, M (corresponding author), Univ Ontario Inst Technol, Oshawa, ON, Canada.
RI Deussen, Oliver/HKF-2004-2023; Collins, Christopher/AAJ-6345-2020; Keim,
   Daniel/X-7749-2019
OI El-Assady, Mennatallah/0000-0001-8526-2613
FU NSERC [RGPIN-2015-03916];  [DEG-777/17];  [DFG-431/16]
FX Funded by DEG-777/17, DFG-431/16, and NSERC RGPIN-2015-03916.
NR 67
TC 31
Z9 34
U1 0
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 374
EP 384
DI 10.1109/TVCG.2018.2864769
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000036
PM 30235133
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Jo, J
   Vernier, F
   Dragicevic, P
   Fekete, JD
AF Jo, Jaemin
   Vernier, Frederic
   Dragicevic, Pierre
   Fekete, Jean-Daniel
TI A Declarative Rendering Model for Multiclass Density Maps
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Scalability; multiclass scatterplots; density maps; aggregation;
   declarative specification; visualization grammar
ID TIME
AB Multiclass maps are scatterplots, multidimensional projections, or thematic geographic maps where data points have a categorical attribute in addition to two quantitative attributes. This categorical attribute is often rendered using shape or color, which does not scale when overplotting occurs. When the number of data points increases, multiclass maps must resort to data aggregation to remain readable. We present multiclass density maps: multiple 2D histograms computed for each of the category values. Multiclass density maps are meant as a building block to improve the expressiveness and scalability of multiclass map visualization. In this article, we first present a short survey of aggregated multiclass maps, mainly from cartography. We then introduce a declarative model a simple yet expressive JSON grammar associated with visual semantics that specifies a wide design space of visualizations for multiclass density maps. Our declarative model is expressive and can be efficiently implemented in visualization front-ends such as modern web browsers. Furthermore, it can be reconfigured dynamically to support data exploration tasks without recomputing the raw data. Finally, we demonstrate how our model can be used to reproduce examples from the past and support exploring data at scale.
C1 [Jo, Jaemin] Seoul Natl Univ, Seoul, South Korea.
   [Vernier, Frederic] Univ Paris Saclay, Univ Paris Sud, CNRS, LIMSI, Paris, France.
   [Dragicevic, Pierre; Fekete, Jean-Daniel] INRIA, Rocquencourt, France.
C3 Seoul National University (SNU); Centre National de la Recherche
   Scientifique (CNRS); Universite Paris Saclay; Inria
RP Jo, J (corresponding author), Seoul Natl Univ, Seoul, South Korea.
EM jmjo@hcil.snu.ac.kr; frederic.vernier@limsi.fr;
   pierre.dragicevic@inria.fr; jean-daniel.fekete@inria.fr
RI Dragicevic, Pierre/HKV-4981-2023; Fekete, Jean-Daniel/N-9175-2018
OI Fekete, Jean-Daniel/0000-0003-3770-8726
FU National Research Foundation of Korea (NRF) - Korea government (MSIP)
   [NRF-2016R1A2B2007153]
FX This work was supported by the National Research Foundation of Korea
   (NRF) grant funded by the Korea government (MSIP) (No.
   NRF-2016R1A2B2007153). Fig. 2c is from Semiology of Graphics: Diagrams,
   Networks, Maps by Jacques Bertin and reprinted by permission of the
   University of Wisconsin Press. (c) 1983 by the Board of Regents of the
   University of Wisconsin System. All rights reserved.
NR 52
TC 18
Z9 22
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 470
EP 480
DI 10.1109/TVCG.2018.2865141
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000045
PM 30136987
OA Green Published
DA 2025-03-07
ER

PT J
AU Molchanov, V
   Linsen, L
AF Molchanov, Vladimir
   Linsen, Lars
TI Shape-preserving Star Coordinates
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Star coordinates; multidimensional data projection; multivariate data
   visualization
ID VISUALIZATION
AB Dimensionality reduction is commonly applied to multidimensional data to reduce the complexity of their analysis. In visual analysis systems, projections embed multidimensional data into 2D or 3D spaces for graphical representation. To facilitate a robust and accurate analysis, essential characteristics of the multidimensional data shall be preserved when projecting. Orthographic star coordinates is a state-of-the-art linear projection method that avoids distortion of multidimensional clusters by restricting interactive exploration to orthographic projections. However, existing numerical methods for computing orthographic star coordinates have a number of limitations when putting them into practice. We overcome these limitations by proposing the novel concept of shape preserving star coordinates where shape preservation is assured using a superset of orthographic projections. Our scheme is explicit, exact, simple, fast, parameter-free, and stable. To maintain a valid shape-preserving star-coordinates configuration during user interaction with one of the star-coordinates axes, we derive an algorithm that only requires us to modify the configuration of one additional compensatory axis. Different design goals can be targeted by using different strategies for selecting the compensatory axis. We propose and discuss four strategies including a strategy that approximates orthographic star coordinates very well and a data-driven strategy. We further present shape-preserving morphing strategies between two shape-preserving configurations, which can be adapted for the generation of data tours. We apply our concept to multiple data analysis scenarios to document its applicability and validate its desired properties.
C1 [Molchanov, Vladimir; Linsen, Lars] Westfalische Wilhelms Univ Munster, Munster, Germany.
C3 University of Munster
RP Molchanov, V (corresponding author), Westfalische Wilhelms Univ Munster, Munster, Germany.
EM molchano@uni-muenster.de; linsen@uni-muenster.de
FU DFG [MO 3050/2-1]
FX This work was supported in part by a grant from DFG (MO 3050/2-1).
NR 30
TC 4
Z9 4
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 449
EP 458
DI 10.1109/TVCG.2018.2865118
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000043
PM 30136983
DA 2025-03-07
ER

PT J
AU Ondov, B
   Jardine, N
   Elmqvist, N
   Franconeri, S
AF Ondov, Brian
   Jardine, Nicole
   Elmqvist, Niklas
   Franconeri, Steven
TI Face to Face: Evaluating Visual Comparison
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Graphical perception; visual perception; visual comparison; crowdsourced
   evaluation
ID ANIMATED TRANSITIONS; PERCEPTION; SYMMETRY; SPACE; SEARCH; DESIGN; MAPS
AB Data are often viewed as a single set of values, but those values frequently must be compared with another set. The existing evaluations of designs that facilitate these comparisons tend to be based on intuitive reasoning, rather than quantifiable measures. We build on this work with a series of crowdsourced experiments that use low-level perceptual comparison tasks that arise frequently in comparisons within data visualizations (e.g., which value changes the most between the two sets of data?). Participants completed these tasks across a variety of layouts: overlaid, two arrangements of juxtaposed small multiples, mirror-symmetric small multiples, and animated transitions. A staircase procedure sought the difficulty level (e.g., value change delta) that led to equivalent accuracy for each layout. Confirming prior intuition, we observe high levels of performance for overlaid versus standard small multiples. However, we also find performance improvements for both mirror symmetric small multiples and animated transitions. While some results are incongruent with common wisdom in data visualization, they align with previous work in perceptual psychology, and thus have potentially strong implications for visual comparison designs.
C1 [Ondov, Brian] NIH, Bldg 10, Bethesda, MD 20892 USA.
   [Ondov, Brian; Elmqvist, Niklas] Univ Maryland, College Pk, MD 20742 USA.
   [Jardine, Nicole; Franconeri, Steven] Northwestern Univ, Evanston, IL USA.
C3 National Institutes of Health (NIH) - USA; University System of
   Maryland; University of Maryland College Park; Northwestern University
RP Ondov, B (corresponding author), NIH, Bldg 10, Bethesda, MD 20892 USA.; Ondov, B (corresponding author), Univ Maryland, College Pk, MD 20742 USA.
EM ondovb@umd.edu; nicole.jardine@northwestern.edu; elm@umd.edu;
   franconeri@northwestern.edu
OI Jardine, Nicole/0000-0001-5472-4029; Elmqvist,
   Niklas/0000-0001-5805-5301
FU intramural Research Program of the National Human Genome Research
   Institute, National Institutes of Health
FX We thank Adil Yalcin of Keshif, LTC for advice and assistance with the
   use of the Amazon Mechanical Turk platform, Catherine Plaisant for
   feedback on experimental design and usability, and our peer reviewers
   for helpful suggestions. Brian Ondov was supported by the intramural
   Research Program of the National Human Genome Research Institute,
   National Institutes of Health. Any opinions, findings, and conclusions
   or recommendations expressed in this material are those of the authors
   and do not necessarily reflect the views of the respective funding
   agencies.
NR 69
TC 41
Z9 46
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 861
EP 871
DI 10.1109/TVCG.2018.2864884
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA HD6IJ
UT WOS:000452640000082
PM 30136952
DA 2025-03-07
ER

PT J
AU Wang, YH
   Wang, YY
   Zhang, HF
   Sun, YQ
   Fu, CW
   Sedlmair, M
   Chen, BQ
   Deussen, O
AF Wang, Yunhai
   Wang, Yanyan
   Zhang, Haifeng
   Sun, Yinqi
   Fu, Chi-Wing
   Sedlmair, Michael
   Chen, Baoquan
   Deussen, Oliver
TI Structure-aware Fisheye Views for Efficient Large Graph Exploration
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Graph Visualization; Focus plus Context Technique; Structure-aware Zoom;
   Graph Layout Technique
ID NAVIGATION; SYSTEM; LAYOUT; LENS
AB Traditional fisheye views for exploring large graphs introduce substantial distortions that often lead to a decreased readability of paths and other interesting structures. To overcome these problems, we propose a framework for structure-aware fisheye views. Using edge orientations as constraints for graph layout optimization allows us not only to reduce spatial and temporal distortions during fisheye zooms, but also to improve the readability of the graph structure. Furthermore, the framework enables us to optimize fisheye lenses towards specific tasks and design a family of new lenses: polyfocal, cluster, and path lenses. A GPU implementation lets us process large graphs with up to 15,000 nodes at interactive rates. A comprehensive evaluation, a user study, and two case studies demonstrate that our structure-aware fisheye views improve layout readability and user performance.
C1 [Wang, Yunhai; Wang, Yanyan; Zhang, Haifeng; Sun, Yinqi] Shandong Univ, Jinan, Shandong, Peoples R China.
   [Chen, Baoquan] Peking Univ, Beijing, Peoples R China.
   [Fu, Chi-Wing] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
   [Sedlmair, Michael] Univ Stuttgart, VISUS, Stuttgart, Germany.
   [Deussen, Oliver] Konstanz Univ, Constance, Germany.
   [Deussen, Oliver] SIAT, Shenzhen VisuCA Key Lab, Shenzhen, Peoples R China.
C3 Shandong University; Peking University; Chinese University of Hong Kong;
   University of Stuttgart; University of Konstanz; Chinese Academy of
   Sciences; Shenzhen Institute of Advanced Technology, CAS
RP Wang, YH (corresponding author), Shandong Univ, Jinan, Shandong, Peoples R China.
EM cloudseawang@gmail.com; yanyanwang93@gmail.com; hiphone.zhang@gmail.com;
   sunyinqi0508@gmail.com; cwfu@cse.cuhk.edu.hk;
   michael.sedlmair@visus.uni-stuttgart.de; baoquan.chen@gmail.com;
   oliver.deussen@uni-konstanz.de
RI wang, yanyan/JLL-4616-2023; Deussen, Oliver/HKF-2004-2023; Fu,
   Chi-Wing/X-4703-2019
OI Fu, Chi Wing/0000-0002-5238-593X; Sun, Yinqi/0009-0000-3783-5472;
   Sedlmair, Michael/0000-0001-7048-9292
FU National Key Research & Development Plan of China [2016YFB1001404]; NSFC
   [61772315]; NSFC-Guangdong Joint Fund [U1501255]; Science Challenge
   Project [TZ2016002]; Shandong Provincial Natural Science Foundation
   [ZR2016FM12]; Open Research Fund of Beijing Key Laboratory of Big Data
   Technology for Food Safety [BUBD-2017KF02]; Beijing Technology and
   Business University; Open Project Program of the State Key Lab of CAD CG
   [A1801]; Zhejiang University; Fundamental Research Funds of Shandong
   University
FX This work is supported by the grants of the National Key Research &
   Development Plan of China (2016YFB1001404), NSFC (61772315),
   NSFC-Guangdong Joint Fund (U1501255), Science Challenge Project (No.
   TZ2016002), Shandong Provincial Natural Science Foundation (ZR2016FM12),
   the Open Research Fund of Beijing Key Laboratory of Big Data Technology
   for Food Safety (No. BUBD-2017KF02), Beijing Technology and Business
   University, the Open Project Program of the State Key Lab of CAD & CG
   (No. A1801), Zhejiang University, and the Fundamental Research Funds of
   Shandong University.
NR 56
TC 18
Z9 21
U1 0
U2 22
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 566
EP 575
DI 10.1109/TVCG.2018.2864911
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000054
PM 30136962
DA 2025-03-07
ER

PT J
AU Nadeem, S
   Gu, XF
   Kaufman, AE
AF Nadeem, Saad
   Gu, Xianfeng
   Kaufman, Arie E.
TI LMap: Shape-Preserving Local Mappings for Biomedical Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Biomedical visualization; virtual colonoscopy; multimodal brain
   visualization; molecular surface visualization; shape-preserving mapping
ID SURFACE; MAPS; GEOMETRY; SUPINE
AB Visualization of medical organs and biological structures is a challenging task because of their complex geometry and the resultant occlusions. Global spherical and planar mapping techniques simplify the complex geometry and resolve the occlusions to aid in visualization. However, while resolving the occlusions these techniques do not preserve the geometric context, making them less suitable for mission-critical biomedical visualization tasks. In this paper, we present a shape-preserving local mapping technique for resolving occlusions locally while preserving the overall geometric context. More specifically, we present a novel visualization algorithm, LMap, for conformally parameterizing and deforming a selected local region-of-interest (ROI) on an arbitrary surface. The resultant shape-preserving local mappings help to visualize complex surfaces while preserving the overall geometric context. The algorithm is based on the robust and efficient extrinsic Ricci flow technique, and uses the dynamic Ricci flow algorithm to guarantee the existence of a local map for a selected ROI on an arbitrary surface. We show the effectiveness and efficacy of our method in three challenging use cases: (1) multimodal brain visualization, (2) optimal coverage of virtual colonoscopy centerline flythrough, and (3) molecular surface visualization.
C1 [Nadeem, Saad; Gu, Xianfeng; Kaufman, Arie E.] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; Stony Brook University
RP Nadeem, S (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM sanadeem@cs.stonybrook.edu; gu@cs.stonybrook.edu; ari@cs.stonybrook.edu
FU McDonnell Center for Systems Neuroscience at Washington University; US
   National Science Foundation [IIP1069147, CNS1302246, IIS1527200,
   NRT1633299, CNS1650499]; Marcus Foundation
FX The VC datasets are courtesy of Stony Brook University Hospital (SBUH)
   and Dr. Richard Choi, Walter Reed Army Medical Center. The brain
   datasets are courtesy Human Connectome Project, WU-Minn Consortium (PIs:
   David Van Essen and Kamil Ugurbil) funded by the 16 NIH Institutes and
   Centers that support the NIH Blueprint for Neuroscience Research; and by
   the McDonnell Center for Systems Neuroscience at Washington University.
   The molecular imaging data is courtesy Dr. Michael Krone of
   Visualization Research Center, University of Stuttgart, Germany. We
   would like to thank Dr. Matthew Barish and Dr. Kevin Baker of SBUH and
   Dr. Hoi-Chung Leung of SBU Neuroscience Department for their help in
   this project. This work has been partially supported by the US National
   Science Foundation grants IIP1069147, CNS1302246, IIS1527200,
   NRT1633299, CNS1650499, and the Marcus Foundation.
NR 36
TC 2
Z9 3
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2018
VL 24
IS 12
BP 3111
EP 3122
DI 10.1109/TVCG.2017.2772237
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GZ0TW
UT WOS:000449079000009
PM 29990124
OA Bronze, Green Submitted, Green Accepted
DA 2025-03-07
ER

PT J
AU Wei, XZ
   Qiu, SQ
   Zhu, L
   Feng, RL
   Tian, YB
   Xi, JT
   Zheng, YY
AF Wei, Xiangzhi
   Qiu, Siqi
   Zhu, Lin
   Feng, Ruiliang
   Tian, Yaobin
   Xi, Juntong
   Zheng, Youyi
TI Toward Support-Free 3D Printing: A Skeletal Approach for Partitioning
   Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE 3D printing; skeleton; model partition; support-free
ID MESH SEGMENTATION; OBJECTS; OPTIMIZATION; PACKING
AB Minimizing support structures is crucial in reducing 3D printing material and time. Partition-based methods are efficient means in realizing this objective. Although some algorithms exist for support-free fabrication of solid models, no algorithm ever considers the problem of support-free fabrication for shell models (i.e., hollowed meshes). In this paper, we present a skeleton-based algorithm for partitioning a 3D surface model into the least number of parts for 3D printing without using any support structure. To achieve support-free fabrication while minimizing the effect of the seams and cracks that are inevitably induced by the partition, which affect the aesthetics and strength of the final assembled surface, we put forward an optimization system with the minimization of the number of partitions and the total length of the cuts, under the constraints of support-free printing angle. Our approach is particularly tailored for shell models, and it can be applicable to solid models as well. We first rigorously show that the optimization problem is NP-hard and then propose a stochastic method to find an optimal solution to the objectives. We propose a polynomial-time algorithm for a special case when the skeleton graph satisfies the requirement that the number of partitioned parts and the degree of each node are bounded by a small constant. We evaluate our partition method on a number of 3D models and validate our method by 3D printing experiments.
C1 [Wei, Xiangzhi; Qiu, Siqi; Zhu, Lin; Feng, Ruiliang; Xi, Juntong] Shanghai Jiao Tong Univ, Sch Mech Engn, Dept Intelligent Mfg & Informat Engn, Shanghai, Peoples R China.
   [Tian, Yaobin] Hong Kong Univ Sci & Technol, Dept Ind Engn & Logist Management, Hong Kong, Hong Kong, Peoples R China.
   [Zheng, Youyi] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
C3 Shanghai Jiao Tong University; Hong Kong University of Science &
   Technology; Zhejiang University
RP Zheng, YY (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM antonwei@sjtu.edu.cn; siqiqiu@sjtu.edu.cn; zhulin0728@sjtu.edu.cn;
   fengnaliang@sjtu.edu.cn; ybtian@ust.hk; jtxi@sjtu.edu.cn;
   zyy@cad.zju.edu.cn
OI Wei, Xiangzhi/0000-0002-4541-0017
FU Shanghai Sailing Program [16YF1405500]; Shanghai Jiao Tong University
   [AF0200163]; State Key Laboratory of Mechanical Systems and Vibration
   [MSVZD201505]; National Natural Science Foundation of China [51605290,
   61502306]; China Young 1000 Talents Program
FX This work was supported in part by Shanghai Sailing Program No.
   16YF1405500, Shanghai Jiao Tong University Grant No. AF0200163, State
   Key Laboratory of Mechanical Systems and Vibration Grant No.
   MSVZD201505, and The National Natural Science Foundation of China No.
   51605290, No. 61502306, and the China Young 1000 Talents Program.
   Xiangzhi Wei and Siqi Qiu are co-first author.
NR 56
TC 56
Z9 63
U1 1
U2 64
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2018
VL 24
IS 10
BP 2799
EP 2812
DI 10.1109/TVCG.2017.2767047
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GS7PN
UT WOS:000443894900011
PM 29989969
DA 2025-03-07
ER

PT J
AU Lai, WS
   Huang, YJ
   Joshi, N
   Buehler, C
   Yang, MH
   Kang, SB
AF Lai, Wei-Sheng
   Huang, Yujia
   Joshi, Neel
   Buehler, Christopher
   Yang, Ming-Hsuan
   Kang, Sing Bing
TI Semantic-Driven Generation of Hyperlapse from 360 Degree Video
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE 360 degree videos; hyperlapse; video stabilization; semantic
   segmentation; spatial-temporal saliency
ID MODEL
AB We present a system for converting a fully panoramic (360 degree) video into a normal field-of-view (NFOV) hyperlapse for an optimal viewing experience. Our system exploits visual saliency and semantics to non-uniformly sample in space and time for generating hyperlapses. In addition, users can optionally choose objects of interest for customizing the hyperlapses. We first stabilize an input 360 degree video by smoothing the rotation between adjacent frames and then compute regions of interest and saliency scores. An initial hyperlapse is generated by optimizing the saliency and motion smoothness followed by the saliency-aware frame selection. We further smooth the result using an efficient 2D video stabilization approach that adaptively selects the motion model to generate the final hyperlapse. We validate the design of our system by showing results for a variety of scenes and comparing against the state-of-the-art method through a large-scale user study.
C1 [Lai, Wei-Sheng; Yang, Ming-Hsuan] Univ Calif Merced, Dept Elect & Engn & Comp Sci, Merced, CA 95340 USA.
   [Huang, Yujia] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.
   [Joshi, Neel; Buehler, Christopher; Kang, Sing Bing] Microsoft Artificial Intelligence & Res AI&R, Redmond, WA 98052 USA.
C3 University of California System; University of California Merced;
   Carnegie Mellon University
RP Lai, WS (corresponding author), Univ Calif Merced, Dept Elect & Engn & Comp Sci, Merced, CA 95340 USA.
EM wlai24@ucmerced.edu; yujiah1@andrew.cmu.edu; neel@microsoft.com;
   chbuehle@microsoft.com; mliyang@ucmerced.edu; sbkang@microsoft.com
RI Yang, Ming-Hsuan/T-9533-2019
OI Yang, Ming-Hsuan/0000-0003-4848-2304
FU Direct For Computer & Info Scie & Enginr [1149783] Funding Source:
   National Science Foundation; Div Of Information & Intelligent Systems
   [1149783] Funding Source: National Science Foundation
NR 31
TC 40
Z9 45
U1 0
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2018
VL 24
IS 9
BP 2610
EP 2621
DI 10.1109/TVCG.2017.2750671
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GP3XC
UT WOS:000440787200012
PM 28910772
DA 2025-03-07
ER

PT J
AU Lu, XQ
   Wu, SH
   Chen, HH
   Yeung, SK
   Chen, WZ
   Zwicker, M
AF Lu, Xuequan
   Wu, Shihao
   Chen, Honghua
   Yeung, Sai-Kit
   Chen, Wenzhi
   Zwicker, Matthias
TI GPF: GMM-Inspired Feature-Preserving Point Set Filtering
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE GPF; gaussian mixture model; point set filtering; feature preserving
ID GAUSSIAN MIXTURE MODEL; OBJECTS
AB Point set filtering, which aims at reconstructing noise-free point sets from their corresponding noisy inputs, is a fundamental problem in 3D geometry processing. The main challenge of point set filtering is to preserve geometric features of the underlying geometry while at the same time removing the noise. State-of-the-art point set filtering methods still struggle with this issue: some are not designed to recover sharp features, and others cannot well preserve geometric features, especially fine-scale features. In this paper, we propose a novel approach for robust feature-preserving point set filtering, inspired by the Gaussian Mixture Model (GMM). Taking a noisy point set and its filtered normals as input, our method can robustly reconstruct a high-quality point set which is both noise-free and feature-preserving. Various experiments show that our approach can soundly outperform the selected state-of-the-art methods, in terms of both filtering quality and reconstruction accuracy.
C1 [Lu, Xuequan; Chen, Wenzhi] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
   [Wu, Shihao] Univ Bern, Inst Comp Sci, CH-3012 Bern, Switzerland.
   [Chen, Honghua] Nanjing Normal Univ, Coll Educ Sci, Nanjing 210097, Jiangsu, Peoples R China.
   [Yeung, Sai-Kit] Singapore Univ Technol & Design, Pilliar Informat Syst Technol & Design, Singapore 487372, Singapore.
   [Zwicker, Matthias] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
C3 Zhejiang University; University of Bern; Nanjing Normal University;
   Singapore University of Technology & Design; University System of
   Maryland; University of Maryland College Park
RP Chen, WZ (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou 310027, Zhejiang, Peoples R China.
EM xuequanlu@zju.edu.cn; wu@inf.unibe.ch; chenhonghuacn@gmail.com;
   saikit@sutd.edu.sg; chenwz@zju.edu.cn; zwicker@cs.umd.edu
RI 陳, 文誌/AAI-6255-2021; Wu, Shihao/ADU-7023-2022
OI Lu, Xuequan/0000-0003-0959-408X; Zwicker, Matthias/0000-0001-8630-5515;
   Yeung, Sai-Kit/0000-0001-7974-0607
FU Singapore MOE Academic Research Fund [MOE2013-T2-1-159]; SUTD-MIT
   International Design Center Grant [IDG31300106]; SUTD Digital
   Manufacturing and Design (DManD) Centre; National Research Foundation
   (NRF) of Singapore; National Research Foundation, Prime Minister's
   Office, Singapore under its IDM Futures Funding Initiative
FX This work is supported in part by Singapore MOE Academic Research Fund
   MOE2013-T2-1-159 and SUTD-MIT International Design Center Grant
   IDG31300106. We acknowledge the in-part support of the SUTD Digital
   Manufacturing and Design (DManD) Centre which is supported by the
   National Research Foundation (NRF) of Singapore. This research is also
   supported in part by the National Research Foundation, Prime Minister's
   Office, Singapore under its IDM Futures Funding Initiative.
NR 44
TC 47
Z9 52
U1 3
U2 22
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2018
VL 24
IS 8
BP 2315
EP 2326
DI 10.1109/TVCG.2017.2725948
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GL6DJ
UT WOS:000437269000005
PM 28708561
DA 2025-03-07
ER

PT J
AU Leimkühler, T
   Kellnhofer, P
   Ritschel, T
   Myszkowski, K
   Seidel, HP
AF Leimkuehler, Thomas
   Kellnhofer, Petr
   Ritschel, Tobias
   Myszkowski, Karol
   Seidel, Hans-Peter
TI Perceptual Real-Time 2D-to-3D Conversion Using Cue Fusion
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Depth cues; stereo; image-based rendering; perceptual reasoning; video
   analysis; viewing algorithms; pixel classification; real-time systems
ID DEPTH; VIDEO
AB We propose a system to infer binocular disparity from a monocular video stream in real-time. Different from classic reconstruction of physical depth in computer vision, we compute perceptually plausible disparity, that is numerically inaccurate, but results in a very similar overall depth impression with plausible overall layout, sharp edges, fine details and agreement between luminance and disparity. We use several simple monocular cues to estimate disparity maps and confidence maps of low spatial and temporal resolution in real-time. These are complemented by spatially-varying, appearance-dependent and class-specific disparity prior maps, learned from example stereo images. Scene classification selects this prior at runtime. Fusion of prior and cues is done by means of robust MAP inference on a dense spatio-temporal conditional random field with high spatial and temporal resolution. Using normal distributions allows this in constant-time, parallel per-pixel work. We compare our approach to previous 2D-to-3D conversion systems in terms of different metrics, as well as a user study and validate our notion of perceptually plausible disparity.
C1 [Leimkuehler, Thomas; Myszkowski, Karol; Seidel, Hans-Peter] MPI Informat, D-66123 Saarbrucken, Saarland, Germany.
   [Kellnhofer, Petr] MIT CSAIL, Cambridge, MA 02139 USA.
   [Ritschel, Tobias] UCL, London WC1E 6BT, England.
C3 Max Planck Society; Massachusetts Institute of Technology (MIT);
   University of London; University College London
RP Leimkühler, T (corresponding author), MPI Informat, D-66123 Saarbrucken, Saarland, Germany.
EM fleimkueh@mpi-inf.mpg.de; pkellnho@mit.edu; t.ritschel@ucl.ac.uk;
   karol@mpi-inf.mpg.de; hps@mpi-inf.mpg.de
OI Leimkuehler, Thomas/0009-0006-7784-7957; Myszkowski,
   Karol/0000-0002-8505-4141
NR 53
TC 3
Z9 4
U1 0
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2018
VL 24
IS 6
BP 2037
EP 2050
DI 10.1109/TVCG.2017.2703612
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE7EY
UT WOS:000431397300014
PM 28504938
OA Green Published
DA 2025-03-07
ER

PT J
AU Peck, TC
   Doan, M
   Bourne, KA
   Good, JJ
AF Peck, Tabitha C.
   Doan, My
   Bourne, Kimberly A.
   Good, Jessica J.
TI The Effect of Gender Body-Swap Illusions on Working Memory and
   Stereotype Threat
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 25th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 18-22, 2018
CL Reutlingen, GERMANY
SP IEEE, IEEE Comp Soc, IEEE Comp Soc Visualizat & Graph Tech Comm, VICON, Digital Project, ART, Haption, MiddleVR, VR ON, VISCON, BARCO, WorldViz, Disney Res, Chinese Acad Sci, Comp Network Informat Ctr, KUKA
DE Virtual reality; body-swap illusions; virtual embodiment; avatars;
   stereotype threat; working memory
ID IMMERSIVE VIRTUAL-REALITY; INDIVIDUAL-DIFFERENCES; TEST-PERFORMANCE;
   MATH; OWNERSHIP; REPRESENTATION; EXPERIENCE; CAPACITY; ATTITUDE; AVATARS
AB The underrepresentation of women in technical and STEM fields is a well-known problem, and stereotype threatening situations have been linked to the inability to recruit and retain women into these fields. Virtual reality enables the unique ability to perform body-swap illusions, and research has shown that these illusions can change participant behavior. Characteristically people take on the traits of the avatar they are embodying. We hypothesized that female participants embodying male avatars when a stereotype threat was made salient would demonstrate stereotype lift. We tested our hypothesis through a between-participants user study in an immersive virtual environment by measuring working memory. Our results support that stereotype threat can be induced in an immersive virtual environment, and that stereotype lift is possible with fully-immersive body-swap illusions. Additionally, our results suggest that participants in a gender-swapped avatar without an induced stereotype threat have significantly impaired working memory; however, this impairment is lifted when a threat is made salient. We discuss possible theories as to why a body-swap illusion from a female participant into a male avatar would only increase working memory impairment when not under threat, as well as applications and future research directions. Our results offer additional insight into understanding the cognitive effects of body-swap illusions, and provide evidence that virtual reality may be an applicable tool for decreasing the gender gap in technology.
C1 [Peck, Tabitha C.; Doan, My; Bourne, Kimberly A.; Good, Jessica J.] Davidson Coll, Davidson, NC 28036 USA.
C3 Davidson College
RP Peck, TC (corresponding author), Davidson Coll, Davidson, NC 28036 USA.
EM tapeck@davidson.edu; mydoan@davidson.edu; kibourne@davidson.edu;
   jegood@davidson.edu
RI Peck, Tabitha/AAH-2032-2021
OI Peck, Tabitha/0000-0002-3667-7713; Bourne, Kimberly/0000-0003-3705-091X
FU Davidson College's Faculty Study and Research grant
FX The authors wish to thank Tim Chartier for voicing the experiment
   instructions and Davidson College's Faculty Study and Research grant for
   helping to support this work.
NR 61
TC 36
Z9 38
U1 3
U2 30
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2018
VL 24
IS 4
BP 1604
EP 1612
DI 10.1109/TVCG.2018.2793598
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FZ6BS
UT WOS:000427682500023
PM 29543177
DA 2025-03-07
ER

PT J
AU Wenskovitch, J
   Crandell, I
   Ramakrishnan, N
   House, L
   Leman, S
   North, C
AF Wenskovitch, John
   Crandell, Ian
   Ramakrishnan, Naren
   House, Leanna
   Leman, Scotland
   North, Chris
TI Towards a Systematic Combination of Dimension Reduction and Clustering
   in Visual Analytics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Dimension reduction; clustering; algorithms; visual analytics
ID NODE-LINK; EXPLORATION; INFORMATION; PROJECTION; ALGORITHM; DIAGRAMS;
   DISTANCE; LAYOUT
AB Dimension reduction algorithms and clustering algorithms are both frequently used techniques in visual analytics. Both families of algorithms assist analysts in performing related tasks regarding the similarity of observations and finding groups in datasets. Though initially used independently, recent works have incorporated algorithms from each family into the same visualization systems. However, these algorithmic combinations are often ad hoc or disconnected, working independently and in parallel rather than integrating some degree of interdependence. A number of design decisions must be addressed when employing dimension reduction and clustering algorithms concurrently in a visualization system, including the selection of each algorithm, the order in which they are processed, and how to present and interact with the resulting projection. This paper contributes an overview of combining dimension reduction and clustering into a visualization system, discussing the challenges inherent in developing a visualization system that makes use of both families of algorithms.
C1 [Wenskovitch, John; Ramakrishnan, Naren; North, Chris] Virginia Tech, Dept Comp Sci, Blacksburg, VA 24061 USA.
   [Crandell, Ian; House, Leanna; Leman, Scotland] Virginia Tech, Dept Stat, Blacksburg, VA 24061 USA.
C3 Virginia Polytechnic Institute & State University; Virginia Polytechnic
   Institute & State University
RP Wenskovitch, J (corresponding author), Virginia Tech, Dept Comp Sci, Blacksburg, VA 24061 USA.
EM jw87@cs.vt.edu; ian85@vt.edu; naren@cs.vt.edu; lhouse@vt.edu;
   leman@vt.edu; north@cs.vt.edu
RI Wenskovitch, John/AAY-4371-2020
OI Ramakrishnan, Naren/0000-0002-1821-9743; Wenskovitch,
   John/0000-0002-0573-6442
FU NSF [IIS-1447416, IIS-1633363, DGE-1545362]; General Dynamics Mission
   Systems; Div Of Information & Intelligent Systems; Direct For Computer &
   Info Scie & Enginr [1447416] Funding Source: National Science Foundation
FX This research was supported by NSF Grants IIS-1447416, IIS-1633363, and
   DGE-1545362, as well as by a grant from General Dynamics Mission
   Systems. The authors would like to recognize the role of comments from
   reviewers and discussions with InfoVis Lab @ VT research group members
   in improving this work.
NR 93
TC 69
Z9 78
U1 0
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 131
EP 141
DI 10.1109/TVCG.2017.2745258
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400015
PM 28866581
DA 2025-03-07
ER

PT J
AU Hu, KM
   Yan, DM
   Bommes, D
   Alliez, P
   Benes, B
AF Hu, Kaimo
   Yan, Dong-Ming
   Bommes, David
   Alliez, Pierre
   Benes, Bedrich
TI Error-Bounded and Feature Preserving Surface Remeshing with Minimal
   Angle Improvement
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Surface remeshing; error-bounded; feature preserving; minimal angle
   improvement; saliency function
AB Surface remeshing is a key component in many geometry processing applications. The typical goal consists in finding a mesh that is (1) geometrically faithful to the original geometry, (2) as coarse as possible to obtain a low-complexity representation and (3) free of bad elements that would hamper the desired application (e.g., the minimum interior angle is above an application-dependent threshold). Our algorithm is designed to address all three optimization goals simultaneously by targeting prescribed bounds on approximation error delta, minimal interior angle theta and maximum mesh complexity N (number of vertices). The approximation error bound d is a hard constraint, while the other two criteria are modeled as optimization goals to guarantee feasibility. Our optimization framework applies carefully prioritized local operators in order to greedily search for the coarsest mesh with minimal interior angle above theta and approximation error bounded by delta. Fast runtime is enabled by a local approximation error estimation, while implicit feature preservation is obtained by specifically designed vertex relocation operators. Experiments show that for reasonable angle bounds (theta <= 35 degrees) our approach delivers high-quality meshes with implicitly preserved features (no tagging required) and better balances between geometric fidelity, mesh complexity and element quality than the state-of-the-art.
C1 [Hu, Kaimo; Benes, Bedrich] Purdue Univ, 610 Purdue Mall, W Lafayette, IN 47907 USA.
   [Yan, Dong-Ming] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Bommes, David] Rhein Westfal TH Aachen, Schinkelstr 2, D-52062 Aachen, Germany.
   [Alliez, Pierre] Inria Sophia Antipolis Mediterranee, 2004 Route Lucioles BP 93, F-06902 Sophia Antipolis, France.
C3 Purdue University System; Purdue University; Chinese Academy of
   Sciences; Institute of Automation, CAS; RWTH Aachen University
RP Hu, KM (corresponding author), Purdue Univ, 610 Purdue Mall, W Lafayette, IN 47907 USA.
EM hukaimo02@gmail.com; yandongming@gmail.com; bommes@aices.rwth-aachen.de;
   pierre.alliez@inria.fr; bbenes@purdue.edu
RI ; Benes, Bedrich/A-8150-2016
OI Bommes, David/0000-0002-3190-1341; Benes, Bedrich/0000-0002-5293-2112;
   Yan, Dong-Ming/0000-0003-2209-2404; Hu, Kaimo/0000-0003-3692-8380
FU European Research Council (ERC Starting Grant Robust Geometry
   Processing) [257474]; National Science Foundation of China [61373071,
   61372168, 61620106003]; German Research Foundation (DFG) [GSC 111];
   European Research Council (ERC) [257474] Funding Source: European
   Research Council (ERC)
FX We wish to thank Mario Botsch for providing the source code and results
   of their method, and Pascal Frey for providing the MMGS software. This
   work was partially supported by the European Research Council (ERC
   Starting Grant Robust Geometry Processing # 257474), the National
   Science Foundation of China (61373071, 61372168, 61620106003) and the
   German Research Foundation (DFG, grant GSC 111, Aachen Institute for
   Advanced Study in Computational Engineering Science).
NR 48
TC 44
Z9 51
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2017
VL 23
IS 12
BP 2560
EP 2573
DI 10.1109/TVCG.2016.2632720
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6YZ
UT WOS:000414393700007
PM 28114021
OA Green Published, Green Submitted
DA 2025-03-07
ER

PT J
AU Wen, Q
   Xu, F
   Yong, JH
AF Wen, Quan
   Xu, Feng
   Yong, Jun-Hai
TI Real-Time 3D Eye Performance Reconstruction for RGBD Cameras
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Eye reconstruction; facial animation; gaze tracking; multilinear model;
   RGBD camera
ID GAZE ESTIMATION; TRACKING; MOTION
AB This paper proposes a real-time method for 3D eye performance reconstruction using a single RGBD sensor. Combined with facial surface tracking, our method generates more pleasing facial performance with vivid eye motions. In our method, a novel scheme is proposed to estimate eyeball motions by minimizing the differences between a rendered eyeball and the recorded image. Our method considers and handles different appearances of human irises, lighting variations and highlights on images via the proposed eyeball model and the L0-based optimization. Robustness and real-time optimization are achieved through the novel 3D Taylor expansion-based linearization. Furthermore, we propose an online bidirectional regression method to handle occlusions and other tracking failures on either of the two eyes from the information of the opposite eye. Experiments demonstrate that our technique achieves robust and accurate eye performance reconstruction for different iris appearances, with various head/face/eye motions, and under different lighting conditions.
C1 [Wen, Quan; Xu, Feng; Yong, Jun-Hai] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
   [Wen, Quan; Xu, Feng; Yong, Jun-Hai] Minist Educ China, Key Lab Informat Syst Secur, Beijing 100084, Peoples R China.
   [Wen, Quan; Xu, Feng; Yong, Jun-Hai] Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University
RP Wen, Q (corresponding author), Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.; Wen, Q (corresponding author), Minist Educ China, Key Lab Informat Syst Secur, Beijing 100084, Peoples R China.; Wen, Q (corresponding author), Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
EM wenq013@gmail.com; xufeng2003@gmail.com; yongjh@tsinghua.edu.cn
FU NSFC [61671268, 61672307]; National Key Technologies R&D Program of
   China [2015BAF23B03]
FX This work was supported by the NSFC (No. 61671268, 61672307) and the
   National Key Technologies R&D Program of China (No. 2015BAF23B03). Feng
   Xu is the corresponding author.
NR 67
TC 11
Z9 12
U1 2
U2 28
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2017
VL 23
IS 12
BP 2586
EP 2598
DI 10.1109/TVCG.2016.2641442
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FL6YZ
UT WOS:000414393700009
PM 28026772
DA 2025-03-07
ER

PT J
AU Xu, XM
   Zhong, LY
   Xie, MS
   Liu, XT
   Qin, J
   Wong, TT
AF Xu, Xuemiao
   Zhong, Linyuan
   Xie, Minshan
   Liu, Xueting
   Qin, Jing
   Wong, Tien-Tsin
TI ASCII Art Synthesis from Natural Photographs
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE ASCII art synthesis; non-classical receptive field modulation; texture
   suppression
ID CONTOUR-DETECTION; SURROUND SUPPRESSION; INHIBITION; MODEL
AB While ASCII art is a worldwide popular art form, automatic generating structure-based ASCII art from natural photographs remains challenging. The major challenge lies on extracting the perception-sensitive structure from the natural photographs so that a more concise ASCII art reproduction can be produced based on the structure. However, due to excessive amount of texture in natural photos, extracting perception-sensitive structure is not easy, especially when the structure may be weak and within the texture region. Besides, to fit different target text resolutions, the amount of the extracted structure should also be controllable. To tackle these challenges, we introduce a visual perception mechanism of non-classical receptive field modulation (non-CRF modulation) from physiological findings to this ASCII art application, and propose a new model of non-CRF modulation which can better separate the weak structure from the crowded texture, and also better control the scale of texture suppression. Thanks to our non-CRF model, more sensible ASCII art reproduction can be obtained. In addition, to produce more visually appealing ASCII arts, we propose a novel optimization scheme to obtain the optimal placement of proportional-font characters. We apply our method on a rich variety of images, and visually appealing ASCII art can be obtained in all cases.
C1 [Xu, Xuemiao] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Guangdong, Peoples R China.
   [Zhong, Linyuan; Xie, Minshan] South China Univ Technol, Dept Comp Sci & Engn, Guangzhou, Guangdong, Peoples R China.
   [Liu, Xueting; Wong, Tien-Tsin] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
   [Liu, Xueting; Wong, Tien-Tsin] Chinese Univ Hong Kong, Shenzhen Res Inst, Hong Kong, Hong Kong, Peoples R China.
   [Qin, Jing] Hong Kong Polytech Univ, Sch Nursing, Hong Kong, Hong Kong, Peoples R China.
C3 South China University of Technology; South China University of
   Technology; Chinese University of Hong Kong; CUHK Shenzhen Research
   Institute; Chinese University of Hong Kong; Hong Kong Polytechnic
   University
RP Xu, XM (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou, Guangdong, Peoples R China.
EM xuemx@scut.edu.cn; z.linyuan@mail.scut.edu.cn; csmsxie@mail.scut.edu.cn;
   xtliu@cse.cuhk.edu.hk; jqin@szu.edu.cn; ttwong@cse.cuhk.edu.hk
RI Liu, Xueting/AAG-9648-2019; Qin, Jing/J-9807-2016
OI Xie, Minshan/0000-0002-6288-1611; xu, xuemiao/0000-0002-8006-3663; Qin,
   Jing/0000-0002-7059-0929
FU NSFC [61272293, 61300137, 61472145, 61233012]; NSFG [S2013010014973];
   RGC Fund [CUHK14200915]; Science and Technology Planning Major Project
   of Guangdong Province [2015A070711001]; Open Project Program of
   Guangdong Key Lab of Popular High Performance Computers; Shenzhen Key
   Lab of Service Computing and Applications [SZU-GDPHPCL2015]
FX This work was supported by the funding from the NSFC (Grant No.
   61272293, 61300137, 61472145, 61233012) and NSFG (Grant No.
   S2013010014973), RGC Fund (Grant No. CUHK14200915), Science and
   Technology Planning Major Project of Guangdong Province (Grant No.
   2015A070711001), Open Project Program of Guangdong Key Lab of Popular
   High Performance Computers and Shenzhen Key Lab of Service Computing and
   Applications (Grant No.SZU-GDPHPCL2015).
NR 25
TC 6
Z9 6
U1 1
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2017
VL 23
IS 8
BP 1910
EP 1923
DI 10.1109/TVCG.2016.2569084
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EZ8LR
UT WOS:000404977900003
PM 27323365
DA 2025-03-07
ER

PT J
AU Hu, JX
   Hamidian, H
   Zhong, ZC
   Hua, J
AF Hu, Jiaxi
   Hamidian, Hajar
   Zhong, Zichun
   Hua, Jing
TI Visualizing Shape Deformations with Variation of Geometric Spectrum
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Geometry-based Technique; Spectral Analysis; Biomedical Visualization
ID REGISTRATION; OPTIMIZATION; SURFACES
AB This paper presents a novel approach based on spectral geometry to quantify and visualize non-isometric deformations of 3D surfaces by mapping two manifolds. The proposed method can determine multi-scale, non-isometric deformations through the variation of Laplace-Beltrami spectrum of two shapes. Given two triangle meshes, the spectra can be varied from one to another with a scale function defined on each vertex. The variation is expressed as a linear interpolation of eigenvalues of the two shapes. In each iteration step, a quadratic programming problem is constructed, based on our derived spectrum variation theorem and smoothness energy constraint, to compute the spectrum variation. The derivation of the scale function is the solution of such a problem. Therefore, the final scale function can be solved by integral of the derivation from each step, which, in turn, quantitatively describes non-isometric deformations between two shapes. To evaluate the method, we conduct extensive experiments on synthetic and real data. We employ real epilepsy patient imaging data to quantify the shape variation between the left and right hippocampi in epileptic brains. In addition, we use longitudinal Alzheimer data to compare the shape deformation of diseased and healthy hippocampus. In order to show the accuracy and effectiveness of the proposed method, we also compare it with spatial registration-based methods, e. g., non-rigid Iterative Closest Point (ICP) and voxel-based method. These experiments demonstrate the advantages of our method.
C1 [Hu, Jiaxi; Hamidian, Hajar; Zhong, Zichun; Hua, Jing] Wayne State Univ, Detroit, MI 48202 USA.
C3 Wayne State University
RP Hua, J (corresponding author), Wayne State Univ, Detroit, MI 48202 USA.
EM jiaxihu@wayne.edu; nasim.hamidian@wayne.edu; zichunzhong@wayne.edu;
   jinghua@wayne.edu
FU  [NSF CNS-1647200];  [IIS-0915933];  [IIS-0937586];  [ZJNSF LZ16F020002]
FX The authors wish to thank Dr. Hamid Soltanian-Zadeh from Department of
   Radiology at Henry Ford Health System for providing the data for some of
   the experiments and Dr. Farshad Fotouhi for his valuable input. The
   research is supported in part by grants NSF CNS-1647200, IIS-0915933,
   IIS-0937586 and ZJNSF LZ16F020002.
NR 44
TC 5
Z9 7
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 721
EP 730
DI 10.1109/TVCG.2016.2598790
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600075
PM 27875186
DA 2025-03-07
ER

PT J
AU Xu, PP
   Mei, HH
   Ren, L
   Chen, W
AF Xu, Panpan
   Mei, Honghui
   Ren, Liu
   Chen, Wei
TI ViDX: Visual Diagnostics of Assembly Line Performance in Smart Factories
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Temporal Data; Marley's Graph; Visual Analytics; Manufacturing; Smart
   Factory; Connected Industry; Industry 4.0
ID INDUSTRIE 4.0; VISUALIZATION
AB Visual analytics plays a key role in the era of connected industry (or industry 4.0, industrial internet) as modern machines and assembly lines generate large amounts of data and effective visual exploration techniques are needed for troubleshooting, process optimization, and decision making. However, developing effective visual analytics solutions for this application domain is a challenging task due to the sheer volume and the complexity of the data collected in the manufacturing processes. We report the design and implementation of a comprehensive visual analytics system, VbX It supports both real-time tracking of assembly line performance and historical data exploration to identify inefficiencies, locate anomalies, and form hypotheses about their causes and effects. The system is designed based on a set of requirements gathered through discussions with the managers and operators from manufacturing sites. It features interlinked views displaying data at different levels of detail. In particular, we apply and extend the Mareys graph by introducing a time-aware outlier-preserving visual aggregation technique to support effective troubleshooting in manufacturing processes. We also introduce two novel interaction techniques, namely the quantiles brush and samples brush, for the users to interactively steer the outlier detection algorithms. We evaluate the system with example use cases and an in-depth user interview, both conducted together with the managers and operators from manufacturing plants. The result demonstrates its effectiveness and reports a successful pilot application of visual analytics for manufacturing in smart factories.
C1 [Xu, Panpan; Ren, Liu] Bosch Res North America, Palo Alto, CA USA.
   [Mei, Honghui; Chen, Wei] Zhejiang Univ, Hangzhou, Peoples R China.
C3 Zhejiang University
RP Xu, PP (corresponding author), Bosch Res North America, Palo Alto, CA USA.
EM panpan.xu@us.bosch.com; meihonghui@cad.zju.edu.cn; liu.ren@us.bosch.com;
   chenwei@cad.zju.edu.cn
RI Chen, Wei/AAR-9817-2020
FU 973 Program of China [2015CB352503]; NSFC [61232012, 61422211]
FX We would like to thank Kenneth Podd and his colleagues from Bosch Plant
   in Anderson in the U.S. for the helpful discussions and user study
   support. In addition, we would like to thank Race Oswald and Alex
   Makarov for preparing the 3D models, editing the video and proofreading
   the paper. Finally, we thank the VAST reviewers for their valuable
   feedback. The work was performed while Honghui Mei was an intern at
   Bosch Research North America. Wei Chen is supported by 973 Program of
   China (2015CB352503), NSFC (61232012, 61422211).
NR 36
TC 89
Z9 103
U1 0
U2 36
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 291
EP 300
DI 10.1109/TVCG.2016.2598664
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600032
PM 27875145
DA 2025-03-07
ER

PT J
AU Yan, DM
   Wonka, P
AF Yan, Dong-Ming
   Wonka, Peter
TI Non-Obtuse Remeshing with Centroidal Voronoi Tessellation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Centroidal voronoi tessellation; non-obtuse remeshing; restricted
   voronoi diagram
ID TRIANGULAR MESHES; COMPUTATION; GEOMETRY
AB We present a novel remeshing algorithm that avoids triangles with small (acute) angles and those with large (obtuse) angles. Our solution is based on an extension of Centroidal Voronoi Tesselation (CVT). We augment the original CVT formulation with a penalty term that penalizes short Voronoi edges, while the CVT term helps to avoid small angles. Our results show significant improvements in remeshing quality over the state of the art.
C1 [Yan, Dong-Ming] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.
   [Yan, Dong-Ming; Wonka, Peter] King Abdullah Univ Sci & Technol, Thuwal 239556900, Saudi Arabia.
   [Wonka, Peter] Arizona State Univ, Tempe, AZ 85287 USA.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; King Abdullah
   University of Science & Technology; Arizona State University; Arizona
   State University-Tempe
RP Yan, DM (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100190, Peoples R China.; Yan, DM (corresponding author), King Abdullah Univ Sci & Technol, Thuwal 239556900, Saudi Arabia.
EM yandongming@gmail.com; pwonka@gmail.com
OI Yan, Dong-Ming/0000-0003-2209-2404
FU KAUST Visual Computing Center; National Natural Science Foundation of
   China [61372168, 61372190, 61571439, 61572502]; China National 863
   Program [2015AA016402]; Scientific Research Foundation for the Returned
   Overseas Chinese Scholars of State Education Ministry of China; State
   Key Laboratory of Virtual Reality Technology and Systems, Beihang
   University [BUAA-VR-15KF-06]
FX We would like to thank Zhonggui Chen, Fernando de Goes and Sebastien
   Valette for providing the data and executables, Bruno Levy for sharing
   the code for feature-sensitive CVT, and Lubin Fan for the rendering.
   This work was partially supported by the KAUST Visual Computing Center,
   the National Natural Science Foundation of China (Nos. 61372168,
   61372190, 61571439, and 61572502), the China National 863 Program (No.
   2015AA016402), the Scientific Research Foundation for the Returned
   Overseas Chinese Scholars of State Education Ministry of China, the open
   funding project of the State Key Laboratory of Virtual Reality
   Technology and Systems, Beihang University (No. BUAA-VR-15KF-06).
NR 53
TC 32
Z9 34
U1 0
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2016
VL 22
IS 9
BP 2136
EP 2144
DI 10.1109/TVCG.2015.2505279
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DU4FD
UT WOS:000382166900006
PM 26661470
DA 2025-03-07
ER

PT J
AU Kwon, OH
   Muelder, C
   Lee, K
   Ma, KL
AF Kwon, Oh-Hyun
   Muelder, Chris
   Lee, Kyungwon
   Ma, Kwan-Liu
TI A Study of Layout, Rendering, and Interaction Methods for Immersive
   Graph Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Graph visualization; virtual reality; immersive environments;
   head-mounted display
ID STEREO; MOTION
AB Information visualization has traditionally limited itself to 2D representations, primarily due to the prevalence of 2D displays and report formats. However, there has been a recent surge in popularity of consumer grade 3D displays and immersive head-mounted displays (HMDs). The ubiquity of such displays enables the possibility of immersive, stereoscopic visualization environments. While techniques that utilize such immersive environments have been explored extensively for spatial and scientific visualizations, contrastingly very little has been explored for information visualization. In this paper, we present our considerations of layout, rendering, and interaction methods for visualizing graphs in an immersive environment. We conducted a user study to evaluate our techniques compared to traditional 2D graph visualization. The results show that participants answered significantly faster with a fewer number of interactions using our techniques, especially for more difficult tasks. While the overall correctness rates are not significantly different, we found that participants gave significantly more correct answers using our techniques for larger graphs.
C1 [Kwon, Oh-Hyun; Muelder, Chris; Ma, Kwan-Liu] Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.
   [Kwon, Oh-Hyun] Ajou Univ, Suwon 441749, South Korea.
   [Lee, Kyungwon] Ajou Univ, Dept Digital Media, Suwon 441749, South Korea.
C3 University of California System; University of California Davis; Ajou
   University; Ajou University
RP Kwon, OH (corresponding author), Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA.; Kwon, OH (corresponding author), Ajou Univ, Suwon 441749, South Korea.
EM kw@ucdavis.edu; cwmuelder@ucdavis.edu; kwlee@ajou.ac.kr;
   ma@cs.ucdavis.edu
RI Kwon, Oh-Hyun/V-8085-2019
OI Lee, Kyungwon/0000-0003-3756-3985
FU National Research Foundation of Korea via BK21 PLUS; U.S. National
   Science Foundation [NSF DRL-1323214, NSF DE-FC02-12ER26072]; UC Davis's
   RISE program
FX This research was supported in part by the National Research Foundation
   of Korea via BK21 PLUS, by the U.S. National Science Foundation via NSF
   DRL-1323214 and NSF DE-FC02-12ER26072, and by UC Davis's RISE program.
NR 58
TC 123
Z9 144
U1 0
U2 41
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2016
VL 22
IS 7
BP 1802
EP 1815
DI 10.1109/TVCG.2016.2520921
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DO0NI
UT WOS:000377475200003
PM 26812726
OA Bronze
DA 2025-03-07
ER

PT J
AU Friston, S
   Karlström, P
   Steed, A
AF Friston, Sebastian
   Karlstroem, Per
   Steed, Anthony
TI The Effects of Low Latency on Pointing and Steering Tasks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Latency; indirect input; HCI; Fitts's law; human factors
ID FITTS LAW; PERFORMANCE; DELAY
AB Latency is detrimental to interactive systems, especially pseudo-physical systems that emulate real-world behaviour. It prevents users from making quick corrections to their movement, and causes their experience to deviate from their expectations. Latency is a result of the processing and transport delays inherent in current computer systems. As such, while a number of studies have hypothesized that any latency will have a degrading effect, few have been able to test this for latencies less than similar to 50 ms. In this study we investigate the effects of latency on pointing and steering tasks. We design an apparatus with a latency lower than typical interactive systems, using it to perform interaction tasks based on Fitts's law and the Steering law. We find evidence that latency begins to affect performance at similar to 16 ms, and that the effect is non-linear. Further, we find latency does not affect the various components of an aiming motion equally. We propose a three stage characterisation of pointing movements with each stage affected independently by latency. We suggest that understanding how users execute movement is essential for studying latency at low levels, as high level metrics such as total movement time may be misleading.
C1 [Friston, Sebastian; Steed, Anthony] UCL, Dept Comp Sci, London, England.
   [Karlstroem, Per] Maxeler Technol Ltd, London, England.
C3 University of London; University College London
RP Friston, S; Steed, A (corresponding author), UCL, Dept Comp Sci, London, England.; Karlström, P (corresponding author), Maxeler Technol Ltd, London, England.
EM sebastian.friston.12@ucl.ac.uk; pkarlstrom@maxeler.com;
   a.steed@cs.ucl.ac.uk
OI Steed, Anthony/0000-0001-9034-3020; Friston,
   Sebastian/0000-0002-0061-8519
NR 34
TC 23
Z9 28
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2016
VL 22
IS 5
BP 1605
EP 1615
DI 10.1109/TVCG.2015.2446467
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA DH7ZH
UT WOS:000373012300010
PM 27045915
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Barbosa, A
   Paulovich, FV
   Paiva, A
   Goldenstein, S
   Petronetto, F
   Nonato, LG
AF Barbosa, A.
   Paulovich, F. V.
   Paiva, A.
   Goldenstein, S.
   Petronetto, F.
   Nonato, L. G.
TI Visualizing and Interacting with Kernelized Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Multidimensional projection; visualization; kernel methods
ID DIMENSIONALITY REDUCTION; PROJECTION; LAYOUT
AB Kernel-based methods have experienced a substantial progress in the last years, tuning out an essential mechanism for data classification, clustering and pattern recognition. The effectiveness of kernel-based techniques, though, depends largely on the capability of the underlying kernel to properly embed data in the feature space associated to the kernel. However, visualizing how a kernel embeds the data in a feature space is not so straightforward, as the embedding map and the feature space are implicitly defined by the kernel. In this work, we present a novel technique to visualize the action of a kernel, that is, how the kernel embeds data into a high-dimensional feature space. The proposed methodology relies on a solid mathematical formulation to map kernelized data onto a visual space. Our approach is faster and more accurate than most existing methods while still allowing interactive manipulation of the projection layout, a game-changing trait that other kernel-based projection techniques do not have.
C1 [Barbosa, A.; Paulovich, F. V.; Paiva, A.; Nonato, L. G.] Univ Sao Paulo, Inst Comp & Matemat Computac, Sao Carlos, SP, Brazil.
   [Goldenstein, S.] Univ Estadual Campinas, Inst Comp, Campinas, SP, Brazil.
   [Petronetto, F.] Univ Fed Espirito Santo, Dept Matemat, Vitoria, ES, Spain.
C3 Universidade de Sao Paulo; Universidade Estadual de Campinas
RP Barbosa, A; Paulovich, FV; Paiva, A; Nonato, LG (corresponding author), Univ Sao Paulo, Inst Comp & Matemat Computac, Sao Carlos, SP, Brazil.; Goldenstein, S (corresponding author), Univ Estadual Campinas, Inst Comp, Campinas, SP, Brazil.; Petronetto, F (corresponding author), Univ Fed Espirito Santo, Dept Matemat, Vitoria, ES, Spain.
EM barbosa@icmc.usp.br; paulovic@icmc.usp.br; apneto@icmc.usp.br;
   siome@ic.unicamp.br; fabiano.carmo@ufes.br; gnonato@icmc.usp.br
RI Nonato, Luis/D-5782-2011; Petronetto, Fabiano/ABC-6521-2020; Petronetto,
   Fabiano/I-1742-2016; Paiva, Afonso/E-2593-2011; Paulovich,
   Fernando/G-1329-2010
OI Petronetto, Fabiano/0000-0003-1940-5406; Paiva,
   Afonso/0000-0001-8229-3385; Paulovich, Fernando/0000-0002-2316-760X
FU FAPESP [2011/22749-8, 2013/19760-5, 2014/09546-9]; CNPq [302643/2013-3];
   CAPES; Samsung; Fundacao de Amparo a Pesquisa do Estado de Sao Paulo
   (FAPESP) [14/09546-9] Funding Source: FAPESP
FX The authors acknowledge the financial support from FAPESP
   (#2011/22749-8), (#2013/19760-5), (#2014/09546-9), CNPq
   (#302643/2013-3), CAPES, and a Samsung.
NR 43
TC 13
Z9 13
U1 0
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2016
VL 22
IS 3
BP 1314
EP 1325
DI 10.1109/TVCG.2015.2464797
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE2DC
UT WOS:000370435700012
PM 26829242
DA 2025-03-07
ER

PT J
AU Palacios, J
   Yeh, H
   Wang, WP
   Zhang, Y
   Laramee, RS
   Sharma, R
   Schultz, T
   Zhang, E
AF Palacios, Jonathan
   Yeh, Harry
   Wang, Wenping
   Zhang, Yue
   Laramee, Robert S.
   Sharma, Ritesh
   Schultz, Thomas
   Zhang, Eugene
TI Feature Surfaces in Symmetric Tensor Fields Based on Eigenvalue Manifold
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Tensor field visualization; feature-based visualization; tensor field
   topology; traceless tensors; A-patches; scalar fields
ID VISUALIZATION; GLYPHS; LINES
AB Three-dimensional symmetric tensor fields have a wide range of applications in solid and fluid mechanics. Recent advances in the (topological) analysis of 3D symmetric tensor fields focus on degenerate tensors which form curves. In this paper, we introduce a number of feature surfaces, such as neutral surfaces and traceless surfaces, into tensor field analysis, based on the notion of eigenvalue manifold. Neutral surfaces are the boundary between linear tensors and planar tensors, and the traceless surfaces are the boundary between tensors of positive traces and those of negative traces. Degenerate curves, neutral surfaces, and traceless surfaces together form a partition of the eigenvalue manifold, which provides a more complete tensor field analysis than degenerate curves alone. We also extract and visualize the isosurfaces of tensor modes, tensor isotropy, and tensor magnitude, which we have found useful for domain applications in fluid and solid mechanics. Extracting neutral and traceless surfaces using the Marching Tetrahedra method can cause the loss of geometric and topological details, which can lead to false physical interpretation. To robustly extract neutral surfaces and traceless surfaces, we develop a polynomial description of them which enables us to borrow techniques from algebraic surface extraction, a topic well-researched by the computer-aided design (CAD) community as well as the algebraic geometry community. In addition, we adapt the surface extraction technique, called A-patches, to improve the speed of finding degenerate curves. Finally, we apply our analysis to data from solid and fluid mechanics as well as scalar field analysis.
C1 [Palacios, Jonathan; Sharma, Ritesh] Oregon State Univ, Sch Elect Engn & Comp Sci, Kelley Engn Ctr 1148, Corvallis, OR 97331 USA.
   [Yeh, Harry] Oregon State Univ, Sch Civil & Construct Engn, 208 Owen Hall, Corvallis, OR 97331 USA.
   [Wang, Wenping] Univ Hong Kong, Dept Comp Sci, Pokfulam Rd, Hong Hom, Hong Kong, Peoples R China.
   [Zhang, Yue] Oregon State Univ, Sch Elect Engn & Comp Sci, Kelley Engn Ctr 3117, Corvallis, OR 97331 USA.
   [Laramee, Robert S.] Swansea Univ, Dept Comp Sci, Swansea SA2 8PP, W Glam, Wales.
   [Schultz, Thomas] Univ Bonn, Inst Comp Sci, Friedrich Ebert Allee 144, D-53113 Bonn, Germany.
   [Zhang, Eugene] Oregon State Univ, Sch Elect Engn & Comp Sci, Kelley Engn Ctr 2111, Corvallis, OR 97331 USA.
C3 Oregon State University; Oregon State University; University of Hong
   Kong; Oregon State University; Swansea University; University of Bonn;
   Oregon State University
RP Palacios, J; Sharma, R (corresponding author), Oregon State Univ, Sch Elect Engn & Comp Sci, Kelley Engn Ctr 1148, Corvallis, OR 97331 USA.; Yeh, H (corresponding author), Oregon State Univ, Sch Civil & Construct Engn, 208 Owen Hall, Corvallis, OR 97331 USA.; Wang, WP (corresponding author), Univ Hong Kong, Dept Comp Sci, Pokfulam Rd, Hong Hom, Hong Kong, Peoples R China.; Zhang, Y (corresponding author), Oregon State Univ, Sch Elect Engn & Comp Sci, Kelley Engn Ctr 3117, Corvallis, OR 97331 USA.; Laramee, RS (corresponding author), Swansea Univ, Dept Comp Sci, Swansea SA2 8PP, W Glam, Wales.; Schultz, T (corresponding author), Univ Bonn, Inst Comp Sci, Friedrich Ebert Allee 144, D-53113 Bonn, Germany.; Zhang, E (corresponding author), Oregon State Univ, Sch Elect Engn & Comp Sci, Kelley Engn Ctr 2111, Corvallis, OR 97331 USA.
EM palacijo@eecs.oregonstate.edu; harry@engr.oregonstate.edu;
   wenping@cs.hku.hk; zhangyue@onid.oregonstate.edu;
   R.S.Laramee@swansea.ac.uk; sharmrit@eecs.oregonstate.edu;
   schultz@cs.uni-bonn.de; zhange@eecs.oregonstate.edu
RI Schultz, Thomas/IWV-2288-2023
OI Laramee, Robert S/0000-0002-3874-6145; Sharma,
   Ritesh/0000-0003-1160-3918
FU US National Science Foundation (NSF) [IIS-0917308]
FX Eugene Zhang is partially sponsored by the US National Science
   Foundation (NSF) grant IIS-0917308. In addition, he wishes to
   acknowledge the support received while a guest professor at the
   Max-Planck-Institute of Informatics where some of the initial research
   ideas in this paper were conceived.
NR 35
TC 18
Z9 18
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2016
VL 22
IS 3
BP 1248
EP 1260
DI 10.1109/TVCG.2015.2484343
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE2DC
UT WOS:000370435700007
PM 26441450
OA Bronze
DA 2025-03-07
ER

PT J
AU Zhang, R
   Wang, SW
   Chen, XJ
   Ding, C
   Jiang, L
   Zhou, J
   Liu, LG
AF Zhang, Ran
   Wang, Shiwei
   Chen, Xuejin
   Ding, Chao
   Jiang, Luo
   Zhou, Jie
   Liu, Ligang
TI Designing Planar Deployable Objects via Scissor Structures
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computer graphics; digital fabrication
ID MECHANISMS
AB Scissor structure is used to generate deployable objects for space-saving in a variety of applications, from architecture to aerospace science. While deployment from a small, regular shape to a larger one is easy to design, we focus on a more challenging task: designing a planar scissor structure that deploys from a given source shape into a specific target shape. We propose a two-step constructive method to generate a scissor structure from a high-dimensional parameter space. Topology construction of the scissor structure is first performed to approximate the two given shapes, as well as to guarantee the deployment. Then the geometry of the scissor structure is optimized in order to minimize the connection deflections and maximize the shape approximation. With the optimized parameters, the deployment can be simulated by controlling an anchor scissor unit. Physical deployable objects are fabricated according to the designed scissor structures by using 3D printing or manual assembly. We show a number of results for different shapes to demonstrate that even with fabrication errors, our designed structures can deform fluently between the source and target shapes.
C1 [Zhang, Ran; Chen, Xuejin; Zhou, Jie] Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230026, Anhui, Peoples R China.
   [Wang, Shiwei; Ding, Chao; Jiang, Luo; Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Zhang, R (corresponding author), Univ Sci & Technol China, CAS Key Lab Technol Geospatial Informat Proc & Ap, Hefei 230026, Anhui, Peoples R China.
EM cuminflea@gmail.com; shiweiw7@mail.ustc.edu.cn; xjchen99@ustc.edu.cn;
   dc322@mail.ustc.edu.cn; jluo@mail.ustc.edu.cn; zjsist@gmail.com;
   ligang.liu@gmail.com
RI Liu, Ligang/IZQ-5817-2023; zhang, ran/KZU-6484-2024
OI Jiang, Luo/0000-0002-7578-8723
NR 33
TC 24
Z9 28
U1 4
U2 61
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2016
VL 22
IS 2
BP 1051
EP 1062
DI 10.1109/TVCG.2015.2430322
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DA3TF
UT WOS:000367721100003
PM 26731451
DA 2025-03-07
ER

PT J
AU Hinrichs, U
   Forlini, S
   Moynihan, B
AF Hinrichs, Uta
   Forlini, Stefania
   Moynihan, Bridget
TI Speculative Practices: Utilizing InfoVis to Explore Untapped Literary
   Collections
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Digital Humanities; Interlinked Visualization; Literary Studies;
   Cultural Collections; Science Fiction
ID INFORMATION VISUALIZATION; DESIGN; TREE
AB In this paper we exemplify how information visualization supports speculative thinking, hypotheses testing, and preliminary interpretation processes as part of literary research. While InfoVis has become a buzz topic in the digital humanities, skepticism remains about how effectively it integrates into and expands on traditional humanities research approaches. From an InfoVis perspective, we lack case studies that show the specific design challenges that make literary studies and humanities research at large a unique application area for information visualization. We examine these questions through our case study of the Speculative W@nderverse, a visualization tool that was designed to enable the analysis and exploration of an untapped literary collection consisting of thousands of science fiction short stories. We present the results of two empirical studies that involved general-interest readers and literary scholars who used the evolving visualization prototype as part of their research for over a year. Our findings suggest a design space for visualizing literary collections that is defined by (1) their academic and public relevance, (2) the tension between qualitative vs. quantitative methods of interpretation, (3) result- vs. process-driven approaches to InfoVis, and (4) the unique material and visual qualities of cultural collections. Through the Speculative W@nderverse we demonstrate how visualization can bridge these sometimes contradictory perspectives by cultivating curiosity and providing entry points into literary collections while, at the same time, supporting multiple aspects of humanities research processes.
C1 [Hinrichs, Uta] Univ St Andrews, SACHI Grp, St Andrews KY16 9AJ, Fife, Scotland.
   [Forlini, Stefania; Moynihan, Bridget] Univ Calgary, Dept English, Calgary, AB T2N 1N4, Canada.
C3 University of St Andrews; University of Calgary
RP Hinrichs, U (corresponding author), Univ St Andrews, SACHI Grp, St Andrews KY16 9AJ, Fife, Scotland.
EM uh3@st-andrews.ac.uk; sforlini@ucalgary.ca; bcmoynih@ucalgary.ca
RI Hinrichs, Uta/X-1644-2019
OI Moynihan, Bridget/0000-0003-3032-8069
FU Faculty of Arts at the University of Calgary; SSHRC
FX We would like to thank all graduate students who helped annotate the
   Gibson Anthologies. We also thank the Faculty of Arts at the University
   of Calgary for supporting our open house events and all participants for
   providing invaluable feedback to our research. Furthermore, we thank
   Jason Reid and John Brosz for their technical support with the Gibson
   database, and, last but not least, SSHRC for funding this research.
NR 53
TC 31
Z9 34
U1 2
U2 27
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 429
EP 438
DI 10.1109/TVCG.2015.2467452
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Arts &amp; Humanities Citation Index (A&amp;HCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400048
PM 26529713
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kwon, BC
   Kim, SH
   Lee, S
   Choo, J
   Huh, J
   Yi, JS
AF Kwon, Bum Chul
   Kim, Sung-Hee
   Lee, Sukwon
   Choo, Jaegul
   Huh, Jina
   Yi, Ji Soo
TI VisOHC: Designing Visual Analytics for Online Health Communities
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Online health communities visual analytics; conversation analysis;
   thread visualization; healthcare; design study
ID VISUALIZATION; INFORMATION; SUPPORT; TEXT; CARE
AB Through online health communities (OHCs), patients and caregivers exchange their illness experiences and strategies for overcoming the illness, and provide emotional support. To facilitate healthy and lively conversations in these communities, their members should be continuously monitored and nurtured by OHC administrators. The main challenge of OHO administrators" tasks lies in understanding the diverse dimensions of conversation threads that lead to productive discussions in their communities. In this paper, we present a design study in which three domain expert groups participated, an OHC researcher and two OHC administrators of online health communities, which was conducted to find with a visual analytic solution. Through our design study, we characterized the domain goals of OHC administrators and derived tasks to achieve these goals. As a result of this study, we propose a system called VisOHC, which visualizes individual OHC conversation threads as collapsed boxes a visual metaphor of conversation threads. In addition, we augmented the posters' reply authorship network with marks and/or beams to show conversation dynamics within threads. We also developed unique measures tailored to the characteristics of OHCs which can be encoded for thread visualizations at the users requests. Our observation of the two administrators while using VisOHC showed that it supports their tasks and reveals interesting insights into online health communities. Finally, we share our methodological lessons on probing visual designs together with domain experts by allowing them to freely encode measurements into visual variables.
C1 [Kwon, Bum Chul] Univ Konstanz, Constance, Germany.
   [Kim, Sung-Hee] Univ British Columbia, Vancouver, BC V5Z 1M9, Canada.
   [Choo, Jaegul] Korea Univ, Seoul, South Korea.
   [Huh, Jina] Michigan State Univ, E Lansing, MI 48824 USA.
   [Lee, Sukwon; Yi, Ji Soo] Purdue Univ, W Lafayette, IN 47907 USA.
C3 University of Konstanz; University of British Columbia; Korea
   University; Michigan State University; Purdue University System; Purdue
   University
RP Choo, J (corresponding author), Korea Univ, Seoul, South Korea.
EM bumchul.kwon@uni-konstanz.de; kim731@cs.ubc.ca; sukwon@purdue.edu;
   jchoo@korea.ac.kr; jinahuh@msu.edu; yij@purdue.edu
RI Huh-Yoo, Jina/AAN-7442-2021; Choo, Jaegul/ABF-8315-2020
OI Choo, Jaegul/0000-0003-1071-4835; Huh-Yoo, Jina/0000-0001-5811-9256
FU National Library of Medicine of the National Institutes of Health
   [K01LM011980]
FX We thank John Crowley, Marie Connelly, Aaron Beals, and Jessica
   Ludvigsen for participation and invaluable feedback. Research reported
   in this paper was in part supported by National Library of Medicine of
   the National Institutes of Health under award number K01LM011980.
NR 44
TC 28
Z9 37
U1 1
U2 27
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 71
EP 80
DI 10.1109/TVCG.2015.2467555
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400012
PM 26529688
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Yoghourdjian, V
   Dwyer, T
   Gange, G
   Kieffer, S
   Klein, K
   Marriott, K
AF Yoghourdjian, Vahan
   Dwyer, Tim
   Gange, Graeme
   Kieffer, Steve
   Klein, Karsten
   Marriott, Kim
TI High-Quality Ultra-Compact Grid Layout of Grouped Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Network visualization; graph drawing; power graph; optimization;
   large-neighborhood search
ID GRAPH
AB Prior research into network layout has focused on fast heuristic techniques for layout of large networks, or complex multi-stage pipelines for higher quality layout of small graphs. Improvements to these pipeline techniques, especially for orthogonal-style layout, are difficult and practical results have been slight in recent years. Yet, as discussed in this paper, there remain significant issues in the quality of the layouts produced by these techniques, even for quite small networks. This is especially true when layout with additional grouping constraints is required. The first contribution of this paper is to investigate an ultra-compact, grid-like network layout aesthetic that is motivated by the grid arrangements that are used almost universally by designers in typographical layout. Since the time when these heuristic and pipeline-based graph-layout methods were conceived, generic technologies (MIP, CP and SAT) for solving combinatorial and mixed-integer optimization problems have improved massively. The second contribution of this paper is to reassess whether these techniques can be used for high-quality layout of small graphs. While they are fast enough for graphs of up to 50 nodes we found these methods do not scale up. Our third contribution is a large-neighborhood search meta-heuristic approach that is scalable to larger networks.
C1 [Yoghourdjian, Vahan; Dwyer, Tim; Kieffer, Steve; Klein, Karsten; Marriott, Kim] Monash Univ, Clayton, Vic 3800, Australia.
   [Gange, Graeme] Univ Melbourne, Melbourne, Vic 3010, Australia.
C3 Monash University; University of Melbourne
RP Yoghourdjian, V (corresponding author), Monash Univ, Clayton, Vic 3800, Australia.
EM Vahan.Yoghourdjian@monash.edu; Tim.Dwyer@monash.edu;
   ggange@csse.unimelb.edu.au; Steve.Kieffer@monash.edu;
   Karsten.Klein@monash.edu; Kim.Marriott@monash.edu
OI Gange, Graeme/0000-0002-1354-431X; Dwyer, Tim/0000-0002-9076-9571
FU Australian Research Council [DP140100077]
FX We acknowledge the support of the Australian Research Council through
   Discovery Project DP140100077
NR 42
TC 25
Z9 26
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 339
EP 348
DI 10.1109/TVCG.2015.2467251
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400039
PM 26390477
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Dyken, L
   Usher, W
   Kumar, S
AF Dyken, Landon
   Usher, Will
   Kumar, Sidharth
TI Interactive Isosurface Visualization in Memory Constrained Environments
   Using Deep Learning and Speculative Raycasting
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Isosurfaces; Graphics processing units; Rendering (computer graphics);
   Browsers; Servers; Pipelines; Costs; Deep learning; GPU raycasting;
   isosurface rendering; large-scale data techniques
ID OF-THE-ART; VOLUME; EFFICIENT; COMPRESSION; ALGORITHM
AB New web technologies have enabled the deployment of powerful GPU-based computational pipelines that run entirely in the web browser, opening a new frontier for accessible scientific visualization applications. However, these new capabilities do not address the memory constraints of lightweight end-user devices encountered when attempting to visualize the massive data sets produced by today's simulations and data acquisition systems. We propose a novel implicit isosurface rendering algorithm for interactive visualization of massive volumes within a small memory footprint. We achieve this by progressively traversing a wavefront of rays through the volume and decompressing blocks of the data on-demand to perform implicit ray-isosurface intersections, displaying intermediate results each pass. We improve the quality of these intermediate results using a pretrained deep neural network that reconstructs the output of early passes, allowing for interactivity with better approximates of the final image. To accelerate rendering and increase GPU utilization, we introduce speculative ray-block intersection into our algorithm, where additional blocks are traversed and intersected speculatively along rays to exploit additional parallelism in the workload. Our algorithm is able to trade-off image quality to greatly decrease rendering time for interactive rendering even on lightweight devices. Our entire pipeline is run in parallel on the GPU to leverage the parallel computing power that is available even on lightweight end-user devices. We compare our algorithm to the state of the art in low-overhead isosurface extraction and demonstrate that it achieves 1.7x-5.7x reductions in memory overhead and up to 8.4x reductions in data decompressed.
C1 [Dyken, Landon; Kumar, Sidharth] Univ Illinois, Chicago, IL 61820 USA.
   [Usher, Will] Luminary Cloud, San Mateo, CA 94401 USA.
C3 University of Illinois System; University of Illinois Chicago;
   University of Illinois Chicago Hospital
RP Dyken, L (corresponding author), Univ Illinois, Chicago, IL 61820 USA.
EM ldyke@uic.edu; will@willusher.io; sidharth@uic.edu
OI Dyken, Landon/0009-0008-9546-6685
FU NSF RII Track-4 [2132013]; NSF PPoSS planning [2217036]; NSF PPoSS large
   [2316157]; NSF collaborative research [2221811]
FX This work was supported in part by NSF RII Track-4 under Grant
   2132013,in part by NSF PPoSS planning under Grant 2217036, in part by
   NSF PPoSS large under Grant 2316157, and in part by NSF collaborative
   research under Grant 2221811.
NR 63
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2025
VL 31
IS 2
BP 1582
EP 1597
DI 10.1109/TVCG.2024.3420225
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA R6W2Y
UT WOS:001392823200016
PM 38941206
OA hybrid
DA 2025-03-07
ER

PT J
AU Yan, X
   Yuan, ZH
   Du, YH
   Liao, YH
   Guo, Y
   Cui, SG
   Li, Z
AF Yan, Xu
   Yuan, Zhihao
   Du, Yuhao
   Liao, Yinghong
   Guo, Yao
   Cui, Shuguang
   Li, Zhen
TI Comprehensive Visual Question Answering on Point Clouds through
   Compositional Scene Manipulation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Image color analysis; Visualization; Task
   analysis; Engines; Solid modeling; Semantics; Deep learning; vision and
   language; 3D visual question answering; point cloud processing
AB Visual Question Answering on 3D Point Cloud (VQA-3D) is an emerging yet challenging field that aims at answering various types of textual questions given an entire point cloud scene. To tackle this problem, we propose the CLEVR3D, a large-scale VQA-3D dataset consisting of 171K questions from 8,771 3D scenes. Specifically, we develop a question engine leveraging 3D scene graph structures to generate diverse reasoning questions, covering the questions of objects' attributes (i.e., size, color, and material) and their spatial relationships. Through such a manner, we initially generated 44K questions from 1,333 real-world scenes. Moreover, a more challenging setup is proposed to remove the confounding bias and adjust the context from a common-sense layout. Such a setup requires the network to achieve comprehensive visual understanding when the 3D scene is different from the general co-occurrence context (e.g., chairs always exist with tables). To this end, we further introduce the compositional scene manipulation strategy and generate 127K questions from 7,438 augmented 3D scenes, which can improve VQA-3D models for real-world comprehension. Built upon the proposed dataset, we baseline several VQA-3D models, where experimental results verify that the CLEVR3D can significantly boost other 3D scene understanding tasks.
C1 [Yan, Xu; Yuan, Zhihao; Du, Yuhao; Liao, Yinghong; Cui, Shuguang; Li, Zhen] Chinese Univ Hong Kong, Future Network Intelligence Inst, Sch Sci & Engn, Shenzhen 518172, Peoples R China.
   [Guo, Yao] Shanghai Jiao Tong Univ, Inst Med Robot, Shanghai 200240, Peoples R China.
C3 The Chinese University of Hong Kong, Shenzhen; Shanghai Jiao Tong
   University
RP Li, Z (corresponding author), Chinese Univ Hong Kong, Future Network Intelligence Inst, Sch Sci & Engn, Shenzhen 518172, Peoples R China.
EM xuyan1@link.cuhk.edu.cn; zhihaoyuan@link.cuhk.edu.cn;
   yuhaodu@link.cuhk.edu.cn; yinghongliao@link.cuhk.edu.cn;
   yao.guo@sjtu.edu.cn; shuguangcui@cuhk.edu.cn; lizhen@cuhk.edu.cn
RI Wang, Sheng/O-3465-2015; Zhang, Qian/B-9058-2009; Cui,
   Shuguang/D-4677-2014
OI Wang, Sheng/0000-0003-4210-1670; Yuan, Zhihao/0000-0003-3100-1252;
   Zhang, Qian/0000-0001-9205-1881; Cui, Shuguang/0000-0003-2608-775X;
   Liao, Yinghong/0000-0001-9128-1167
FU Shenzhen General Program [JCYJ20220530143600001]; Basic Research Project
   [HZQB-KCZYZ-2021067]; Shenzhen-Hong Kong Joint [SGDX20211123112401002];
   Shenzhen Outstanding Talents Training Fund; Guangdong Research Project
   [2017ZT07X152, 2019CX01X104]; Guangdong Provincial Key Laboratory of
   Future Networks of Intelligence [2022B1212010001]; Chinese University of
   Hong Kong, Shenzhen; NSFC [61931024, 81922046]; NSFC-Youth [62203296];
   Tencent Open Fund; ITSO
FX This work was supported in part by Shenzhen General Program under Grant
   JCYJ20220530143600001, in part by the Basic Research Project under Grant
   HZQB-KCZYZ-2021067 of Hetao Shenzhen HK S&T Cooperation Zone, in part by
   Shenzhen-Hong Kong Joint Funding under Grant SGDX20211123112401002, in
   part by Shenzhen Outstanding Talents Training Fund, by Guangdong
   Research Project under Grants 2017ZT07X152 and 2019CX01X104, in part by
   the Guangdong Provincial Key Laboratory of Future Networks of
   Intelligence under Grant 2022B1212010001, in part by The Chinese
   University of Hong Kong, Shenzhen, in part by NSFC under Grants 61931024
   and 81922046, in part by NSFC-Youth under Grant 62203296, in part by
   Tencent Open Fund, and in part by ITSO at CUHKSZ.
NR 59
TC 0
Z9 0
U1 6
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2024
VL 30
IS 12
BP 7473
EP 7485
DI 10.1109/TVCG.2023.3340679
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K8D1L
UT WOS:001346124800006
PM 38064324
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Dastan, M
   Fiorentino, M
   Walter, ED
   Diegritz, C
   Uva, AE
   Eck, U
   Navab, N
AF Dastan, Mine
   Fiorentino, Michele
   Walter, Elias D.
   Diegritz, Christian
   Uva, Antonio E.
   Eck, Ulrich
   Navab, Nassir
TI Co-Designing Dynamic Mixed Reality Drill Positioning Widgets: A
   Collaborative Approach with Dentists in a Realistic Setup
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Dentistry; Visualization; Three-dimensional displays; Phantoms;
   Implants; Surgery; Navigation; Dynamic widgets; precise tool
   positioning; usability testing; co-design; dentistry; mixed reality
ID GUIDED IMPLANT-SURGERY; AUGMENTED REALITY; INTERFACES; NAVIGATION;
   TEMPLATE; ACCURACY; SURVIVAL; DISTANCE; SYSTEM
AB Mixed Reality (MR) is proven in the literature to support precise spatial dental drill positioning by superimposing 3D widgets. Despite this, the related knowledge about widget's visual design and interactive user feedback is still limited. Therefore, this study is contributed to by co-designed MR drill tool positioning widgets with two expert dentists and three MR experts. The results of co-design are two static widgets (SWs): a simple entry point, a target axis, and two dynamic widgets (DWs), variants of dynamic error visualization with and without a target axis (DWTA and DWEP). We evaluated the co-designed widgets in a virtual reality simulation supported by a realistic setup with a tracked phantom patient, a virtual magnifying loupe, and a dentist's foot pedal. The user study involved 35 dentists with various backgrounds and years of experience. The findings demonstrated significant results; DWs outperform SWs in positional and rotational precision, especially with younger generations and subjects with gaming experiences. The user preference remains for DWs (19) instead of SWs (16). However, findings indicated that the precision positively correlates with the time trade-off. The post-experience questionnaire (NASA-TLX) showed that DWs increase mental and physical demand, effort, and frustration more than SWs. Comparisons between DWEP and DWTA show that the DW's complexity level influences time, physical and mental demands. The DWs are extensible to diverse medical and industrial scenarios that demand precision.
C1 [Dastan, Mine; Fiorentino, Michele; Uva, Antonio E.] Polytech Univ Bari, Bari, Italy.
   [Walter, Elias D.; Diegritz, Christian] Ludwig Maximilian Univ Munich, Munich, Germany.
   [Eck, Ulrich; Navab, Nassir] Tech Univ Munich, Munich, Germany.
C3 Politecnico di Bari; University of Munich; Technical University of
   Munich
RP Dastan, M (corresponding author), Polytech Univ Bari, Bari, Italy.
EM mine.dastan@poliba.it
RI Uva, Antonio/A-9673-2012; Dastan, Mine/LFR-9751-2024; Fiorentino,
   Michele/M-6976-2015
OI Dastan, Mine/0000-0003-0555-155X; Walter, Elias/0000-0003-4802-2279;
   Fiorentino, Michele/0000-0003-2197-6574; Eck, Ulrich/0000-0002-5322-4724
FU Italian Ministry of Education, University and Research (MUR)
   [L.232/2016]
FX Support from the Italian Ministry of Education, University and Research
   (MUR) under the program "Departments of Excellence"(L.232/2016). The
   authors express their gratitude to the dentists at the Department of
   Conservative Dentistry and Periodontology, LMUK linikum Hospital,
   Munich, for their contribution of the user study.
NR 74
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7053
EP 7063
DI 10.1109/TVCG.2024.3456170
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300004
PM 39250405
DA 2025-03-07
ER

PT J
AU Do, TD
   Benjamin, J
   Protko, CI
   McMahan, RP
AF Do, Tiffany D.
   Benjamin, Juanita
   Protko, Camille Isabella
   McMahan, Ryan P.
TI Cultural Reflections in Virtual Reality: The Effects of User Ethnicity
   in Avatar Matching Experiences on Sense of Embodiment
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Avatars; Cybersickness; Interviews; Virtual environments; Skin; Cultural
   differences; Social groups; Virtual reality; sense of embodiment;
   avatars; diversity
ID GENDER; IMPACT; OWNERSHIP; IDENTITY; MOTION; MEMORY; AGE
AB Matching avatar characteristics to a user can impact sense of embodiment (SoE) in YR. However, few studies have examined how participant demographics may interact with these matching effects. We recruited a diverse and racially balanced sample of 78 participants to investigate the differences among participant groups when embodying both demographically matched and unmatched avatars. We found that participant ethnicity emerged as a significant factor, with Asian and Black participants reporting lower total SoE compared to Hispanic participants. Furthermore, we found that user ethnicity significantly influences ownership (a subscale of SoE), with Asian and Black participants exhibiting stronger effects of matched avatar ethnicity compared to White participants. Additionally, Hispanic participants showed no significant differences, suggesting complex dynamics in ethnic-racial identity. Our results also reveal significant main effects of matched avatar ethnicity and gender on SoE, indicating the importance of considering these factors in VR experiences. These findings contribute valuable insights into understanding the complex dynamics shaping VR experiences across different demographic groups.
C1 [Do, Tiffany D.] Drexel Univ, Philadelphia, PA 19104 USA.
   [Benjamin, Juanita; Protko, Camille Isabella] Univ Cent Florida, Orlando, FL 32816 USA.
   [McMahan, Ryan P.] Virginia Tech, Blacksburg, VA 24061 USA.
C3 Drexel University; State University System of Florida; University of
   Central Florida; Virginia Polytechnic Institute & State University
RP Do, TD (corresponding author), Drexel Univ, Philadelphia, PA 19104 USA.
EM tiffany.do@drexel.edu; juanita.benjamin@ucf.edu; ca916041@ucf.edu;
   rpm@vt.edu
OI Benjamin, Juanita/0000-0003-1308-6678; Do, Tiffany
   D./0000-0003-3323-4586; McMahan, Ryan/0000-0001-9357-9696
FU Doctoral Research Support Award from the College of Graduate Studies at
   The University of Central Florida
FX This work was supported in part by the Doctoral Research Support Award
   from the College of Graduate Studies at The University of Central
   Florida.
NR 80
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7408
EP 7418
DI 10.1109/TVCG.2024.3456211
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300036
PM 39250390
DA 2025-03-07
ER

PT J
AU Yazgan, M
   Sahillioglu, Y
AF Yazgan, Misranur
   Sahillioglu, Yusuf
TI A Partition Based Method for Spectrum-Preserving Mesh Simplification
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Mesh simplification; spectrum-preserving; Laplace-Beltrami spectrum;
   partitioning; Mesh simplification; spectrum-preserving; Laplace-Beltrami
   spectrum; partitioning
AB The majority of the simplification methods focus on preserving the appearance of the mesh, ignoring the spectral properties of the differential operators derived from the mesh. The spectrum of the Laplace-Beltrami operator is essential for a large subset of applications in geometry processing. Coarsening a mesh without considering its spectral properties might result in incorrect calculations on the simplified mesh. Given a 3D triangular mesh, this article aims to simplify the mesh using edge collapses, while focusing on preserving the spectral properties of the associated cotangent Laplace-Beltrami operator. Unlike the existing spectrum-preserving coarsening methods, we consider solely the eigenvalues of the operator in order to preserve the spectrum. The presented method is partition based, that is the input mesh is divided into smaller patches which are simplified individually. We evaluate our method on a variety of meshes, by using functional maps and quantitative norms, to measure how well the eigenvalues and eigenvectors of the Laplace-Beltrami operator computed on the input mesh are maintained by the output mesh. We demonstrate that the achieved spectrum preservation is at least as effective as the existing spectral coarsening methods.
C1 [Yazgan, Misranur; Sahillioglu, Yusuf] Middle East Tech Univ, Dept Comp Engn, TR-06800 Ankara, Turkiye.
C3 Middle East Technical University
RP Sahillioglu, Y (corresponding author), Middle East Tech Univ, Dept Comp Engn, TR-06800 Ankara, Turkiye.
EM yazganmisra@gmail.com; ysahillioglu@gmail.com
OI Sahillioglu, Yusuf/0000-0002-7997-4232
FU TUBITAK [EEEAG-119E572]
FX This work was supported in part by TUBITAK under Grant EEEAG-119E572.
NR 41
TC 0
Z9 0
U1 1
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6839
EP 6850
DI 10.1109/TVCG.2023.3341610
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600006
PM 38090863
DA 2025-03-07
ER

PT J
AU Ma, CF
   Yang, Y
   Guo, J
   Wei, MQ
   Wang, CJ
   Guo, YW
   Wang, WP
AF Ma, Changfeng
   Yang, Yang
   Guo, Jie
   Wei, Mingqiang
   Wang, Chongjun
   Guo, Yanwen
   Wang, Wenping
TI Collaborative Completion and Segmentation for Partial Point Clouds With
   Outliers
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Collaborative completion and segmentation; deep learning; outliers;
   point cloud completion; shape analysis; Collaborative completion and
   segmentation; deep learning; outliers; point cloud completion; shape
   analysis
AB Outliers will inevitably creep into the captured point cloud during 3D scanning, degrading cutting-edge models on various geometric tasks heavily. This paper looks at an intriguing question that whether point cloud completion and segmentation can promote each other to defeat outliers. To answer it, we propose a collaborative completion and segmentation network, termed CS-Net, for partial point clouds with outliers. Unlike most of existing methods, CS-Net does not need any clean (or say outlier-free) point cloud as input or any outlier removal operation. CS-Net is a new learning paradigm that makes completion and segmentation networks work collaboratively. With a cascaded architecture, our method refines the prediction progressively. Specifically, after the segmentation network, a cleaner point cloud is fed into the completion network. We design a novel completion network which harnesses the labels obtained by segmentation together with farthest point sampling to purify the point cloud and leverages KNN-grouping for better generation. Benefited from segmentation, the completion module can utilize the filtered point cloud which is cleaner for completion. Meanwhile, the segmentation module is able to distinguish outliers from target objects more accurately with the help of the clean and complete shape inferred by completion. Besides the designed collaborative mechanism of CS-Net, we establish a benchmark dataset of partial point clouds with outliers. Extensive experiments show clear improvements of our CS-Net over its competitors, in terms of outlier robustness and completion accuracy.
C1 [Ma, Changfeng; Yang, Yang; Guo, Jie; Wang, Chongjun; Guo, Yanwen] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210000, Peoples R China.
   [Wei, Mingqiang] Nanjing Univ Aeronaut & Astronaut, Sch Comp Sci & Technol, Nanjing 210016, Peoples R China.
   [Wang, Wenping] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Nanjing University; Nanjing University of Aeronautics & Astronautics;
   University of Hong Kong
RP Guo, YW (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210000, Peoples R China.
EM changfengma@smail.nju.edu.cn; yyang_nju@outlook.com; guojie@nju.edu.cn;
   mingqiang.wei@gmail.com; chjwang@nju.edu.cn; ywguo@nju.edu.cn;
   wenping@tamu.edu
OI Ma, Changfeng/0000-0001-8732-7038
FU National Natural Science Foundation of China [62032011, 61972194];
   Guangdong Basic and Applied Basic Research Foundation [2022A1515010170]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62032011 and 61972194, and in part by
   Guangdong Basic and Applied Basic Research Foundation under Grant
   2022A1515010170.
NR 74
TC 2
Z9 2
U1 22
U2 34
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6118
EP 6129
DI 10.1109/TVCG.2023.3328354
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000030
PM 37903041
DA 2025-03-07
ER

PT J
AU Filipov, V
   Arleo, A
   Bogl, M
   Miksch, S
AF Filipov, Velitchko
   Arleo, Alessio
   Bogl, Markus
   Miksch, Silvia
TI On Network Structural and Temporal Encodings: A Space and Time Odyssey
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Encoding; Animation; Layout; Visualization; Time factors;
   Topology; Human-centered computing-visualization-graph drawings;
   empirical studies in visualization
ID NODE-LINK; GRAPH; VISUALIZATION; VIOLATIONS
AB The dynamic network visualization design space consists of two major dimensions: network structural and temporal representation. As more techniques are developed and published, a clear need for evaluation and experimental comparisons between them emerges. Most studies explore the temporal dimension and diverse interaction techniques supporting the participants, focusing on a single structural representation. Empirical evidence about performance and preference for different visualization approaches is scattered over different studies, experimental settings, and tasks. This paper aims to comprehensively investigate the dynamic network visualization design space in two evaluations. First, a controlled study assessing participants' response times, accuracy, and preferences for different combinations of network structural and temporal representations on typical dynamic network exploration tasks, with and without the support of standard interaction methods. Second, the best-performing combinations from the first study are enhanced based on participants' feedback and evaluated in a heuristic-based qualitative study with visualization experts on a real-world network. Our results highlight node-link with animation and playback controls as the best-performing combination and the most preferred based on ratings. Matrices achieve similar performance to node-link in the first study but have considerably lower scores in our second evaluation. Similarly, juxtaposition exhibits evident scalability issues in more realistic analysis contexts.
C1 [Filipov, Velitchko] TU Wien, Ctr Visual Analyt Sci & Technol CVAST, A-1040 Vienna, Austria.
C3 Technische Universitat Wien
RP Filipov, V (corresponding author), TU Wien, Ctr Visual Analyt Sci & Technol CVAST, A-1040 Vienna, Austria.
EM velitchko.filipov@tuwien.ac.at; alessio.arleo@tuwien.ac.at;
   markus.boegl@tuwien.ac.at; silvia.miksch@tuwien.ac.at
RI Filipov, Velitchko/JSK-6634-2023; Bögl, Markus/ISS-2644-2023; Arleo,
   Alessio/IRZ-8036-2023
OI Bogl, Markus/0000-0002-8337-4774; Miksch, Silvia/0000-0003-4427-5703;
   Filipov, Velitchko/0000-0001-9592-2179; Arleo,
   Alessio/0000-0003-2008-3651
FU ArtVis [P35767]
FX No Statement Available
NR 75
TC 3
Z9 3
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5847
EP 5860
DI 10.1109/TVCG.2023.3310019
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400064
PM 37647194
OA hybrid
DA 2025-03-07
ER

PT J
AU Li, J
   Kang, D
   Pei, WJ
   Zhe, X
   Zhang, Y
   Bao, LC
   He, ZY
AF Li, Jing
   Kang, Di
   Pei, Wenjie
   Zhe, Xuefei
   Zhang, Ying
   Bao, Linchao
   He, Zhenyu
TI Audio2Gestures: Generating Diverse Gestures From Audio
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Codes; Training; Three-dimensional displays; Dynamics; Decoding; Image
   reconstruction; Hidden Markov models; Cross-model generation; gesture;
   motion generation
ID SCALE
AB People may perform diverse gestures affected by various mental and physical factors when speaking the same sentences. This inherent one-to-many relationship makes co-speech gesture generation from audio particularly challenging. Conventional CNNs/RNNs assume one-to-one mapping, and thus tend to predict the average of all possible target motions, easily resulting in plain/boring motions during inference. So we propose to explicitly model the one-to-many audio-to-motion mapping by splitting the cross-modal latent code into shared code and motion-specific code. The shared code is expected to be responsible for the motion component that is more correlated to the audio while the motion-specific code is expected to capture diverse motion information that is more independent of the audio. However, splitting the latent code into two parts poses extra training difficulties. Several crucial training losses/strategies, including relaxed motion loss, bicycle constraint, and diversity loss, are designed to better train the VAE. Experiments on both 3D and 2D motion datasets verify that our method generates more realistic and diverse motions than previous state-of-the-art methods, quantitatively and qualitatively. Besides, our formulation is compatible with discrete cosine transformation (DCT) modeling and other popular backbones (i.e., RNN, Transformer). As for motion losses and quantitative motion evaluation, we find structured losses/metrics (e.g. STFT) that consider temporal and/or spatial context complement the most commonly used point-wise losses (e.g. PCK), resulting in better motion dynamics and more nuanced motion details. Finally, we demonstrate that our method can be readily used to generate motion sequences with user-specified motion clips on the timeline.
C1 [Li, Jing; He, Zhenyu] Harbin Inst Technol, Shenzhen 518055, Peoples R China.
   [Kang, Di; Bao, Linchao] Tencent AI Lab, Shenzhen 518055, Peoples R China.
C3 Harbin Institute of Technology; Tencent
RP He, ZY (corresponding author), Harbin Inst Technol, Shenzhen 518055, Peoples R China.; Bao, LC (corresponding author), Tencent AI Lab, Shenzhen 518055, Peoples R China.
EM lijing@stu.hit.edu.cn; dkang@tencent.com; wenjiecoder@outlook.com;
   zhexuefei@outlook.com; yinggzhang@tencent.com; linchaobao@gmail.com;
   zhenyuhe@hit.edu.cn
RI Pei, Wenjie/IQT-7671-2023; Bao, Linchao/AAG-9148-2020
OI li, jing/0000-0002-2162-1004; Bao, Linchao/0000-0001-9543-3754
FU National Natural Science Foundation of China [62172126, U2013210,
   62006060]; Shenzhen Fundamental Research Program
   [JCYJ20220818102415032]; Shenzhen Research Council
   [JCYJ20210324120202006]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62172126, in part by the National
   Natural Science Foundation of China under Grants U2013210, 62006060, in
   part by Shenzhen Fundamental Research Program under Grant
   JCYJ20220818102415032, and in part by Shenzhen Research Council under
   Grant JCYJ20210324120202006.
NR 67
TC 1
Z9 1
U1 3
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4752
EP 4766
DI 10.1109/TVCG.2023.3276973
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400030
PM 37195841
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Xiao, YQ
   Bai, HM
   Gao, Y
   Hu, B
   Zheng, J
   Cai, XE
   Rao, JS
   Li, XG
   Hao, AM
AF Xiao, Yanqing
   Bai, Hongming
   Gao, Yang
   Hu, Ben
   Zheng, Jia
   Cai, Xiaoe
   Rao, Jiasheng
   Li, Xiaoguang
   Hao, Aimin
TI Interactive Virtual Ankle Movement Controlled by Wrist sEMG Improves
   Motor Imagery: An Exploratory Study
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Training; Stroke (medical condition); Wrist; Muscles; Legged locomotion;
   Biological system modeling; Real-time systems; VR-based stroke
   rehabilitation training; motor imagery; sEMG-based virtual feedback
ID FUNCTIONAL RECOVERY; ROBOTIC DEVICE; STROKE; REHABILITATION; EMBODIMENT;
   VALIDITY; THREATS; ANGLES; LEGS; HAND
AB Virtual reality (VR) techniques can significantly enhance motor imagery training by creating a strong illusion of action for central sensory stimulation. In this article, we establish a precedent by using surface electromyography (sEMG) of contralateral wrist movement to trigger virtual ankle movement through an improved data-driven approach with a continuous sEMG signal for fast and accurate intention recognition. Our developed VR interactive system can provide feedback training for stroke patients in the early stages, even if there is no active ankle movement. Our objectives are to evaluate: 1) the effects of VR immersion mode on body illusion, kinesthetic illusion, and motor imagery performance in stroke patients; 2) the effects of motivation and attention when utilizing wrist sEMG as a trigger signal for virtual ankle motion; 3) the acute effects on motor function in stroke patients. Through a series of well-designed experiments, we have found that, compared to the 2D condition, VR significantly increases the degree of kinesthetic illusion and body ownership of the patients, and improves their motor imagery performance and motor memory. When compared to conditions without feedback, using contralateral wrist sEMG signals as trigger signals for virtual ankle movement enhances patients' sustained attention and motivation during repetitive tasks. Furthermore, the combination of VR and feedback has an acute impact on motor function. Our exploratory study suggests that the sEMG-based immersive virtual interactive feedback provides an effective option for active rehabilitation training for severe hemiplegia patients in the early stages, with great potential for clinical application.
C1 [Xiao, Yanqing; Rao, Jiasheng] Beihang Univ, Beijing Adv Innovat Ctr Biomed Engn, Sch Biol Sci & Med Engn, Beijing Key Lab Biomat & Neural Regenerat, Beijing 100191, Peoples R China.
   [Bai, Hongming; Gao, Yang; Hu, Ben] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Bai, Hongming; Gao, Yang; Hu, Ben] Chinese Acad Med Sci, Res Unit Virtual Body & Virtual Surg 2019RU004, Beijing 100050, Peoples R China.
   [Zheng, Jia] Capital Med Univ, Beijing Childrens Hosp, Beijing 100045, Peoples R China.
   [Cai, Xiaoe] Beijing Haidian Hosp, Dept Rehabil Med, Beijing 100080, Peoples R China.
   [Li, Xiaoguang] Capital Med Univ, Sch Basic Med Sci, Dept Neurobiol, Beijing 100069, Peoples R China.
C3 Beihang University; Beihang University; Chinese Academy of Medical
   Sciences - Peking Union Medical College; Capital Medical University;
   Capital Medical University
RP Gao, Y (corresponding author), Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Li, XG (corresponding author), Capital Med Univ, Sch Basic Med Sci, Dept Neurobiol, Beijing 100069, Peoples R China.
EM xioyanjingx@126.com; 18813139553@163.com; gaoyangvr@buaa.edu.cn;
   huben@buaa.edu.cn; lwc20002@126.com; cxe930316@163.com;
   raojschina@buaa.edu.cn; lxgchina@sina.com; ham@buaa.edu.cn
RI Gao, Yang/JQV-9627-2023; Zhao, Mingyu/HHS-0141-2022
OI Zheng, Jia/0009-0007-6407-7436; Gao, Yang/0000-0002-9149-3554; Rao,
   Jia-Sheng/0000-0002-5196-0912
FU National Natural Science Foundation of China [62002010, 31970970,
   82271403]; Beijing Advanced Innovation Center for Biomedical Engineering
   [ZF138G1714]; CAMS Innovation Fund for Medical Sciences (CIFMS)
   [2019-I2M-5-016]; Beijing Science and Technology Project
   [Z221100007722001]; Huawei Innovation Research Plan [TC20220616019]; Key
   Research and Development Program of Guangzhou [202206060003]; Open
   Project Program of State Key Laboratory of Virtual Reality Technology
   and Systems [VRLAB2023A05]
FX No Statement Available
NR 75
TC 2
Z9 3
U1 13
U2 32
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5507
EP 5524
DI 10.1109/TVCG.2023.3294342
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400011
PM 37432832
DA 2025-03-07
ER

PT J
AU Ji, B
   Pan, Y
   Yan, YC
   Chen, RZ
   Yang, XK
AF Ji, Bin
   Pan, Ye
   Yan, Yichao
   Chen, Ruizhao
   Yang, Xiaokang
TI StyleVR: Stylizing Character Animations With Normalizing Flows
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Character animation; motion generation; style transfer; normalizing
   flow; virtual reality
AB The significance of artistry in creating animated virtual characters is widely acknowledged, and motion style is a crucial element in this process. There has been a long-standing interest in stylizing character animations with style transfer methods. However, this kind of models can only deal with short-term motions and yield deterministic outputs. To address this issue, we propose a generative model based on normalizing flows for stylizing long and aperiodic animations in the VR scene. Our approach breaks down this task into two sub-problems: motion style transfer and stylized motion generation, both formulated as the instances of conditional normalizing flows with multi-class latent space. Specifically, we encode high-frequency style features into the latent space for varied results and control the generation process with style-content labels for disentangled edits of style and content. We have developed a prototype, StyleVR, in Unity, which allows casual users to apply our method in VR. Through qualitative and quantitative comparisons, we demonstrate that our system outperforms other methods in terms of style transfer as well as stochastic stylized motion generation.
C1 [Ji, Bin; Pan, Ye; Yan, Yichao; Chen, Ruizhao; Yang, Xiaokang] Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China.
C3 Shanghai Jiao Tong University
RP Pan, Y (corresponding author), Shanghai Jiao Tong Univ, Shanghai 200240, Peoples R China.
EM bin.ji@sjtu.edu.cn; whitneypanye@sjtu.edu.cn; yanyichao@sjtu.edu.cn;
   stelledge@sjtu.edu.cn; xkyang@sjtu.edu.cn
RI Yan, Yichao/ADT-5511-2022
OI Ji, Bin/0000-0002-8981-5251; Chen, Ruizhao/0000-0002-3056-0186
FU Shanghai Sailing Program [20YF1421200]; National Natural Science
   Foundation of China (NSFC) [62102255, 62201342]; Shanghai Municipal
   Science and Technology Major Project [2021SHZDZX0102]
FX This work was supported in part by Shanghai Sailing Program under Grant
   20YF1421200, in part by the National Natural Science Foundation of China
   (NSFC) under Grant 62102255, in part by the National Natural Science
   Foundation of China (NSFC) under Grant 62201342 and in part by Shanghai
   Municipal Science and Technology Major Project under Grant
   2021SHZDZX0102.
NR 49
TC 0
Z9 0
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4183
EP 4196
DI 10.1109/TVCG.2023.3259183
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700023
PM 37030765
DA 2025-03-07
ER

PT J
AU Vohra, SK
   Harth, P
   Isoe, Y
   Bahl, A
   Fotowat, H
   Engert, F
   Hege, HC
   Baum, D
AF Vohra, Sumit Kumar
   Harth, Philipp
   Isoe, Yasuko
   Bahl, Armin
   Fotowat, Haleh
   Engert, Florian
   Hege, Hans-Christian
   Baum, Daniel
TI A Visual Interface for Exploring Hypotheses About Neural Circuits
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Neurons; Neural circuits; Visualization; Imaging; Image reconstruction;
   Behavioral sciences; Data visualization; Human-centered computing;
   scientific visualization; visualization systems and tools; web-based
   interaction
ID BRAIN; VISUALIZATION
AB One of the fundamental problems in neurobiological research is to understand how neural circuits generate behaviors in response to sensory stimuli. Elucidating such neural circuits requires anatomical and functional information about the neurons that are active during the processing of the sensory information and generation of the respective response, as well as an identification of the connections between these neurons. With modern imaging techniques, both morphological properties of individual neurons as well as functional information related to sensory processing, information integration and behavior can be obtained. Given the resulting information, neurobiologists are faced with the task of identifying the anatomical structures down to individual neurons that are linked to the studied behavior and the processing of the respective sensory stimuli. Here, we present a novel interactive tool that assists neurobiologists in the aforementioned tasks by allowing them to extract hypothetical neural circuits constrained by anatomical and functional data. Our approach is based on two types of structural data: brain regions that are anatomically or functionally defined, and morphologies of individual neurons. Both types of structural data are interlinked and augmented with additional information. The presented tool allows the expert user to identify neurons using Boolean queries. The interactive formulation of these queries is supported by linked views, using, among other things, two novel 2D abstractions of neural circuits. The approach was validated in two case studies investigating the neural basis of vision-based behavioral responses in zebrafish larvae. Despite this particular application, we believe that the presented tool will be of general interest for exploring hypotheses about neural circuits in other species, genera and taxa.
C1 [Vohra, Sumit Kumar; Harth, Philipp; Hege, Hans-Christian; Baum, Daniel] Zuse Inst Berlin ZIB, Dept Visual & Data Ctr Comp, D-14195 Berlin, Germany.
   [Isoe, Yasuko; Fotowat, Haleh; Engert, Florian] Harvard Univ, Dept Mol & Cellular Biol, Cambridge, MA 02138 USA.
   [Bahl, Armin] Univ Konstanz, Ctr Adv Study Collect Behav, D-78464 Constance, Germany.
C3 Zuse Institute Berlin; Harvard University; University of Konstanz
RP Vohra, SK (corresponding author), Zuse Inst Berlin ZIB, Dept Visual & Data Ctr Comp, D-14195 Berlin, Germany.
EM vohra@zib.de; harth@zib.de; yasuko_isoe@fas.harvard.edu;
   armin.bahl@uni-konstanz.de; haleh.fotowat@wyss.harvard.edu;
   florian@mcb.harvard.edu; hege@zib.de; baum@zib.de
OI Isoe, Yasuko/0000-0002-1391-7921; Vohra, Sumit
   Kumar/0000-0003-2443-7461; Hege, Hans-Christian/0000-0002-6574-0988;
   Harth, Philipp/0000-0001-8831-3893; Baum, Daniel/0000-0003-1550-7245;
   Bahl, Armin/0000-0001-7591-5860
FU National Institute of Neurological Disorders and Stroke (NINDS) of the
   National Institutes of Health (NIH) [1U19NS104653]; Deutsche
   Forschungsgemeinschaft (DFG); SPP 2041 Computational Connectomics
   [347210657]; NIH [U19NS104653, 1R01NS124017]; National Science
   Foundation [IIS-1912293]; Simons Foundation [SCGB 542973]; Deutsche
   Forschungsgemeinschaft (DFG, German Research Foundation) through
   Germany's Excellence Strategy [EXC 2117 422037984]; Emmy Noether Program
   [429442687]; Zukunftskolleg Konstanz
FX This work was supported in part by the National Institute of
   Neurological Disorders and Stroke (NINDS) of the National Institutes of
   Health (NIH) under Grant 1U19NS104653, and in part by the Deutsche
   Forschungsgemeinschaft (DFG), SPP 2041 Computational Connectomics, under
   Grant 347210657. The work of Florian Engert received funding from the
   NIH under Grants U19NS104653 and 1R01NS124017, in part by the National
   Science Foundation under Grant IIS-1912293, and in part by the Simons
   Foundation under Grant SCGB 542973. The work of Armin Bahl was supported
   in part by the Deutsche Forschungsgemeinschaft (DFG, German Research
   Foundation) through Germany's Excellence Strategy under Grant EXC 2117
   422037984, in part by the Emmy Noether Program under Grant 429442687,
   and in part by the Zukunftskolleg Konstanz.
NR 61
TC 0
Z9 0
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3945
EP 3958
DI 10.1109/TVCG.2023.3243668
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700087
PM 37022819
DA 2025-03-07
ER

PT J
AU Zhao, MY
   Huang, XS
   Jiang, JE
   Mou, LT
   Yan, DM
   Ma, L
AF Zhao, Mingyang
   Huang, Xiaoshui
   Jiang, Jingen
   Mou, Luntian
   Yan, Dong-Ming
   Ma, Lei
TI Accurate Registration of Cross-Modality Geometry via Consistent
   Clustering
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Point cloud compression; Geometry; Three-dimensional displays; Laser
   radar; Tensors; Clustering algorithms; Solid modeling; Cross-modality
   geometry; point cloud registration; 3D reconstruction; adaptive fuzzy
   clustering; CAD
ID ITERATIVE CLOSEST POINT; ICP
AB The registration of unitary-modality geometric data has been successfully explored over past decades. However, existing approaches typically struggle to handle cross-modality data due to the intrinsic difference between different models. To address this problem, in this article, we formulate the cross-modality registration problem as a consistent clustering process. First, we study the structure similarity between different modalities based on an adaptive fuzzy shape clustering, from which a coarse alignment is successfully operated. Then, we optimize the result using fuzzy clustering consistently, in which the source and target models are formulated as clustering memberships and centroids, respectively. This optimization casts new insight into point set registration, and substantially improves the robustness against outliers. Additionally, we investigate the effect of fuzzier in fuzzy clustering on the cross-modality registration problem, from which we theoretically prove that the classical Iterative Closest Point (ICP) algorithm is a special case of our newly defined objective function. Comprehensive experiments and analysis are conducted on both synthetic and real-world cross-modality datasets. Qualitative and quantitative results demonstrate that our method outperforms state-of-the-art approaches with higher accuracy and robustness. Our code is publicly available at https://github.com/zikai1/CrossModReg.
C1 [Zhao, Mingyang] Chinese Acad Sci, Beijing Acad Artificial Intelligence, Inst Automat, Beijing 100045, Peoples R China.
   [Zhao, Mingyang; Jiang, Jingen; Yan, Dong-Ming] Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100045, Peoples R China.
   [Huang, Xiaoshui] Shanghai AI Lab, Shanghai 200433, Peoples R China.
   [Jiang, Jingen; Yan, Dong-Ming] Chinese Acad Sci, State Key Lab Multimodal Artificial Intelligence, Inst Automat, Beijing 100045, Peoples R China.
   [Jiang, Jingen; Yan, Dong-Ming] Univ Chinese Acad Sci, Sch AI, Beijing 101408, Peoples R China.
   [Mou, Luntian] Beijing Univ Technol, Beijing Inst Artificial Intelligence, Beijing Key Lab Multimedia & Intelligent Software, Beijing 100021, Peoples R China.
   [Ma, Lei] Peking Univ, Natl Biomed Imaging Ctr, Beijing 100871, Peoples R China.
   [Ma, Lei] Peking Univ, Sch Comp Sci, Beijing Acad Artificial Intelligence, Beijing 100871, Peoples R China.
   [Ma, Lei] Peking Univ, Sch Comp Sci, Natl Key Lab Multimedia Informat Proc, Beijing 100871, Peoples R China.
C3 Beijing Academy of Artificial Intelligence; Chinese Academy of Sciences;
   Institute of Automation, CAS; Chinese Academy of Sciences; Institute of
   Automation, CAS; Shanghai Artificial Intelligence Laboratory; Chinese
   Academy of Sciences; Institute of Automation, CAS; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS; Beijing
   University of Technology; Peking University; Peking University; Beijing
   Academy of Artificial Intelligence; Peking University
RP Yan, DM (corresponding author), Chinese Acad Sci, Inst Automat, Natl Lab Pattern Recognit, Beijing 100045, Peoples R China.; Yan, DM (corresponding author), Chinese Acad Sci, State Key Lab Multimodal Artificial Intelligence, Inst Automat, Beijing 100045, Peoples R China.; Ma, L (corresponding author), Peking Univ, Natl Biomed Imaging Ctr, Beijing 100871, Peoples R China.; Ma, L (corresponding author), Peking Univ, Sch Comp Sci, Beijing Acad Artificial Intelligence, Beijing 100871, Peoples R China.; Ma, L (corresponding author), Peking Univ, Sch Comp Sci, Natl Key Lab Multimedia Informat Proc, Beijing 100871, Peoples R China.
EM zhaomingyang16@mails.ucas.ac.cn; huangxiaoshui@163.com;
   jiangjingen@ia.ac.cn; ltmou@bjut.edu.cn; yandongming@gmail.com;
   lei.ma@pku.edu.cn
RI Mou, Luntian/ACX-6553-2022; Zhao, Ming-yang/ABD-1741-2021; Ma,
   Lei/AEI-0577-2022
OI Ma, Lei/0000-0001-6024-3854; Yan, Dong-Ming/0000-0003-2209-2404; Huang,
   Xiaoshui/0000-0002-3579-538X
FU National Key R&D Program of China [2022ZD0116305]; National Natural
   Science Foundation of China [62172415]; Open Research Fund Program of
   State Key Laboratory of Hydroscience and Engineering, Tsinghua
   University [klhse-2022-D-04]
FX This work was supported in part by National Key R&D Program of China
   under Grant 2022ZD0116305, in part by the National Natural Science
   Foundation of China under Grant 62172415, and in part by the Open
   Research Fund Program of State Key Laboratory of Hydroscience and
   Engineering, Tsinghua University under Grants klhse-2022-D-04.
NR 56
TC 9
Z9 9
U1 9
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4055
EP 4067
DI 10.1109/TVCG.2023.3247169
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700043
PM 37027717
DA 2025-03-07
ER

PT J
AU Zhao, TM
   Gao, P
   Tian, T
   Ma, JY
   Tian, JW
AF Zhao, Tianming
   Gao, Peng
   Tian, Tian
   Ma, Jiayi
   Tian, Jinwen
TI From Noise Addition to Denoising: A Self-Variation Capture Network for
   Point Cloud Optimization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Point cloud compression; Noise reduction; Noise measurement; Manifolds;
   Optimization; Three-dimensional displays; Surface cleaning; Commonality
   capture; noise perturbation; point clouds denoising; self-variation
AB Point clouds obtained from 3D scanners are often noisy and cannot be directly used for subsequent high-level tasks. In this article, we propose a novel point cloud optimization method capable of denoising and homogenizing point clouds. Our idea is based on the assumption that the noise is generally much smaller than the effective signal. We perform noise perturbation on the noisy point cloud to get a new noisy point cloud, called self-variation point cloud. The noisy point cloud and self-variation point cloud have different noise distribution, but the same point cloud distribution. We compute the potential commonality between two noisy point clouds to obtain a clean point cloud. To implement our idea, we propose a Self-Variation Capture Network (SVCNet). We perturb the point cloud features in the latent space to obtain self-variation feature vectors, and capture the commonality between two noisy feature vectors through the feature aggregation and averaging. In addition, an edge constraint module is introduced to suppress low-pass effects during denoising. Our denoising method does not take into account the noise characteristics, and can filter the drift noise located on the underlying surface, resulting in a uniform distribution of the generated point cloud. The experimental results show that our algorithm outperforms the current state-of-the-art algorithms, especially in generating more uniform point clouds. In addition, extended experiments demonstrate the potential of our algorithm for point clouds upsampling.
C1 [Zhao, Tianming; Gao, Peng; Tian, Tian; Tian, Jinwen] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Natl Key Lab Multispectral Informat Intelligent Pr, Wuhan 430074, Peoples R China.
   [Ma, Jiayi] Wuhan Univ, Elect Informat Sch, Wuhan 430072, Peoples R China.
C3 Huazhong University of Science & Technology; Wuhan University
RP Tian, T (corresponding author), Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Natl Key Lab Multispectral Informat Intelligent Pr, Wuhan 430074, Peoples R China.
EM tming@hust.edu.cn; gaopengde@hust.edu.cn; ttian@hust.edu.cn;
   jyma2010@gmail.com; jwtian@mail.hust.edu.cn
RI Ma, Jiayi/Y-2470-2019; Tian, Tian/AAO-6980-2021
OI Ma, Jiayi/0000-0003-3264-3265; Tian, Tian/0000-0003-0148-4900
FU National Natural Science Foundation of China [42071339, 62276192];
   National Key Laboratory Fund [6142113210310]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 42071339 and 62276192 and in part by
   National Key Laboratory Fund under Grant 6142113210310.
NR 44
TC 4
Z9 5
U1 25
U2 37
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3413
EP 3426
DI 10.1109/TVCG.2022.3231680
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700069
PM 37015380
DA 2025-03-07
ER

PT J
AU Zhong, CL
   Sang, XZ
   Yan, BB
   Li, H
   Chen, D
   Qin, XJ
   Chen, S
   Ye, XQ
AF Zhong, Chongli
   Sang, Xinzhu
   Yan, Binbin
   Li, Hui
   Chen, Duo
   Qin, Xiujuan
   Chen, Shuo
   Ye, Xiaoqian
TI Real-Time High-Quality Computer-Generated Hologram Using Complex-Valued
   Convolutional Neural Network
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Neural networks; Computer architecture; Three-dimensional displays; Deep
   learning; Image reconstruction; Holography; Convolution; neural models;
   virtual and augmented reality
ID SPATIAL LIGHT-MODULATOR; ANGULAR SPECTRUM METHOD; 3-DIMENSIONAL DISPLAY;
   PHASE; PROPAGATION; IMAGE
AB Holographic displays are ideal display technologies for virtual and augmented reality because all visual cues are provided. However, real-time high-quality holographic displays are difficult to achieve because the generation of high-quality computer-generated hologram (CGH) is inefficient in existing algorithms. Here, complex-valued convolutional neural network (CCNN) is proposed for phase-only CGH generation. The CCNN-CGH architecture is effective with a simple network structure based on the character design of complex amplitude. A holographic display prototype is set up for optical reconstruction. Experiments verify that state-of-the-art performance is achieved in terms of quality and generation speed in existing end-to-end neural holography methods using the ideal wave propagation model. The generation speed is three times faster than HoloNet and one-sixth faster than Holo-encoder, and the Peak Signal to Noise Ratio (PSNR) is increased by 3 dB and 9 dB, respectively. Real-time high-quality CGHs are generated in 1920 x 1072 and 3840 x 2160 resolutions for dynamic holographic displays.
C1 [Zhong, Chongli; Sang, Xinzhu; Yan, Binbin; Chen, Duo; Qin, Xiujuan; Chen, Shuo; Ye, Xiaoqian] Beijing Univ Posts & Telecommun, State Key Lab Informat Photon & Opt Commun, Beijing 100876, Peoples R China.
   [Li, Hui] Tsinghua Univ, Beijing Natl Res Ctr Informat Sci & Technol, Beijing 100084, Peoples R China.
C3 Beijing University of Posts & Telecommunications; Tsinghua University
RP Sang, XZ; Yan, BB (corresponding author), Beijing Univ Posts & Telecommun, State Key Lab Informat Photon & Opt Commun, Beijing 100876, Peoples R China.
EM zclda@bupt.edu.cn; xzsang@bupt.edu.cn; yanbinbin@bupt.edu.cn;
   huilijtsd@163.com; chenduo15@aliyun.com; 18801175281@163.com;
   shuochen365@bupt.edu.cn; xiaoqianye@bupt.edu.cn
RI z, cl/KZU-4124-2024
OI Zhong, Chongli/0000-0001-8917-7512
FU National Natural Science Foundation of China [62075016, 62175017]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62075016, 62175017.
NR 51
TC 11
Z9 11
U1 14
U2 38
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3709
EP 3718
DI 10.1109/TVCG.2023.3239670
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700006
PM 37022034
DA 2025-03-07
ER

PT J
AU Shen, HW
   Kiyokawa, K
AF Shen, Han-Wei
   Kiyokawa, Kiyoshi
TI Message from the Editor-in-Chief and from the Associate Editor-in-Chief
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
AB Welcome to the <italic>13th IEEE Transactions on Visualization and Computer Graphics (TVCG)</italic> special issue on IEEE Virtual Reality and 3D User Interfaces. This volume contains a total of 80 full papers selected for and presented at the IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2024), held in Orlando, Florida, USA, from March 16 to 21, 2024.
C1 [Shen, Han-Wei] Ohio State Univ, Columbus, OH 43210 USA.
   [Kiyokawa, Kiyoshi] Nara Inst Sci & Technol, Ikoma, Japan.
C3 University System of Ohio; Ohio State University; Nara Institute of
   Science & Technology
RP Shen, HW (corresponding author), Ohio State Univ, Columbus, OH 43210 USA.
EM shen.94@osu.edu; kiyo@is.naist.jp
RI Shen, Han-wei/A-4710-2012
NR 0
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 0viii
EP 0viii
DI 10.1109/TVCG.2024.3369809
PG 1
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400004
OA Bronze
DA 2025-03-07
ER

PT J
AU Hong, JY
   Maciejewski, R
   Trubuil, A
   Isenberg, T
AF Hong, Jiayi
   Maciejewski, Ross
   Trubuil, Alain
   Isenberg, Tobias
TI Visualizing and Comparing Machine Learning Predictions to Improve
   Human-AI Teaming on the Example of Cell Lineage
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cell lineage; comparing ML predictions; human -AI teaming; machine
   learning; plant biology; visual analytics; visualization
ID DIVISION; ANALYTICS; PLATFORM
AB We visualize the predictions of multiple machine learning models to help biologists as they interactively make decisions about cell lineage-the development of a (plant) embryo from a single ovum cell. Based on a confocal microscopy dataset, traditionally biologists manually constructed the cell lineage, starting from this observation and reasoning backward in time to establish their inheritance. To speed up this tedious process, we make use of machine learning (ML) models trained on a database of manually established cell lineages to assist the biologist in cell assignment. Most biologists, however, are not familiar with ML, nor is it clear to them which model best predicts the embryo's development. We thus have developed a visualization system that is designed to support biologists in exploring and comparing ML models, checking the model predictions, detecting possible ML model mistakes, and deciding on the most likely embryo development. To evaluate our proposed system, we deployed our interface with six biologists in an observational study. Our results show that the visual representations of machine learning are easily understandable, and our tool, LineageD+, could potentially increase biologists' working efficiency and enhance the understanding of embryos.
C1 [Hong, Jiayi; Maciejewski, Ross] Arizona State Univ, Tempe, AZ 85281 USA.
   [Trubuil, Alain] Univ Paris Saclay, INRAE, F-69100 Villeurbanne, France.
   [Isenberg, Tobias] Univ Paris Saclay, CNRS, Inria, F-91120 Palaiseau, France.
C3 Arizona State University; Arizona State University-Tempe; Universite
   Paris Saclay; INRAE; Universite Paris Saclay; Centre National de la
   Recherche Scientifique (CNRS); Inria; Institut Polytechnique de Paris;
   Ecole Polytechnique
RP Isenberg, T (corresponding author), Univ Paris Saclay, CNRS, Inria, F-91120 Palaiseau, France.
EM jiayi.hong@hotmail.com; rmacieje@asu.edu; alain.trubuil@inra.fr;
   tobias.isenberg@inria.fr
RI ; Isenberg, Tobias/A-7575-2008
OI Hong, Jiayi/0000-0002-1332-5045; Isenberg, Tobias/0000-0001-7953-8644
FU Inria#x0027;s Naviscope Project
FX No Statement Available
NR 47
TC 1
Z9 1
U1 4
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2024
VL 30
IS 4
BP 1956
EP 1969
DI 10.1109/TVCG.2023.3302308
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JN9X1
UT WOS:001173975500003
PM 37665712
OA Green Submitted
DA 2025-03-07
ER

PT J
AU He, TY
   Zhong, YY
   Isenberg, P
   Isenberg, T
AF He, Tingying
   Zhong, Yuanyang
   Isenberg, Petra
   Isenberg, Tobias
TI Design Characterization for Black-and-White Textures in Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Shape; Image color analysis; Bars;
   Encoding; Electronic mail; Aesthetics; textures; icons; black and white;
   visualization; visual representations; categorical data; design;
   perception
ID PERCEPTION; FEATURES; STATISTICS; ELEMENTS; TEXTONS
AB We investigate the use of 2D black-and-white textures for the visualization of categorical data and contribute a summary of texture attributes, and the results of three experiments that elicited design strategies as well as aesthetic and effectiveness measures. Black-and-white textures are useful, for instance, as a visual channel for categorical data on low-color displays, in 2D/3D print, to achieve the aesthetic of historic visualizations, or to retain the color hue channel for other visual mappings. We specifically study how to use what we call geometric and iconic textures. Geometric textures use patterns of repeated abstract geometric shapes, while iconic textures use repeated icons that may stand for data categories. We parameterized both types of textures and developed a tool for designers to create textures on simple charts by adjusting texture parameters. 30 visualization experts used our tool and designed 66 textured bar charts, pie charts, and maps. We then had 150 participants rate these designs for aesthetics. Finally, with the top-rated geometric and iconic textures, our perceptual assessment experiment with 150 participants revealed that textured charts perform about equally well as non-textured charts, and that there are some differences depending on the type of chart.
C1 [He, Tingying; Isenberg, Petra; Isenberg, Tobias] Univ Paris Saclay, CNRS, Inria, LISN, Gif Sur Yvette, France.
   [Zhong, Yuanyang] Tencent Technol Shenzhen Co Ltd, Shenzhen, Peoples R China.
C3 Centre National de la Recherche Scientifique (CNRS); Inria; Universite
   Paris Saclay; Tencent
RP He, TY (corresponding author), Univ Paris Saclay, CNRS, Inria, LISN, Gif Sur Yvette, France.
EM tingying.he@inria.fr; zoniaczhong@tencent.com; petra.isenberg@inria.fr;
   tobias.isenberg@inria.fr
RI He, Tingying/KHW-4844-2024; Isenberg, Tobias/A-7575-2008
OI Isenberg, Tobias/0000-0001-7953-8644; Isenberg,
   Petra/0000-0002-2948-6417; He, Tingying/0000-0002-9670-5587
NR 53
TC 0
Z9 0
U1 1
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 1019
EP 1029
DI 10.1109/TVCG.2023.3326941
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500114
PM 37883265
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Piccolotto, N
   Bögl, M
   Muehlmann, C
   Nordhausen, K
   Filzmoser, P
   Schmidt, J
   Miksch, S
AF Piccolotto, Nikolaus
   Boegl, Markus
   Muehlmann, Christoph
   Nordhausen, Klaus
   Filzmoser, Peter
   Schmidt, Johanna
   Miksch, Silvia
TI Data Type Agnostic Visual Sensitivity Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visual analytics; parameter space analysis; sensitivity analysis;
   spatial blind source separation
ID LARGE-SCALE SYSTEMS; UNCERTAINTY ANALYSIS; SPACE; EXPLORATION;
   ANALYTICS; VISUALIZATION
AB Modern science and industry rely on computational models for simulation, prediction, and data analysis. Spatial blind source separation (SBSS) is a model used to analyze spatial data. Designed explicitly for spatial data analysis, it is superior to popular non-spatial methods, like PCA. However, a challenge to its practical use is setting two complex tuning parameters, which requires parameter space analysis. In this paper, we focus on sensitivity analysis (SA). SBSS parameters and outputs are spatial data, which makes SA difficult as few SA approaches in the literature assume such complex data on both sides of the model. Based on the requirements in our design study with statistics experts, we developed a visual analytics prototype for data type agnostic visual sensitivity analysis that fits SBSS and other contexts. The main advantage of our approach is that it requires only dissimilarity measures for parameter settings and outputs (Fig. 1). We evaluated the prototype heuristically with visualization experts and through interviews with two SBSS experts. In addition, we show the transferability of our approach by applying it to microclimate simulations. Study participants could confirm suspected and known parameter-output relations, find surprising associations, and identify parameter subspaces to examine in the future. During our design study and evaluation, we identified challenging future research opportunities.
C1 [Piccolotto, Nikolaus; Boegl, Markus; Muehlmann, Christoph; Filzmoser, Peter; Miksch, Silvia] TU Wien, Vienna, Austria.
   [Nordhausen, Klaus] Univ Jyvaskyla, Jyvaskyla, Finland.
   [Schmidt, Johanna] VRVis GmbH, Vienna, Austria.
C3 Technische Universitat Wien; University of Jyvaskyla
RP Piccolotto, N (corresponding author), TU Wien, Vienna, Austria.
EM nikolaus.piccolotto@tuwien.ac.at; markus.bogl@tuwien.ac.at;
   christoph.muehlmann@tuwien.ac.at; klaus.k.nordhausen@jyu.fi;
   peter.filzmoser@tuwien.ac.at; johanna.schmidt@vrvis.at;
   silvia.miksch@tuwien.ac.at
RI Filzmoser, Peter/A-7737-2015; Bögl, Markus/ISS-2644-2023; Nordhausen,
   Klaus/A-8644-2008
OI Muehlmann, Christoph/0000-0001-7330-8434; Bogl,
   Markus/0000-0002-8337-4774; Schmidt, Johanna/0000-0002-9638-6344;
   Piccolotto, Nikolaus/0000-0001-6876-6502; Filzmoser,
   Peter/0000-0002-8014-4682; Miksch, Silvia/0000-0003-4427-5703;
   Nordhausen, Klaus/0000-0002-3758-8501
FU Austrian Science Fund (FWF)
FX No Statement Available
NR 72
TC 0
Z9 0
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 1106
EP 1116
DI 10.1109/TVCG.2023.3327203
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500029
PM 37922175
OA hybrid, Green Submitted
DA 2025-03-07
ER

PT J
AU Troidl, J
   Warchol, S
   Choi, J
   Matelsky, J
   Dhanyasi, N
   Wang, XY
   Wester, B
   Wei, DL
   Lichtman, JW
   Pfister, H
   Beyer, J
AF Troidl, Jakob
   Warchol, Simon
   Choi, Jinhan
   Matelsky, Jordan
   Dhanyasi, Nagaraju
   Wang, Xueying
   Wester, Brock
   Wei, Donglai
   Lichtman, Jeff W.
   Pfister, Hanspeter
   Beyer, Johanna
TI VIMO - Visual Analysis of Neuronal Connectivity Motifs
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Neurons; Visualization; Three-dimensional displays; Synapses;
   Morphology; Brain; Rendering (computer graphics); Visual motif analysis;
   Focus&Context; Scientific visualization; Neuroscience; Connectomics
ID EXPLORATION; TOOL
AB Recent advances in high-resolution connectomics provide researchers with access to accurate petascale reconstructions of neuronal circuits and brain networks for the first time. Neuroscientists are analyzing these networks to better understand information processing in the brain. In particular, scientists are interested in identifying specific small network motifs, i.e., repeating subgraphs of the larger brain network that are believed to be neuronal building blocks. Although such motifs are typically small (e.g., 2-6 neurons), the vast data sizes and intricate data complexity present significant challenges to the search and analysis process. To analyze these motifs, it is crucial to review instances of a motif in the brain network and then map the graph structure to detailed 3D reconstructions of the involved neurons and synapses. We present Vimo, an interactive visual approach to analyze neuronal motifs and motif chains in large brain networks. Experts can sketch network motifs intuitively in a visual interface and specify structural properties of the involved neurons and synapses to query large connectomics datasets. Motif instances (MIs) can be explored in high-resolution 3D renderings. To simplify the analysis of MIs, we designed a continuous focus&context metaphor inspired by visual abstractions. This allows users to transition from a highly-detailed rendering of the anatomical structure to views that emphasize the underlying motif structure and synaptic connectivity. Furthermore, Vimo supports the identification of motif chains where a motif is used repeatedly (e.g., 2-4 times) to form a larger network structure. We evaluate Vimo in a user study and an in-depth case study with seven domain experts on motifs in a large connectome of the fruit fly, including more than 21,000 neurons and 20 million synapses. We find that Vimo enables hypothesis generation and confirmation through fast analysis iterations and connectivity highlighting.
C1 [Troidl, Jakob; Warchol, Simon; Pfister, Hanspeter; Beyer, Johanna] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
   [Dhanyasi, Nagaraju; Wang, Xueying; Lichtman, Jeff W.] Harvard Univ, Dept Cellular& Mol Biol, Cambridge, MA USA.
   [Matelsky, Jordan; Wester, Brock] Johns Hopkins Univ, Appl Phys Lab, Baltimore, MD USA.
   [Matelsky, Jordan] Univ Penn, Dept Bioengn, Philadelphia, PA USA.
   [Choi, Jinhan; Wei, Donglai] Boston Coll, Dept Comp Sci, Chestnut Hill, MA USA.
C3 Harvard University; Harvard University; Johns Hopkins University; Johns
   Hopkins University Applied Physics Laboratory; University of
   Pennsylvania; Boston College
RP Troidl, J (corresponding author), Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
EM jakob.troidl@gmail.com; simonwarchol@g.harvard.edu; jinhan.choi@bc.edu;
   Jordan.Matelsky@jhuapl.edu; nagarajudhanyasi@gmail.com;
   snowsd@gmail.com; brock.wester@jhuapl.edu; donglai.wei@bc.edu;
   jeff@mcb.harvard.edu; pfister@g.harvard.edu; jbeyer@g.harvard.edu
RI Wang, Xueying/J-5350-2019
OI Warchol, Simon/0000-0001-9067-6888; Pfister,
   Hanspeter/0000-0002-3620-2582
FU NSF
FX No Statement Available
NR 62
TC 1
Z9 1
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 748
EP 758
DI 10.1109/TVCG.2023.3327388
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500039
PM 37883279
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Walchshofer, C
   Dhanoa, V
   Streit, M
   Meyer, M
AF Walchshofer, Conny
   Dhanoa, Vaishal
   Streit, Marc
   Meyer, Miriah
TI Transitioning to a Commercial Dashboarding System: Socio-Technical
   Observations and Opportunities
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Interview study; socio-technical challenges; visual analytics
ID VISUAL ANALYTICS; VISUALIZATION; DISCOVERY; DESIGN
AB Many long-established, traditional manufacturing businesses are becoming more digital and data-driven to improve their production. These companies are embracing visual analytics in these transitions through their adoption of commercial dashboarding systems. Although a number of studies have looked at the technical challenges of adopting these systems, very few have focused on the socio-technical issues that arise. In this paper, we report on the results of an interview study with 17 participants working in a range of roles at a long-established, traditional manufacturing company as they adopted Microsoft Power BI. The results highlight a number of socio-technical challenges the employees faced, including difficulties in training, using and creating dashboards, and transitioning to a modern digital company. Based on these results, we propose a number of opportunities for both companies and visualization researchers to improve these difficult transitions, as well as opportunities for rethinking how we design dashboarding systems for real-world use.
C1 [Walchshofer, Conny; Streit, Marc] Johannes Kepler Univ Linz, Linz, Austria.
   [Dhanoa, Vaishal] Pro 2 Future GmbH, Graz, Austria.
   [Meyer, Miriah] Linkoping Univ, Linkoping, Sweden.
C3 Johannes Kepler University Linz; Linkoping University
RP Walchshofer, C (corresponding author), Johannes Kepler Univ Linz, Linz, Austria.
EM conny.walchshofer@jku.at; vaishali.dhanoa@pro2future.at;
   marc.streit@jku.at; miriah.meyer@liu.se
OI Streit, Marc/0000-0001-9186-2092; Reis, Conny/0000-0003-3942-8445;
   Dhanoa, Vaishali/0000-0002-0493-8616
FU Wallenberg AI, Autonomous Systems and Software Program (WASP)
FX No Statement Available
NR 62
TC 0
Z9 0
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 381
EP 391
DI 10.1109/TVCG.2023.3326525
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500030
PM 37878440
OA hybrid
DA 2025-03-07
ER

PT J
AU Yan, L
   Liang, X
   Guo, HQ
   Wang, B
AF Yan, Lin
   Liang, Xin
   Guo, Hanqi
   Wang, Bei
TI TopoSZ: Preserving Topology in Error-Bounded Lossy Compression
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Lossy compression; contour tree; topology preservation; topological data
   analysis; topology in visualization
ID CONTOUR TREES; SCALAR FIELDS; SIMPLIFICATION; VISUALIZATION; SYMMETRY
AB Existing error-bounded lossy compression techniques control the pointwise error during compression to guarantee the integrity of the decompressed data. However, they typically do not explicitly preserve the topological features in data. When performing post hoc analysis with decompressed data using topological methods, preserving topology in the compression process to obtain topologically consistent and correct scientific insights is desirable. In this paper, we introduce TopoSZ, an error-bounded lossy compression method that preserves the topological features in 2D and 3D scalar fields. Specifically, we aim to preserve the types and locations of local extrema as well as the level set relations among critical points captured by contour trees in the decompressed data. The main idea is to derive topological constraints from contour-tree-induced segmentation from the data domain, and incorporate such constraints with a customized error-controlled quantization strategy from the SZ compressor (version 1.4). Our method allows users to control the pointwise error and the loss of topological features during the compression process with a global error bound and a persistence threshold.
C1 [Yan, Lin] Argonne Natl Lab, Lemont, IL 60439 USA.
   [Liang, Xin] Univ Kentucky, Lexington, KY USA.
   [Guo, Hanqi] Ohio State Univ, Columbus, OH USA.
   [Wang, Bei] Univ Utah, Salt Lake City, UT USA.
C3 United States Department of Energy (DOE); Argonne National Laboratory;
   University of Kentucky; University System of Ohio; Ohio State
   University; Utah System of Higher Education; University of Utah
RP Yan, L (corresponding author), Argonne Natl Lab, Lemont, IL 60439 USA.
EM lyan@anl.gov; xliang@uky.edu; guo.2154@osu.edu; beiwang@sci.utah.edu
RI Liang, Xin/LGY-5316-2024; Guo, Hanqi/AAL-1929-2021; Wang,
   Bei/ABH-7125-2022; Guo, Hanqi/ADW-4234-2022
OI Wang, Bei/0000-0002-9240-0700; Liang, Xin/0000-0002-0630-1600; Guo,
   Hanqi/0000-0001-7776-1834
FU DOE
FX No Statement Available
NR 68
TC 2
Z9 2
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 1302
EP 1312
DI 10.1109/TVCG.2023.3326920
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500003
PM 37930917
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ye, HY
   Li, CH
   Li, Y
   Wang, CB
AF Ye, Huayuan
   Li, Chenhui
   Li, Yang
   Wang, Changbo
TI InvVis: Large-Scale Data Embedding for Invertible Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Steganography; Image restoration; Visualization;
   Image reconstruction; Metadata; QR codes; Information visualization;
   information steganography; invertible visualization; invertible neural
   network
ID IMAGES; INFORMATION
AB We present InvVis, a new approach for invertible visualization, which is reconstructing or further modifying a visualization from an image. InvVis allows the embedding of a significant amount of data, such as chart data, chart information, source code, etc., into visualization images. The encoded image is perceptually indistinguishable from the original one. We propose a new method to efficiently express chart data in the form of images, enabling large-capacity data embedding. We also outline a model based on the invertible neural network to achieve high-quality data concealing and revealing. We explore and implement a variety of application scenarios of InvVis. Additionally, we conduct a series of evaluation experiments to assess our method from multiple perspectives, including data embedding quality, data restoration accuracy, data encoding capacity, etc. The result of our experiments demonstrates the great potential of InvVis in invertible visualization.
C1 [Ye, Huayuan; Li, Chenhui; Li, Yang; Wang, Changbo] East China Normal Univ, Sch Comp Sci & Technol, Shanghai, Peoples R China.
C3 East China Normal University
RP Li, CH (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai, Peoples R China.
EM huayuan221@gmail.com; chli@cs.ecnu.edu.cn; yli@cs.ecnu.edu.cn;
   cbwang@cs.ecnu.edu.cn
RI Li, Chenhui/AAR-3682-2020
OI Wang, Changbo/0000-0001-8940-6418; ye, huayuan/0009-0008-8208-2017; Li,
   Chenhui/0000-0001-9835-2650
FU NSFC
FX No Statement Available
NR 57
TC 3
Z9 3
U1 2
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 1139
EP 1149
DI 10.1109/TVCG.2023.3326597
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500106
PM 37871072
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Wurster, SW
   Guo, HQ
   Shen, HW
   Peterka, T
   Xu, JY
AF Wurster, Skylar W.
   Guo, Hanqi
   Shen, Han-Wei
   Peterka, Tom
   Xu, Jiayi
TI Deep Hierarchical Super Resolution for Scientific Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Spatial resolution; Rendering (computer graphics); Octrees; Data models;
   Computational modeling; Interpolation; Isosurfaces; Deep learning;
   super-resolution; hierarchical data
ID ADAPTIVE MESH REFINEMENT; SUPERRESOLUTION; VISUALIZATION; FRAMEWORK;
   EFFICIENT
AB We present a novel technique for hierarchical super resolution (SR) with neural networks (NNs), which upscales volumetric data represented with an octree data structure to a high-resolution uniform gridwith minimal seam artifacts on octree node boundaries. Our method uses existing state-of-the-art SR models and adds flexibility to upscale input data with varying levels of detail across the domain, instead of only uniform grid data that are supported in previous approaches.The key is to use a hierarchy of SR NNs, each trained to perform 2x SR between two levels of detail, with a hierarchical SR algorithm that minimizes seam artifacts by starting from the coarsest level of detail and working up.We show that our hierarchical approach outperforms baseline interpolation and hierarchical upscaling methods, and demonstrate the usefulness of our proposed approach across three use cases including data reduction using hierarchical downsampling+SR instead of uniform downsampling+SR, computation savings for hierarchical finite-time Lyapunov exponent field calculation, and super-resolving low-resolution simulation results for a high-resolution approximation visualization.
C1 [Wurster, Skylar W.; Guo, Hanqi; Shen, Han-Wei; Xu, Jiayi] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43201 USA.
   [Peterka, Tom] Argonne Natl Lab, Math & Comp Sci Div, Argonne, IL 60439 USA.
C3 University System of Ohio; Ohio State University; United States
   Department of Energy (DOE); Argonne National Laboratory
RP Wurster, SW (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43201 USA.
EM wurster.18@osu.edu; guo.2154@osu.edu; shen.94@osu.edu;
   tpeterka@mcs.anl.gov; xu.2205@buckeyemail.osu.edu
RI Shen, Han-wei/A-4710-2012; Guo, Hanqi/AAL-1929-2021; Guo,
   Hanqi/ADW-4234-2022
OI Xu, Jiayi/0000-0002-9091-6412; Shen, Han-Wei/0000-0002-1211-2320;
   Wurster, Skylar/0000-0001-6685-615X; Guo, Hanqi/0000-0001-7776-1834
FU US Department of Energy SciDAC program [DE-SC0021360]; National Science
   Foundation Division of Information and Intelligent Systems
   [IIS-1955764]; National Science Foundation Office of Advanced
   Cyberinfrastructure [OAC-2112606]; Advanced Scientific Computing
   Research, Office of Science, U.S. Department of Energy
   [DE-AC02-06CH11357]; U.S. Department of Energy (DOE) [DE-SC0021360]
   Funding Source: U.S. Department of Energy (DOE)
FX This work was supported in part by the US Department of Energy SciDAC
   program under Grant DE-SC0021360, in part by the National Science
   Foundation Division of Information and Intelligent Systems under Grant
   IIS-1955764, in part by the National Science Foundation Office of
   Advanced Cyberinfrastructure under Grant OAC-2112606, and in part by the
   Advanced Scientific Computing Research, Office of Science, U.S.
   Department of Energy, under Grant DE-AC02-06CH11357, program manager
   Margaret Lentz.
NR 64
TC 7
Z9 8
U1 5
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5483
EP 5495
DI 10.1109/TVCG.2022.3214420
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300046
PM 36251892
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Chang, BF
   Sun, GD
   Li, T
   Huang, HC
   Liang, RH
AF Chang, Baofeng
   Sun, Guodao
   Li, Tong
   Huang, Houchao
   Liang, Ronghua
TI MUSE: Visual Analysis of Musical Semantic Sequence
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Musical semantic sequence; semantic analysis; temporal sequence; feature
   extraction
ID VISUALIZATION
AB Visualization has the capacity of converting auditory perceptions of music into visual perceptions, which consequently opens the door to music visualization (e.g., exploring group style transitions and analyzing performance details). Current research either focuses on low-level analysis without constructing and comparing music group characteristics, or concentrates on high-level group analysis without analyzing and exploring detailed information. To fill this gap, integrating the high-level group analysis and low-level details exploration of music, we design a musical semantic sequence visualization analytics prototype system (MUSE) that mainly combines a distribution view and a semantic detail view, assisting analysts in obtaining the group characteristics and detailed interpretation. In the MUSE, we decompose the music into note sequences for modeling and abstracting music into three progressively fine-grained pieces of information (i.e., genres, instruments and notes). The distribution view integrates a new density contour, which considers sequence distance and semantic similarity, and helps analysts quickly identify the distribution features of the music group. The semantic detail view displays the music note sequences and combines the window moving to avoid visual clutter while ensuring the presentation of complete semantic details. To prove the usefulness and effectiveness of MUSE, we perform two case studies based on real-world music MIDI data. In addition, we conduct a quantitative user study and an expert evaluation.
C1 [Chang, Baofeng; Sun, Guodao; Li, Tong; Liang, Ronghua] Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Zhejiang, Peoples R China.
   [Huang, Houchao] Zhejiang Univ Technol, Coll Informat Engn, Hangzhou 310023, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology; Zhejiang University of Technology
RP Sun, GD (corresponding author), Zhejiang Univ Technol, Coll Comp Sci & Technol, Hangzhou 310023, Zhejiang, Peoples R China.
EM baofeng.chang@zjut.edu.cn; guodao@zjut.edu.cn; tongli@zjut.edu.cn;
   houchaohuang@zjut.edu.cn; rhliang@zjut.edu.cn
RI Sun, Guodao/AAN-4428-2021
OI Li, Tong/0000-0002-4642-9932; Chang, Baofeng/0000-0002-1028-716X
FU National Key Research and Development Program of China [2020YFB1707700];
   National Natural Science Foundation of China [61972356, 62036009]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFB1707700 and in part by
   the National Natural Science Foundation of China under Grants 61972356
   and 62036009.
NR 57
TC 2
Z9 2
U1 2
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2023
VL 29
IS 9
BP 4015
EP 4030
DI 10.1109/TVCG.2022.3175364
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA O2BA8
UT WOS:001041912300021
PM 35609098
DA 2025-03-07
ER

PT J
AU Li, YR
   Wang, JP
   Dai, X
   Wang, L
   Yeh, CCM
   Zheng, Y
   Zhang, W
   Ma, KL
AF Li, Yiran
   Wang, Junpeng
   Dai, Xin
   Wang, Liang
   Yeh, Chin-Chia Michael
   Zheng, Yan
   Zhang, Wei
   Ma, Kwan-Liu
TI How Does Attention Work in Vision Transformers? A Visual Analytics
   Attempt
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Head; Transformers; Visual analytics; Task analysis; Measurement;
   Heating systems; Deep learning; explainable artificial intelligence;
   multi-head self-attention; vision transformer; visual analytics
AB Vision transformer (ViT) expands the success of transformer models from sequential data to images. The model decomposes an image into many smaller patches and arranges them into a sequence. Multi-head self-attentions are then applied to the sequence to learn the attention between patches. Despite many successful interpretations of transformers on sequential data, little effort has been devoted to the interpretation of ViTs, and many questions remain unanswered. For example, among the numerous attention heads, which one is more important? How strong are individual patches attending to their spatial neighbors in different heads? What attention patterns have individual heads learned? In this work, we answer these questions through a visual analytics approach. Specifically, we first identify what heads are more important in ViTs by introducing multiple pruning-based metrics. Then, we profile the spatial distribution of attention strengths between patches inside individual heads, as well as the trend of attention strengths across attention layers. Third, using an autoencoder-based learning solution, we summarize all possible attention patterns that individual heads could learn. Examining the attention strengths and patterns of the important heads, we answer why they are important. Through concrete case studies with experienced deep learning experts on multiple ViTs, we validate the effectiveness of our solution that deepens the understanding of ViTs from head importance, head attention strength, and head attention pattern.
C1 [Li, Yiran; Ma, Kwan-Liu] Univ Calif Davis, Davis, CA 95616 USA.
   [Wang, Junpeng; Dai, Xin; Wang, Liang; Yeh, Chin-Chia Michael; Zheng, Yan; Zhang, Wei] Visa Res, Palo Alto, CA 94301 USA.
C3 University of California System; University of California Davis
RP Li, YR (corresponding author), Univ Calif Davis, Davis, CA 95616 USA.
EM ranli@ucdavis.edu; junpenwa@visa.com; xidai@visa.com; liawang@visa.com;
   miyeh@visa.com; yazheng@visa.com; wzhan@visa.com; klma@ucdavis.edu
RI Yeh, Michael/J-1738-2019; Zheng, Yanlong/ADS-5844-2022
OI Wang, Junpeng/0000-0002-1130-9914
FU National Institute of Health [1R01CA270454-01, 1R01CA273058-01]
FX This work was supported in part by the National Institute of Health
   under Grants 1R01CA270454-01 and 1R01CA273058-01.
NR 31
TC 9
Z9 9
U1 2
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2023
VL 29
IS 6
BP 2888
EP 2900
DI 10.1109/TVCG.2023.3261935
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F4DZ2
UT WOS:000981880500005
PM 37027263
OA Green Submitted, Green Accepted
DA 2025-03-07
ER

PT J
AU Yuan, N
   Wang, PH
   Meng, WL
   Chen, SM
   Xu, J
   Xin, SQ
   He, Y
   Wang, WP
AF Yuan, Na
   Wang, Peihui
   Meng, Wenlong
   Chen, Shuangmin
   Xu, Jian
   Xin, Shiqing
   He, Ying
   Wang, Wenping
TI A Variational Framework for Curve Shortening in Various Geometric
   Domains
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Heating systems; Three-dimensional displays; Point cloud compression;
   Shortest path problem; Costs; Approximation algorithms; Surface waves;
   Variational method; shortest paths; geodesics; least-cost paths
ID COMPUTATION; PATHS; GRAPH
AB Geodesics measure the shortest distance (either locally or globally) between two points on a curved surface and serve as a fundamental tool in digital geometry processing. Suppose that we have a parameterized path (gamma)(t) = x(u(t),v(t)) on a surface x = x(u,v) with gamma(0)=p and gamma(1)=q . We formulate the two-point geodesic problem into a minimization problem integral H-1(0)(?xu(u ')(t)+xv(v ')(t)?)dt , where H(s) satisfies H(0)=0,H '(s) > 0 and H ''(s) >= 0 for s > 0 . In our implementation, we choose H(s)=(e)s(2)-1 and show that it has several unique advantages over other choices such as H (s) = s(2) and H(s) = s. It is also a minimizer of the traditional geodesic length variational and able to guarantee the uniqueness and regularity in terms of curve parameterization. In the discrete setting, we construct the initial path by a sequence of moveable points {xi}(n)(i=1) and minimize n-expressionry sumexpressiontion H-n(i=1)(?x(i)-x(i+1)?) . The resulting points are evenly spaced along the path. It's obvious that our algorithm can deal with parametric surfaces. Considering that meshes, point clouds and implicit surfaces can be transformed into a signed distance function (SDF), we also discuss its implementation on a general SDF. Finally, we show that our method can be extended to solve a general least-cost path problem. We validate the proposed algorithm in terms of accuracy, performance and scalability, and demonstrate the advantages by extensive comparisons.
C1 [Yuan, Na; Meng, Wenlong; Xin, Shiqing] Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Shan Dong, Peoples R China.
   [Wang, Peihui] ArcSoft, Hangzhou 310012, Zhejiang, Peoples R China.
   [Chen, Shuangmin] Qingdao Univ Sci & Technol, Sch Informat & Technol, Qingdao 266101, Shandong, Peoples R China.
   [Xu, Jian] Chinese Acad Sci, Ningbo Inst Mat Technol & Engn, Beijing 100045, Peoples R China.
   [He, Ying] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Wang, Wenping] Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Shandong University; Qingdao University of Science & Technology; Chinese
   Academy of Sciences; Ningbo Institute of Materials Technology and
   Engineering, CAS; Nanyang Technological University; University of Hong
   Kong
RP Xin, SQ (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan 250100, Shan Dong, Peoples R China.
EM yuanna_sdu@163.com; peihui_wang@163.com; longwuya@163.com;
   csmqq@163.com; xujian@nimte.ac.cn; xinshiqing@sdu.edu.cn;
   yhe@ntu.edu.sg; wenping@cs.hku.hk
RI Wang, Pei-Hui/F-7682-2014; He, Ying/A-3708-2011
OI He, Ying/0000-0002-6749-4485; Xin, Shiqing/0000-0001-8452-8723
FU National Key R&D Program of China [2021YFB1715900]; National Natural
   Science Foundation of China [62002190, 62272277, 52075526, 91860204];
   NSF of Shandong Province [ZR2020MF036]; Singapore Ministry of Education
   [RG20/20, T2EP20220-0014]; Strategic Priority Research Program of the
   Chinese Academy of Sciences [XDA21010205]
FX This work was supported in part by the National Key R & D Program of
   China under Grant 2021YFB1715900, in part by the National Natural
   Science Foundation of China under Grants 62002190, 62272277, 52075526,
   and 91860204, in part by the NSF of Shandong Province under Grant
   ZR2020MF036, in part by the Singapore Ministry of Education under Grants
   RG20/20 and T2EP20220-0014, and in part by the Strategic Priority
   Research Program of the Chinese Academy of Sciences under Grant
   XDA21010205
NR 61
TC 2
Z9 3
U1 2
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2023
VL 29
IS 4
BP 1951
EP 1963
DI 10.1109/TVCG.2021.3135021
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D9DT5
UT WOS:000971666900004
PM 34905492
DA 2025-03-07
ER

PT J
AU Wang, SD
   Zeng, W
   Chen, X
   Ye, Y
   Qiao, Y
   Fu, CW
AF Wang, Shidong
   Zeng, Wei
   Chen, Xi
   Ye, Yu
   Qiao, Yu
   Fu, Chi-Wing
TI ActFloor-GAN: Activity-Guided Adversarial Networks for Human-Centric
   Floorplan Design
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Layout; Generative adversarial networks; Buildings; Predictive models;
   Computer architecture; Topology; Optimization; Floorplan design; room
   layout; human-centric; GAN
ID LAYOUT DESIGN; CONFIGURATION; REGISTRATION
AB We present a novel two-stage approach for automated floorplan design in residential buildings with a given exterior wall boundary. Our approach has the unique advantage of being human-centric, that is, the generated floorplans can be geometrically plausible, as well as topologically reasonable to enhance resident interaction with the environment. From the input boundary, we first synthesize a human-activity map that reflects both the spatial configuration and human-environment interaction in an architectural space. We propose to produce the human-activity map either automatically by a pre-trained generative adversarial network (GAN) model, or semi-automatically by synthesizing it with user manipulation of the furniture. Second, we feed the human-activity map into our deep framework ActFloor-GAN to guide a pixel-wise prediction of room types. We adopt a re-formulated cycle-consistency constraint in ActFloor-GAN to maximize the overall prediction performance, so that we can produce high-quality room layouts that are readily convertible to vectorized floorplans. Experimental results show several benefits of our approach. First, a quantitative comparison with prior methods shows superior performance of leveraging the human-activity map in predicting piecewise room types. Second, a subjective evaluation by architects shows that our results have compelling quality as professionally-designed floorplans and much better than those generated by existing methods in terms of the room layout topology. Last, our approach allows manipulating the furniture placement, considers the human activities in the environment, and enables the incorporation of user-design preferences.
C1 [Wang, Shidong; Chen, Xi; Qiao, Yu] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.
   [Wang, Shidong] Shandong Univ, Jinan 250100, Shandong, Peoples R China.
   [Zeng, Wei] Hong Kong Univ Sci & Technol, Guangzhou, Peoples R China.
   [Chen, Xi] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Ye, Yu] Tongji Univ, Shanghai, Peoples R China.
   [Fu, Chi-Wing] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology,
   CAS; Shandong University; Hong Kong University of Science & Technology;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Tongji University; Chinese University of Hong Kong
RP Zeng, W (corresponding author), Hong Kong Univ Sci & Technol, Guangzhou, Peoples R China.
EM sdwang96@gmail.com; weizeng@ust.hk; xi.chen2@siat.ac.cn;
   yye@tongji.edu.cn; yu.qiao@siat.ac.cn; cwfu@cse.cuhk.edu.hk
RI Fu, Chi-Wing/X-4703-2019; Qiao, Yu/ABD-5787-2021
OI Zeng, Wei/0000-0002-5600-8824; Wang, Shidong/0000-0003-2850-8319; Fu,
   Chi Wing/0000-0002-5238-593X
FU Fundamental Research Funds for the Central Universities [22120210540];
   Research Grants Council of the Hong Kong Special Administrative Region
   [CUHK 14206320]
FX This work was supported in part by the Fundamental Research Funds for
   the Central Universities under Grant 22120210540 and in part by the
   Research Grants Council of the Hong Kong Special Administrative Region
   under Grant CUHK 14206320.
NR 57
TC 12
Z9 12
U1 25
U2 76
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2023
VL 29
IS 3
BP 1610
EP 1624
DI 10.1109/TVCG.2021.3126478
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8N3OW
UT WOS:000925059900001
PM 34752396
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Bauer, D
   Wu, Q
   Ma, KL
AF Bauer, David
   Wu, Qi
   Ma, Kwan-Liu
TI FoVolNet: Fast Volume Rendering using Foveated Deep Neural Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Volume data; volume visualization; deep learning; foveated rendering;
   neural reconstruction
ID OCCLUSION SHADING MODEL; AMBIENT OCCLUSION; OPTICAL-FLOW
AB Volume data is found in many important scientific and engineering applications. Rendering this data for visualization at high quality and interactive rates for demanding applications such as virtual reality is still not easily achievable even using professional-grade hardware. We introduce FoVolNet-a method to significantly increase the performance of volume data visualization. We develop a cost-effective foveated rendering pipeline that sparsely samples a volume around a focal point and reconstructs the full-frame using a deep neural network. Foveated rendering is a technique that prioritizes rendering computations around the user's focal point. This approach leverages properties of the human visual system, thereby saving computational resources when rendering data in the periphery of the user's field of vision. Our reconstruction network combines direct and kernel prediction methods to produce fast, stable, and perceptually convincing output. With a slim design and the use of quantization, our method outperforms state-of-the-art neural reconstruction techniques in both end-to-end frame times and visual quality. We conduct extensive evaluations of the system's rendering performance, inference speed, and perceptual properties, and we provide comparisons to competing neural image reconstruction techniques. Our test results show that FoVolNet consistently achieves significant time saving over conventional rendering while preserving perceptual quality.
C1 [Bauer, David; Wu, Qi; Ma, Kwan-Liu] Univ Calif Davis, Davis, CA 95616 USA.
C3 University of California System; University of California Davis
RP Bauer, D (corresponding author), Univ Calif Davis, Davis, CA 95616 USA.
EM davbauer@ucdavis.edu; qadwu@ucdavis.edu; klma@ucdavis.edu
RI wu, qirui/GLU-4942-2022; Bauer, David/JDN-0139-2023
OI WU, Qi/0000-0003-0342-9366
FU Department of Energy [SC-DE0019486]; Intel's oneAPI Centers of
   Excellence grant
FX This research is sponsored in part by the Department of Energy through
   grant SC-DE0019486 and Intel's oneAPI Centers of Excellence grant.
NR 66
TC 11
Z9 13
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2023
VL 29
IS 1
BP 515
EP 525
DI 10.1109/TVCG.2022.3209498
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7F6YZ
UT WOS:000901991800007
PM 36155446
OA Green Submitted, Green Accepted
DA 2025-03-07
ER

PT J
AU Meinecke, C
   Wrisley, DJ
   Janicke, S
AF Meinecke, Christofer
   Wrisley, David Joseph
   Janicke, Stefan
TI Explaining Semi-Supervised Text Alignment Through Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Semantics; Task analysis; Visual analytics; Pipelines;
   Data visualization; Collaboration; Text alignment; word embeddings;
   human-in-the-loop; visualization in the humanities; professional reading
ID DIGITAL HUMANITIES; VISUAL ANALYTICS
AB The analysis of variance in complex text traditions is an arduous task when carried out manually. Text alignment algorithms provide domain experts with a robust alternative to such repetitive tasks. Existing white-box approaches allow the digital humanities to establish syntax-based metrics taking into account the spelling, morphology and order of words. However, they produce limited results, as semantic meanings are typically not taken into account. Our interdisciplinary collaboration between visualization and digital humanities combined a semi-supervised text alignment approach based on word embeddings that take not only syntactic but also semantic text features into account, thereby improving the overall quality of the alignment. In our collaboration, we developed different visual interfaces that communicate the word distribution in high-dimensional vector space generated by the underlying neural network for increased transparency, assessment of the tool's reliability and overall improved hypothesis generation. We further offer visual means to enable the expert reader to feed domain knowledge into the system at multiple levels with the aim of improving both the product and the process of text alignment. This ultimately illustrates how visualization can engage with and augment complex modes of reading in the humanities.
C1 [Meinecke, Christofer] Univ Leipzig, D-04109 Leipzig, Germany.
   [Wrisley, David Joseph] New York Univ Abu Dhabi, Abu Dhabi 25586, U Arab Emirates.
   [Janicke, Stefan] Univ Southern Denmark, DK-5230 Odense, Denmark.
C3 Leipzig University; New York University; New York University Abu Dhabi;
   University of Southern Denmark
RP Meinecke, C (corresponding author), Univ Leipzig, D-04109 Leipzig, Germany.
EM cmeinecke@informatik.uni-leipzig.de; djw12@nyu.edu;
   stjaenicke@imada.sdu.dk
OI Meinecke, Christofer/0000-0002-5637-9975; Wrisley, David
   Joseph/0000-0002-0355-1487
NR 68
TC 6
Z9 6
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4797
EP 4809
DI 10.1109/TVCG.2021.3105899
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400060
PM 34406941
DA 2025-03-07
ER

PT J
AU Shi, L
   Hu, JN
   Tan, ZH
   Tao, J
   Ding, JY
   Jin, Y
   Wu, YJ
   Thompson, PM
AF Shi, Lei
   Hu, Junnan
   Tan, Zhihao
   Tao, Jun
   Ding, Jiayan
   Jin, Yan
   Wu, Yanjun
   Thompson, Paul M.
TI MV<SUP>2</SUP>Net: Multi-Variate Multi-View Brain Network Comparison
   Over Uncertain Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Brain network; visual comparison; multivariate analysis
ID WHITE-MATTER INTEGRITY; ALZHEIMERS-DISEASE; FUNCTIONAL CONNECTIVITY;
   CORPUS-CALLOSUM; CLINICAL CORE; FIBER TRACTS; DIFFUSION; VISUALIZATION;
   DTI; PATTERNS
AB Visually identifying effective bio-markers from human brain networks poses non-trivial challenges to the field of data visualization and analysis. Existing methods in the literature and neuroscience practice are generally limited to the study of individual connectivity features in the brain (e.g., the strength of neural connection among brain regions). Pairwise comparisons between contrasting subject groups (e.g., the diseased and the healthy controls) are normally performed. The underlying neuroimaging and brain network construction process is assumed to have 100 percent fidelity. Yet, real-world user requirements on brain network visual comparison lean against these assumptions. In this work, we present MV(2)Net, a visual analytics system that tightly integrates multivariate multi-view visualization for brain network comparison with an interactive wrangling mechanism to deal with data uncertainty. On the analysis side, the system integrates multiple extraction methods on diffusion and geometric connectivity features of brain networks, an anomaly detection algorithm for data quality assessment, single- and multi-connection feature selection methods for biomarker detection. On the visualization side, novel designs are introduced which optimize network comparisons among contrasting subject groups and related connectivity features. Our design provides level-of-detail comparisons, from juxtaposed and explicit-coding views for subject group comparisons, to high-order composite view for correlation of network comparisons, and to fiber tract detail view for voxel-level comparisons. The proposed techniques are inspired and evaluated in expert studies, as well as through case analyses on diffusion and geometric bio-markers of certain neurology diseases. Results in these experiments demonstrate the effectiveness and superiority of MV(2)Net over state-of-the-art approaches.
C1 [Shi, Lei] Beihang Univ, Sch Comp Sci & Engn, Beijing 100083, Peoples R China.
   [Hu, Junnan; Tao, Jun] Sun Yat Sen Univ, Natl Supercomp Ctr, Sch Data & Comp Sci, Guangzhou 510275, Peoples R China.
   [Tan, Zhihao; Wu, Yanjun] Chinese Acad Sci, Inst Software, Beijing 100049, Peoples R China.
   [Ding, Jiayan] Tongji Univ, Coll Design & Innovat, Shanghai 200092, Peoples R China.
   [Jin, Yan] Univ Texas MD Anderson Canc Ctr, Dept Radiat Oncol, Houston, TX 77030 USA.
   [Thompson, Paul M.] Univ Southern Calif, Imaging Genet Ctr, Mark & Mary Stevens Inst Neuroimaging & Informat, Los Angeles, CA 90007 USA.
C3 Beihang University; Sun Yat Sen University; Chinese Academy of Sciences;
   Institute of Software, CAS; Tongji University; University of Texas
   System; UTMD Anderson Cancer Center; University of Southern California
RP Tao, J (corresponding author), Sun Yat Sen Univ, Natl Supercomp Ctr, Sch Data & Comp Sci, Guangzhou 510275, Peoples R China.; Wu, YJ (corresponding author), Chinese Acad Sci, Inst Software, Beijing 100049, Peoples R China.
EM leishi@buaa.edu.cn; hujn3@mail2.sysu.edu.cn;
   tanzhihao18@mails.ucas.edu.cn; taoj23@mail.sysu.edu.cn;
   dingjy@tongji.edu.cn; yjinz@ucla.edu; yanjun@iscas.ac.cn; pthomp@usc.edu
RI Thompson, Paul/C-4194-2018
OI Thompson, Paul/0000-0002-4720-8867
FU NSFC [61772504, 61902446]; Fundamental Research Funds for the Central
   Universities; SKLSDE
FX This work was supported in part by NSFC under Grants 61772504 and
   61902446, in part by the Fundamental Research Funds for the Central
   Universities, and in part by SKLSDE.
NR 81
TC 1
Z9 1
U1 1
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4640
EP 4657
DI 10.1109/TVCG.2021.3098123
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400049
PM 34283716
DA 2025-03-07
ER

PT J
AU Worrallo, AG
   Hartley, T
AF Worrallo, Adam Grant
   Hartley, Thomas
TI Robust Optical Based Hand Interaction for Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Sensors; Optical sensors; Tracking; Sensor systems; Data aggregation;
   Resists; Headphones; Gestures; hand interaction; optical trackers;
   virtual reality
ID SENSOR
AB Using optical sensors to track hand gestures in virtual reality (VR) simulations requires issues such as occlusion, field-of-view, accuracy and stability of sensors to be addressed or mitigated. We introduce an optical hand-based interaction system that comprises two Leap Motion sensors mounted onto a VR headset at different orientations. Our system collects sensor data from the leap motions, combines and processes it to produce optimal hand tracking data, that minimises the effect of sensor occlusion and noise. This contrasts with previous systems that do not use multiple head-mounted sensors or incorporate hand-data aggregation. We also present a study that compares the proposed system with glove-based and traditional motion controller-based interaction. We investigate hand interactions effect on the feeling of naturalness and immersion. The results show that the use of two head-mounted sensors and the data aggregation system increased the number of valid hands presented to the user and can be successfully applied to VR. The user study shows that there is a strong preference for the proposed system due to the natural feeling and freeing interaction. The absence of an indirect interface such as gloves or controllers was found to aid in creating a more natural and immersive experience.
C1 [Worrallo, Adam Grant; Hartley, Thomas] Univ Wolverhampton, Wolverhampton WV1 1LY, England.
C3 University of Wolverhampton
RP Worrallo, AG (corresponding author), Univ Wolverhampton, Wolverhampton WV1 1LY, England.
EM adam.worrallo@wlv.ac.uk; t.hartley2@wlv.ac.uk
NR 38
TC 4
Z9 4
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4186
EP 4197
DI 10.1109/TVCG.2021.3083411
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400017
PM 34033541
DA 2025-03-07
ER

PT J
AU Luong, T
   Holz, C
AF Luong, Tiffany
   Holz, Christian
TI Characterizing Physiological Responses to Fear, Frustration, and Insight
   in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 21st IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 17-22, 2022
CL Singapore, SINGAPORE
SP IEEE, IEEE Comp Soc, IEEE VGTC, ACM SIGGRAPH, Zoom, Qualcomm, Nvidia, Oppo, Advent2 Labs Consultat, Serl Io, Hiverlab
DE Virtual Reality; Affective Computing; Physiological Measures
ID ELEMENTS; EMOTION; BLINK
AB Physiological sensing often complements studies of human behavior in virtual reality (VR) to detect users' affective and cognitive states. Some psychological states, such as fear and frustration, can be particularly hard to differentiate from a physiological perspective as they are close in the arousal and valence emotional space. Moreover, it is largely unclear how users' physiological reactions are expressed in response to transient psychological states such as fear, frustration, and insight-especially since these are rich indicators for characterizing users' responses to dynamic systems but are hard to capture in highly interactive settings. We conducted a study (N = 24) to analyze participants' pulmonary, electrodermal, cardiac, and pupillary responses to moments of fear, frustration, and insight in immersive settings. Participants interacted in five VR environments, throughout which we measured their physiological reactions and analyzed the patterns we observed. We also measured subjective fear and frustration using questionnaires. We found differences between fear and frustration pupillary, respiratory, and electrodermal responses, as well as between the pupillary changes that followed fear in a horror game and those that followed fear in a vertigo experiment. We present the relationships between fear levels, frustration levels, and their physiological responses. To detect these affective events and states, we introduce user-independent binary classification models that achieved an average micro F1 score of 71% for detecting fear in a horror game, 75% for fear of vertigo, 76% for frustration, and 75% for insight, showing the promise for detecting these states from passive and objective signals.
C1 [Luong, Tiffany; Holz, Christian] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich
RP Luong, T (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM tiffany.luong@inf.ethz.ch; christian.holz@inf.ethz.ch
RI Holz, Christian/AAV-4925-2020
OI Holz, Christian/0000-0001-9655-9519
NR 64
TC 4
Z9 4
U1 2
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2022
VL 28
IS 11
BP 3917
EP 3927
DI 10.1109/TVCG.2022.3203113
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 5M3ZS
UT WOS:000871038200034
PM 36048988
DA 2025-03-07
ER

PT J
AU Sidenmark, L
   Parent, M
   Wu, CH
   Chan, J
   Glueck, M
   Wigdor, D
   Grossman, T
   Giordano, M
AF Sidenmark, Ludwig
   Parent, Mark
   Wu, Chi-Hao
   Chan, Joannes
   Glueck, Michael
   Wigdor, Daniel
   Grossman, Tovi
   Giordano, Marcello
TI Weighted Pointer: Error-aware Gaze-based Interaction through Fallback
   Modalities
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 21st IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 17-22, 2022
CL Singapore, SINGAPORE
SP IEEE
DE Eye tracking; Gaze interaction; Virtual Reality; Adaptive interfaces;
   Accessibility
ID EYE-GAZE; SELECTION
AB Gaze-based interaction is a fast and ergonomic type of hands-free interaction that is often used with augmented and virtual reality when pointing at targets. Such interaction, however, can be cumbersome whenever user, tracking, or environmental factors cause eye tracking errors. Recent research has suggested that fallback modalities could be leveraged to ensure stable interaction irrespective of the current level of eye tracking error. This work thus presents Weighted Pointer interaction, a collection of error-aware pointing techniques that determine whether pointing should be performed by gaze, a fallback modality, or a combination of the two, depending on the level of eye tracking error that is present. These techniques enable users to accurately point at targets when eye tracking is accurate and inaccurate. A virtual reality target selection study demonstrated that Weighted Pointer techniques were more performant and preferred over techniques that required the use of manual modality switching.
C1 [Sidenmark, Ludwig] Real Labs, Lancaster, England.
   [Sidenmark, Ludwig] Univ Lancaster, Lancaster, England.
   [Parent, Mark; Wu, Chi-Hao; Chan, Joannes; Glueck, Michael; Wigdor, Daniel; Giordano, Marcello] Real Labs Res, Lancaster, England.
   [Grossman, Tovi] Univ Toronto, Toronto, ON, Canada.
C3 Lancaster University; University of Toronto
RP Sidenmark, L (corresponding author), Real Labs, Lancaster, England.; Sidenmark, L (corresponding author), Univ Lancaster, Lancaster, England.
EM l.sidenmark@lancaster.ac.uk; mrkprnt@fb.com; eddywu@fb.com;
   joanneschan@fb.com; mglueck@fb.com; dwigdor@fb.com;
   tovi@dgp.toronto.edu; giordanoml@fb.com
RI Glueck, Michael/AAK-3015-2020; Wu, chihao/G-3512-2011
NR 80
TC 18
Z9 18
U1 5
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2022
VL 28
IS 11
BP 3585
EP 3595
DI 10.1109/TVCG.2022.3203096
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 5M3ZS
UT WOS:000871038200004
PM 36048981
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Wang, XY
   Ye, H
   Sandor, C
   Zhang, WZ
   Fu, HB
AF Wang, Xuanyu
   Ye, Hui
   Sandor, Christian
   Zhang, Weizhan
   Fu, Hongbo
TI Predict-and-Drive: Avatar Motion Adaption in Room-Scale Augmented
   Reality Telepresence with Heterogeneous Spaces
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 21st IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 17-22, 2022
CL Singapore, SINGAPORE
SP IEEE, IEEE Comp Soc, IEEE VGTC, ACM SIGGRAPH, Zoom, Qualcomm, Nvidia, Oppo, Advent2 Labs Consultat, Serl Io, Hiverlab
DE AR Telepresence; Avatar Motion Adaption; Heterogeneous Spaces;
   Redirected Walking
ID WALKING
AB Avatar-mediated symmetric Augmented Reality (AR) telepresence has emerged with the ability to empower users located in different remote spaces to interact with each other in 3D through avatars. However, different spaces have heterogeneous structures and features, which bring difficulties in synchronizing avatar motions with real user motions and adapting avatar motions to local scenes. To overcome these issues, existing methods generate mutual movable spaces or retarget the placement of avatars. However, these methods limit the telepresence experience in a small sub-area space, fix the positions of users and avatars, or adjust the beginning/ending positions of avatars without presenting smooth transitions. Moreover, the delay between the avatar retargeting and users' real transitions can break the semantic synchronization between users' verbal conversation and perceived avatar motion. In this paper, we first examine the impact of the aforementioned transition delay and explore the preferred transition style with the existence of such delay through user studies. With the results showing a significant negative effect of avatar transition delay and providing the design choice of the transition style, we propose a Predict-and-Drive controller to diminish the delay and present the smooth transition of the telepresence avatar. We also introduce a grouping component as an upgrade to immediately calculate a coarse virtual target once the user initiates a transition, which could further eliminate the avatar transition delay. Once having the coarse virtual target or an exactly predicted target, we find the corresponding target for the avatar according to the pre-constructed mapping of objects of interest between two spaces. The avatar control component maintains an artificial potential field of the space and drives the avatar towards the target while respecting the obstacles in the physical environment. We further conduct ablation studies to evaluate the effectiveness of our proposed components.
C1 [Wang, Xuanyu; Zhang, Weizhan] Xi An Jiao Tong Univ, Sch Comp Sci & Technol, MOEKLINNS Lab, Xian, Peoples R China.
   [Wang, Xuanyu; Ye, Hui; Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
   [Sandor, Christian] Univ Paris Saclay, CNRS, Lab Intetdisciplinaire Sci Numer LISN, Paris, France.
C3 Xi'an Jiaotong University; City University of Hong Kong; Universite
   Paris Saclay; Centre National de la Recherche Scientifique (CNRS)
RP Zhang, WZ (corresponding author), Xi An Jiao Tong Univ, Sch Comp Sci & Technol, MOEKLINNS Lab, Xian, Peoples R China.; Fu, HB (corresponding author), City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
EM xwang2247-c@my.cityu.edu.hk; hui.ye@cityu.edu.hk;
   chris.sandor@gmail.com; zhangwzh@xjtu.edu.cn; hongbofu@cityu.edu.hk
RI zhang, guoqing/GXG-4800-2022; Wang, Xuanyu/HGA-5924-2022
OI Ye, Hui/0000-0001-9539-9920; Zhang, Weizhan/0000-0003-0330-5435; Wang,
   Xuanyu/0000-0002-6040-6905; SANDOR, Christian/0000-0002-3990-2728
FU National Key Research and Development Program of China [2020AAA0108800];
   City University of Hong Kong [7005729, 9667234, 7005590, 9229094];
   National Natural Science Foundation of China [62172326, 62137002];
   Innovative Research Group of the National Natural Science Foundation of
   China [61721002]; Innovation Research Team of Ministry of Education [IRT
   17R86]; Project of China Knowledge Centre for Engineering Science and
   Technology; Project of Chinese academy of engineering "The Online and
   Offline Mixed Educational Service System for 'The Belt and Road'
   Training in MOOC China"; RGC Postdoc Fellowship Scheme
FX We thank the anonymous reviewers for their constructive comments, and
   the interview and user study participants for their time. This work was
   supported by grants from National Key Research and Development Program
   of China (No. 2020AAA0108800), the City University of Hong Kong (No.
   7005729, 9667234, 7005590, 9229094), the National Natural Science
   Foundation of China (No. 62172326 and 62137002), Innovative Research
   Group of the National Natural Science Foundation of China (No.
   61721002), Innovation Research Team of Ministry of Education (No. IRT
   17R86), Project of China Knowledge Centre for Engineering Science and
   Technology, Project of Chinese academy of engineering "The Online and
   Offline Mixed Educational Service System for `The Belt and Road'
   Training in MOOC China". Hui Ye was supported by the RGC Postdoc
   Fellowship Scheme.
NR 41
TC 9
Z9 9
U1 3
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2022
VL 28
IS 11
BP 3705
EP 3714
DI 10.1109/TVCG.2022.3203109
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 5M3ZS
UT WOS:000871038200015
PM 36049006
OA Green Published
DA 2025-03-07
ER

PT J
AU Sakano, Y
   Ando, H
AF Sakano, Yuichi
   Ando, Hiroshi
TI Conditions of a Multi-View 3D Display for Accurate Reproduction of
   Perceived Glossiness
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Two dimensional displays; Head; Crosstalk;
   Tracking; Visualization; Visual systems; Autostereoscopic display; head
   motion; stereopsis; material perception; perceptual validation; human
   factors
ID MOTION IN-DEPTH; 3-DIMENSIONAL SHAPE; STILL IMAGES; CROSS-TALK;
   PERCEPTION; ILLUMINATION; HIGHLIGHTS; INFORMATION; TELEVISION;
   SMOOTHNESS
AB Visualizing objects as they are perceived in the real world is often critical in our daily experiences. We previously focused on objects' surface glossiness visualized with a 3D display and found that a multi-view 3D display reproduces perceived glossiness more accurately than a 2D display. This improvement of glossiness reproduction can be explained by the fact that a glossy surface visualized by a multi-view 3D display appropriately provides luminance differences between the two eyes and luminance changes accompanying the viewer's lateral head motion. In the present study, to determine the requirements of a multi-view 3D display for the accurate reproduction of perceived glossiness, we developed a simulator of a multi-view 3D display to independently and simultaneously manipulate the viewpoint interval and the magnitude of the optical inter-view crosstalk. Using the simulator, we conducted a psychophysical experiment and found that glossiness reproduction is most accurate when the viewpoint interval is small and there is just a small (but not too small) amount of crosstalk. We proposed a simple yet perceptually valid model that quantitatively predicts the reproduction accuracy of perceived glossiness.
C1 [Sakano, Yuichi] Natl Inst Informat & Commun Technol, Ctr Informat & Neural Networks CiNet, Suita, Osaka 5650871, Japan.
   [Sakano, Yuichi] Osaka Univ, Suita, Osaka 5650871, Japan.
   [Ando, Hiroshi] Natl Inst Informat & Commun Technol, Ctr Informat & Neural Networks CiNet, Kyoto 6190289, Japan.
   [Ando, Hiroshi] Osaka Univ, Kyoto 6190289, Japan.
C3 National Institute of Information & Communications Technology (NICT) -
   Japan; Osaka University; National Institute of Information &
   Communications Technology (NICT) - Japan; Osaka University
RP Sakano, Y (corresponding author), Natl Inst Informat & Commun Technol, Ctr Informat & Neural Networks CiNet, Suita, Osaka 5650871, Japan.; Sakano, Y (corresponding author), Osaka Univ, Suita, Osaka 5650871, Japan.
EM yuichi@nict.go.jp; h-ando@nict.go.jp
RI Sakano, Yuichi/D-1955-2011; Ando, Hiroshi/A-8570-2015
OI Sakano, Yuichi/0000-0002-4106-6386
NR 95
TC 3
Z9 3
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2022
VL 28
IS 10
BP 3336
EP 3350
DI 10.1109/TVCG.2021.3063182
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4G5UL
UT WOS:000849261100002
PM 33651695
OA hybrid
DA 2025-03-07
ER

PT J
AU Bao, HJ
   Xie, WJ
   Qian, QH
   Chen, DP
   Zhai, SJ
   Wang, N
   Zhang, GF
AF Bao, Hujun
   Xie, Weijian
   Qian, Quanhao
   Chen, Danpeng
   Zhai, Shangjin
   Wang, Nan
   Zhang, Guofeng
TI Robust Tightly-Coupled Visual-Inertial Odometry with Pre-built Maps in
   High Latency Situations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Location awareness; Real-time systems; Visualization; Point cloud
   compression; Global Positioning System; Trajectory; Servers; Pre-built
   Map; VIO; Tightly-Coupled; High Latency
ID MONOCULAR SLAM; LARGE-SCALE; LOCALIZATION; LIDAR; OPTIMIZATION;
   VERSATILE; VISION; FUSION
AB In this paper, we present a novel monocular visual-inertial odometry system with pre-built maps deployed on the remote server, which can robustly run in real-time on a mobile device even in high latency situations. By tightly coupling VIO with geometric priors from pre-built maps, our system can tolerate the high latency and low frequency of global localization service, which is especially suitable for practical applications when the localization service is deployed on the remote server. Firstly, sparse point clouds are obtained from the dense mesh by the ray casting method according to the localization results. The dense mesh can be reconstructed from the point clouds generated by Structure-from-Motion. We directly use the sparse point clouds in feature tracking and state update to suppress drift. In the process of feature tracking, the high local accuracy of VIO is fully utilized to effectively remove outliers and make our system robust. The experiments on EurocMav datasets and simulation datasets show that compared with state-of-the-art methods, our method can achieve better results in terms of both precision and robustness. The effectiveness of the proposed method is further demonstrated through a real-time AR demo on a mobile phone with the aid of visual localization on the remote server.
C1 [Bao, Hujun; Xie, Weijian; Chen, Danpeng; Zhang, Guofeng] Zhejiang Univ, State Key Lab CADCG, Hangzhou, Zhejiang, Peoples R China.
   [Xie, Weijian; Qian, Quanhao; Chen, Danpeng; Zhai, Shangjin; Wang, Nan] SenseTime Res, Hangzhou, Zhejiang, Peoples R China.
   [Xie, Weijian; Qian, Quanhao; Chen, Danpeng; Zhai, Shangjin; Wang, Nan] Tetras AI, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Zhang, GF (corresponding author), Zhejiang Univ, State Key Lab CADCG, Hangzhou, Zhejiang, Peoples R China.
EM baohujun@zju.edu.cn; xieweijian@sensetime.com;
   qianquanhao1@sensetime.com; chendanpeng@tetras.ai;
   zhaishangjin@sensetime.com; wangnan@tetras.ai; zhangguofeng@zju.edu.cn
RI Zhang, Ge/K-9118-2019; wang, nan/AAS-6287-2021
FU NSF of China [61932003]
FX The authors would like to thank Youji Feng, Liyang Zhou, Fei Jiao,
   Mingxuan Jiang, Chongshan Sheng, Yuequ Cai for their kind help in the
   development of the global localization service and the real-time AR
   demo. This work was partially supported by NSF of China (No. 61932003).
NR 52
TC 6
Z9 7
U1 1
U2 31
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY 1
PY 2022
VL 28
IS 5
BP 2212
EP 2222
DI 10.1109/TVCG.2022.3150495
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1R1AK
UT WOS:000803110400033
PM 35167466
DA 2025-03-07
ER

PT J
AU Debarba, HG
   Chague, S
   Charbonnier, C
AF Debarba, Henrique Galvan
   Chague, Sylvain
   Charbonnier, Caecilia
TI On the Plausibility of Virtual Body Animation Features in Virtual
   Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Animation; Virtual environments; Avatars; Tracking; Solid modeling;
   Color; Self-representing avatar animation; character animation;
   plausibility illusion; presence; sense of control; agency
ID EMOTION CONTAGION; EMBODIMENT; APPEARANCE; PLACE; SENSE
AB We present two experiments to assess the relative impact of different levels of body animation fidelity on plausibility illusion (Psi). The first experiment presents a virtual character that is not controlled by the user (n=13), while the second experiment presents a user-controlled virtual avatar (n=24, all male). Psi concerns how realistic and coherent the events in a virtual environment look and feel and is part of Slater's proposition of two orthogonal components of presence in virtual reality (VR). In the experiments, the face, hands, upper and lower bodies of the character or self-avatar were manipulated to present different degrees of animation fidelity, such as no animation, procedural animation, and motion captured animation. Participants started the experiment experiencing the best animation configuration. Then, animation features were reduced to limit the amount of captured information made available to the system. Participants had to move from this basic animation configuration towards a more complete one, and declare when the avatar animation realism felt equivalent to the initial and most complete configuration, which could happen before all animation features were maxed out. Participants in the self-avatar experiment were also asked to rate how each animation feature affected their sense of control of the virtual body. We found that a virtual body with upper and lower body animated using eight tracked rigid bodies and inverse kinematics (IK) was often perceived as equivalent to a professional capture pipeline relying on 53 markers. Compared to what standard VR kits in the market are offering, i.e., a tracked headset and two hand controllers, we found that foot tracking, followed by mouth animation and finger tracking, were the features that added the most to the sense of control of a self-representing avatar. In addition, these features were often among the first to be improved in both experiments.
C1 [Debarba, Henrique Galvan] IT Univ Copenhagen, DK-2300 Copenhagen, Denmark.
   [Chague, Sylvain; Charbonnier, Caecilia] Artanim Fdn, CH-1217 Meyrin, Switzerland.
C3 IT University Copenhagen
RP Debarba, HG (corresponding author), IT Univ Copenhagen, DK-2300 Copenhagen, Denmark.
EM hgdebarba@gmail.com; sylvain.chague@artanim.ch;
   caecilia.charbonnier@artanim.ch
RI Galvan Debarba, Henrique/D-8081-2015; Charbonnier,
   Caecilia/HLV-7680-2023
OI Galvan Debarba, Henrique/0000-0003-2090-9409; Charbonnier,
   Caecilia/0000-0002-7018-885X
FU European Commission as part of the H2020 program [762111]; H2020 -
   Industrial Leadership [762111] Funding Source: H2020 - Industrial
   Leadership
FX The authors would like to thank the anonymous reviewers for their
   comments. They also thank Entropy Studio, for directing the cinematic
   capture used in the first experiment. This work was funded by the
   European Commission as part of the H2020 program, under the Grant
   agreement 762111, VRTogether.
NR 35
TC 10
Z9 10
U1 1
U2 30
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2022
VL 28
IS 4
BP 1880
EP 1893
DI 10.1109/TVCG.2020.3025175
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZH9CR
UT WOS:000761227900013
PM 32946397
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Seinfeld, S
   Feuchtner, T
   Pinzek, J
   Müller, J
AF Seinfeld, Sofia
   Feuchtner, Tiare
   Pinzek, Johannes
   Mueller, Joerg
TI Impact of Information Placement and User Representations in VR on
   Performance and Embodiment
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Task analysis; Virtual environments; Keyboards; Tools;
   Input devices; Computer science; Virtual reality; notifications;
   attention; near space; virtual representations; input devices
ID ILLUSORY OWNERSHIP; SPACE; ATTENTION
AB Human sensory processing is sensitive to the proximity of stimuli to the body. It is therefore plausible that these perceptual mechanisms also modulate the detectability of content in VR, depending on its location. We evaluate this in a user study and further explore the impact of the user's representation during interaction. We also analyze how embodiment and motor performance are influenced by these factors. In a dual-task paradigm, participants executed a motor task, either through virtual hands, virtual controllers, or a keyboard. Simultaneously, they detected visual stimuli appearing in different locations. We found that, while actively performing a motor task in the virtual environment, performance in detecting additional visual stimuli is higher when presented near the user's body. This effect is independent of how the user is represented and only occurs when the user is also engaged in a secondary task. We further found improved motor performance and increased embodiment when interacting through virtual tools and hands in VR, compared to interacting with a keyboard. This article contributes to better understanding the detectability of visual content in VR, depending on its location in the virtual environment, as well as the impact of different user representations on information processing, embodiment, and motor performance.
C1 [Seinfeld, Sofia; Pinzek, Johannes; Mueller, Joerg] Univ Bayreuth, Inst Comp Sci, D-95447 Bayreuth, Germany.
   [Feuchtner, Tiare] Aarhus Univ, Comp Sci Dept, DK-8200 Aarhus N, Denmark.
C3 University of Bayreuth; Aarhus University
EM sofia.seinfeld@uni-bayreuth.de; tiare.feuchtner@acm.org;
   s2jopinz@stmail.uni-bayreuth.de; joerg.mueller@uni-bayreuth.de
RI Seinfeld, Sofia/ABA-5769-2020
OI Seinfeld, Sofia/0000-0001-9649-0785; Feuchtner,
   Tiare/0000-0002-9922-5538
FU European Union [737087]; Innovation Fund Denmark (MADE Digital project,
   IFD Grant) [6151-00006B]
FX This research has received funding from the European Union's Horizon
   2020 research and innovation program under Grant agreement #737087
   (Levitate). This work was also supported in part by the Innovation Fund
   Denmark (MADE Digital project, IFD Grant no. 6151-00006B). The authors
   would like to thank Jan Milosch and Timm Seltmann for their help in
   conducting the study.
NR 49
TC 10
Z9 11
U1 0
U2 29
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2022
VL 28
IS 3
BP 1545
EP 1556
DI 10.1109/TVCG.2020.3021342
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA YP1EJ
UT WOS:000748371200007
PM 32877336
OA Green Submitted, hybrid
DA 2025-03-07
ER

PT J
AU Han, J
   Zheng, H
   Chen, D
   Wang, CL
AF Han, Jun
   Zheng, Hao
   Chen, Danny Z.
   Wang, Chaoli
TI STNet: An End-to-End Generative Framework for Synthesizing
   Spatiotemporal Super-Resolution Volumes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Time-varying data; generative adversarial network; spatiotemporal
   super-resolution
ID FLOW
AB We present STNet, an end-to-end generative framework that synthesizes spatiotemporal super-resolution volumes with high fidelity for time-varying data. STNet includes two modules: a generator and a spatiotemporal discriminator. The input to the generator is two low-resolution volumes at both ends, and the output is the intermediate and the two-ending spatiotemporal super-resolution volumes. The spatiotemporal discriminator, leveraging convolutional long short-term memory, accepts a spatiotemporal super-resolution sequence as input and predicts a conditional score for each volume based on its spatial (the volume itself) and temporal (the previous volumes) information. We propose an unsupervised pre-training stage using cycle loss to improve the generalization of STNet. Once trained, STNet can generate spatiotemporal super-resolution volumes from low-resolution ones, offering scientists an option to save data storage (i.e., sparsely sampling the simulation output in both spatial and temporal dimensions). We compare STNet with the baseline bicubic+linear interpolation, two deep learning solutions (SSR+TSR, STD), and a state-of-the-art tensor compression solution (TTHRESH) to show the effectiveness of STNet.
C1 [Han, Jun; Zheng, Hao; Chen, Danny Z.; Wang, Chaoli] Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
C3 University of Notre Dame
RP Han, J (corresponding author), Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
EM jhan5@nd.edu; hzheng3@nd.edu; dchen@nd.edu; chaoli.wang@nd.edu
RI Wang, Chaoli/AAJ-5173-2020; Han, Jun/AAR-8756-2021; zheng,
   hao/HHM-6949-2022
OI Zheng, Hao/0000-0002-9790-7607
FU U.S. National Science Foundation [IIS-1455886, CCF-1617735, CNS-1629914,
   DUE-1833129, IIS-1955395, IIS-2101696, OAC-2104158]
FX This research was supported in part by the U.S. National Science
   Foundation through grants IIS-1455886, CCF-1617735, CNS-1629914,
   DUE-1833129, IIS-1955395, IIS-2101696, and OAC-2104158. The authors
   would like to thank the anonymous reviewers for their insightful
   comments.
NR 65
TC 26
Z9 31
U1 0
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 270
EP 280
DI 10.1109/TVCG.2021.3114815
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000040
PM 34587051
DA 2025-03-07
ER

PT J
AU Park, H
   Das, N
   Duggal, R
   Wright, AP
   Shaikh, O
   Hohman, F
   Chau, DH
AF Park, Haekyu
   Das, Nilaksh
   Duggal, Rahul
   Wright, Austin P.
   Shaikh, Omar
   Hohman, Fred
   Chau, Duen Horng (Polo)
TI NeuroCartography: Scalable Automatic Visual Summarization of Concepts in
   Deep Neural Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Neurons; Dogs; Feature extraction; Visualization; Faces; Deep learning;
   Semantics; Deep learning interpretability; visual analytics; scalable
   summarization; neuron clustering; neuron embedding
ID OBJECT DETECTION
AB Existing research on making sense of deep neural networks often focuses on neuron-level interpretation, which may not adequately capture the bigger picture of how concepts are collectively encoded by multiple neurons. We present Neurocartography, an interactive system that scalably summarizes and visualizes concepts learned by neural networks. It automatically discovers and groups neurons that detect the same concepts, and describes how such neuron groups interact to form higher-level concepts and the subsequent predictions. Neurocartography introduces two scalable summarization techniques: (1) neuron clustering groups neurons based on the semantic similarity of the concepts detected by neurons (e.g., neurons detecting "dog faces" of different breeds are grouped); and (2) neuron embedding encodes the associations between related concepts based on how often they co-occur (e.g., neurons detecting "dog face" and "dog tail" are placed closer in the embedding space). Key to our scalable techniques is the ability to efficiently compute all neuron pairs' relationships, in time linear to the number of neurons instead of quadratic time. Neurocartography scales to large data, such as the ImageNet dataset with 1.2M images. The system's tightly coordinated views integrate the scalable techniques to visualize the concepts and their relationships, projecting the concept associations to a 2D space in Neuron Projection View, and summarizing neuron clusters and their relationships in Graph View. Through a large-scale human evaluation, we demonstrate that our technique discovers neuron groups that represent coherent, human-meaningful concepts. And through usage scenarios, we describe how our approaches enable interesting and surprising discoveries, such as concept cascades of related and isolated concepts. The Neurocartography visualization runs in modern browsers and is open-sourced.
C1 [Park, Haekyu; Das, Nilaksh; Duggal, Rahul; Wright, Austin P.; Shaikh, Omar; Hohman, Fred; Chau, Duen Horng (Polo)] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Hohman, Fred] Apple, Cupertino, CA USA.
C3 University System of Georgia; Georgia Institute of Technology; Apple Inc
RP Park, H (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM haekyu@gatech.edu; nilakshdas@gatech.edu; rahulduggal@gatech.edu;
   apwright@gatech.edu; oshaikh@gatech.edu; fredhohman@gatech.edu;
   polo@gatech.edu
RI Park, Haekyu/MEP-7630-2025; Das, Nilaksh/AHC-3955-2022
FU DARPA [HR00112030001]; NSF [IIS-1563816, CNS-1704701]
FX We thank Hannah Kim, the Georgia Tech Visualization Lab, and the
   anonymous reviewers for their support and constructive feedback. This
   work was supported in part by DARPA (HR00112030001), NSF grants
   IIS-1563816, CNS-1704701, and gifts from Intel, NVIDIA, Google.
NR 58
TC 9
Z9 11
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 813
EP 823
DI 10.1109/TVCG.2021.3114858
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000084
PM 34587079
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Tovanich, N
   Soulie, N
   Heulot, N
   Isenberg, P
AF Tovanich, Natkamon
   Soulie, Nicolas
   Heulot, Nicolas
   Isenberg, Petra
TI MiningVis: Visual Analytics of the Bitcoin Mining Economy
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Bitcoin; Tools; Economics; Data mining; Visual analytics; Hardware;
   Security; Visual analytics; Bitcoin; Bitcoin mining; mining pools; pool
   hopping
ID ENERGY
AB We present a visual analytics tool, MiningVis, to explore the long-term historical evolution and dynamics of the Bitcoin mining ecosystem. Bitcoin is a cryptocurrency that attracts much attention but remains difficult to understand. Particularly important to the success, stability, and security of Bitcoin is a component of the system called "mining." Miners are responsible for validating transactions and are incentivized to participate by the promise of a monetary reward. Mining pools have emerged as collectives of miners that ensure a more stable and predictable income. MiningVis aims to help analysts understand the evolution and dynamics of the Bitcoin mining ecosystem, including mining market statistics, multi-measure mining pool rankings, and pool hopping behavior. Each of these features can be compared to external data concerning pool characteristics and Bitcoin news. In order to assess the value of MiningVis, we conducted online interviews and insight-based user studies with Bitcoin miners. We describe research questions tackled and insights made by our participants and illustrate practical implications for visual analytics systems for Bitcoin mining.
C1 [Tovanich, Natkamon; Heulot, Nicolas] Paris Saclay, IRT SystemX, F-91120 Palaiseau, France.
   [Tovanich, Natkamon; Isenberg, Petra] Univ Paris Saclay, LISN, INRIA, CNRS, F-91405 Orsay, France.
   [Soulie, Nicolas] Univ Paris Saclay, Univ Evry, IMT BS, LITEM, F-91025 Evry, France.
C3 Universite Paris Saclay; Inria; Microsoft; Centre National de la
   Recherche Scientifique (CNRS); Universite Paris Saclay; IMT - Institut
   Mines-Telecom; Institut Mines-Telecom Business School; Universite Paris
   Saclay
RP Tovanich, N (corresponding author), Paris Saclay, IRT SystemX, F-91120 Palaiseau, France.
EM natkamon.tovanich@irt-systemx.fr; nicolas.soulie@imt-bs.eu;
   nicolas.heulot@irt-systemx.fr; petra.isenberg@inria.fr
RI Tovanich, Natkamon/IUM-4274-2023
OI Tovanich, Natkamon/0000-0001-9680-9282; Soulie,
   Nicolas/0000-0002-5045-3054
FU French Program Investissements d'Avenir
FX The authors wish to thank all participants in the user study, Catherine
   Plaisant for valuable feedback to improve our work and Teppakorn
   Thanuthanad for technical advice on developing the tool. This research
   work has been carried out under the leadership of the Institute for
   Technological Research SystemX, and therefore granted with public funds
   within the scope of the French Program Investissements d'Avenir.
NR 71
TC 10
Z9 13
U1 1
U2 33
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 868
EP 878
DI 10.1109/TVCG.2021.3114821
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000089
PM 34596542
OA Green Published
DA 2025-03-07
ER

PT J
AU Hansen, LH
   Fleck, P
   Stranner, M
   Schmalstieg, D
   Arth, C
AF Hansen, Lasse H.
   Fleck, Philipp
   Stranner, Marco
   Schmalstieg, Dieter
   Arth, Clemens
TI Augmented Reality for Subsurface Utility Engineering, Revisited
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Three-dimensional displays; Location awareness;
   Excavation; Global Positioning System; Daylighting; Surface
   reconstruction; Augmented Reality; Infrastructure; Computer Graphics;
   Localization
AB Civil engineering is a primary domain for new augmented reality technologies. In this work, the area of subsurface utility engineering is revisited, and new methods tackling well-known, yet unsolved problems are presented. We describe our solution to the outdoor localization problem, which is deemed one of the most critical issues in outdoor augmented reality, proposing a novel, lightweight hardware platform to generate highly accurate position and orientation estimates in a global context. Furthermore, we present new approaches to drastically improve realism of outdoor data visualizations. First, a novel method to replace physical spray markings by indistinguishable virtual counterparts is described. Second, the visualization of 3D reconstructions of real excavations is presented, fusing seamlessly with the view onto the real environment. We demonstrate the power of these new methods on a set of different outdoor scenarios.
C1 [Hansen, Lasse H.] Aalborg Univ, Aalborg, Denmark.
   [Fleck, Philipp; Stranner, Marco; Schmalstieg, Dieter] Graz Univ Technol, Graz, Austria.
   [Arth, Clemens] AR4 GmbH, Graz, Austria.
C3 Aalborg University; Graz University of Technology
RP Hansen, LH (corresponding author), Aalborg Univ, Aalborg, Denmark.
EM lhha@build.aau.dk; philipp.fleck@icg.tugraz.at;
   marco.stranner@icg.tugraz.at; dieter.schmalstieg@icg.tugraz.at;
   clemens@ar4.io
OI Arth, Clemens/0000-0001-6949-4713; Stranner, Marco/0000-0001-8175-9059;
   Hansen, Lasse Hedegaard/0000-0002-6476-1242
FU FFG [859208]
FX This work was supported in part by a grant from FFG (#859208).
NR 45
TC 17
Z9 20
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2021
VL 27
IS 11
BP 4119
EP 4128
DI 10.1109/TVCG.2021.3106479
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WN2ZT
UT WOS:000711642700006
PM 34449372
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Sugimoto, M
   Iwai, D
   Ishida, K
   Punpongsanon, P
   Sato, K
AF Sugimoto, Masatoki
   Iwai, Daisuke
   Ishida, Koki
   Punpongsanon, Parinya
   Sato, Kosuke
TI Directionally Decomposing Structured Light for Projector Calibration
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Calibration; Cameras; Lenses; Focusing; Apertures; Shape; Prototypes;
   Projector calibration; projection mapping; spatial augmented reality
ID NONRIGID SURFACE
AB Intrinsic projector calibration is essential in projection mapping (PM) applications, especially in dynamic PM. However, due to the shallow depth-of-field (DOF) of a projector, more work is needed to ensure accurate calibration. We aim to estimate the intrinsic parameters of a projector while avoiding the limitation of shallow DOF. As the core of our technique, we present a practical calibration device that requires a minimal working volume directly in front of the projector lens regardless of the projector's focusing distance and aperture size. The device consists of a flat-bed scanner and pinhole-array masks. For calibration, a projector projects a series of structured light patterns in the device. The pinholes directionally decompose the structured light, and only the projected rays that pass through the pinholes hit the scanner plane. For each pinhole, we extract a ray passing through the optical center of the projector. Consequently, we regard the projector as a pinhole projector that projects the extracted rays only, and we calibrate the projector by applying the standard camera calibration technique, which assumes a pinhole camera model. Using a proof-of-concept prototype, we demonstrate that our technique can calibrate projectors with different focusing distances and aperture sizes at the same accuracy as a conventional method. Finally, we confirm that our technique can provide intrinsic parameters accurate enough for a dynamic PM application, even when a projector is placed too far from a projection target for a conventional method to calibrate the projector using a fiducial object of reasonable size.
C1 [Sugimoto, Masatoki; Iwai, Daisuke; Ishida, Koki; Punpongsanon, Parinya; Sato, Kosuke] Osaka Univ, Grad Sch Engn Sci, Suita, Osaka, Japan.
   [Iwai, Daisuke] Japan Sci & Technol Agency, PRESTO, Kawaguchi, Saitama, Japan.
C3 Osaka University; Japan Science & Technology Agency (JST)
RP Sugimoto, M (corresponding author), Osaka Univ, Grad Sch Engn Sci, Suita, Osaka, Japan.
RI Iwai, Daisuke/R-8174-2019; PUNPONGSANON, PARINYA/B-4884-2013
OI Iwai, Daisuke/0000-0002-3493-5635
FU JST, PRESTO, Japan [JPMJPR19J2]
FX The authors would like to thank Anselm Grundhofer for valuable
   discussions. This work was supported by JST, PRESTO Grant Number
   JPMJPR19J2, Japan.
NR 35
TC 6
Z9 6
U1 6
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2021
VL 27
IS 11
BP 4161
EP 4170
DI 10.1109/TVCG.2021.3106511
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WN2ZT
UT WOS:000711642700010
PM 34449387
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Chen, J
   Ling, M
   Li, R
   Isenberg, P
   Isenberg, T
   Sedlmair, M
   Möller, T
   Laramee, RS
   Shen, HW
   Wünsche, K
   Wang, QR
AF Chen, Jian
   Ling, Meng
   Li, Rui
   Isenberg, Petra
   Isenberg, Tobias
   Sedlmair, Michael
   Moeller, Torsten
   Laramee, Robert S.
   Shen, Han-Wei
   Wuensche, Katharina
   Wang, Qiru
TI VIS30K: A Collection of Figures and Tables From IEEE Visualization
   Conference Publications
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Conferences; Metadata; Tools; Data
   mining; Electronic mail; Visualization; IEEE VIS; InfoVis; SciVis; VAST;
   dataset; bibliometrics; images; figures; tables
ID OF-THE-ART; INFORMATION; EXPLORATION; EXTRACTION
AB We present the VIS30K dataset, a collection of 29,689 images that represents 30 years of figures and tables from each track of the IEEE Visualization conference series (Vis, SciVis, InfoVis, VAST). VIS30K's comprehensive coverage of the scientific literature in visualization not only reflects the progress of the field but also enables researchers to study the evolution of the state-of-the-art and to find relevant work based on graphical content. We describe the dataset and our semi-automatic collection process, which couples convolutional neural networks (CNN) with curation. Extracting figures and tables semi-automatically allows us to verify that no images are overlooked or extracted erroneously. To improve quality further, we engaged in a peer-search process for high-quality figures from early IEEE Visualization papers. With the resulting data, we also contribute VISImageNavigator (VIN, visimagenavigator.github.io), a web-based tool that facilitates searching and exploring VIS30K by author names, paper keywords, title and abstract, and years.
C1 [Chen, Jian; Ling, Meng; Li, Rui; Shen, Han-Wei] Ohio State Univ, Columbus, OH 43210 USA.
   [Isenberg, Petra; Isenberg, Tobias] Univ Paris Saclay, CNRS, INRIA, LISN, F-91190 St Aubin, France.
   [Sedlmair, Michael] Univ Stuttgart, D-70174 Stuttgart, Germany.
   [Moeller, Torsten; Wuensche, Katharina] Univ Vienna, A-1010 Vienna, Austria.
   [Laramee, Robert S.; Wang, Qiru] Univ Nottingham, Nottingham NG7 2RD, England.
C3 University System of Ohio; Ohio State University; Centre National de la
   Recherche Scientifique (CNRS); Universite Paris Saclay; Inria;
   University of Stuttgart; University of Vienna; University of Nottingham
RP Chen, J (corresponding author), Ohio State Univ, Columbus, OH 43210 USA.
EM chen.8028@osu.edu; ling.253@osu.edu; li.8950@osu.edu;
   petra.isenberg@inria.fr; tobias.isenberg@inria.fr;
   michael.sedlmair@visus.uni-stuttgart.de; torsten.moeller@univie.ac.at;
   robert.laramee@nottingham.ac.uk; shen.94@osu.edu;
   katharina.wuenschel@univie.ac.at; qiru.wang@nottingham.ac.uk
RI jian, chan/ABG-5546-2021; Shen, Han-wei/A-4710-2012; Isenberg,
   Tobias/A-7575-2008
OI Moller, Torsten/0000-0003-1192-0710; LI, RUI/0000-0002-8166-9667; Wang,
   Qiru/0000-0003-3397-308X; Ling, Meng/0000-0001-6597-5448; Isenberg,
   Tobias/0000-0001-7953-8644; Chen, Jian/0000-0002-1599-0831; Isenberg,
   Petra/0000-0002-2948-6417
FU NSF [OAC-1945347, CNS-1531491, IIS-1302755]; NIST [MSE-10NANB12H181];
   FFG ICT of the Future program via the ViSciPub project [867378]; EPSRC
   [EP/S010238/1, EP/S010238/2] Funding Source: UKRI
FX The authors would like to thank Roger Crawfis for his hard copies of
   early conference proceedings and David H. Laidlaw for conference
   proceedings CDs. This work was partly supported by NSF OAC-1945347, NIST
   MSE-10NANB12H181, NSF CNS-1531491, NSF IIS-1302755 and the FFG ICT of
   the Future program via the ViSciPub project (no. 867378).
NR 44
TC 25
Z9 26
U1 1
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2021
VL 27
IS 9
BP 3826
EP 3833
DI 10.1109/TVCG.2021.3054916
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TS3CP
UT WOS:000679532000017
PM 33502982
OA Bronze, Green Submitted
DA 2025-03-07
ER

PT J
AU He, C
   Micallef, L
   He, LY
   Peddinti, G
   Aittokallio, T
   Jacucci, G
AF He, Chen
   Micallef, Luana
   He, Liye
   Peddinti, Gopal
   Aittokallio, Tero
   Jacucci, Giulio
TI Characterizing the Quality of Insight by Interactions: A Case Study
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Tools; Task analysis; Visualization; Market
   research; Pattern analysis; Cognitive science; Insight; interaction;
   interaction pattern; entity; visualization; insight-based evaluation
ID ANALYTIC PROVENANCE; DATA EXPLORATION; VISUALIZATION; INFORMATION;
   SENSEMAKING; USERS; COMPUTATION; KNOWLEDGE; FRAMEWORK
AB Understanding the quality of insight has become increasingly important with the trend of allowing users to post comments during visual exploration, yet approaches for qualifying insight are rare. This article presents a case study to investigate the possibility of characterizing the quality of insight via the interactions performed. To do this, we devised the interaction of a visualization tool-MediSyn-for insight generation. MediSyn supports five types of interactions: selecting, connecting, elaborating, exploring, and sharing. We evaluated MediSyn with 14 participants by allowing them to freely explore the data and generate insights. We then extracted seven interaction patterns from their interaction logs and correlated the patterns to four aspects of insight quality. The results show the possibility of qualifying insights via interactions. Among other findings, exploration actions can lead to unexpected insights; the drill-down pattern tends to increase the domain values of insights. A qualitative analysis shows that using domain knowledge to guide exploration can positively affect the domain value of derived insights. We discuss the study's implications, lessons learned, and future research opportunities.
C1 [He, Chen; Jacucci, Giulio] Univ Helsinki, Dept Comp Sci, Helsinki 00100, Finland.
   [Micallef, Luana] Univ Copenhagen, Human Ctr Comp, Dept Comp Sci, Copenhagen, Denmark.
   [He, Liye; Aittokallio, Tero] Univ Helsinki, Inst Mol Med Finland, SF-00100 Helsinki, Finland.
   [Peddinti, Gopal] VTT Tech Res Ctr Finland Oy, Espoo 02150, Finland.
C3 University of Helsinki; University of Copenhagen; University of
   Helsinki; VTT Technical Research Center Finland
RP He, C (corresponding author), Univ Helsinki, Dept Comp Sci, Helsinki 00100, Finland.
EM chen.he@helsinki.fi; liye.he@helsinki.fi; gopal.peddinti@vtt.fi;
   tero.aittokallio@helsinki.fi; giulio.jacucci@helsinki.fi
RI He, Liye/AAW-5928-2020; Jacucci, Giulio/G-8803-2011; Peddinti,
   Gopal/X-7223-2019; Peddinti, Gopal/G-4872-2016; Aittokallio,
   Tero/B-6583-2009; He, Liye/C-8843-2015
OI Peddinti, Gopal/0000-0002-8767-968X; He, Chen/0000-0003-2055-4468;
   Aittokallio, Tero/0000-0002-0886-9769; He, Liye/0000-0002-6632-2112
FU Academy of Finland [295504, 286440]; Academy of Finland (AKA) [286440,
   295504] Funding Source: Academy of Finland (AKA)
FX This work was supported by the Academy of Finland (Grants no. 295504,
   and 286440).
NR 70
TC 9
Z9 9
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2021
VL 27
IS 8
BP 3410
EP 3424
DI 10.1109/TVCG.2020.2977634
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TC7PK
UT WOS:000668831500005
PM 32142444
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Adhikary, J
   Vertanen, K
AF Adhikary, Jiban
   Vertanen, Keith
TI Text Entry in Virtual Environments using Speech and a Midair Keyboard
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Terms Text Entry; Speech Recognition; Virtual Reality (VR); Head Mounted
   Display (HMD); Midair Gestures
ID RECOGNITION
AB Entering text in virtual environments can be challenging, especially without auxiliary input devices. We investigate text input in virtual reality using hand-tracking and speech. Our system visualizes users" hands in the virtual environment, allowing typing on an auto-correcting midair keyboard. It also supports speaking a sentence and then correcting errors by selecting alternative words proposed by a speech recognizer. We conducted a user study in which participants wrote sentences with and without speech. Using only the keyboard. users wrote at 11 words-per-minute at a 1.2% error rate. Speaking and correcting sentences was faster and more accurate at 28 words-per-minute and a 0.5% error rate. Participants achieved this performance despite half of sentences containing an uncommon out-of-vocabulary word (e.g. proper name). For sentences with only in-vocabulary words, performance using speech and midair keyboard corrections was faster at 36 words-per-minute with a low 0.3% error rate.
C1 [Adhikary, Jiban; Vertanen, Keith] Michigan Technol Univ, Dept Comp Sci, Houghton, MI 49931 USA.
C3 Michigan Technological University
RP Adhikary, J (corresponding author), Michigan Technol Univ, Dept Comp Sci, Houghton, MI 49931 USA.
EM jiban@mtu.edu; vertanen@mtu.edu
RI Adhikary, Jiban Krishna/ACQ-9913-2022
OI Vertanen, Keith/0000-0002-7814-2450; Adhikary, Jiban
   Krishna/0000-0002-5471-5090
FU NSF [IIS-1750193]
FX This material is based upon work supported by the NSF under Grant No.
   IIS-1750193.
NR 57
TC 29
Z9 31
U1 0
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2021
VL 27
IS 5
BP 2648
EP 2658
DI 10.1109/TVCG.2021.3067776
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR5YU
UT WOS:000661120200014
PM 33750704
DA 2025-03-07
ER

PT J
AU Chai, SM
   Fu, XM
   Liu, LG
AF Chai, Shuangming
   Fu, Xiao-Ming
   Liu, Ligang
TI Voting for Distortion Points in Geometric Processing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Distortion; Mesh generation; Surface treatment; Topology; Manuals;
   Distortion measurement; Robustness; Distortion points;
   parameterizations; low isometric distortion
AB Low isometric distortion is often required for mesh parameterizations. A configuration of some vertices, where the distortion is concentrated, provides a way to mitigate isometric distortion, but determining the number and placement of these vertices is non-trivial. We call these vertices distortion points. We present a novel and automatic method to detect distortion points using a voting strategy. Our method integrates two components: candidate generation and candidate voting. Given a closed triangular mesh, we generate candidate distortion points by executing a three-step procedure repeatedly: (1) randomly cut an input to a disk topology; (2) compute a low conformal distortion parameterization; and (3) detect the distortion points. Finally, we count the candidate points and generate the final distortion points by voting. We demonstrate that our algorithm succeeds when employed on various closed meshes with a genus of zero or higher. The distortion points generated by our method are utilized in three applications, including planar parameterization, semi-automatic landmark correspondence, and isotropic remeshing. Compared to other state-of-the-art methods, our method demonstrates stronger practical robustness in distortion point detection.
C1 [Chai, Shuangming; Fu, Xiao-Ming; Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Fu, XM (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
EM kfckfckf@mail.ustc.edu.cn; fuxm@ustc.edu.cn; lgliu@ustc.edu.cn
RI Liu, Ligang/IZQ-5817-2023; Fu, Xiao-Ming/V-8253-2019; Chai,
   Shuangming/AAR-8926-2021
OI Chai, Shuangming/0000-0002-9475-7906; Fu, Xiao-Ming/0000-0001-8479-0107
FU National Natural Science Foundation of China [61802359, 61672482,
   11626253]; Anhui Provincial Natural Science Foundation [1808085QF208];
   Fundamental Research Funds for the Central Universities [WK0010460006,
   WK0010450004]
FX The authors would like to thank the anonymous reviewers for their
   constructive suggestions and comments. This work is supported by the
   National Natural Science Foundation of China (61802359, 61672482,
   11626253), the Anhui Provincial Natural Science Foundation
   (1808085QF208), and the Fundamental Research Funds for the Central
   Universities (WK0010460006, WK0010450004).
NR 56
TC 5
Z9 7
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2021
VL 27
IS 4
BP 2469
EP 2480
DI 10.1109/TVCG.2019.2947420
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO8XL
UT WOS:000623420400016
PM 31634132
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Itoh, Y
   Langlotz, T
   Zollmann, S
   Iwai, D
   Kiyoshi, K
   Amano, T
AF Itoh, Yuta
   Langlotz, Tobias
   Zollmann, Stefanie
   Iwai, Daisuke
   Kiyoshi, Kiyokawa
   Amano, Toshiyuki
TI Computational Phase-Modulated Eyeglasses
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Adaptive optics; Lenses; Cameras; Optical imaging; Optical modulation;
   Mirrors; Computational eyeglasses; augmented vision; phase modulation;
   LCoS; spatial light modulator
AB We present computational phase-modulated eyeglasses, a see-through optical system that modulates the view of the user using phase-only spatial light modulators (PSLM). A PSLM is a programmable reflective device that can selectively retardate, or delay, the incoming light rays. As a result, a PSLM works as a computational dynamic lens device. We demonstrate our computational phase-modulated eyeglasses with either a single PSLM or dual PSLMs and show that the concept can realize various optical operations including focus correction, bi-focus, image shift, and field of view manipulation, namely optical zoom. Compared to other programmable optics, computational phase-modulated eyeglasses have the advantage in terms of its versatility. In addition, we also presents some prototypical focus-loop applications where the lens is dynamically optimized based on distances of objects observed by a scene camera. We further discuss the implementation, applications but also discuss limitations of the current prototypes and remaining issues that need to be addressed in future research.
C1 [Itoh, Yuta] Tokyo Inst Technol, Tokyo 1528550, Japan.
   [Itoh, Yuta] RIKEN AIP, Tokyo 1030027, Japan.
   [Langlotz, Tobias; Zollmann, Stefanie] Univ Otago, Dunedin 9016, New Zealand.
   [Iwai, Daisuke] Osaka Univ, Osaka 5650871, Japan.
   [Kiyoshi, Kiyokawa] Nara Inst Sci & Technol, Nara 6300192, Japan.
   [Amano, Toshiyuki] Wakayama Univ, Fac Syst Engn, Wakayama 6408510, Japan.
C3 Institute of Science Tokyo; Tokyo Institute of Technology; RIKEN;
   University of Otago; Osaka University; Nara Institute of Science &
   Technology; Wakayama University
RP Itoh, Y (corresponding author), Tokyo Inst Technol, Tokyo 1528550, Japan.; Itoh, Y (corresponding author), RIKEN AIP, Tokyo 1030027, Japan.
EM yuta.itoh@c.titech.ac.jp; tobias.langlotz@otago.ac.nz;
   stefanie.zollmann@otago.ac.nz; daisuke.iwai@sys.es.osaka-u.ac.jp;
   kiyo@is.naist.jp; amano@sys.wakayama-u.ac.jp
RI Iwai, Daisuke/R-8174-2019; Langlotz, Tobias/LKN-4608-2024
OI Langlotz, Tobias/0000-0003-1275-2026; Iwai, Daisuke/0000-0002-3493-5635;
   Amano, Toshiyuki/0000-0003-4146-1375; Zollmann,
   Stefanie/0000-0002-4690-5409; Itoh, Yuta/0000-0002-5901-797X
FU Marsden Fund Council from Government funding [52421]; Catalyst Seed
   grant [52421]; JST PRESTO, Japan [JPMJPR17J2]
FX This work is partially supported by the Marsden Fund Council from
   Government funding and a Catalyst Seed grant, both administered by the
   Royal Society of NZ, and Callaghan Innovation, host of the Science for
   Technological Innovation National Science Challenge, Seed Project 52421,
   and by JST PRESTO Grant Number JPMJPR17J2, Japan.
NR 57
TC 7
Z9 7
U1 1
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2021
VL 27
IS 3
BP 1916
EP 1928
DI 10.1109/TVCG.2019.2947038
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QA9EF
UT WOS:000613744500004
PM 31613772
DA 2025-03-07
ER

PT J
AU Zhang, DB
   Lu, XQ
   Qin, H
   He, Y
AF Zhang, Dongbo
   Lu, Xuequan
   Qin, Hong
   He, Ying
TI Pointfilter: Point Cloud Filtering via Encoder-Decoder Modeling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Noise measurement; Machine learning;
   Robustness; Tuning; Network architecture; Filtering; Automatic point
   cloud filtering; deep learning; autoencoder; feature-preserving
   denoising
AB Point cloud filtering is a fundamental problem in geometry modeling and processing. Despite of significant advancement in recent years, the existing methods still suffer from two issues: 1) they are either designed without preserving sharp features or less robust in feature preservation; and 2) they usually have many parameters and require tedious parameter tuning. In this article, we propose a novel deep learning approach that automatically and robustly filters point clouds by removing noise and preserving their sharp features. Our point-wise learning architecture consists of an encoder and a decoder. The encoder directly takes points (a point and its neighbors) as input, and learns a latent representation vector which goes through the decoder to relate the ground-truth position with a displacement vector. The trained neural network can automatically generate a set of clean points from a noisy input. Extensive experiments show that our approach outperforms the state-of-the-art deep learning techniques in terms of both visual quality and quantitative error metrics. The source code and dataset can be found at https://github.com/dongbo-BUAA-VR/Pointfilter.
C1 [Zhang, Dongbo] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Lu, Xuequan] Deakin Univ, Sch Informat Technol, Geelong, Vic 3216, Australia.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [He, Ying] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 Beihang University; Deakin University; State University of New York
   (SUNY) System; Stony Brook University; Nanyang Technological University
RP Lu, XQ (corresponding author), Deakin Univ, Sch Informat Technol, Geelong, Vic 3216, Australia.
EM zhangdongbo9212@163.com; xuequan.lu@deakin.edu.au;
   qin@cs.stonybrook.edu; yhe@ntu.edu.sg
RI He, Ying/A-3708-2011
OI He, Ying/0000-0002-6749-4485; QIN, HONG/0000-0001-7699-1355; Lu,
   Xuequan/0000-0003-0959-408X
FU National Key R&D Program of China [2017YFF0106407]; Deakin University
   [CY01-251301-F003-PJ03906-PG00447]; National Natural Science Foundation
   of China [61532002]; National Science Foundation of USA [IIS-0949467,
   IIS-1047715, IIS-1812606, IIS-1715985, IIS-1049448]; AcRF;  [PJ06625]
FX The authors would like to thank the anonymous reviewers for their
   detailed and constructive comments. This research was supported in part
   by the National Key R&D Program of China under Grant No. 2017YFF0106407,
   Deakin University internal grant (CY01-251301-F003-PJ03906-PG00447) and
   industry Grant (PJ06625), AcRF 20/20, National Natural Science
   Foundation of China under Grant No. 61532002, and National Science
   Foundation of USA (NO. IIS-0949467, IIS-1047715, IIS-1812606,
   IIS-1715985, and IIS-1049448). They also thank Nvidia for donating a GPU
   (initial tests) and Chi Wang for discussions at the early stage of the
   project. Dongbo Zhang and Xuequan Lu are joint first author in thiswork.
NR 45
TC 83
Z9 92
U1 6
U2 65
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2021
VL 27
IS 3
BP 2015
EP 2027
DI 10.1109/TVCG.2020.3027069
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QA9EF
UT WOS:000613744500011
PM 32986553
OA Green Submitted, Bronze
DA 2025-03-07
ER

PT J
AU Dasu, K
   Ma, KL
   Ma, J
   Frazier, J
AF Dasu, Keshav
   Ma, Kwan-Liu
   Ma, Joyce
   Frazier, Jennifer
TI Sea of Genes: A Reflection on Visualising Metagenomic Data for Museums
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Narrative visualization; storytelling; animation; evaluation; user
   studies; informal learning environments
ID NARRATIVE VISUALIZATION; ANIMATIONS; ANNOTATION; TREE
AB We examine the process of designing an exhibit to communicate scientific findings from a complex dataset and unfamiliar domain to the public in a science museum. Our exhibit sought to communicate new lessons based on scientific findings from the domain of metagenomics. This multi-user exhibit had three goals: (1) to inform the public about microbial communities and their daily cycles; (2) to link microbes' activity to the concept of gene expression; (3) and to highlight scientists' use of gene expression data to understand the role of microbes. To address these three goals, we derived visualization designs with three corresponding stories, each corresponding to a goal. We present three successive rounds of design and evaluation of our attempts to convey these goals. We could successfully present one story but had limited success with our second and third goals. This work presents a detailed account of an attempt to explain tightly coupled relationships through storytelling and animation in a multi-user, informal learning environment to a public with varying prior knowledge on the domain and identify lessons for future design.
C1 [Dasu, Keshav; Ma, Kwan-Liu] Univ Calif Davis, Davis, CA 95616 USA.
   [Ma, Joyce; Frazier, Jennifer] Exploratorium, San Francisco, CA USA.
C3 University of California System; University of California Davis
RP Dasu, K (corresponding author), Univ Calif Davis, Davis, CA 95616 USA.
EM kdasu@ucdavis.edu; ma@ucdavis.edu; jma@exploratorium.edu;
   jfrazier@exploratorium.edu
FU U.S. National Science Foundation [DRL-1323214, DRL-1322828,
   IIS-1528203]; Gordon and Betty Moore Foundation
FX The authors wish to thank Elisha Wood-Charlson and Ed DeLong for sharing
   the SCOPE dataset and for their time and expertise. We would also like
   to thank Eric Rodenbeck, Nicolette Hayes, and Andrew Wong of Stamen
   Design for collaborating on the design and development of the
   prototypes, and Meghan Kroning, Tamara Kubacki, Katherine Nammacher, and
   Janine Penticuff for evaluation assistance. This research has been
   supported in part by the U.S. National Science Foundation through grants
   DRL-1323214, DRL-1322828, and IIS-1528203 and by the Gordon and Betty
   Moore Foundation. Any opinions, findings, and conclusions or
   recommendations expressed in this material are those of the authors and
   do not necessarily reflect the view of the Foundations.
NR 68
TC 6
Z9 8
U1 2
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 935
EP 945
DI 10.1109/TVCG.2020.3030412
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100078
PM 33108288
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kasica, S
   Berret, C
   Munzner, T
AF Kasica, Stephen
   Berret, Charles
   Munzner, Tamara
TI Table Scraps: An Actionable Framework for Multi-Table Data Wrangling
   From An Artifact Study of Computational Journalism
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Journalism; Taxonomy; Tools; Data analysis; Encoding; Task analysis;
   Cleaning; Computational journalism; Data journalism; Data wrangling
ID VISUALIZATION; PROVENANCE
AB For the many journalists who use data and computation to report the news, data wrangling is an integral part of their work. Despite an abundance of literature on data wrangling in the context of enterprise data analysis, little is known about the specific operations, processes, and pain points journalists encounter while performing this tedious, time-consuming task. To better understand the needs of this user group, we conduct a technical observation study of 50 public repositories of data and analysis code authored by 33 professional journalists at 26 news organizations. We develop two detailed and cross-cutting taxonomies of data wrangling in computational journalism, for actions and for processes. We observe the extensive use of multiple tables, a notable gap in previous wrangling analyses. We develop a concise, actionable framework for general multi-table data wrangling that includes wrangling operations documented in our taxonomy that are without clear parallels in other work. This framework, the first to incorporate tables as first-class objects, will support future interactive wrangling tools for both computational journalism and general-purpose use. We assess the generative and descriptive power of our framework through discussion of its relationship to our set of taxonomies.
C1 [Kasica, Stephen; Munzner, Tamara] Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.
   [Berret, Charles] Univ British Columbia, Sch Journalism Writing & Media, Vancouver, BC, Canada.
C3 University of British Columbia; University of British Columbia
RP Kasica, S (corresponding author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.
EM kasica@cs.ubc.ca; cberret@mail.ubc.ca; tmm@cs.ubc.ca
RI Munzner, Tamara/HKP-2536-2023
FU NSERC [CREATE 386138851, RGPIN-2014-06309]
FX We thank Jurgen Bernard, Anamaria Crisan, Madison Elliott, Zipeng Liu,
   Joanna McGrenere, Francis Nguyen, Michael Oppermann, and Ben Shneiderman
   for their formative feedback. This work was supported by NSERC CREATE
   386138851 and Discovery RGPIN-2014-06309.
NR 55
TC 10
Z9 13
U1 1
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 957
EP 966
DI 10.1109/TVCG.2020.3030462
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100080
PM 33074823
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ono, JP
   Castelo, S
   Lopez, R
   Bertini, E
   Freire, J
   Silva, C
AF Ono, Jorge Piazentin
   Castelo, Sonia
   Lopez, Roque
   Bertini, Enrico
   Freire, Juliana
   Silva, Claudio
TI <i>PipelineProfiler:</i> A Visual Analytics Tool for the Exploration of
   AutoML Pipelines
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Automatic Machine Learning; Pipeline Visualization; Model Evaluation
ID EFFICIENT
AB In recent years, a wide variety of automated machine learning (AutoML) methods have been proposed to generate end-to-end ML pipelines. While these techniques facilitate the creation of models, given their black-box nature, the complexity of the underlying algorithms, and the large number of pipelines they derive, they are difficult for developers to debug. It is also challenging for machine learning experts to select an AutoML system that is well suited for a given problem. In this paper, we present the Pipeline Profiler, an interactive visualization tool that allows the exploration and comparison of the solution space of machine learning (ML) pipelines produced by AutoML systems. PipelineProfiler is integrated with Jupyter Notebook and can be combined with common data science tools to enable a rich set of analyses of the ML pipelines, providing users a better understanding of the algorithms that generated them as well as insights into how they can be improved. We demonstrate the utility of our tool through use cases where PipelineProfiler is used to better understand and improve a real-world AutoML system. Furthermore, we validate our approach by presenting a detailed analysis of a think-aloud experiment with six data scientists who develop and evaluate AutoML tools.
C1 [Ono, Jorge Piazentin; Castelo, Sonia; Lopez, Roque; Bertini, Enrico; Freire, Juliana; Silva, Claudio] NYU, New York, NY 10003 USA.
C3 New York University
RP Ono, JP (corresponding author), NYU, New York, NY 10003 USA.
EM jorgehpo@nyu.edu; s.castelo@nyu.edu; rlopez@nyu.edu;
   enrico.bertini@nyu.edu; juliana.freire@nyu.edu; csilva@nyu.edu
RI Piazentin Ono, Jorge Henrique/ABF-2441-2021; Bertini,
   Enrico/ABG-1278-2020; Freire, Juliana/AAQ-4484-2020
OI Castelo Quispe, Sonia/0000-0001-6881-3006; Lopez,
   Roque/0000-0003-3484-1783
FU DARPA D3M program; NSF [CNS-1229185, CCF-1533564, CNS-1544753,
   CNS-1730396, CNS-1828576]
FX This work was partially supported by the DARPA D3M program and NSF
   awards CNS-1229185, CCF-1533564, CNS-1544753, CNS-1730396, and
   CNS-1828576. Any opinions, findings, and conclusions or recommendations
   expressed in this material are those of the authors and do not
   necessarily reflect the views of NSF and DARPA.
NR 66
TC 22
Z9 25
U1 1
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 390
EP 400
DI 10.1109/TVCG.2020.3030361
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100027
PM 33048694
DA 2025-03-07
ER

PT J
AU Chen, Z
   Panozzo, D
   Dumas, J
AF Chen, Zhen
   Panozzo, Daniele
   Dumas, Jeremie
TI Half-Space Power Diagrams and Discrete Surface Offsets
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Surface morphology; Shape; Three-dimensional displays; Two dimensional
   displays; Data structures; Computational modeling; Fabrication; Geometry
   processing; offset; voronoi diagram; power diagram; dexels; layered
   depth images
ID COMPUTATION; ALGORITHM; IMAGES
AB We present an efficient, trivially parallelizable algorithm to compute offset surfaces of shapes discretized using a dexel data structure. Our algorithm is based on a two-stage sweeping procedure that is simple to implement and efficient, entirely avoiding volumetric distance field computations typical of existing methods. Our construction is based on properties of half-space power diagrams, where each seed is only visible by a half-space, which were never used before for the computation of surface offsets. The primary application of our method is interactive modeling for digital fabrication. Our technique enables a user to interactively process high-resolution models. It is also useful in a plethora of other geometry processing tasks requiring fast, approximate offsets, such as topology optimization, collision detection, and skeleton extraction. We present experimental timings, comparisons with previous approaches, and provide a reference implementation in the supplemental material, which can be found on the Computer Society Digital Library at http://doi.ieeecomputersociety.org/10.1109/TVCG.2019.2945961.
C1 [Chen, Zhen] Univ Sci & Technol China, Math, Hefei 230027, Anhui, Peoples R China.
   [Panozzo, Daniele; Dumas, Jeremie] NYU, Comp Sci, Courant Inst Math Sci, New York, NY 10012 USA.
   [Dumas, Jeremie] Adobe Res, Seattle, WA 98103 USA.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; New York University; Adobe Systems Inc.
RP Dumas, J (corresponding author), NYU, Comp Sci, Courant Inst Math Sci, New York, NY 10012 USA.
EM csyzzkdcz@gmail.com; panozzo@nyu.edu; jeremie.dumas@ens-lyon.org
RI Chen, Zhen/AGY-9090-2022
OI Chen, Zhen/0000-0002-0012-4447; Panozzo, Daniele/0000-0003-1183-2454;
   Chen, Zhen/0000-0002-6766-9046; Dumas, Jeremie/0000-0001-7304-9882
FU NSF CAREER award [IIS-1652515]; NSF [OAC-1835712]
FX This work was supported in part by the NSF CAREER award IIS-1652515, the
   NSF grant OAC-1835712, a gift from Adobe, and a gift from nTopology.
NR 60
TC 0
Z9 0
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2020
VL 26
IS 10
BP 2970
EP 2981
DI 10.1109/TVCG.2019.2945961
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NJ9AP
UT WOS:000566336800004
PM 31613769
OA Green Submitted, Bronze
DA 2025-03-07
ER

PT J
AU Han, XG
   Hou, KC
   Du, D
   Qiu, YD
   Cui, SG
   Zhou, K
   Yu, YZ
AF Han, Xiaoguang
   Hou, Kangcheng
   Du, Dong
   Qiu, Yuda
   Cui, Shuguang
   Zhou, Kun
   Yu, Yizhou
TI CaricatureShop: Personalized and Photorealistic Caricature Sketching
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Photorealistic caricature; sketch-based face exaggeration; facial
   details enhancing
AB In this paper, we propose the first sketching system for interactively personalized and photorealistic face caricaturing. Input an image of a human face, the users can create caricature photos by manipulating its facial feature curves. Our system first performs exaggeration on the recovered 3D face model, which is conducted by assigning the laplacian of each vertex a scaling factor according to the edited sketches. The mapping between 2D sketches and the vertex-wise scaling field is constructed by a novel deep learning architecture. Our approach allows outputting different exaggerations when applying the same sketching on different input figures in term of their different geometric characteristics, which makes the generated results "personalized". With the obtained 3D caricature model, two images are generated, one obtained by applying 2D warping guided by the underlying 3D mesh deformation and the other obtained by re-rendering the deformed 3D textured model. These two images are then seamlessly integrated to produce our final output. Due to the severe stretching of meshes, the rendered texture is of blurry appearances. A deep learning approach is exploited to infer the missing details for enhancing these blurry regions. Moreover, a relighting operation is invented to further improve the photorealism of the result. These further make our results "photorealistic". The qualitative experiment results validated the efficiency of our sketching system.
C1 [Han, Xiaoguang; Qiu, Yuda; Cui, Shuguang] Shenzhen Res Institue Big Data, Shenzhen 518172, Peoples R China.
   [Han, Xiaoguang; Qiu, Yuda; Cui, Shuguang] Chinese Univ Hong Kong Shenzhen, Shenzhen 518172, Peoples R China.
   [Hou, Kangcheng; Zhou, Kun] Zhejiang Univ, Hangzhou 310058, Peoples R China.
   [Du, Dong] Univ Sci & Technol China, Shenzhen Res Inst Big Data, Hefei Shi 230022, Anhui Sheng, Peoples R China.
   [Yu, Yizhou] Univ Hong Kong, Pokfulam, Hong Kong, Peoples R China.
C3 The Chinese University of Hong Kong, Shenzhen; Zhejiang University;
   Shenzhen Research Institute of Big Data; Chinese Academy of Sciences;
   University of Science & Technology of China, CAS; University of Hong
   Kong
RP Han, XG (corresponding author), Shenzhen Res Institue Big Data, Shenzhen 518172, Peoples R China.; Han, XG (corresponding author), Chinese Univ Hong Kong Shenzhen, Shenzhen 518172, Peoples R China.
EM hanxiaoguang@cuhk.edu.cn; kangchenghou@gmail.com;
   dongdu@mail.ustc.edu.cn; 218012028@link.cuhk.edu.cn;
   shuguangcui@cuhk.edu.cn; kunzhou@acm.org; yizhouy@acm.org
RI Han, Xiaoguang/AFH-8561-2022; Du, Dong/AEG-5685-2022; Hou,
   Kangcheng/GRS-6097-2022; Zhou, Kun/ITT-3967-2023; Cui,
   Shuguang/D-4677-2014; /F-3345-2010
OI Cui, Shuguang/0000-0003-2608-775X; Du, Dong/0000-0001-5481-389X;
   /0000-0002-0470-5548; Han, Xiaoguang/0000-0003-0162-3296
FU Shenzhen Fundamental Research Fund [KQTD2015033114415450]; Hong Kong
   Research Grants Council under General Research Funds [HKU17206218]; The
   Pearl River Talent Recruitment Program Innovative and Entrepreneurial
   Teams in 2017 [2017ZT07X152]
FX This work was partially supported by Shenzhen Fundamental Research Fund
   under Grant No. KQTD2015033114415450, and "The Pearl River Talent
   Recruitment Program Innovative and Entrepreneurial Teams in 2017" under
   grant No. 2017ZT07X152, and Hong Kong Research Grants Council under
   General Research Funds (HKU17206218).
NR 45
TC 15
Z9 17
U1 0
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2020
VL 26
IS 7
BP 2349
EP 2361
DI 10.1109/TVCG.2018.2886007
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MB9QV
UT WOS:000542933100002
PM 30575537
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Weissker, T
   Bimberg, P
   Froehlich, B
AF Weissker, Tim
   Bimberg, Pauline
   Froehlich, Bernd
TI Getting There Together: Group Navigation in Distributed Virtual
   Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 27th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 22-26, 2020
CL Atlanta, GA
SP IEEE, IEEE Comp Soc
DE Navigation; Virtual environments; Collaboration; Task analysis;
   Teleportation; Avatars; Virtual Reality; Collaborative Virtual
   Environments; Remote Collaboration; Group Navigation; Teleportation;
   Jumping
ID REALITY
AB We analyzed the design space of group navigation tasks in distributed virtual environments and present a framework consisting of techniques to form groups, distribute responsibilities, navigate together, and eventually split up again. To improve joint navigation, our work focused on an extension of the Multi-Ray Jumping technique that allows adjusting the spatial formation of two distributed users as part of the target specification process. The results of a quantitative user study showed that these adjustments lead to significant improvements in joint two-user travel, which is evidenced by more efficient travel sequences and lower task loads imposed on the navigator and the passenger. In a qualitative expert review involving all four stages of group navigation, we confirmed the effective and efficient use of our technique in a more realistic use-case scenario and concluded that remote collaboration benefits from fluent transitions between individual and group navigation.
C1 [Weissker, Tim; Bimberg, Pauline; Froehlich, Bernd] Bauhaus Univ Weimar, Virtual Real & Visualizat Res Grp, Weimar, Germany.
C3 Bauhaus-Universitat Weimar
RP Weissker, T (corresponding author), Bauhaus Univ Weimar, Virtual Real & Visualizat Res Grp, Weimar, Germany.
EM tim.weissker@uni-weimar.de; clara.pauline.bimberg@uni-weimar.de;
   bernd.froehlich@uni-weimar.de
OI Weissker, Tim/0000-0001-9119-811X
FU European Unions Horizon 2020 Framework Programme for Research and
   Innovation under the Specific Grant Agreement [785907]; German Ministry
   of Education and Research (BMBF) [03PSIPT5A]
FX Our research has received funding from the European Unions Horizon 2020
   Framework Programme for Research and Innovation under the Specific Grant
   Agreement No. 785907 (Human Brain Project SGA2) and from the German
   Ministry of Education and Research (BMBF) under grant 03PSIPT5A
   (Provenance Analytics). We would like to thank the participants of our
   studies and the members of the Virtual Reality and Visualization
   Research Group at Bauhaus-Universita Weimar
   (http://www.uni-weimar.de/vr).
NR 53
TC 23
Z9 23
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2020
VL 26
IS 5
BP 1860
EP 1870
DI 10.1109/TVCG.2020.2973474
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA LA1WU
UT WOS:000523746000005
PM 32070981
DA 2025-03-07
ER

PT J
AU Jiang, HY
   Yan, DM
   Zhang, XP
   Wonka, P
AF Jiang, Haiyong
   Yan, Dong-Ming
   Zhang, Xiaopeng
   Wonka, Peter
TI Selection Expressions for Procedural Modeling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Grammar; Computational modeling; Windows; Adaptation models;
   Merging; Procedural modeling; building modeling; selections; grammars
AB We introduce a new approach for procedural modeling. Our main idea is to select shapes using selection-expressions instead of simple string matching used in current state-of-the-art grammars like CGA shape and CGA++. A selection-expression specifies how to select a potentially complex subset of shapes from a shape hierarchy, e.g., "select all tall windows in the second floor of the main building facade". This new way of modeling enables us to express modeling ideas in their global context rather than traditional rules that operate only locally. To facilitate selection-based procedural modeling we introduce the procedural modeling language SelEx. An important implication of our work is that enforcing important constraints, such as alignment and same size constraints can be done by construction. Therefore, our procedural descriptions can generate facade and building variations without violating alignment and sizing constraints that plague the current state of the art. While the procedural modeling of architecture is our main application domain, we also demonstrate that our approach nicely extends to other man-made objects.
C1 [Jiang, Haiyong; Yan, Dong-Ming; Zhang, Xiaopeng] Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
   [Jiang, Haiyong; Wonka, Peter] King Abdullah Univ Sci & Technol, Thuwal 239556900, Saudi Arabia.
   [Jiang, Haiyong; Yan, Dong-Ming; Zhang, Xiaopeng] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; King Abdullah
   University of Science & Technology; Chinese Academy of Sciences;
   University of Chinese Academy of Sciences, CAS
RP Yan, DM (corresponding author), Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.; Yan, DM (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
EM haiyong.jiang@nlpr.ia.ac.cn; yandongming@gmail.com;
   xiaopeng.zhang@ia.ac.cn; pwonka@gmail.com
FU Visual Computing Center (VCC) at KAUST through the CARF program;
   National Natural Science Foundation of China [61620106003, 61802362,
   61772523, 61331018]
FX We would like to thank Michael Schwarz for developing an initial version
   of the language and procedural modeling system with us in 2015/2016. He
   proposed the concepts of virtual, attached, and contained shapes and
   contributed to the development of the navigation-based selection and
   constraint handling. He also created Fig. 1 and suggested the term
   selection expression. We also had multiple helpful discussions with
   Peter Rautek and Liangliang Nan about SBML. Fuzhang Wu helped with the
   comparison to CGA. Further, we would like to acknowledge funding from
   the Visual Computing Center (VCC) at KAUST through the CARF program and
   the National Natural Science Foundation of China (61620106003, 61802362,
   61772523, and 61331018).
NR 32
TC 6
Z9 6
U1 1
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2020
VL 26
IS 4
BP 1775
EP 1788
DI 10.1109/TVCG.2018.2877614
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KU2OG
UT WOS:000519547200012
PM 30369446
DA 2025-03-07
ER

PT J
AU Eulzer, P
   Engelhardt, S
   Lichtenberg, N
   de Simone, R
   Lawonn, K
AF Eulzer, Pepe
   Engelhardt, Sandy
   Lichtenberg, Nils
   de Simone, Raffaele
   Lawonn, Kai
TI Temporal Views of Flattened Mitral Valve Geometries
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Mitral valve; quantification; medical visualization; parameterization;
   spatio-temporal visualization
ID 3-DIMENSIONAL TRANSESOPHAGEAL ECHOCARDIOGRAPHY; VISUALIZATION; FLOW;
   ANATOMY
AB The mitral valve, one of the four valves in the human heart, controls the bloodflow between the left atrium and ventricle and may suffer from various pathologies. Malfunctioning valves can be treated by reconstructive surgeries, which have to be carefully planned and evaluated. While current research focuses on the modeling and segmentation of the valve, we base our work on existing segmentations of patient-specific mitral valves, that are also time-resolved (3D+t) over the cardiac cycle. The interpretation of the data can be ambiguous, due to the complex surface of the valve and multiple time steps. We therefore propose a software prototype to analyze such 3D+t data, by extracting pathophysiological parameters and presenting them via dimensionally reduced visualizations. For this, we rely on an existing algorithm to unroll the convoluted valve surface towards a flattened 2D representation. In this paper, we show that the 3D+t data can be transferred to 3D or 2D representations in a way that allows the domain expert to faithfully grasp important aspects of the cardiac cycle. In this course, we not only consider common pathophysiological parameters, but also introduce new observations that are derived from landmarks within the segmentation model. Our analysis techniques were developed in collaboration with domain experts and a survey showed that the insights have the potential to support mitral valve diagnosis and the comparison of the pre- and post-operative condition of a patient.
C1 [Eulzer, Pepe; Lichtenberg, Nils; Lawonn, Kai] Univ Koblenz Landau, Mainz, Germany.
   [Engelhardt, Sandy] Mannheim Univ Appl Sci, Mannheim, Germany.
   [de Simone, Raffaele] Heidelberg Univ Hosp, Heidelberg, Germany.
C3 University of Koblenz & Landau; Ruprecht Karls University Heidelberg
RP Eulzer, P (corresponding author), Univ Koblenz Landau, Mainz, Germany.
EM eulzer@uni-koblenz.de; s.engelhardt@hs-mannheim.de;
   nlichtenberg@uni-koblenz.de; raffaele.de.simone@med.uni-heidelberg.de;
   lawonn@uni-koblenz.de
RI Eulzer, Pepe/GWC-0439-2022
OI Eulzer, Pepe/0000-0002-0161-5678
FU German Research Foundation (DFG) [398787259, EN1197/2-1, DE 2131/2-1, LA
   3855/1-1]
FX The work was supported by the German Research Foundation (DFG) project
   398787259, EN1197/2-1, DE 2131/2-1 and LA 3855/1-1.
NR 43
TC 10
Z9 10
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 971
EP 980
DI 10.1109/TVCG.2019.2934337
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100090
PM 31425104
DA 2025-03-07
ER

PT J
AU Jallepalli, A
   Levine, JA
   Kirby, RM
AF Jallepalli, Ashok
   Levine, Joshua A.
   Kirby, Robert M.
TI The Effect of Data Transformations on Scalar Field Topological Analysis
   of High-Order FEM Solutions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE High-Order Finite Element Methods; Filtering Techniques; Scalar Field
   Visualization; Topological Analysis
ID DISCONTINUOUS GALERKIN SOLUTIONS; CONSERVING SIAC FILTERS; COMPUTING
   CONTOUR TREES; STREAMLINE INTEGRATION; ACCURACY; COMPUTATION; ROBUST
AB High-order finite element methods (HO-FEM) are gaining popularity in the simulation community due to their success in solving complex flow dynamics. There is an increasing need to analyze the data produced as output by these simulations. Simultaneously, topological analysis tools are emerging as powerful methods for investigating simulation data. However, most of the current approaches to topological analysis have had limited application to HO-FEM simulation data for two reasons. First, the current topological tools are designed for linear data (polynomial degree one), but the polynomial degree of the data output by these simulations is typically higher (routinely up to polynomial degree six). Second, the simulation data and derived quantities of the simulation data have discontinuities at element boundaries, and these discontinuities do not match the input requirements for the topological tools. One solution to both issues is to transform the high-order data to achieve low-order, continuous inputs for topological analysis. Nevertheless, there has been little work evaluating the possible transformation choices and their downstream effect on the topological analysis. We perform an empirical study to evaluate two commonly used data transformation methodologies along with the recently introduced L-SIAC filter for processing high-order simulation data. Our results show diverse behaviors are possible. We offer some guidance about how best to consider a pipeline of topological analysis of HO-FEM simulations with the currently available implementations of topological analysis.
C1 [Jallepalli, Ashok; Kirby, Robert M.] Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.
   [Levine, Joshua A.] Univ Arizona, Dept Comp Sci, Tucson, AZ 85721 USA.
C3 Utah System of Higher Education; University of Utah; University of
   Arizona
RP Jallepalli, A (corresponding author), Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.
EM ashokj@sci.utah.edu; josh@email.arizona.edu; kirby@sci.utah.edu
OI Kirby, Robert/0000-0001-5712-4141
FU U.S. Department of Energy, Office of Science, Office of Advanced
   Scientific Computing Research [DE-SC-0019039]; ARO [W911NF-15-1-0222]
FX The authors also wish to thank Professor Spencer Sherwin (Imperial
   College London, UK), Mr. Alexandre Sidot, and the Nektar++ Group for the
   counter-rotating vortex data and helpful discussions. This material is
   based upon work supported by the U.S. Department of Energy, Office of
   Science, Office of Advanced Scientific Computing Research, under Award
   Number(s) DE-SC-0019039. The authors acknowledge support from ARO
   W911NF-15-1-0222 (Program Manager Dr. Mike Coyle).
NR 73
TC 2
Z9 3
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 162
EP 172
DI 10.1109/TVCG.2019.2934338
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100015
PM 31425105
OA Green Published, Green Submitted
DA 2025-03-07
ER

PT J
AU Jardine, N
   Ondov, BD
   Elmqvist, N
   Franconeri, S
AF Jardine, Nicole
   Ondov, Brian D.
   Elmqvist, Niklas
   Franconeri, Steven
TI The Perceptual Proxies of Visual Comparison
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Graphical perception; visual perception; visual comparison; crowdsourced
   evaluation
ID ATTENTION; FEATURES
AB Perceptual tasks in visualizations often involve comparisons. Of two sets of values depicted in two charts, which set had values that were the highest overall? Which had the widest range? Prior empirical work found that the performance on different visual comparison tasks (e.g., ;biggest delta;, ;biggest correlation;) varied widely across different combinations of marks and spatial arrangements. In this paper, we expand upon these combinations in an empirical evaluation of two new comparison tasks: the ;biggest mean; and ;biggest range; between two sets of values. We used a staircase procedure to titrate the difficulty of the data comparison to assess which arrangements produced the most precise comparisons for each task. We find visual comparisons of biggest mean and biggest range are supported by some chart arrangements more than others, and that this pattern is substantially different from the pattern for other tasks. To synthesize these dissonant findings, we argue that we must understand which features of a visualization are actually used by the human visual system to solve a given task. We call these perceptual proxies. For example, when comparing the means of two bar charts, the visual system might use a ;Mean length; proxy that isolates the actual lengths of the bars and then constructs a true average across these lengths. Alternatively, it might use a ;Hull Area; proxy that perceives an implied hull bounded by the bars of each chart and then compares the areas of these hulls. We propose a series of potential proxies across different tasks, marks, and spatial arrangements. Simple models of these proxies can be empirically evaluated for their explanatory power by matching their performance to human performance across these marks, arrangements, and tasks. We use this process to highlight candidates for perceptual proxies that might scale more broadly to explain performance in visual comparison.
C1 [Jardine, Nicole; Franconeri, Steven] Northwestern Univ, Evanston, IL 60208 USA.
   [Jardine, Nicole] Cook Cty Assessors Off, Chicago, IL USA.
   [Ondov, Brian D.] NIH, Bldg 10, Bethesda, MD 20892 USA.
   [Ondov, Brian D.; Elmqvist, Niklas] Univ Maryland, College Pk, MD 20742 USA.
C3 Northwestern University; National Institutes of Health (NIH) - USA;
   University System of Maryland; University of Maryland College Park
RP Jardine, N (corresponding author), Northwestern Univ, Evanston, IL 60208 USA.
EM njardine@cookcountyassessor.com; ondovb@umd.edu; elm@umd.edu;
   franconeri@northwestern.edu
OI Elmqvist, Niklas/0000-0001-5805-5301; Jardine,
   Nicole/0000-0001-5472-4029
FU U.S. National Science Foundation [DRL-1661264]; Intramural Research
   Program of the National Human Genome Research Institute, a part of the
   U.S. National Institutes of Health; Northwestern University
FX Nicole Jardine was supported by the U.S. National Science Foundation
   grant number DRL-1661264 while affiliated with Northwestern University.
   Brian Ondov was supported by the Intramural Research Program of the
   National Human Genome Research Institute, a part of the U.S. National
   Institutes of Health. Any opinions, findings, and conclusions or
   recommendations expressed in this material are those of the authors and
   do not necessarily reflect the views of the respective funding agencies.
NR 29
TC 26
Z9 32
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 1012
EP 1021
DI 10.1109/TVCG.2019.2934786
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA KB0CF
UT WOS:000506166100094
PM 31443016
DA 2025-03-07
ER

PT J
AU Krekhov, A
   Cmentowski, S
   Waschk, A
   Krüger, J
AF Krekhov, Andrey
   Cmentowski, Sebastian
   Waschk, Andre
   Kruger, Jens
TI Deadeye Visualization Revisited: Investigation of Preattentiveness and
   Applicability in Virtual Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Popout; virtual reality; preattentive vision; volume rendering;
   dichoptic presentation; binocular rivalry
ID VISUAL-SEARCH; QUANTITATIVE DEPTH; STEREO; PERCEPTION; ATTENTION;
   MOTION; MODEL; FORM; BINOCULARITY; CONSTRAINTS
AB Visualizations rely on highlighting to attract and guide our attention. To make an object of interest stand out independently from a number of distractors, the underlying visual cue, e.g., color, has to be preattentive. In our prior work, we introduced Deadeye as an instantly recognizable highlighting technique that works by rendering the target object for one eye only. In contrast to prior approaches, Deadeye excels by not modifying any visual properties of the target. However, in the case of 2D visualizations, the method requires an additional setup to allow dichoptic presentation, which is a considerable drawback. As a follow-up to requests from the community, this paper explores Deadeye as a highlighting technique for 3D visualizations, because such stereoscopic scenarios support dichoptic presentation out of the box. Deadeye suppresses binocular disparities for the target object, so we cannot assume the applicability of our technique as a given fact. With this motivation, the paper presents quantitative evaluations of Deadeye in VR, including configurations with multiple heterogeneous distractors as an important robustness challenge. After confirming the preserved preattentiveness (all average accuracies above 90%) under such real-world conditions, we explore VR volume rendering as an example application scenario for Deadeye. We depict a possible workflow for integrating our technique, conduct an exploratory survey to demonstrate benefits and limitations, and finally provide related design implications.
C1 [Krekhov, Andrey; Cmentowski, Sebastian; Waschk, Andre; Kruger, Jens] Univ Duisburg Essen, Ctr Visual Data Anal & Comp Graph COVIDAG, Duisburg, Germany.
   [Kruger, Jens] Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.
C3 University of Duisburg Essen; Utah System of Higher Education;
   University of Utah
RP Krekhov, A (corresponding author), Univ Duisburg Essen, Ctr Visual Data Anal & Comp Graph COVIDAG, Duisburg, Germany.
EM andrey.krekhov@uni-due.de; sebastian.cmentowski@uni-due.de;
   jens.krueger@uni-due.de; jens@sci.utah.edu
OI Cmentowski, Sebastian/0000-0003-4555-6187
FU NINR -National Institute of Nursing Research [R01NR014852]
FX We are immensely grateful to Christine Pickett for her comments that
   greatly improved the manuscript. We would also like to show our
   gratitude to the anonymous reviewers of the original Deadeye paper and
   the IEEE VIS community for motivating us to explore Deadeye in virtual
   reality environments. This research was made possible in part by Award
   Number R01NR014852 from the NINR -National Institute of Nursing
   Research.
NR 85
TC 12
Z9 16
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 547
EP 557
DI 10.1109/TVCG.2019.2934370
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA KB0CF
UT WOS:000506166100051
PM 31425106
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Spinner, T
   Schlegel, U
   Schäfer, H
   El-Assady, M
AF Spinner, Thilo
   Schlegel, Udo
   Schaefer, Hanna
   El-Assady, Mennatallah
TI explAIner: A Visual Analytics Framework for Interactive and Explainable
   Machine Learning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Explainable AI; Interactive Machine Learning; Deep Learning; Visual
   Analytics; Interpretability; Explainability
AB We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.
C1 [Spinner, Thilo; Schlegel, Udo; Schaefer, Hanna; El-Assady, Mennatallah] Univ Konstanz, Constance, Germany.
C3 University of Konstanz
RP Spinner, T (corresponding author), Univ Konstanz, Constance, Germany.
EM thiolo.spinner@uni-konstanz.de; udo.schlegel@uni-konstanz.de;
   hanna.schaefer@uni-konstanz.de; mennatallah.el-assady@uni-konstanz.de
RI Schlegel, Udo/AAI-9385-2021; Hauptmann, Hanna/R-3492-2016
OI El-Assady, Mennatallah/0000-0001-8526-2613; Spinner,
   Thilo/0000-0002-1168-1804; Hauptmann, Hanna/0000-0002-6840-5341
FU European Union's Horizon 2020 research and innovation programme [825041,
   826494]; H2020 - Industrial Leadership [825041] Funding Source: H2020 -
   Industrial Leadership; H2020 Societal Challenges Programme [826494]
   Funding Source: H2020 Societal Challenges Programme
FX This work has received funding from the European Union's Horizon 2020
   research and innovation programme under grant agreements No 825041 and
   No 826494.
NR 81
TC 147
Z9 165
U1 7
U2 93
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 1064
EP 1074
DI 10.1109/TVCG.2019.2934629
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100099
PM 31442998
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Wexler, J
   Pushkarna, M
   Bolukbasi, T
   Wattenberg, M
   Viégas, F
   Wilson, J
AF Wexler, James
   Pushkarna, Mahima
   Bolukbasi, Tolga
   Wattenberg, Martin
   Viegas, Fernanda
   Wilson, Jimbo
TI The What-If Tool: Interactive Probing of Machine Learning Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Interactive Machine Learning; Model Debugging; Model Comparison
AB A key challenge in developing and deploying Machine Learning (ML) systems is understanding their performance across a wide range of inputs. To address this challenge, we created the What-If Tool, an open-source application that allows practitioners to probe, visualize, and analyze ML systems, with minimal coding. The What-If Tool lets practitioners test performance in hypothetical situations, analyze the importance of different data features, and visualize model behavior across multiple models and subsets of input data. It also lets practitioners measure systems according to multiple ML fairness metrics. We describe the design of the tool, and report on real-life usage at different organizations.
C1 [Wexler, James; Pushkarna, Mahima; Bolukbasi, Tolga; Wattenberg, Martin; Viegas, Fernanda; Wilson, Jimbo] Google Res, Mountain View, CA 94043 USA.
C3 Google Incorporated
RP Wexler, J (corresponding author), Google Res, Mountain View, CA 94043 USA.
EM jwexler@google.com; mahimap@google.com; tolgb@google.com;
   wattenberg@google.com; viegas@google.com; jimbo@google.com
OI Wattenberg, Martin/0000-0003-0904-4862
NR 30
TC 281
Z9 313
U1 2
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 56
EP 65
DI 10.1109/TVCG.2019.2934619
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100006
PM 31442996
OA Green Submitted, hybrid
DA 2025-03-07
ER

PT J
AU Feng, M
   Peck, E
   Harrison, L
AF Feng, Mi
   Peck, Evan
   Harrison, Lane
TI Patterns and Pace: Quantifying Diverse Exploration Behavior with
   Visualizations on the Web
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Interaction; Visualization; Quantitative Evaluation
AB The diverse and vibrant ecosystem of interactive visualizations on the web presents an opportunity for researchers and practitioners to observe and analyze how everyday people interact with data visualizations. However, existing metrics of visualization interaction behavior used in research do not fully reveal the breadth of peoples' open-ended explorations with visualizations. One possible way to address this challenge is to determine high-level goals for visualization interaction metrics, and infer corresponding features from user interaction data that characterize different aspects of peoples' explorations of visualizations. In this paper, we identify needs for visualization behavior measurement, and develop corresponding candidate features that can be inferred from users' interaction data. We then propose metrics that capture novel aspects of peoples' open-ended explorations, including exploration uniqueness and exploration pacing. We evaluate these metrics along with four other metrics recently proposed in visualization literature by applying them to interaction data from prior visualization studies. The results of these evaluations suggest that these new metrics 1) reveal new characteristics of peoples' use of visualizations, 2) can be used to evaluate statistical differences between visualization designs, and 3) are statistically independent of prior metrics used in visualization research. We discuss implications of these results for future studies, including the potential for applying these metrics in visualization interaction analysis, as well as emerging challenges in developing and selecting metrics depicting visualization explorations.
C1 [Feng, Mi; Harrison, Lane] Worcester Polytech Inst, Worcester, MA 01609 USA.
   [Peck, Evan] Bucknell Univ, Lewisburg, PA 17837 USA.
C3 Worcester Polytechnic Institute; Bucknell University
RP Feng, M (corresponding author), Worcester Polytech Inst, Worcester, MA 01609 USA.
EM mfeng2@wpi.edu; evan.peck@bucknell.edu; lane@cs.wpi.edu
OI Harrison, Lane/0000-0003-3029-2799
NR 46
TC 20
Z9 23
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 501
EP 511
DI 10.1109/TVCG.2018.2865117
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000048
PM 30188824
DA 2025-03-07
ER

PT J
AU Gogolouis, A
   Tsandilas, T
   Palpanas, T
   Bezerianos, A
AF Gogolouis, Anna
   Tsandilas, Theophanis
   Palpanas, Themis
   Bezerianos, Anastasia
TI Comparing Similarity Perception in Time Series Visualizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Time series; similarity perception; automatic similarity search; line
   charts; horizon graphs; colorfields; evaluation
ID EPILEPTIFORM DISCHARGES; SPIKES
AB A common challenge faced by many domain experts working with time series data is how to identify and compare similar patterns. This operation is fundamental in high-level tasks, such as detecting recurring phenomena or creating clusters of similar temporal sequences. While automatic measures exist to compute time series similarity, human intervention is often required to visually inspect these automatically generated results. The visualization literature has examined similarity perception and its relation to automatic similarity measures for line charts, but has not yet considered if alternative visual representations, such as horizon graphs and colorfields, alter this perception. Motivated by how neuroscientists evaluate epileptiform patterns, we conducted two experiments that study how these three visualization techniques affect similarity perception in EEG signals. We seek to understand if the time series results returned from automatic similarity measures are perceived in a similar manner, irrespective of the visualization technique; and if what people perceive as similar with each visualization aligns with different automatic measures and their similarity constraints. Our findings indicate that horizon graphs align with similarity measures that allow local variations in temporal position or speed (i.e., dynamic time warping) more than the two other techniques. On the other hand, horizon graphs do not align with measures that are insensitive to amplitude and y-offset scaling (i.e., measures based on z-normalization), but the inverse seems to be the case for line charts and colorfields. Overall, our work indicates that the choice of visualization affects what temporal patterns we consider as similar, i.e., the notion of similarity in time series is not visualization independent.
C1 [Gogolouis, Anna; Tsandilas, Theophanis] Univ Paris Sud, INRIA, Orsay, France.
   [Gogolouis, Anna; Tsandilas, Theophanis; Bezerianos, Anastasia] Univ Paris Saclay, Paris, France.
   [Tsandilas, Theophanis] CNRS, Paris, France.
   [Palpanas, Themis] Univ Paris 05, Paris, France.
   [Bezerianos, Anastasia] Univ Paris Sud, Orsay, France.
   [Bezerianos, Anastasia] INRIA, CNRS, Paris, France.
C3 Inria; Universite Paris Saclay; Microsoft; Universite Paris Saclay;
   Centre National de la Recherche Scientifique (CNRS); Universite Paris
   Cite; Universite Paris Saclay; Inria; Centre National de la Recherche
   Scientifique (CNRS)
RP Gogolouis, A (corresponding author), Univ Paris Sud, INRIA, Orsay, France.; Gogolouis, A (corresponding author), Univ Paris Saclay, Paris, France.
EM anna.gogolou@inria.fr; fanis@lri.fr; themis@mi.parisdescartes.fr;
   anastasia.bezerianos@lri.fr
OI Bezerianos, Anastasia/0000-0002-7142-2548
NR 70
TC 49
Z9 58
U1 0
U2 37
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 523
EP 533
DI 10.1109/TVCG.2018.2865077
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000050
PM 30136982
OA Green Published, Green Submitted
DA 2025-03-07
ER

PT J
AU Zhang, MH
   Lin, HW
   Zhang, K
   Yu, JH
AF Zhang, Mohan
   Lin, Hongwei
   Zhang, Kang
   Yu, Jinhui
TI Computer Simulation and Generation of Moving Sand Pictures
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Moving sand picture; tessellation; segmentation; color arrangement;
   physical simulation
ID ANIMATING SAND; MODEL
AB Moving sand pictures are interesting devices that can be used to generate an infinite number of unique scenes when repeatedly being flipped over. However, little work has been done on attempting to simulate the process of picture formulation. In this paper, we present an approach capable of generating images in the style of moving sand pictures. Our system defines moving sand pictures in a few steps, such as initialization, segmentation and physical simulation, so that a variety of moving sand pictures including mountain ridges, desert, clouds and even regular patterns can be generated by either automatic or semi-automatic via interaction during initialization and segmentation. Potential applications of our approach range from advertisements, posters, post cards, packaging, to digital arts.
C1 [Zhang, Mohan; Yu, Jinhui] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
   [Lin, Hongwei] Zhejiang Univ, Sch Math Sci, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
   [Zhang, Kang] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA.
   [Yu, Jinhui] Harbin Finance Univ, Dept Comp Sci, Harbin 150030, Heilongjiang, Peoples R China.
C3 Zhejiang University; Zhejiang University; University of Texas System;
   University of Texas Dallas; Harbin Finance University
RP Zhang, MH (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310058, Zhejiang, Peoples R China.
EM 631964148@qq.com; hwlin@zju.edu.cn; kzhang@utdallas.edu;
   jhyu@cad.zju.edu.cn
OI Zhang, Kang/0000-0003-3802-7535; Lin, Hongwei/0000-0002-9337-9624
FU Natural Science Foundation of China [61772463, 61379069]; National Key
   RD Plan of China [2016YFB1001501]; Fundamental Research Funds for the
   Central Universities [2017XZZX009-03]
FX This work has been supported by the Natural Science Foundation of China
   (61772463 and 61379069), the National Key R&D Plan of China
   (2016YFB1001501), and the Fundamental Research Funds for the Central
   Universities (2017XZZX009-03).
NR 33
TC 3
Z9 3
U1 0
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2018
VL 24
IS 12
BP 3058
EP 3068
DI 10.1109/TVCG.2017.2779799
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GZ0TW
UT WOS:000449079000005
PM 29990197
DA 2025-03-07
ER

PT J
AU Merenda, C
   Kim, H
   Tanous, K
   Gabbard, JL
   Feichtl, B
   Misu, T
   Suga, C
AF Merenda, Coleman
   Kim, Hyungil
   Tanous, Kyle
   Gabbard, Joseph L.
   Feichtl, Blake
   Misu, Teruhisa
   Suga, Chihiro
TI Augmented Reality Interface Design Approaches for Goal-directed and
   Stimulus-driven Driving Tasks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 17th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 16-20, 2018
CL Munich, GERMANY
SP IEEE
DE Mixed-reality; augmented reality; driving; head-up displays
ID PERFORMANCE; BEHAVIOR; SYSTEMS
AB The automotive industry is rapidly developing new in-vehicle technologies that can provide drivers with information to aid awareness and promote quicker response times. Particularly, vehicles with augmented reality (AR) graphics delivered via head-up displays (HUDs) are nearing mainstream commercial feasibility and will be widely implemented over the next decade. Though AR graphics have been shown to provide tangible benefits to drivers in scenarios like forward collision warnings and navigation, they also create many new perceptual and sensory issues for drivers. For some time now, designers have focused on increasing the realism and quality of virtual graphics delivered via HUDs, and recently have begun testing more advanced 3D HUD systems that deliver volumetric spatial information to drivers. However, the realization of volumetric graphics adds further complexity to the design and delivery of AR cues, and moreover, parameters in this new design space must be clearly and operationally defined and explored. In this work, we present two user studies that examine how driver performance and visual attention are affected when using fixed and animated AR HUD interface design approaches in driving scenarios that require top-down and bottom-up cognitive processing. Results demonstrate that animated design approaches can produce some driving gains (e.g., in goal-directed navigation tasks) but often come at the cost of response time and distance. Our discussion yields AR HUD design recommendations and challenges some of the existing assumptions of world-fixed conformal graphic approaches to design.
C1 [Merenda, Coleman; Kim, Hyungil; Tanous, Kyle; Gabbard, Joseph L.; Feichtl, Blake] Virginia Tech, Blacksburg, VA USA.
   [Misu, Teruhisa; Suga, Chihiro] Honda Res Inst, Columbus, OH 43212 USA.
C3 Virginia Polytechnic Institute & State University; Honda Motor Company;
   Honda USA
RP Merenda, C (corresponding author), Virginia Tech, Blacksburg, VA USA.
EM cjm120@vt.edu; hci.kim@vt.edu; ktanous@vt.edu; jgabbard@vt.edu;
   blake96@vt.edu; tmisu@honda-ri.com; C-Suga@hra.com
RI Kim, Hyungil/K-6944-2019; Misu, Teruhisa/KCY-4391-2024
OI Gabbard, Joseph/0000-0002-7488-676X; Kim, Hyungil/0000-0001-9014-7484
NR 35
TC 48
Z9 52
U1 8
U2 116
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2018
VL 24
IS 11
BP 2875
EP 2885
DI 10.1109/TVCG.2018.2868531
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA GZ0TM
UT WOS:000449077900006
PM 30235134
DA 2025-03-07
ER

PT J
AU Wang, BB
   Holzschuch, N
AF Wang, Beibei
   Holzschuch, Nicolas
TI Point-Based Rendering for Homogeneous Participating Media with
   Refractive Boundaries
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Participating media; point-based rendering; interactive editing
ID SCATTERING
AB Illumination effects in translucent materials are a combination of several physical phenomena: refraction at the surface, absorption and scattering inside the material. Because refraction can focus light deep inside the material, where it will be scattered, practical illumination simulation inside translucent materials is difficult. In this paper, we present an a Point-Based Global Illumination method for light transport on homogeneous translucent materials with refractive boundaries. We start by placing light samples inside the translucent material and organizing them into a spatial hierarchy. At rendering, we gather light from these samples for each camera ray. We compute separately the sample contributions for single, double and multiple scattering, and add them. We present two implementations of our algorithm: an offline version for high-quality rendering and an interactive GPU implementation. The offline version provides significant speed-ups and reduced memory footprints compared to state-of-the-art algorithms, with no visible impact on quality. The GPU version yields interactive frame rates: 30 fps when moving the viewpoint, 25 fps when editing the light position or the material parameters.
C1 [Wang, Beibei] Nanjing Univ Sci & Technol, Nanjing 210094, Jiangsu, Peoples R China.
   [Wang, Beibei] Shandong Univ, Dept Comp Sci & Technol, Jinan 250101, Shandong, Peoples R China.
   [Holzschuch, Nicolas] Univ Grenoble Alpes, INRIA, CNRS, LJK, F-38400 Grenoble, France.
C3 Nanjing University of Science & Technology; Shandong University; Centre
   National de la Recherche Scientifique (CNRS); Communaute Universite
   Grenoble Alpes; Universite Grenoble Alpes (UGA); Inria
RP Wang, BB (corresponding author), Nanjing Univ Sci & Technol, Nanjing 210094, Jiangsu, Peoples R China.; Wang, BB (corresponding author), Shandong Univ, Dept Comp Sci & Technol, Jinan 250101, Shandong, Peoples R China.
EM bebei.wang@gmail.com; nicolas.holzschuch@inria.fr
RI Holzschuch, Nicolas/E-8861-2014
FU National Key R&D Program of China [2017YFB0203000]; National Natural
   Science Foundation of China [61472224, 61472225]; Natural Science
   Foundation of Jiangsu [BK20170857]; ANR [ANR-15-CE38-0005]; Agence
   Nationale de la Recherche (ANR) [ANR-15-CE38-0005] Funding Source:
   Agence Nationale de la Recherche (ANR)
FX We thank the reviewer for the valuable comments. We thank Huw Bowles and
   Cyril Crassin for their helping. This work has been partially supported
   by the National Key R&D Program of China under grant no. 2017YFB0203000,
   the National Natural Science Foundation of China under grant no.
   61472224, 61472225, the Natural Science Foundation of Jiangsu under
   grant no. BK20170857 and ANR project ANR-15-CE38-0005 "Materials".
NR 41
TC 5
Z9 7
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2018
VL 24
IS 10
BP 2743
EP 2757
DI 10.1109/TVCG.2017.2768525
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GS7PN
UT WOS:000443894900007
PM 29990082
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Guo, YL
   Liu, XP
   Xiong, C
   Xu, XM
   Fu, CW
AF Guo, Yulong
   Liu, Xiaopei
   Xiong, Chi
   Xu, Xuemiao
   Fu, Chi-Wing
TI Towards High-Quality Visualization of Superfluid Vortices
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Superfluid dynamics; vortex structure; visual analysis
ID VORTEX VISUALIZATION; HELIUM; TRANSPORT; TIME
AB Superfluidity is a special state of matter exhibiting macroscopic quantum phenomena and acting like a fluid with zero viscosity. In such a state, superfluid vortices exist as phase singularities of the model equation with unique distributions. This paper presents novel techniques to aid the visual understanding of superfluid vortices based on the state-of-the-art non-linear Klein-Gordon equation, which evolves a complex scalar field, giving rise to special vortex lattice/ring structures with dynamic vortex formation, reconnection, and Kelvin waves, etc. By formulating a numerical model with theoretical physicists in superfluid research, we obtain high-quality superfluid flow data sets without noise-like waves, suitable for vortex visualization. By further exploring superfluid vortex properties, we develop a new vortex identification and visualization method: a novel mechanism with velocity circulation to overcome phase singularity and an orthogonal-plane strategy to avoid ambiguity. Hence, our visualizations can help reveal various superfluid vortex structures and enable domain experts for related visual analysis, such as the steady vortex lattice/ring structures, dynamic vortex string interactions with reconnections and energy radiations, where the famous Kelvin waves and decaying vortex tangle were clearly observed. These visualizations have assisted physicists to verify the superfluid model, and further explore its dynamic behavior more intuitively.
C1 [Guo, Yulong; Liu, Xiaopei] ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 200031, Peoples R China.
   [Xiong, Chi] Nanyang Technol Univ, Inst Adv Studies, Singapore 639798, Singapore.
   [Xu, Xuemiao] South China Univ Technol, Dept Comp Sci & Engn, Guangzhou 510630, Guangdong, Peoples R China.
   [Fu, Chi-Wing] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Sha Tin, Hong Kong, Peoples R China.
C3 ShanghaiTech University; Nanyang Technological University; South China
   University of Technology; Chinese University of Hong Kong
RP Liu, XP (corresponding author), ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 200031, Peoples R China.
EM guoyl@shanghaitech.edu.cn; liuxp@shanghaitech.edu.cn;
   xiongchi@ntu.edu.sg; xuemx@scut.edu.cn; cwfu@cse.cuhk.edu.hk
RI Guo, Yulong/AFD-6338-2022; Fu, Chi-Wing/X-4703-2019; Xiong,
   Chi/AAF-9134-2021
OI xu, xuemiao/0000-0002-8006-3663; Fu, Chi Wing/0000-0002-5238-593X
FU National Science Foundation of China (NSFC)-Outstanding Youth Foundation
   [61502305]; ShanghaiTech University; Guangdong high-level personnel of
   special support program [2016TQ03X319]; CUHK [4055061]
FX The authors would like to thank all anonymous reviewers for their
   constructive comments. This work is supported by National Science
   Foundation of China (NSFC)-Outstanding Youth Foundation (Grant No.
   61502305), ShanghaiTech University startup funding, Guangdong high-level
   personnel of special support program (Grant No. 2016TQ03X319), and the
   CUHK strategic recruitment fund and direct grant (4055061).
NR 49
TC 5
Z9 6
U1 4
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2018
VL 24
IS 8
BP 2440
EP 2455
DI 10.1109/TVCG.2017.2719684
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GL6DJ
UT WOS:000437269000014
PM 28650819
OA Bronze, Green Submitted, Green Accepted
DA 2025-03-07
ER

PT J
AU Said, SH
   Tamaazousti, M
   Bartoli, A
AF Said, Souheil Hadj
   Tamaazousti, Mohamed
   Bartoli, Adrien
TI Image-Based Models for Specularity Propagation in Diminished Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Diminished reality; specularity; propagation; rendering; isocontours;
   brightest point; illumination variation
ID ILLUMINATION; REFLECTION
AB The aim of Diminished Reality (DR) is to remove a target object in a live video stream seamlessly. In our approach, the area of the target object is replaced with new texture that blends with the rest of the image. The result is then propagated to the next frames of the video. One of the important stages of this technique is to update the target region with respect to the illumination change. This is a complex and recurrent problem when the viewpoint changes. We show that the state-of-the-art in DR fails in solving this problem, even under simple scenarios. We then use local illumination models to address this problem. According to these models, the variation in illumination only affects the specular component of the image. In the context of DR, the problem is therefore solved by propagating the specularities in the target area. We list a set of structural properties of specularities which we incorporate in two new models for specularity propagation. Our first model includes the same property as the previous approaches, which is the smoothness of illumination variation, but has a different estimation method based on the Thin-Plate Spline. Our second model incorporates more properties of the specularity's shape on planar surfaces. Experimental results on synthetic and real data show that our strategy substantially improves the rendering quality compared to the state-of-the-art in DR.
C1 [Said, Souheil Hadj; Tamaazousti, Mohamed] CEA, LIST, F-91191 Gif Sur Yvette, France.
   [Said, Souheil Hadj] Inst Pascal, ENCOV Res Grp, F-63178 Clermont Ferrand, France.
   [Bartoli, Adrien] Inst Pascal, F-63178 Clermont Ferrand, France.
C3 Universite Paris Saclay; CEA
RP Said, SH (corresponding author), CEA, LIST, F-91191 Gif Sur Yvette, France.; Said, SH (corresponding author), Inst Pascal, ENCOV Res Grp, F-63178 Clermont Ferrand, France.
EM souheil.hadj-said@cea.fr; mohamed.tamaazousti@cea.fr;
   adrien.bartoli@gmail.com
FU EUs FP7 through the ERC research grant [307483 FLEXABLE]
FX This research has received funding from the EUs FP7 through the ERC
   research grant 307483 FLEXABLE.
NR 39
TC 2
Z9 2
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2018
VL 24
IS 7
BP 2140
EP 2152
DI 10.1109/TVCG.2017.2705687
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH3SG
UT WOS:000433321900007
PM 28541208
DA 2025-03-07
ER

PT J
AU Wang, YH
   Feng, K
   Chu, XW
   Zhang, J
   Fu, CW
   Sedlmair, M
   Yu, XH
   Chen, BQ
AF Wang, Yunhai
   Feng, Kang
   Chu, Xiaowei
   Zhang, Jian
   Fu, Chi-Wing
   Sedlmair, Michael
   Yu, Xiaohui
   Chen, Baoquan
TI A Perception-Driven Approach to Supervised Dimensionality Reduction for
   Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Dimensionality reduction; supervised; visual class separation;
   high-dimensional data
ID PROJECTION
AB Dimensionality reduction (DR) is a common strategy for visual analysis of labeled high-dimensional data. Low-dimensional representations of the data help, for instance, to explore the class separability and the spatial distribution of the data. Widely-used unsupervised DR methods like PCA do not aim to maximize the class separation, while supervised DR methods like LDA often assume certain spatial distributions and do not take perceptual capabilities of humans into account. These issues make them ineffective for complicated class structures. Towards filling this gap, we present a perception-driven linear dimensionality reduction approach that maximizes the perceived class separation in projections. Our approach builds on recent developments in perception-based separation measures that have achieved good results in imitating human perception. We extend these measures to be density-aware and incorporate them into a customized simulated annealing algorithm, which can rapidly generate a near optimal DR projection. We demonstrate the effectiveness of our approach by comparing it to state-of-the-art DR methods on 93 datasets, using both quantitative measure and human judgments. We also provide case studies with class-imbalanced and unlabeled data.
C1 [Wang, Yunhai; Chu, Xiaowei; Yu, Xiaohui; Chen, Baoquan] Shandong Univ, Sch Comp Sci & Technol, Jinan 250000, Shandong, Peoples R China.
   [Feng, Kang] Shandong Univ, IRC, Jinan, Shandong, Peoples R China.
   [Fu, Chi-Wing] Chinese Univ Hong Kong, Sha Tin, Hong Kong, Peoples R China.
   [Zhang, Jian] Chinese Acad Sci, CNIC, Beijing 100190, Peoples R China.
   [Zhang, Jian] Chinese Acad Sci, Supercomp Ctr, Comp Network Informat Ctr, Beijing, Peoples R China.
   [Sedlmair, Michael] Univ Vienna, A-1010 Vienna, Austria.
C3 Shandong University; Shandong University; Chinese University of Hong
   Kong; Chinese Academy of Sciences; Chinese Academy of Sciences; Computer
   Network Information Center, CAS; University of Vienna
RP Wang, YH (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan 250000, Shandong, Peoples R China.
EM cloudseawang@gmail.com; falcon6130@gmail.com; cuxiaoxie@gmail.com;
   zhangjian@sccas.cn; philip.chiwing.fu@gmail.com;
   michael.sedlmair@univie.ac.at; xhyu05@gmail.com; baoquan.chen@gmail.com
RI Fu, Chi-Wing/X-4703-2019; Yu, Hui/JED-7628-2023
OI Yu, Xiaohui/0000-0001-8170-2327; Sedlmair, Michael/0000-0001-7048-9292;
   Fu, Chi Wing/0000-0002-5238-593X
FU NSFC-Guangdong Joint Fund [U1501255]; NSFC [61379091, 91630204];
   National Key Research & Development Plan of China [2016YFB1001404];
   Shandong Provincial Natural Science Foundation [2016ZRE27617];
   Fundamental Research Funds of Shandong University
FX This work is supported by the grants of NSFC-Guangdong Joint Fund
   (U1501255), NSFC (61379091, 91630204), the National Key Research &
   Development Plan of China (2016YFB1001404), Shandong Provincial Natural
   Science Foundation (2016ZRE27617) and the Fundamental Research Funds of
   Shandong University. Yunhai Wang and Kang Feng are joint first authors.
NR 49
TC 36
Z9 42
U1 2
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2018
VL 24
IS 5
BP 1828
EP 1840
DI 10.1109/TVCG.2017.2701829
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE7EW
UT WOS:000431397100011
PM 28489538
DA 2025-03-07
ER

PT J
AU Luo, BC
   Xu, F
   Richardt, C
   Yong, JH
AF Luo, Bicheng
   Xu, Feng
   Richardt, Christian
   Yong, Jun-Hai
TI Parallax360: Stereoscopic 360° Scene Representation for Head-Motion
   Parallax
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 25th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 18-22, 2018
CL Reutlingen, GERMANY
SP IEEE, IEEE Comp Soc, IEEE Comp Soc Visualizat & Graph Tech Comm, VICON, Digital Project, ART, Haption, MiddleVR, VR ON, VISCON, BARCO, WorldViz, Disney Res, Chinese Acad Sci, Comp Network Informat Ctr, KUKA
DE 360 degrees scene capture; scene representation; head-motion parallax; 6
   degrees-of-freedom (6-DoF); image-based rendering
AB We propose a novel 360 degrees scene representation for converting real scenes into stereoscopic 3D virtual reality content with head-motion parallax. Our image-based scene representation enables efficient synthesis of novel views with six degrees-of-freedom (6-DoF) by fusing motion fields at two scales: (1) disparity motion fields carry implicit depth information and are robustly estimated from multiple laterally displaced auxiliary viewpoints, and (2) pairwise motion fields enable real-time flow-based blending, which improves the visual fidelity of results by minimizing ghosting and view transition artifacts. Based on our scene representation, we present an end-to-end system that captures real scenes with a robotic camera arm, processes the recorded data, and finally renders the scene in a head-mounted display in real time (more than 40 Hz). Our approach is the first to support head-motion parallax when viewing real 360 degrees scenes. We demonstrate compelling results that illustrate the enhanced visual experience - and hence sense of immersion-achieved with our approach compared to widely-used stereoscopic panoramas.
C1 [Luo, Bicheng; Xu, Feng; Yong, Jun-Hai] Tsinghua Univ, Sch Software, Beijing, Peoples R China.
   [Richardt, Christian] Univ Bath, Bath, Avon, England.
C3 Tsinghua University; University of Bath
RP Xu, F (corresponding author), Tsinghua Univ, Sch Software, Beijing, Peoples R China.
EM luobc14@mails.tsinghua.edu.cn; feng-xu@tsinghua.edu.cn;
   christian@richardt.name; yongjh@tsinghua.edu.cn
OI Richardt, Christian/0000-0001-6716-9845
FU NSFC [61671268, 61727808, 61672307]; National Key Technologies R&D
   Program of China [2015BAF23B03]; EPSRC grant CAMERA [EP/M023281/1];
   EPSRC [EP/M023281/1] Funding Source: UKRI
FX This work was supported by the NSFC (No. 61671268, 61727808, 61672307),
   the National Key Technologies R&D Program of China (No. 2015BAF23B03)
   and EPSRC grant CAMERA (EP/M023281/1).
NR 47
TC 33
Z9 38
U1 0
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2018
VL 24
IS 4
BP 1545
EP 1553
DI 10.1109/TVCG.2018.2794071
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FZ6BS
UT WOS:000427682500017
PM 29543172
OA Green Published
DA 2025-03-07
ER

PT J
AU Wang, B
   Mueller, K
AF Wang, Bing
   Mueller, Klaus
TI The Subspace Voyager: Exploring High-Dimensional Data along a Continuum
   of Salient 3D Subspaces
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE High-dimensional data; subspace navigation; trackball; PCA; ant colony
   optimization1
ID VISUAL EXPLORATION; TOOL
AB Analyzing high-dimensional data and finding hidden patterns is a difficult problem and has attracted numerous research efforts. Automated methods can be useful to some extent but bringing the data analyst into the loop via interactive visual tools can help the discovery process tremendously. An inherent problem in this effort is that humans lack the mental capacity to truly understand spaces exceeding three spatial dimensions. To keep within this limitation, we describe a framework that decomposes a high-dimensional data space into a continuum of generalized 3D subspaces. Analysts can then explore these 3D subspaces individually via the familiar trackball interface while using additional facilities to smoothly transition to adjacent subspaces for expanded space comprehension. Since the number of such subspaces suffers from combinatorial explosion, we provide a set of data-driven subspace selection and navigation tools which can guide users to interesting subspaces and views. A subspace trail map allows users to manage the explored subspaces, keep their bearings, and return to interesting subspaces and views. Both trackball and trail map are each embedded into a word cloud of attribute labels which aid in navigation. We demonstrate our system via several use cases in a diverse set of application areas-cluster analysis and refinement, information discovery, and supervised training of classifiers. We also report on a user study that evaluates the usability of the various interactions our system provides.
C1 [Wang, Bing; Mueller, Klaus] SUNY Stony Brook, Dept Comp Sci, Visual Analyt & Imaging Lab, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; Stony Brook University
RP Wang, B (corresponding author), SUNY Stony Brook, Dept Comp Sci, Visual Analyt & Imaging Lab, Stony Brook, NY 11794 USA.
EM wang12@cs.sunysb.edu; mueller@cs.sunysb.edu
FU NSF [IIS 1527200, IIS 1117132]; MSIP (Ministry of Science, ICT and
   Future Planning), Korea under the "ITCCP Program"; Div Of Information &
   Intelligent Systems; Direct For Computer & Info Scie & Enginr [1527200]
   Funding Source: National Science Foundation
FX This research was partially supported by NSF grants IIS 1527200 and IIS
   1117132, as well as the MSIP (Ministry of Science, ICT and Future
   Planning), Korea, under the "ITCCP Program" directed by NIPA. We thank
   Eric Papenhausen for proofreading the manuscript, and we also thank the
   reviewers and in particular the Associate Editor for their valuable
   comments.
NR 45
TC 15
Z9 18
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2018
VL 24
IS 2
BP 1204
EP 1222
DI 10.1109/TVCG.2017.2672987
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FR8BW
UT WOS:000419299900015
PM 28252403
OA Green Submitted, Bronze
DA 2025-03-07
ER

PT J
AU Stein, M
   Janetzko, H
   Lamprecht, A
   Breitkreutz, T
   Zimmermann, P
   Goldlücke, B
   Schreck, T
   Andrienko, G
   Grossniklaus, M
   Keim, DA
AF Stein, Manuel
   Janetzko, Halldor
   Lamprecht, Andreas
   Breitkreutz, Thorsten
   Zimmermann, Philipp
   Goldluecke, Bastian
   Schreck, Tobias
   Andrienko, Gennady
   Grossniklaus, Michael
   Keim, Daniel A.
TI Bring it to the Pitch: Combining Video and Movement Data to Enhance Team
   Sport Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE visual analytics; sport analytics; immersive analytics
ID VISUALIZATION; IMAGE; TRANSFORMATION
AB Analysts in professional team sport regularly perform analysis to gain strategic and tactical insights into player and team behavior. Goals of team sport analysis regularly include identification of weaknesses of opposing teams. or assessing performance and improvement potential of a coached team. Current analysis workflows are typically based on the analysis of team videos. Also, analysts can rely on techniques from Information Visualization. to depict e.g., player or ball trajectories. However. video analysis is typically a time-consuming process. where the analyst needs to memorize and annotate scenes. In contrast. visualization typically relies on an abstract data model, often using abstract visual mappings. and is not directly linked to the observed movement context anymore. We propose a visual analytics system that tightly integrates team sport video recordings with abstract visualization of underlying trajectory data. We apply appropriate computer vision techniques to extract trajectory data from video input. Furthermore, we apply advanced trajectory and movement analysis techniques to derive relevant team sport analytic measures for region. event and player analysis in the case of soccer analysis. Our system seamlessly integrates video and visualization modalities, enabling analysts to draw on the advantages of both analysis forms. Several expert studies conducted with team sport analysts indicate the effectiveness of our integrated approach.
C1 [Stein, Manuel; Lamprecht, Andreas; Breitkreutz, Thorsten; Zimmermann, Philipp; Goldluecke, Bastian; Grossniklaus, Michael; Keim, Daniel A.] Univ Konstanz, Constance, Germany.
   [Janetzko, Halldor] Univ Zurich, Zurich, Switzerland.
   [Schreck, Tobias] Graz Univ Technol, Graz, Austria.
   [Andrienko, Gennady] Fraunhofer IAIS, St Augustin, Germany.
   [Andrienko, Gennady] City Univ London, London, England.
C3 University of Konstanz; University of Zurich; Graz University of
   Technology; City St Georges, University of London; City, University of
   London
RP Stein, M (corresponding author), Univ Konstanz, Constance, Germany.
EM manuel.stein@uni-konstanz.de; Halldor.Janetzko@geo.uzh.ch;
   andreas.lamprechi@uni-konstanz.de; thorsten.breitkreutz@uni-konstanz.de;
   philipp.zimmermann@uni-konstanz.de; bastian.goldhucke@uni-konstanz.de;
   Tobias.Schreck@cgv.tugraz.at; Gennady.Andrienko@iais.fraunhofer.de;
   michael.grossniklaus@uni-konstanz.de; daniel.keim@uni-konstanz.de
RI Keim, Daniel/X-7749-2019; Grossniklaus, Michael/JWO-5296-2024;
   Andrienko, Gennady/B-6486-2014
OI Andrienko, Gennady/0000-0002-8574-6295; Stein,
   Manuel/0000-0002-7198-1438; Janetzko, Halldor/0000-0002-0038-5292;
   Schreck, Tobias/0000-0003-0778-8665
FU ERC Starting Grant;  [SFB Transregio 161]
FX This work was supported by the ERC Starting Grant "Light Field Imaging
   and Analysis" and the SFB Transregio 161 "Quantitative Methods for
   Visual Computing".
NR 43
TC 109
Z9 133
U1 5
U2 50
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 13
EP 22
DI 10.1109/TVCG.2017.2745181
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400004
PM 28866578
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhang, J
   Guo, HQ
   Hong, F
   Yuan, XR
   Peterka, T
AF Zhang, Jiang
   Guo, Hanqi
   Hong, Fan
   Yuan, Xiaoru
   Peterka, Tom
TI Dynamic Load Balancing Based on Constrained K-D Tree Decomposition for
   Parallel Particle Tracing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Parallel particle tracing; dynamic load balancing; k-d trees;
   performance analysis
ID COHERENT STRUCTURES; FLOW; VISUALIZATION
AB We propose a dynamically load-balanced algorithm for parallel particle tracing, which periodically attempts to evenly redistribute particles across processes based on k-d tree decomposition. Each process is assigned with (1) a statically partitioned, axis-aligned data block that partially overlaps with neighboring blocks in other processes and (2) a dynamically determined k-d tree leaf node that bounds the active particles for computation; the bounds of the k-d tree nodes are constrained by the geometries of data blocks. Given a certain degree of overlap between blocks, our method can balance the number of particles as much as possible. Compared with other load-balancing algorithms for parallel particle tracing, the proposed method does not require any preanalysis, does not use any heuristics based on flow features, does not make any assumptions about seed distribution, does not move any data blocks during the run, and does not need any master process for work redistribution. Based on a comprehensive performance study up to 8K processes on a Blue Gene/Q system, the proposed algorithm outperforms baseline approaches in both load balance and scalability on various flow visualization and analysis problems.
C1 [Zhang, Jiang; Hong, Fan; Yuan, Xiaoru] Peking Univ, Sch EECS, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.
   [Guo, Hanqi; Peterka, Tom] Argonne Natl Lab, Math & Comp Sci Div, Lemont, IL 60439 USA.
C3 Peking University; United States Department of Energy (DOE); Argonne
   National Laboratory
RP Zhang, J (corresponding author), Peking Univ, Sch EECS, Minist Educ, Key Lab Machine Percept, Beijing, Peoples R China.
EM jiang.zhang@pku.edu.cn; hguo@anl.gov; fan.hong@pku.edu.cn;
   xiaoru.yuan@pku.edu.cn; tpeterka@mcs.anl.gov
RI Yuan, Xiaoru/E-1798-2013; Guo, Hanqi/AAL-1929-2021; Guo,
   Hanqi/ADW-4234-2022
OI Guo, Hanqi/0000-0001-7776-1834; Yuan, Xiaoru/0000-0002-7233-980X
FU NSFC [61672055]; National Program on Key Basic Research Project (973
   Program) [2015CB352503]; U.S. Department of Energy, Office of Science
   [DE-AC02-06CH11357]
FX We thank the anonymous reviewers for their comments. This work is
   supported by NSFC No. 61672055 and the National Program on Key Basic
   Research Project (973 Program) No. 2015CB352503. This work is also
   supported by the U.S. Department of Energy, Office of Science, under
   contract number DE-AC02-06CH11357.
NR 37
TC 26
Z9 35
U1 0
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 954
EP 963
DI 10.1109/TVCG.2017.2744059
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400094
PM 28866518
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kitajima, Y
   Iwai, D
   Sato, K
AF Kitajima, Yuki
   Iwai, Daisuke
   Sato, Kosuke
TI Simultaneous Projection and Positioning of Laser Projector Pixels
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY SEP 09-13, 2017
CL Nantes, FRANCE
SP IEEE
DE Dynamic projection mapping; spatial augmented reality; laser projector;
   light pen; geometric registration
ID LAMPS
AB This paper presents a novel projected pixel localization principle for online geometric registration in dynamic projection mapping applications. We propose applying a time measurement of a laser projector raster-scanning beam using a photosensor to estimate its position while the projector displays meaningful visual information to human observers. Based on this principle, we develop two types of position estimation techniques. One estimates the position of a projected beam when it directly illuminates a photosensor. The other localizes a beam by measuring the reflection from a retro-reflective marker with the photosensor placed in the optical path of the projector. We conduct system evaluations using prototypes to validate this method as well as to confirm the applicability of our principle. In addition, we discuss the technical limitations of the prototypes based on the evaluation results. Finally, we build several dynamic projection mapping applications to demonstrate the feasibility of our principle.
C1 [Kitajima, Yuki; Iwai, Daisuke; Sato, Kosuke] Osaka Univ, Grad Sch Engn Sci, Suita, Osaka, Japan.
C3 Osaka University
RP Kitajima, Y (corresponding author), Osaka Univ, Grad Sch Engn Sci, Suita, Osaka, Japan.
RI Iwai, Daisuke/R-8174-2019
OI Iwai, Daisuke/0000-0002-3493-5635
FU JSPS KAKEHNHI [JP16H02859]; Grants-in-Aid for Scientific Research
   [15H05925] Funding Source: KAKEN
FX This work was supported by JSPS KAKEHNHI Grant Number JP16H02859. The
   authors would like to thank Yula Itoh for providing us the information
   on disassembling the laser projector, and Takumi Kawahara for helping us
   in the drone projection experiment.
NR 45
TC 17
Z9 19
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2017
VL 23
IS 11
BP 2419
EP 2429
DI 10.1109/TVCG.2017.2734478
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FI7XT
UT WOS:000412214000008
PM 28809693
DA 2025-03-07
ER

PT J
AU Nie, YW
   Zhang, ZS
   Sun, H
   Su, T
   Li, GQ
AF Nie, Yongwei
   Zhang, Zhensong
   Sun, Hanqiu
   Su, Tan
   Li, Guiqing
TI Homography Propagation and Optimization for Wide-Baseline Street Image
   Interpolation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image interpolation; street view synthesis; homography propagation;
   homography-constrained warping
ID OPTICAL-FLOW; VIEW
AB Wide-baseline street image interpolation is useful but very challenging. Existing approaches either rely on heavyweight 3D reconstruction or computationally intensive deep networks. We present a lightweight and efficient method which uses simple homography computing and refining operators to estimate piecewise smooth homographies between input views. To achieve the goal, we show how to combine homography fitting and homography propagation together based on reliable and unreliable superpixel discrimination. Such a combination, other than using homography fitting only, dramatically increases the accuracy and robustness of the estimated homographies. Then, we integrate the concepts of homography and mesh warping, and propose a novel homography-constrained warping formulation which enforces smoothness between neighboring homographies by utilizing the first-order continuity of the warped mesh. This further eliminates small artifacts of overlapping, stretching, etc. The proposed method is lightweight and flexible, allows wide-baseline interpolation. It improves the state of the art and demonstrates that homography computation suffices for interpolation. Experiments on city and rural datasets validate the efficiency and effectiveness of our method.
C1 [Nie, Yongwei; Li, Guiqing] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
   [Zhang, Zhensong; Sun, Hanqiu; Su, Tan] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Shatin, Hong Kong, Peoples R China.
C3 South China University of Technology; Chinese University of Hong Kong
RP Nie, YW (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Guangdong, Peoples R China.
EM nieyongwei@scut.edu.cn; zszhang@cse.cuhk.edu.hk; hanqiu@cse.cuhk.edu.hk;
   tsu@cse.cuhk.edu.hk; ligq@scut.edu.cn
RI Tanfeng, Sun/J-7469-2015
FU NSFC [61602183, 61379087, 61572202]; UGC [4055060]; China Postdoctoral
   Science Foundation Grant [2016M590780]; Key Project of Natural Science
   Foundation of Guangdong, China [S2013020012795]
FX The authors would like to thank the anonymous reviewers for their
   valuable comments. This work was partly supported by the NSFC (No.
   61602183, 61379087, and 61572202), UGC grant for research (No. 4055060),
   China Postdoctoral Science Foundation Grant (No. 2016M590780), and the
   Key Project of Natural Science Foundation of Guangdong, China (No.
   S2013020012795).
NR 42
TC 8
Z9 10
U1 0
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2017
VL 23
IS 10
BP 2328
EP 2341
DI 10.1109/TVCG.2016.2618878
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FG0VB
UT WOS:000409496700011
PM 27775524
DA 2025-03-07
ER

PT J
AU Crouser, RJ
   Franklin, L
   Endert, A
   Cook, K
AF Crouser, R. Jordan
   Franklin, Lyndsey
   Endert, Alex
   Cook, Kris
TI Toward Theoretical Techniques for Measuring the Use of Human Effort in
   Visual Analytic Systems
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Theoretical models; human oracle; visual analytics; mixed initiative
   systems; semantic interaction; sensemaking
ID VISUALIZATION; COGNITION; MODEL; PERFORMANCE; FRAMEWORK; DESIGN; WORK
AB Visual analytic systems have long relied on user studies and standard datasets to demonstrate advances to the state of the art, as well as to illustrate the efficiency of solutions to domain-specific challenges. This approach has enabled some important comparisons between systems, but unfortunately the narrow scope required to facilitate these comparisons has prevented many of these lessons from being generalized to new areas. At the same time, advanced visual analytic systems have made increasing use of human-machine collaboration to solve problems not tractable by machine computation alone. To continue to make progress in modeling user tasks in these hybrid visual analytic systems, we must strive to gain insight into what makes certain tasks more complex than others. This will require the development of mechanisms for describing the balance to be struck between machine and human strengths with respect to analytical tasks and workload. In this paper, we argue for the necessity of theoretical tools for reasoning about such balance in visual analytic systems and demonstrate the utility of the Human Oracle Model for this purpose in the context of sensemaking in visual analytics. Additionally, we make use of the Human Oracle Model to guide the development of a new system through a case study in the domain of cybersecurity.
C1 [Crouser, R. Jordan] Smith Coll, Northampton, MA 01063 USA.
   [Franklin, Lyndsey; Cook, Kris] Pacific Northwest Natl Lab, Richland, WA USA.
   [Endert, Alex] Georgia Tech, Atlanta, GA USA.
C3 Smith College; United States Department of Energy (DOE); Pacific
   Northwest National Laboratory; University System of Georgia; Georgia
   Institute of Technology
RP Crouser, RJ (corresponding author), Smith Coll, Northampton, MA 01063 USA.
EM jcrouser@smith.edu; lyndsey.franklin@pnnl.gov; endert@gatech.edu;
   kris.cook@pnnl.gov
RI Franklin, Lyndsey/JMQ-2427-2023; Cook, Kris/HRE-1562-2023
OI Franklin, Lyndsey/0000-0002-4494-7111; Crouser, R.
   Jordan/0000-0001-9936-0791
FU Human Computation and Visualization Laboratory at Smith College
FX The authors wish to thank Samantha Behrens, Zheng "Alice" Mu, and the
   rest of the Human Computation and Visualization Laboratory at Smith
   College for their implementation and design efforts in support of this
   work. The research described in this paper is part of the Analysis In
   Motion Initiative at Pacific Northwest National Laboratory. It was
   conducted under the Laboratory Directed Research and Development Program
   at PNNL, a multi-program national laboratory operated by Battelle for
   the U.S. Department of Energy.
NR 87
TC 12
Z9 13
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 121
EP 130
DI 10.1109/TVCG.2016.2598460
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600015
PM 27514049
DA 2025-03-07
ER

PT J
AU Kim, M
   Kang, K
   Park, D
   Choo, J
   Elmqvist, N
AF Kim, Minjeong
   Kang, Kyeongpil
   Park, Deokgun
   Choo, Jaegul
   Elmqvist, Niklas
TI TopicLens: Efficient Multi-Level Visual Topic Exploration of Large-Scale
   Document Collections
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE topic modeling; nonnegative matrix factorization; t-distributed
   stochastic neighbor embedding; magic lens; text analytics
ID NONNEGATIVE MATRIX; VISUALIZATION; ANALYTICS
AB Topic modeling, which reveals underlying topics of a document corpus, has been actively adopted in visual analytics for large-scale document collections. However, due to its significant processing time and non-interactive nature, topic modeling has so far not been tightly integrated into a visual analytics workflow. Instead, most such systems are limited to utilizing a fixed, initial set of topics. Motivated by this gap in the literature, we propose a novel interaction technique called TopicLens that allows a user to dynamically explore data through a lens interface where topic modeling and the corresponding 2D embedding are efficiently computed on the fly. To support this interaction in real time while maintaining view consistency, we propose a novel efficient topic modeling method and a semi-supervised 2D embedding algorithm. Our work is based on improving state-of-the-art methods such as nonnegative matrix factorization and t-distributed stochastic neighbor embedding. Furthermore, we have built a web-based visual analytics system integrated with TopicLens. We use this system to measure the performance and the visualization quality of our proposed methods. We provide several scenarios showcasing the capability of TopicLens using real-world datasets.
C1 [Kim, Minjeong; Kang, Kyeongpil; Choo, Jaegul] Korea Univ, Seoul, South Korea.
   [Park, Deokgun; Elmqvist, Niklas] Univ Maryland, College Pk, MD 20742 USA.
C3 Korea University; University System of Maryland; University of Maryland
   College Park
RP Choo, J (corresponding author), Korea Univ, Seoul, South Korea.
EM mj1642@korea.ac.kr; rudvlf0313@korea.ac.kr; intuinno@umd.edu;
   jchoo@korea.ac.kr; elm@umd.edu
RI Choo, Jaegul/ABF-8315-2020; Kang, Kyeongpil/LKK-5996-2024
OI Choo, Jaegul/0000-0003-1071-4835; Elmqvist, Niklas/0000-0001-5805-5301;
   Park, Deokgun/0000-0003-0054-9944
FU NIH [R01GM114267]; National Research Foundation of Korea (NRF) - Korea
   government (MSIP) [NRF-2016R1C1B2015924]
FX Research reported in this publication was partially supported by NIH
   grant R01GM114267 and by the National Research Foundation of Korea (NRF)
   grant funded by the Korea government (MSIP) (No. NRF-2016R1C1B2015924).
   Any opinions, findings, and conclusions or recommendations expressed in
   this article are those of the authors and do not necessarily reflect the
   views of the funding agencies.
NR 51
TC 29
Z9 39
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 151
EP 160
DI 10.1109/TVCG.2016.2598445
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600018
DA 2025-03-07
ER

PT J
AU Li, Q
   Xu, P
   Chan, YY
   Wang, Y
   Wang, ZP
   Qu, HM
   Ma, XJ
AF Li, Quan
   Xu, Peng
   Chan, Yeuk Yin
   Wang, Yun
   Wang, Zhipeng
   Qu, Huamin
   Ma, Xiaojuan
TI A Visual Analytics Approach for Understanding Reasons behind Snowballing
   and Comeback in MOBA Games
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Game play data visualization; visual knowledge discovery; visual
   knowledge representation; and game reconstruction
ID GAMEPLAY DATA; VISUALIZATION
AB To design a successful Multiplayer Online Battle Arena (MOBA) game, the ratio of snowballing and comeback occurrences to all matches played must be maintained at a certain level to ensure its fairness and engagement. Although it is easy to identify these two types of occurrences, game developers often find it difficult to determine their causes and triggers with so many game design choices and game parameters involved. In addition, the huge amounts of MOBA game data are often heterogeneous, multi-dimensional and highly dynamic in terms of space and time, which poses special challenges for analysts. In this paper, we present a visual analytics system to help game designers find key events and game parameters resulting in snowballing or comeback occurrences in MOBA game data. We follow a user-centered design process developing the system with game analysts and testing with real data of a trial version MOBA game from NetEase Inc.. We apply novel visualization techniques in conjunction with well-established ones to depict the evolution of players' positions, status and the occurrences of events. Our system can reveal players' strategies and performance throughout a single match and suggest patterns, e. g., specific player' actions and game events, that have led to the final occurrences. We further demonstrate a workflow of leveraging human analyzed patterns to improve the scalability and generality of match data analysis. Finally, we validate the usability of our system by proving the identified patterns are representative in snowballing or comeback matches in a one-month-long MOBA tournament dataset.
C1 [Li, Quan; Chan, Yeuk Yin; Wang, Yun; Qu, Huamin; Ma, Xiaojuan] Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
   [Xu, Peng] NetEase Inc, Hangzhou, Peoples R China.
   [Wang, Zhipeng] China Acad Art, Hangzhou, Peoples R China.
C3 Hong Kong University of Science & Technology; China Academy of Art
RP Li, Q (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
EM qliba@connect.usrt.hk; hzxupeng@corp.netease.com;
   yychanae@connect.usrt.hk; ywangch@connect.usrt.hk;
   wackwang007@gmail.com; huamin@cse.ust.hk; mxj@cse.ust.hk
RI Wang, Zhipeng/AAX-5431-2020
OI Ma, Xiaojuan/0000-0002-9847-7784; Chan, Gromit
   Yeuk-Yin/0000-0003-1356-4406; Li, Quan/0000-0003-2249-0728
FU HK RGC GRF [16208514]
FX The authors would like to thank the game experts in NetEase Inc. for
   providing the feedback and the anonymous reviewers for their valuable
   comments. This research was supported in part by HK RGC GRF 16208514.
NR 41
TC 33
Z9 36
U1 2
U2 35
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 211
EP 220
DI 10.1109/TVCG.2016.2598415
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600024
PM 27514043
OA Green Published
DA 2025-03-07
ER

PT J
AU Xie, C
   Zhong, W
   Mueller, K
AF Xie, Cong
   Zhong, Wen
   Mueller, Klaus
TI A Visual Analytics Approach for Categorical Joint Distribution
   Reconstruction from Marginal Projections
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Joint Distribution Reconstruction; Solution Space; High-dimensional
   Data; Multivariate Data; Parallel Coordinates
ID HIT-AND-RUN; VISUALIZATION; CONVERGENCE; POINTS; ART
AB Oftentimes multivariate data are not available as sets of equally multivariate tuples, but only as sets of projections into subspaces spanned by subsets of these attributes. For example, one may find data with five attributes stored in six tables of two attributes each, instead of a single table of five attributes. This prohibits the visualization of these data with standard high-dimensional methods, such as parallel coordinates or MDS, and there is hence the need to reconstruct the full multivariate (joint) distribution from these marginal ones. Most of the existing methods designed for this purpose use an iterative procedure to estimate the joint distribution. With insufficient marginal distributions and domain knowledge, they lead to results whose joint errors can be large. Moreover, enforcing smoothness for regularizations in the joint space is not applicable if the attributes are not numerical but categorical. We propose a visual analytics approach that integrates both anecdotal data and human experts to iteratively narrow down a large set of plausible solutions. The solution space is populated using a Monte Carlo procedure which uniformly samples the solution space. A level-of-detail high dimensional visualization system helps the user understand the patterns and the uncertainties. Constraints that narrow the solution space can then be added by the user interactively during the iterative exploration, and eventually a subset of solutions with narrow uncertainty intervals emerges.
C1 [Xie, Cong; Zhong, Wen; Mueller, Klaus] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; Stony Brook University
RP Xie, C (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM coxie@cs.stonybrook.edu; wezzhong@cs.stonybrook.edu;
   mueller@cs.stonybrook.edu
OI Xie, Cong/0000-0001-5760-7522
FU NSF [IIS 1527112]; MSIP (Ministry of Science, ICT and Future Planning),
   Korea under the "IT Consilience Creative Program (ITCCP)"
   [NIPA-2013-H0203-13-1001]; Direct For Computer & Info Scie & Enginr; Div
   Of Information & Intelligent Systems [1527200] Funding Source: National
   Science Foundation
FX This research was partially supported by NSF grant IIS 1527112 and the
   MSIP (Ministry of Science, ICT and Future Planning), Korea, under the
   "IT Consilience Creative Program (ITCCP)" (NIPA-2013-H0203-13-1001)
   supervised by NIPA. Finally, we would like to thank Hongsen Liao for the
   helpful discussions on solution space analysis.
NR 34
TC 15
Z9 19
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 51
EP 60
DI 10.1109/TVCG.2016.2598479
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600008
PM 27514059
DA 2025-03-07
ER

PT J
AU Marchand, E
   Uchiyama, H
   Spindler, F
AF Marchand, Eric
   Uchiyama, Hideaki
   Spindler, Fabien
TI Pose Estimation for Augmented Reality: A Hands-On Survey
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Survey; augmented reality; vision-based camera localization; pose
   estimation; PnP; SLAM; motion estimation; homography; keypoint matching;
   code examples
ID VISUAL TRACKING; 3D TRACKING; REGISTRATION; LOCALIZATION; KEYPOINT;
   FEATURES; MODELS; EDGE
AB Augmented reality (AR) allows to seamlessly insert virtual objects in an image sequence. In order to accomplish this goal, it is important that synthetic elements are rendered and aligned in the scene in an accurate and visually acceptable way. The solution of this problem can be related to a pose estimation or, equivalently, a camera localization process. This paper aims at presenting a brief but almost self-contented introduction to the most important approaches dedicated to vision-based camera localization along with a survey of several extension proposed in the recent years. For most of the presented approaches, we also provide links to code of short examples. This should allow readers to easily bridge the gap between theoretical aspects and practical implementations.
C1 [Marchand, Eric] Univ Rennes 1, IRISA, Inria Rennes Bretagne Atlantique, Rennes, France.
   [Uchiyama, Hideaki] Kyushu Univ, Fukuoka, Japan.
   [Spindler, Fabien] Inria Rennes Bretagne Atlantique, Rennes, France.
C3 Universite de Rennes; Kyushu University; Universite de Rennes
RP Marchand, E (corresponding author), Univ Rennes 1, IRISA, Inria Rennes Bretagne Atlantique, Rennes, France.
EM Eric.Marchand@irisa.fr; uchiyama@limu.ait.kyushu-u.ac.jp;
   Fabien.Spindler@inria.fr
RI Marchand, Eric/AAF-2809-2019
OI Spindler, Fabien/0009-0004-8619-6864; Marchand, Eric/0000-0001-7096-5236
NR 151
TC 432
Z9 477
U1 9
U2 133
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2016
VL 22
IS 12
BP 2633
EP 2651
DI 10.1109/TVCG.2015.2513408
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EB4RJ
UT WOS:000387360500011
PM 26731768
OA Green Published
HC Y
HP N
DA 2025-03-07
ER

PT J
AU Guo, XQ
   Yu, Z
   Kang, SB
   Lin, HT
   Yu, JY
AF Guo, Xinqing
   Yu, Zhan
   Kang, Sing Bing
   Lin, Haiting
   Yu, Jingyi
TI Enhancing Light Fields through Ray-Space Stitching
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Light field enhancement; image based rendering
ID STEREO
AB Light fields (LFs) have been shown to enable photorealistic visualization of complex scenes. In practice, however, an LF tends to have a relatively small angular range or spatial resolution, which limits the scope of virtual navigation. In this paper, we show how seamless virtual navigation can be enhanced by stitching multiple LFs. Our technique consists of two key components: LF registration and LF stitching. To register LFs, we use what we call the ray-space motion matrix (RSMM) to establish pairwise ray-ray correspondences. Using Plucker coordinates, we show that the RSMM is a 5 x 6 matrix, which reduces to a 5 x 5 matrix under pure translation and/or in-plane rotation. The final LF stitching is done using multi-resolution, high-dimensional graph-cut in order to account for possible scene motion, imperfect RSMM estimation, and/or undersampling. We show how our technique allows us to create LFs with various enhanced features: extended horizontal and/or vertical field-of-view, larger synthetic aperture and defocus blur, and larger parallax.
C1 [Guo, Xinqing; Lin, Haiting; Yu, Jingyi] Univ Delaware, Dept Comp & Informat Sci, Newark, DE 19716 USA.
   [Yu, Zhan] Adobe Syst Inc, San Jose, CA USA.
   [Kang, Sing Bing] Microsoft Res, Redmond, WA USA.
C3 University of Delaware; Adobe Systems Inc.; Microsoft
RP Guo, XQ (corresponding author), Univ Delaware, Dept Comp & Informat Sci, Newark, DE 19716 USA.
EM xinqing@udel.edu; yshmzhan@udel.edu; sbkang@microsoft.com;
   haiting@udel.edu; yu@eecis.udel.edu
FU National Science Foundation [IIS-CAREER-0845268, IIS-1218156]
FX This project was partially supported by the National Science Foundation
   under grants IIS-CAREER-0845268 and IIS-1218156. The authors would like
   to thank Clemens Birklbauer and Oliver Bimber for providing the source
   code of [39] and useful suggestions. The authors would also like to
   thank anonymous TVCG reviewers for their insightful comments. Xinqing
   Guo and Zhan Yu are the co-first author. Jingyi Yu is the corresponding
   author.
NR 43
TC 38
Z9 42
U1 1
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2016
VL 22
IS 7
BP 1852
EP 1861
DI 10.1109/TVCG.2015.2476805
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DO0NI
UT WOS:000377475200007
PM 26357400
OA Bronze
DA 2025-03-07
ER

PT J
AU Vinkler, M
   Havran, V
   Bittner, J
   Sochor, J
AF Vinkler, Marek
   Havran, Vlastimil
   Bittner, Jiri
   Sochor, Jiri
TI Parallel On-Demand Hierarchy Construction on Contemporary GPUs
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE GPU; bounding volume hierarchies; kd-trees; lazy build; ray tracing
AB We present the first parallel on-demand spatial hierarchy construction algorithm targeting ray tracing on many-core processors such as GPUs. The method performs simultaneous ray traversal and spatial hierarchy construction focused on the parts of the data structure being traversed. The method is based on a versatile framework built around a task pool and runs entirely on the GPU. We show that the on-demand construction can improve rendering times compared to full hierarchy construction. We evaluate our method on both object (BVH) and space (kd-tree) subdivision data structures and compare them mutually. The on-demand method is particularly beneficial for rendering large scenes with high occlusion. We also present SAH kd-tree builder that outperforms previous state-of-the-art builders running on the GPU.
C1 [Vinkler, Marek; Sochor, Jiri] Masaryk Univ, Fac Informat, CS-60177 Brno, Czech Republic.
   [Havran, Vlastimil; Bittner, Jiri] Czech Tech Univ, Fac Elect Engn, CR-16635 Prague, Czech Republic.
C3 Masaryk University Brno; Czech Technical University Prague
RP Vinkler, M (corresponding author), Masaryk Univ, Fac Informat, CS-60177 Brno, Czech Republic.
EM xvinkl@fi.muni.cz; havran@fel.cvut.cz; bittner@fel.cvut.cz;
   sochor@fi.muni.cz
RI Sochor, Jiri/D-7067-2013; Bittner, Jiri/B-1677-2010; Vinkler,
   Marek/I-7391-2012; Havran, Vlastimil/B-4530-2014
OI Bittner, Jiri/0000-0002-5818-934X; Havran, Vlastimil/0000-0002-3329-8814
FU Czech Science Foundation [P202/11/1883, P202/12/2413]; Grant Agency of
   the Czech Technical University in Prague [SGS13/214/OHK3/3T/13]; UNC for
   Powerplant model, project [29]
FX The authors would like to thank Marko Dabrovic for Sibenik model, DAZ3D
   (www.daz3d.com) for Fairy Forest model, Frank Meinl at Crytek for the
   Crytek Sponza model, Prof. C. Sequin for Sodahall model, Samuli Laine
   and Tero Karras for Hairball model, Guillermo M. Leal Llaguno for San
   Miguel model, the UNC for Powerplant model, project [29] for MPII model,
   and Stanford repository for other 3D models. They would also like to
   thank Tero Karras, Timo Aila, and Samuli Laine for releasing their GPU
   ray tracing framework. Our research was partially supported by the Czech
   Science Foundation under research programs P202/11/1883 (Argie) and
   P202/12/2413 (Opalis) and the Grant Agency of the Czech Technical
   University in Prague, grant No. SGS13/214/OHK3/3T/13.
NR 29
TC 2
Z9 3
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2016
VL 22
IS 7
BP 1886
EP 1898
DI 10.1109/TVCG.2015.2465898
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DO0NI
UT WOS:000377475200010
PM 27244710
DA 2025-03-07
ER

PT J
AU Pick, S
   Weyers, B
   Hentschel, B
   Kuhlen, TW
AF Pick, Sebastian
   Weyers, Benjamin
   Hentschel, Bernd
   Kuhlen, Torsten W.
TI Design and Evaluation of Data Annotation Workflows for CAVE-like Virtual
   Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE Virtual Reality Conference (IEEE VR)
CY MAR 19-23, 2016
CL Greenville, SC
SP IEEE, IEEE Comp Soc, IEEE Comp Soc Visualizat & Graph Tech Comm, Clemson Univ
DE Terms Virtual reality; data annotation; interaction design; user study
AB Data annotation finds increasing use in Virtual Reality applications with the goal to support the data analysis process, such as architectural reviews. In this context, a variety of different annotation systems for application to immersive virtual environments have been presented. While many interesting interaction designs for the data annotation workflow have emerged from them, important details and evaluations are often omitted. In particular, we observe that the process of handling metadata to interactively create and manage complex annotations is often not covered in detail. In this paper, we strive to improve this situation by focusing on the design of data annotation workflows and their evaluation. We propose a workflow design that facilitates the most important annotation operations, i.e., annotation creation, review, and modification. Our workflow design is easily extensible in terms of supported annotation and metadata types as well as interaction techniques, which makes it suitable for a variety of application scenarios. To evaluate it, we have conducted a user study in a CAVE -like virtual environment in which we compared our design to two alternatives in terms of a realistic annotation creation task. Our design obtained good results in terms of task performance and user experience.
C1 [Pick, Sebastian; Weyers, Benjamin; Hentschel, Bernd; Kuhlen, Torsten W.] Rhein Westfal TH Aachen, Visual comp Inst, Aachen, Germany.
C3 RWTH Aachen University
RP Pick, S; Weyers, B; Hentschel, B; Kuhlen, TW (corresponding author), Rhein Westfal TH Aachen, Visual comp Inst, Aachen, Germany.
EM pick@vr.rwth-aachen.de; weyers@vr.rwth-aachen.de;
   hentschel@vr.rwth-aachen.de; kuhlen@vr.rwth-aachen.de
FU German Research Foundation DFG as part of the Cluster of Excellence
   "Integrative Production Technology for High-Wage Countries"
FX This research was funded by the German Research Foundation DFG as part
   of the Cluster of Excellence "Integrative Production Technology for
   High-Wage Countries". The authors also thank the Laboratory for Machine
   Tools and Production Engineering (WZL) of RWTH Aachen University for
   providing the factory data set shown in some figures.
NR 37
TC 13
Z9 16
U1 0
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2016
VL 22
IS 4
BP 1452
EP 1461
DI 10.1109/TVCG.2016.2518086
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DH5RN
UT WOS:000372849600015
PM 26780799
DA 2025-03-07
ER

PT J
AU Gu, Y
   Wang, CL
   Peterka, T
   Jacob, R
   Kim, SH
AF Gu, Yi
   Wang, Chaoli
   Peterka, Tom
   Jacob, Robert
   Kim, Seung Hyun
TI Mining Graphs for Understanding Time-Varying Volumetric Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Time-varying data visualization; graph simplification; community
   detection; visual recommendation
AB A notable recent trend in time-varying volumetric data analysis and visualization is to extract data relationships and represent them in a low-dimensional abstract graph view for visual understanding and making connections to the underlying data. Nevertheless, the ever-growing size and complexity of data demands novel techniques that go beyond standard brushing and linking to allow significant reduction of cognition overhead and interaction cost. In this paper, we present a mining approach that automatically extracts meaningful features from a graph-based representation for exploring time-varying volumetric data. This is achieved through the utilization of a series of graph analysis techniques including graph simplification, community detection, and visual recommendation. We investigate the most important transition relationships for time-varying data and evaluate our solution with several time-varying data sets of different sizes and characteristics. For gaining insights from the data, we show that our solution is more efficient and effective than simply asking users to extract relationships via standard interaction techniques, especially when the data set is large and the relationships are complex. We also collect expert feedback to confirm the usefulness of our approach.
C1 [Gu, Yi; Wang, Chaoli] Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
   [Peterka, Tom; Jacob, Robert] Argonne Natl Lab, Div Math & Comp Sci, Argonne, IL 60439 USA.
   [Kim, Seung Hyun] Ohio State Univ, Dept Mech & Aerosp Engn, Columbus, OH 43210 USA.
C3 University of Notre Dame; United States Department of Energy (DOE);
   Argonne National Laboratory; University System of Ohio; Ohio State
   University
RP Gu, Y (corresponding author), Univ Notre Dame, Dept Comp Sci & Engn, Notre Dame, IN 46556 USA.
EM ygu5@nd.edu; chaoli.wang@nd.edu; tpeterka@mcs.anl.gov;
   jacob@mcs.anl.gov; kim.5061@osu.edu
RI Wang, Chaoli/AAJ-5173-2020; Jacob, Robert/D-2580-2011; Kim,
   Seung/N-7343-2016
FU U.S. National Science Foundation [IIS-1456763, IIS-1455886]; U.S.
   Department of Energy [DE-FC02-06ER25777]; Advanced Scientific Computing
   Research, Office of Science, U.S. Department of Energy
   [DE-AC02-06CH11357]; U.S. Department of Energy (DOE) [DE-FC02-06ER25777]
   Funding Source: U.S. Department of Energy (DOE); Direct For Computer &
   Info Scie & Enginr; Div Of Information & Intelligent Systems [1456763]
   Funding Source: National Science Foundation
FX This research was supported in part by the U.S. National Science
   Foundation through grants IIS-1456763 and IIS-1455886, the U.S.
   Department of Energy with Agreement No. DE-FC02-06ER25777, and the
   Advanced Scientific Computing Research, Office of Science, U.S.
   Department of Energy, under Contract No. DE-AC02-06CH11357. Special
   thanks to Dr. Amanda Sgroi for her narration of the accompanying video
   and the anonymous reviewers for their insightful comments.
NR 27
TC 14
Z9 18
U1 0
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 965
EP 974
DI 10.1109/TVCG.2015.2468031
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400102
PM 26529740
DA 2025-03-07
ER

PT J
AU Lu, YF
   Steptoe, M
   Burke, S
   Wang, H
   Tsai, JY
   Davulcu, H
   Montgomery, D
   Corman, SR
   Maciejewski, R
AF Lu, Yafeng
   Steptoe, Michael
   Burke, Sarah
   Wang, Hong
   Tsai, Jiun-Yi
   Davulcu, Hasan
   Montgomery, Douglas
   Corman, Steven R.
   Maciejewski, Ross
TI Exploring Evolving Media Discourse Through Event Cueing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Media Analysis; Time Series Analysis; Event Detection
ID VISUAL ANALYTICS; TEXT COLLECTIONS
AB Online news, microblogs and other media documents all contain valuable insight regarding events and responses to events. Underlying these documents is the concept of framing, a process in which communicators act (consciously or unconsciously) to construct a point of view that encourages facts to be interpreted by others in a particular manner. As media discourse evolves, how topics and documents are framed can undergo change, shifting the discussion to different viewpoints or rhetoric. What causes these shifts can be difficult to determine directly; however, by linking secondary datasets and enabling visual exploration, we can enhance the hypothesis generation process. In this paper, we present a visual analytics framework for event cueing using media data. As discourse develops over time, our framework applies a time series intervention model which tests to see if the level of framing is different before or after a given date. If the model indicates that the times before and after are statistically significantly different, this cues an analyst to explore related datasets to help enhance their understanding of what (if any) events may have triggered these changes in discourse. Our framework consists of entity extraction and sentiment analysis as lenses for data exploration and uses two different models for intervention analysis. To demonstrate the usage of our framework, we present a case study on exploring potential relationships between climate change framing and conflicts in Africa.
C1 [Lu, Yafeng; Steptoe, Michael; Burke, Sarah; Wang, Hong; Tsai, Jiun-Yi; Davulcu, Hasan; Montgomery, Douglas; Corman, Steven R.; Maciejewski, Ross] Arizona State Univ, Tempe, AZ 85287 USA.
C3 Arizona State University; Arizona State University-Tempe
RP Lu, YF (corresponding author), Arizona State Univ, Tempe, AZ 85287 USA.
EM lyafeng@asu.edu; msteptoe@asu.edu; seburke2@asu.edu; hxwang@asu.edu;
   jtsai8@asu.edu; HasanDavulcu@asu.edu; doug.montgomery@asu.edu;
   steve.corman@asu.edu; rmacieje@asu.edu
OI Tsai, Jiun-Yi/0000-0001-7333-2821
FU Department of Defense; NSF [1350573]; Div Of Information & Intelligent
   Systems; Direct For Computer & Info Scie & Enginr [1350573] Funding
   Source: National Science Foundation
FX Some of the material presented here was sponsored by Department of
   Defense and is approved for public release, case number 15-365 and upon
   work supported by the NSF under Grant No. 1350573.
NR 41
TC 29
Z9 36
U1 0
U2 35
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 220
EP 229
DI 10.1109/TVCG.2015.2467991
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400027
PM 26529702
DA 2025-03-07
ER

PT J
AU Hedayati, M
   Kay, M
AF Hedayati, Maryam
   Kay, Matthew
TI What University Students Learn In Visualization Classes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Bars; Interviews; Current
   measurement; Particle measurements; Museums; visualization literacy;
   visualization pedagogy; graph comprehension; visualization expertise
ID STATISTICAL LITERACY; GRAPH LITERACY
AB As a step towards improving visualization literacy, this work investigates how students approach reading visualizations differently after taking a university-level visualization course. We asked students to verbally walk through their process of making sense of unfamiliar visualizations, and conducted a qualitative analysis of these walkthroughs. Our qualitative analysis found that after taking a visualization course, students engaged with visualizations in more sophisticated ways: they were more likely to exhibit design empathy by thinking critically about the tradeoffs behind why a chart was designed in a particular way, and were better able to deconstruct a chart to make sense of it. We also gave students a quantitative assessment of visualization literacy and found no evidence of scores improving after the class, likely because the test we used focused on a different set of skills than those emphasized in visualization classes. While current measurement instruments for visualization literacy are useful, we propose developing standardized assessments for additional aspects of visualization literacy, such as deconstruction and design empathy. We also suggest that these additional aspects could be incorporated more explicitly in visualization courses. All supplemental materials are available at https://osf.io/w5pum/.
C1 [Hedayati, Maryam; Kay, Matthew] Northwestern Univ, Evanston, IL 60208 USA.
C3 Northwestern University
RP Hedayati, M (corresponding author), Northwestern Univ, Evanston, IL 60208 USA.
EM maryam.hedayati@u.northwestern.edu; mjskay@northwestern.edu
RI Kay, Matthew/AAN-2490-2021; Hedatayti, Maryam/KBQ-8003-2024
OI Kay, Matthew/0000-0001-9446-0419; Hedayati, Maryam/0000-0002-0874-5918
FU National Science Foundation [2120750]
FX The authors wish to thank the instructors of the classes we recruited
   from, and members of the MU Collective lab for their feedback. This work
   was supported in part by a grant from the National Science Foundation
   (#2120750).
NR 58
TC 1
Z9 1
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2025
VL 31
IS 1
BP 1072
EP 1082
DI 10.1109/TVCG.2024.3456291
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N9Y7G
UT WOS:001367808800006
PM 39259632
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Li, MZ
   Carr, H
   Rubel, O
   Wang, B
   Weber, GH
AF Li, Mingzhe
   Carr, Hamish
   Rubel, Oliver
   Wang, Bei
   Weber, Gunther H.
TI Distributed Augmentation, Hypersweeps, and Branch Decomposition of
   Contour Trees for Scientific Exploration
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE branch decomposition; Contour trees; parallel algorithms; computational
   topology; computational topology; topological data analysis; topological
   data analysis; computational topology; topological data analysis
ID PARALLEL COMPUTATION; TOPOLOGY
AB Contour trees describe the topology of level sets in scalar fields and are widely used in topological data analysis and visualization. A main challenge of utilizing contour trees for large-scale scientific data is their computation at scale using high-performance computing. To address this challenge, recent work has introduced distributed hierarchical contour trees for distributed computation and storage of contour trees. However, effective use of these distributed structures in analysis and visualization requires subsequent computation of geometric properties and branch decomposition to support contour extraction and exploration. In this work, we introduce distributed algorithms for augmentation, hypersweeps, and branch decomposition that enable parallel computation of geometric properties, and support the use of distributed contour trees as query structures for scientific exploration. We evaluate the parallel performance of these algorithms and apply them to identify and extract important contours for scientific visualization.
C1 [Li, Mingzhe; Wang, Bei] Univ Utah, Salt Lake City, UT 84112 USA.
   [Carr, Hamish] Univ Leeds, Leeds, England.
   [Rubel, Oliver; Weber, Gunther H.] Lawrence Berkeley Natl Lab, Berkeley, CA USA.
C3 Utah System of Higher Education; University of Utah; University of
   Leeds; United States Department of Energy (DOE); Lawrence Berkeley
   National Laboratory
RP Li, MZ (corresponding author), Univ Utah, Salt Lake City, UT 84112 USA.
EM mingzhefluorite@gmail.com; h.carr@leeds.ac.uk; oruebel@lbl.gov;
   beiwang@sci.utah.edu; ghweber@lbl.gov
RI Wang, Bei/ABH-7125-2022; Weber, Gunther/AAA-9678-2019
OI Rubel, Oliver/0000-0001-9902-1984; LI, MINGZHE/0000-0003-0355-1919;
   Wang, Bei/0000-0002-9240-0700; Weber, Gunther H./0000-0002-1794-1398
FU U.S. Department of Energy (DOE), Office of Science, Advanced Scientific
   Computing Research (ASCR) program; Exascale Computing Project
   [17-SC-20-SC, DE-AC02-05CH11231]; National Energy Research Scientific
   Computing Center (NERSC), a Department of Energy Office of Science User
   Facility [ASCR-ERCAP0026937]; DOE [DE-SC0021015]; National Science
   Foundation (NSF) [IIS-2145499]; NSF [IIS-1910733]; University of Leeds
FX This research is supported by the U.S. Department of Energy (DOE),
   Office of Science, Advanced Scientific Computing Research (ASCR) program
   and the Exascale Computing Project (17-SC-20-SC), a collaborative effort
   of the DOE Office of Science and the National Nuclear Security
   Administration under Contract No. DE-AC02-05CH11231 to the Lawrence
   Berkeley National Laboratory. This research used resources of the
   National Energy Research Scientific Computing Center (NERSC), a
   Department of Energy Office of Science User Facility using NERSC award
   ASCR-ERCAP0026937. Additionally, Mingzhe Li and Bei Wang are partially
   supported by DOE DE-SC0021015, National Science Foundation (NSF)
   IIS-2145499 and NSF IIS-1910733. Hamish Carr is supported by University
   of Leeds.
NR 44
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2025
VL 31
IS 1
BP 152
EP 162
DI 10.1109/TVCG.2024.3456322
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA O3I5E
UT WOS:001370107500006
PM 39255093
OA Green Submitted, Green Accepted
DA 2025-03-07
ER

PT J
AU Wang, S
   Zhu, MZ
   Hu, YQ
   Li, DY
   Yuan, FS
   Yu, JZ
AF Wang, Shaoan
   Zhu, Mingzhu
   Hu, Yaoqing
   Li, Dongyue
   Yuan, Fusong
   Yu, Junzhi
TI CylinderTag: An Accurate and Flexible Marker for Cylinder-Shape Objects
   Pose Estimation Based on Projective Invariants
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Location awareness; Deformation; Visualization; Pose estimation; Shape;
   Manifolds; Generators; CylinderTag; cylindrical object; fiducial marker;
   pose estimation; projective invariants
ID TRACKING; GENERATION; SYSTEM
AB High-precision pose estimation based on visual markers has been a thriving research topic in the field of computer vision. However, the suitability of traditional flat markers on curved objects is limited due to the diverse shapes of curved surfaces, which hinders the development of high-precision pose estimation for curved objects. Therefore, this paper proposes a novel visual marker called CylinderTag, which is designed for developable curved surfaces such as cylindrical surfaces. CylinderTag is a cyclic marker that can be firmly attached to objects with a cylindrical shape. Leveraging the manifold assumption, the cross-ratio in projective invariance is utilized for encoding in the direction of zero curvature on the surface. Additionally, to facilitate the usage of CylinderTag, we propose a heuristic search-based marker generator and a high-performance recognizer as well. Moreover, an all-encompassing evaluation of CylinderTag properties is conducted by means of extensive experimentation, covering detection rate, detection speed, dictionary size, localization jitter, and pose estimation accuracy. CylinderTag showcases superior detection performance from varying view angles in comparison to traditional visual markers, accompanied by higher localization accuracy. Furthermore, CylinderTag boasts real-time detection capability and an extensive marker dictionary, offering enhanced versatility and practicality in a wide range of applications. Experimental results demonstrate that the CylinderTag is a highly promising visual marker for use on cylindrical-like surfaces, thus offering important guidance for future research on high-precision visual localization of cylinder-shaped objects.
C1 [Wang, Shaoan; Hu, Yaoqing; Li, Dongyue; Yu, Junzhi] Peking Univ, Coll Engn, Dept Adv Mfg & Robot, State Key Lab Turbulence & Complex Syst, Beijing 100871, Peoples R China.
   [Zhu, Mingzhu] Fuzhou Univ, Dept Mech Engn, Fuzhou 350000, Peoples R China.
   [Yuan, Fusong] Peking Univ Sch & Hosp Stomatol, Ctr Digital Dent, Natl Engn Lab Digital & Mat Technol Stomatol, Beijing 100190, Peoples R China.
   [Yu, Junzhi] Natl Key Lab Cross Media Gen Artificial Intelligen, Beijing 100871, Peoples R China.
C3 Peking University; Fuzhou University
RP Yu, JZ (corresponding author), Peking Univ, Coll Engn, Dept Adv Mfg & Robot, State Key Lab Turbulence & Complex Syst, Beijing 100871, Peoples R China.; Yu, JZ (corresponding author), Natl Key Lab Cross Media Gen Artificial Intelligen, Beijing 100871, Peoples R China.
EM wangshaoan@stu.pku.edu.cn; mzz@fzu.edu.cn; 2101111894@stu.pku.edu.cn;
   2001111648@stu.pku.edu.cn; yuanfusong@bjmu.edu.cn; junzhi.yu@ia.ac.cn
RI Yu, Junzhi/A-7876-2010; Wang, Shaoan/ITV-1130-2023
OI Yu, Junzhi/0000-0002-6347-572X; Li, Dongyue/0000-0002-7824-8546; Wang,
   Shaoan/0000-0001-8175-1567
FU National Key Research and Development Program of China [2020YFB1312800];
   National Natural Science Foundation of China [62373109, 82371019];
   Peking University Clinical Scientist Training Program [BMU2023PYJH019];
   Fundamental Research Funds for the Central Universities
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2020YFB1312800, in part by the
   National Natural Science Foundation of China under Grants 62373109 and
   82371019, in part by the Peking University Clinical Scientist Training
   Program BMU2023PYJH019, and in part by the Fundamental Research Funds
   for the Central Universities.
NR 46
TC 1
Z9 1
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2024
VL 30
IS 12
BP 7486
EP 7499
DI 10.1109/TVCG.2024.3350901
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K8D1L
UT WOS:001346124800009
PM 38190668
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Yuan, CWH
   Yu, TW
   Pan, JY
   Lin, WC
AF Yuan, Chao-Wen Hsuan
   Yu, Tzu-Wei
   Pan, Jia-Yu
   Lin, Wen-Chieh
TI KGScope: Interactive Visual Exploration of Knowledge Graphs With
   Embedding-Based Guidance
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Knowledge graphs; Data visualization; Task analysis; Semantics; Visual
   analytics; Navigation; Load modeling; Interactive visual exploration;
   knowledge graph; knowledge graph embedding
ID LARGE-SCALE; SYSTEM
AB Knowledge graphs have been commonly used to represent relationships between entities and are utilized in the industry to enhance service qualities. As knowledge graphs integrate data from a variety of sources, they can also be useful references for data analysts. However, there is a lack of effective tools to make the most of the rich information in knowledge graphs. Existing knowledge graph exploration systems are ineffective because they did not consider various user needs and characteristics of knowledge graphs. Exploratory approaches specifically designed to uncover and summarize insights in knowledge graphs have not been well studied yet. In this article, we propose KGScope that supports interactive visual explorations and provides embedding-based guidance to derive insights from knowledge graphs. We demonstrate KGScope with usage scenarios and assess its efficacy in supporting the exploration of knowledge graphs with a user study. The results show that KGScope supports knowledge graph exploration effectively by providing useful information and helping explore the entire network.
C1 [Yuan, Chao-Wen Hsuan; Yu, Tzu-Wei; Lin, Wen-Chieh] Natl Yang Ming Chiao Tung Univ, Coll Comp Sci, Hsinchu 30010, Taiwan.
   [Pan, Jia-Yu] Google Inc, Mountain View, CA 94043 USA.
C3 National Yang Ming Chiao Tung University; Google Incorporated
RP Lin, WC (corresponding author), Natl Yang Ming Chiao Tung Univ, Coll Comp Sci, Hsinchu 30010, Taiwan.
EM starlightwinnie@gmail.com; ap9940506@gmail.com; jiayu.pan@gmail.com;
   wclin@cs.nctu.edu.tw
OI Yu, Tzu-Wei/0009-0003-5053-9688; Hsuan Yuan, Chao
   Wen/0009-0008-7134-9089
FU Taiwan National Science and Technology Council [112-2221-E-A49-057-MY3]
FX This work was supported in part by Taiwan National Science and
   Technology Council under Grant 112-2221-E-A49-057-MY3.
NR 59
TC 1
Z9 1
U1 28
U2 31
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2024
VL 30
IS 12
BP 7702
EP 7716
DI 10.1109/TVCG.2024.3360690
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K8D1L
UT WOS:001346124800011
PM 38294914
DA 2025-03-07
ER

PT J
AU Cao, Y
   Meng, XQ
   Mok, PY
   Lee, TY
   Liu, XT
   Li, P
AF Cao, Yu
   Meng, Xiangqiao
   Mok, P. Y.
   Lee, Tong-Yee
   Liu, Xueting
   Li, Ping
TI AnimeDiffusion: Anime Diffusion Colorization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image color analysis; Training; Task analysis; Faces; Image synthesis;
   Computational modeling; Noise reduction; Line drawing colorization;
   diffusion models; reference-based colorization; conditional GAN
AB Being essential in animation creation, colorizing anime line drawings is usually a tedious and time-consuming manual task. Reference-based line drawing colorization provides an intuitive way to automatically colorize target line drawings using reference images. The prevailing approaches are based on generative adversarial networks (GANs), yet these methods still cannot generate high-quality results comparable to manually-colored ones. In this article, a new AnimeDiffusion approach is proposed via hybrid diffusions for the automatic colorization of anime face line drawings. This is the first attempt to utilize the diffusion model for reference-based colorization, which demands a high level of control over the image synthesis process. To do so, a hybrid end-to-end training strategy is designed, including phase 1 for training diffusion model with classifier-free guidance and phase 2 for efficiently updating color tone with a target reference colored image. The model learns denoising and structure-capturing ability in phase 1, and in phase 2, the model learns more accurate color information. Utilizing our hybrid training strategy, the network convergence speed is accelerated, and the colorization performance is improved. Our AnimeDiffusion generates colorization results with semantic correspondence and color consistency. In addition, the model has a certain generalization performance for line drawings of different line styles. To train and evaluate colorization methods, an anime face line drawing colorization benchmark dataset, containing 31,696 training data and 579 testing data, is introduced and shared. Extensive experiments and user studies have demonstrated that our proposed AnimeDiffusion outperforms state-of-the-art GAN-based methods and another diffusion-based model, both quantitatively and qualitatively.
C1 [Cao, Yu; Mok, P. Y.] Hong Kong Polytech Univ, Sch Fash & Text, Hong Kong, Peoples R China.
   [Meng, Xiangqiao; Li, Ping] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
   [Meng, Xiangqiao; Li, Ping] Hong Kong Polytech Univ, Sch Design, Hong Kong, Peoples R China.
   [Mok, P. Y.] Lab Artificial Intelligence Design, Hong Kong, Peoples R China.
   [Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 70101, Taiwan.
   [Liu, Xueting] St Francis Univ, Sch Comp & Informat Sci, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University; Hong Kong Polytechnic University; Hong
   Kong Polytechnic University; National Cheng Kung University; Saint
   Francis University Hong Kong
RP Mok, PY (corresponding author), Hong Kong Polytech Univ, Sch Fash & Text, Hong Kong, Peoples R China.; Li, P (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.; Li, P (corresponding author), Hong Kong Polytech Univ, Sch Design, Hong Kong, Peoples R China.
EM yu-daniel.cao@connect.polyu.hk; xiangqiao.meng@connect.polyu.hk;
   tracy.mok@polyu.edu.hk; tonylee@mail.ncku.edu.tw; tliu@cihe.edu.hk;
   p.li@polyu.edu.hk
RI Li, Ping/AAO-2019-2020
OI Cao, Yu (Daniel)/0000-0002-9761-0723; Li, Ping/0000-0002-1503-0240; Mok,
   Tracy/0000-0002-0635-5318; Liu, Xueting/0000-0002-0868-5353
FU Innovation and Technology Commission of Hong Kong [ITP/028/21TP];
   National Science and Technology Council, Taiwan
   [110-2221-E-006-135-MY3]; Hong Kong Polytechnic University [P0044520,
   P0048387, P0042740, P0035358, P0043906, P0030419]
FX This work was supported in part by the Innovation and Technology
   Commission of Hong Kong under Grant ITP/028/21TP, in part by the
   National Science and Technology Council under Grant
   110-2221-E-006-135-MY3, Taiwan, and in part by The Hong Kong Polytechnic
   University under Grants P0044520, P0048387, P0042740, P0035358,
   P0043906, and P0030419.
NR 44
TC 2
Z9 2
U1 11
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6956
EP 6969
DI 10.1109/TVCG.2024.3357568
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F2M7M
UT WOS:001308219500004
PM 38261497
DA 2025-03-07
ER

PT J
AU Oral, B
   Dragicevic, P
   Telea, A
   Dimara, E
AF Oral, Basak
   Dragicevic, Pierre
   Telea, Alexandru
   Dimara, Evanthia
TI Decoupling Judgment and Decision Making: A Tale of Two Tails
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cognition; decision making; judgment; psychology; visualization;
   Cognition; decision making; judgment; psychology; visualization
ID CONFIDENCE-INTERVALS; GRAPHICAL DISPLAYS; VISUALIZATION; INFORMATION;
   COMMUNICATION; DESIGN
AB Is it true that if citizens understand hurricane probabilities, they will make more rational decisions for evacuation? Finding answers to such questions is not straightforward in the literature because the terms "judgment" and "decision making" are often used interchangeably. This terminology conflation leads to a lack of clarity on whether people make suboptimal decisions because of inaccurate judgments of information conveyed in visualizations or because they use alternative yet currently unknown heuristics. To decouple judgment from decision making, we review relevant concepts from the literature and present two preregistered experiments (N = 601) to investigate if the task (judgment versus decision making), the scenario (sports versus humanitarian), and the visualization (quantile dotplots, density plots, probability bars) affect accuracy. While experiment 1 was inconclusive, we found evidence for a difference in experiment 2. Contrary to our expectations and previous research, which found decisions less accurate than their direct-equivalent judgments, our results pointed in the opposite direction. Our findings further revealed that decisions were less vulnerable to status-quo bias, suggesting decision makers may disfavor responses associated with inaction. We also found that both scenario and visualization types can influence people's judgments and decisions. Although effect sizes are not large and results should be interpreted carefully, we conclude that judgments cannot be safely used as proxy tasks for decision making, and discuss implications for visualization research and beyond. Materials and preregistrations are available at https://osf.io/ufzp5/?view_only=adc0f78a23804c31bf7fdd9385cb264f.
C1 [Oral, Basak; Telea, Alexandru; Dimara, Evanthia] Univ Utrecht, NL-3584 CS Utrecht, Netherlands.
   [Dragicevic, Pierre] Inria Bordeaux, F-33405 Talence, France.
C3 Utrecht University
RP Oral, B (corresponding author), Univ Utrecht, NL-3584 CS Utrecht, Netherlands.
EM e.oral@uu.nl; pierre.dragice@gmail.com; a.c.telea@uu.nl;
   evanthia.dimara@gmail.com
RI Dragicevic, Pierre/HKV-4981-2023
OI Dimara, Evanthia/0000-0001-5212-7888; Telea, Alexandru
   Cristian/0000-0003-0750-0502
FU Department of Information and Computing Sciences at Utrecht University
FX This work was supported by the Department of Information and Computing
   Sciences at Utrecht University.
NR 87
TC 0
Z9 0
U1 4
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6928
EP 6940
DI 10.1109/TVCG.2023.3346640
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600018
PM 38145516
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Lin, ZH
   Hu, CX
   Jia, JZ
   Li, S
AF Lin, Zehui
   Hu, Chenxiao
   Jia, Jinzhu
   Li, Sheng
TI Hypothesis Testing for Progressive Kernel Estimation and VCM Framework
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Photonics; Kernel; Estimation; Testing; Analysis of variance; Lighting;
   Merging; bidirectional path sampling; f-test; hypothesis testing; kernel
   estimation; progressive photon mapping; radius; statistical model;
   vertex connection and merging
ID STILL
AB Identifying an appropriate radius for unbiased kernel estimation is crucial for the efficiency of radiance estimation. However, determining both the radius and unbiasedness still faces big challenges. In this paper, we first propose a statistical model of photon samples and associated contributions for progressive kernel estimation, under which the kernel estimation is unbiased if the null hypothesis of this statistical model stands. Then, we present a method to decide whether to reject the null hypothesis about the statistical population (i.e., photon samples) by the F-test in the Analysis of Variance. Hereby, we implement a progressive photon mapping (PPM) algorithm, wherein the kernel radius is determined by this hypothesis test for unbiased radiance estimation. Second, we propose VCM+, a reinforcement of Vertex Connection and Merging (VCM), and derive its theoretically unbiased formulation. VCM+ combines hypothesis testing-based PPM with bidirectional path tracing (BDPT) via multiple importance sampling (MIS), wherein our kernel radius can leverage the contributions from PPM and BDPT. We test our new algorithms, improved PPM and VCM+, on diverse scenarios with different lighting settings. The experimental results demonstrate that our method can alleviate light leaks and visual blur artifacts of prior radiance estimate algorithms. We also evaluate the asymptotic performance of our approach and observe an overall improvement over the baseline in all testing scenarios.
C1 [Lin, Zehui; Hu, Chenxiao; Li, Sheng] Peking Univ, Sch Comp Sci, Beijing 100871, Peoples R China.
   [Li, Sheng] Peking Univ, Natl Biomed Imaging Ctr, Beijing 100871, Peoples R China.
   [Jia, Jinzhu] Peking Univ, Dept Biostat, Beijing, Peoples R China.
   [Jia, Jinzhu] Peking Univ, Ctr Stat Sci, Beijing, Peoples R China.
C3 Peking University; Peking University; Peking University; Peking
   University
RP Li, S (corresponding author), Peking Univ, Sch Comp Sci, Beijing 100871, Peoples R China.
EM zehui@pku.edu.cn; hinevenwob@qq.com; jzjia@math.pku.edu.cn;
   lisheng@pku.edu.cn
OI Hu, Chenxiao/0009-0000-3306-6653; Jia, Jinzhu/0000-0002-6554-2463; Lin,
   Zehui/0009-0008-7855-4296; Li, Sheng/0000-0002-8901-2184
FU National Key Research and Development Program of China [2022ZD0160805];
   NSFC of China [62172013]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2022ZD0160805, and in par tby
   NSFC of China under Grant 62172013.
NR 40
TC 1
Z9 1
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4709
EP 4723
DI 10.1109/TVCG.2023.3274595
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400010
PM 37159325
DA 2025-03-07
ER

PT J
AU Nguyen, W
   Gramann, K
   Gehrke, L
AF Nguyen, Willy
   Gramann, Klaus
   Gehrke, Lukas
TI Modeling the Intent to Interact With VR Using Physiological Features
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Electromyography; Electroencephalography; Muscles; Electrodes; Task
   analysis; Physiology; Brain modeling; Brain-computer interfaces;
   electroencephalography; virtual reality
AB Objective: Mixed-Reality (XR) technologies promise a user experience (UX) that rivals the interactive experience with the real-world. The key facilitators in the design of such a natural UX are that the interaction has zero lag and that users experience no excess mental load. This is difficult to achieve due to technical constraints such as motion-to-photon latency as well as false-positives during gesture-based interaction. Methods: In this paper, we explored the use of physiological features to model the user's intent to interact with a virtual reality (VR) environment. Accurate predictions about when users want to express an interaction intent could overcome the limitations of an interactive device that lags behind the intention of a user. We computed time-domain features from electroencephalography (EEG) and electromyography (EMG) recordings during a grab-and-drop task in VR and cross-validated a Linear Discriminant Analysis (LDA) for three different combinations of (1) EEG, (2) EMG and (3) EEG-EMG features. Results & Conclusion: We found the classifiers to detect the presence of a pre-movement state from background idle activity reflecting the users' intent to interact with the virtual objects (EEG: 62% +/- 10%, EMG: 72% +/- 9%, EEG-EMG: 69% +/- 10%) above simulated chance level. The features leveraged in our classification scheme have a low computational cost and are especially useful for fast decoding of users' mental states. Our work is a further step towards a useful classification of users' intent to interact, as a high temporal resolution and speed of detection is crucial. This facilitates natural experiences through zero-lag adaptive interfaces.
C1 [Nguyen, Willy] Univ Paris Saclay, F-91190 Gif Sur Yvette, France.
   [Gramann, Klaus; Gehrke, Lukas] TU Berlin, D-10623 Berlin, Germany.
C3 Universite Paris Saclay; Technical University of Berlin
RP Nguyen, W (corresponding author), Univ Paris Saclay, F-91190 Gif Sur Yvette, France.
EM willy.nguyen@hotmail.fr; klaus.gramann@tu-berlin.de;
   lukas.gehrke@tu-berlin.de
RI Gramann, Klaus/AAD-7313-2019; Gehrke, Lukas/KAM-0750-2024
OI Gramann, Klaus/0000-0003-2673-1832; Gehrke, Lukas/0000-0003-3661-1973;
   NGUYEN, WILLY/0000-0002-2904-8011
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
   [GR2627/13-1]
FX The work of Lukas Gehrke was supported by the Deutsche
   Forschungsgemeinschaft (DFG, German Research Foundation) under Grant
   GR2627/13-1.
NR 44
TC 1
Z9 1
U1 2
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5893
EP 5900
DI 10.1109/TVCG.2023.3308787
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400055
PM 37624723
DA 2025-03-07
ER

PT J
AU Zhou, XC
   Li, BS
   Benes, B
   Fei, SL
   Pirk, S
AF Zhou, Xiaochen
   Li, Bosheng
   Benes, Bedrich
   Fei, Songlin
   Pirk, Soren
TI DeepTree: Modeling Trees With Situated Latents
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Measurement; Solid modeling; Three-dimensional displays; Shape;
   Computational modeling; Neural networks; Pipelines; Botanical tree
   models; deep learning; generative methods; shape modeling
ID VIRTUAL CLIMBING PLANTS
AB In this article, we propose DeepTree, a novel method for modeling trees based on learning developmental rules for branching structures instead of manually defining them. We call our deep neural model "situated latent" because its behavior is determined by the intrinsic state -encoded as a latent space of a deep neural model- and by the extrinsic (environmental) data that is "situated" as the location in the 3D space and on the tree structure. We use a neural network pipeline to train a situated latent space that allows us to locally predict branch growth only based on a single node in the branch graph of a tree model. We use this representation to progressively develop new branch nodes, thereby mimicking the growth process of trees. Starting from a root node, a tree is generated by iteratively querying the neural network on the newly added nodes resulting in the branching structure of the whole tree. Our method enables generating a wide variety of tree shapes without the need to define intricate parameters that control their growth and behavior. Furthermore, we show that the situated latents can also be used to encode the environmental response of tree models, e.g., when trees grow next to obstacles. We validate the effectiveness of our method by measuring the similarity of our tree models and by procedurally generated ones based on a number of established metrics for tree form.
C1 [Zhou, Xiaochen; Li, Bosheng; Benes, Bedrich] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
   [Fei, Songlin] Purdue Univ, Dept Forestry & Nat Resources, W Lafayette, IN 47907 USA.
   [Pirk, Soren] Univ Kiel, Dept Comp Sci, D-24143 Kiel, Germany.
C3 Purdue University System; Purdue University; Purdue University System;
   Purdue University; University of Kiel
RP Benes, B (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.
EM zhou1178@purdue.edu; li2343@purdue.edu; bbenes@purdue.edu;
   sfei@purdue.edu; soeren.pirk@gmail.com
RI Fei, Songlin/JTD-3325-2023; Benes, Bedrich/A-8150-2016
OI Fei, Songlin/0000-0003-2772-0166; Benes, Bedrich/0000-0002-5293-2112;
   Zhou, Xiaochen/0000-0003-2280-7799; LI, BOSHENG/0009-0006-6490-1184;
   Pirk, Soren/0000-0003-1937-9797
FU Foundation for Food and Agriculture Research, United States [602757];
   USDA NIFA [2023-68012-38992]
FX This research was partially supported in part by the Foundation for Food
   and Agriculture Research, United States Grant under Grant 602757 to
   Benes, and in part by USDA NIFA, under Grant #2023-68012-38992 to
   Benesand Fei.
NR 57
TC 8
Z9 8
U1 1
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5795
EP 5809
DI 10.1109/TVCG.2023.3307887
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400053
PM 37610910
OA Green Submitted
DA 2025-03-07
ER

PT J
AU McGuffin, MJ
   Servera, R
   Forest, M
AF McGuffin, Michael J.
   Servera, Ryan
   Forest, Marie
TI Path Tracing in 2D, 3D, and Physicalized Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Layout; Task analysis; Headphones; Data
   visualization; Mice; Visualization; 3D printing; augmented reality; data
   physicalization; graph visualization; path finding; path following;
   tangible
ID VISUALIZATION; EXPLORATION; INTERFACES; DISPLAYS; STEREO
AB It is common to advise against using 3D to visualize abstract data such as networks, however Ware and Mitchell's 2008 study showed that path tracing in a network is less error prone in 3D than in 2D. It is unclear, however, if 3D retains its advantage when the 2D presentation of a network is improved using edge-routing, and when simple interaction techniques for exploring the network are available. We address this with two studies of path tracing under new conditions. The first study was preregistered, involved 34 users, and compared 2D and 3D layouts that the user could rotate and move in virtual reality with a handheld controller. Error rates were lower in 3D than in 2D, despite the use of edge-routing in 2D and the use of mouse-driven interactive highlighting of edges. The second study involved 12 users and investigated data physicalization, comparing 3D layouts in virtual reality versus physical 3D printouts of networks augmented with a Microsoft HoloLens headset. No difference was found in error rate, but users performed a variety of actions with their fingers in the physical condition which can inform new interaction techniques.
C1 [McGuffin, Michael J.; Servera, Ryan; Forest, Marie] Ecole Technol Super, Montreal, PQ H3C 1K3, Canada.
C3 University of Quebec; Ecole de Technologie Superieure - Canada
RP McGuffin, MJ (corresponding author), Ecole Technol Super, Montreal, PQ H3C 1K3, Canada.
EM michael.mcguffin@etsmtl.ca; servera.ryan@gmail.com;
   marie.forest@etsmtl.ca
OI McGuffin, Michael/0000-0001-7782-5754
FU Microsoft Research; NSERC
FX This work was supported by Microsoft Research and NSERC.
NR 70
TC 3
Z9 3
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3564
EP 3577
DI 10.1109/TVCG.2023.3238989
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700027
PM 37021998
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ye, YL
   Huang, R
   Zeng, W
AF Ye, Yilin
   Huang, Rong
   Zeng, Wei
TI VISAtlas: An Image-Based Exploration and Query System for Large
   Visualization Collections via Neural Image Embedding
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Feature extraction; Task analysis;
   Layout; Taxonomy; Semantics; Visualization collection; image embedding;
   visual query; image visualization; design pattern
ID INFORMATION VISUALIZATION; VISUAL ANALYSIS; KNOWLEDGE; DESIGN; SPACE
AB High-quality visualization collections are beneficial for a variety of applications including visualization reference and data-driven visualization design. The visualization community has created many visualization collections, and developed interactive exploration systems for the collections. However, the systems are mainly based on extrinsic attributes like authors and publication years, whilst neglect intrinsic property (i.e., visual appearance) of visualizations, hindering visual comparison and query of visualization designs. This paper presents VISAtlas, an image-based approach empowered by neural image embedding, to facilitate exploration and query for visualization collections. To improve embedding accuracy, we create a comprehensive collection of synthetic and real-world visualizations, and use it to train a convolutional neural network (CNN) model with a triplet loss for taxonomical classification of visualizations. Next, we design a coordinated multiple view (CMV) system that enables multi-perspective exploration and design retrieval based on visualization embeddings. Specifically, we design a novel embedding overview that leverages contextual layout framework to preserve the context of the embedding vectors with the associated visualization taxonomies, and density plot and sampling techniques to address the overdrawing problem. We demonstrate in three case studies and one user study the effectiveness of VISAtlas in supporting comparative analysis of visualization collections, exploration of composite visualizations, and image-based retrieval of visualization designs. The studies reveal that real-world visualization collections (e.g., Beagle and VIS30K) better accord with the richness and diversity of visualization designs than synthetic collections (e.g., Data2Vis), inspiring composite visualizations are identified in real-world collections, and distinct design patterns exist in visualizations from different sources.
C1 [Ye, Yilin; Huang, Rong; Zeng, Wei] Hong Kong Univ Sci & Technol Guangzhou, Guangzhou 511400, Guangdong, Peoples R China.
   [Ye, Yilin; Huang, Rong; Zeng, Wei] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology (Guangzhou); Hong Kong
   University of Science & Technology
RP Zeng, W (corresponding author), Hong Kong Univ Sci & Technol Guangzhou, Guangzhou 511400, Guangdong, Peoples R China.; Zeng, W (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
EM yyebd@connect.ust.hk; rhuang421@connect.hkust-gz.edu.cn; weizeng@ust.hk
RI Ye, Yilin/GRR-8394-2022
OI Ye, Yilin/0000-0001-8874-5928; Zeng, Wei/0000-0002-5600-8824; Huang,
   Rong/0000-0002-6807-3148
FU National Natural Science Foundation of China [62172398]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62172398.
NR 76
TC 5
Z9 5
U1 10
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3224
EP 3240
DI 10.1109/TVCG.2022.3229023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700076
PM 37015539
DA 2025-03-07
ER

PT J
AU Cauquis, J
   Peillard, E
   Dominjon, L
   Duval, T
   Moreau, G
AF Cauquis, Julien
   Peillard, Etienne
   Dominjon, Lionel
   Duval, Thierry
   Moreau, Guillaume
TI Investigating Whether the Mass of a Tool Replica Influences Virtual
   Training Learning Outcomes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual Reality; Virtual Training; Prop Design; Weight Perception; User
   Study
ID COGNITIVE LOAD; ENVIRONMENT; REALITY
AB Virtual Reality (VR) has emerged as a promising solution to address the pressing concern of transferring know-how in the manufacturing industry. Making an immersive training experience often involves designing an instrumented replica of a tool whose use is to be learned through virtual training. The process of making a replica can alter its mass, making it different from that of the original tool. As far as we know, the influence of this difference on learning outcomes has never been evaluated. To investigate this subject, an immersive training experience was designed with pre and post-training phases under real conditions, dedicated to learning the use of a rotary tool. 80 participants took part in this study, split into three groups: a control group performing the virtual training using a replica with the same mass as the original tool (m = 100%), a second group that used a replica with a lighter mass than the original tool (m= 50%) and a third group using a replica heavier than the original tool (m = 150%). Despite variations in the mass of the replica used for training, this study revealed that the learning outcomes remained comparable across all groups, while also demonstrating significant enhancements in certain performance measures, including task completion time. Overall, these findings provide useful insights regarding the design of tool replicas for immersive training.
C1 [Cauquis, Julien; Dominjon, Lionel] CLARTE, Nice, France.
   [Cauquis, Julien; Peillard, Etienne; Duval, Thierry; Moreau, Guillaume] IMT Atlantique, UMR CNRS 6285, Lab STICC, Brest, France.
C3 IMT - Institut Mines-Telecom; IMT Atlantique; Universite de Bretagne
   Occidentale
RP Cauquis, J (corresponding author), CLARTE, Nice, France.; Cauquis, J (corresponding author), IMT Atlantique, UMR CNRS 6285, Lab STICC, Brest, France.
EM julien.cauquis@clarte-lab.fr; etienne.peillard@imt-atlantique.fr;
   lionel.dominjon@clarte-lab.fr; thierry.duval@imt-atlantique.fr;
   guillaume.moreau@imt-atlantique.fr
RI Duval, Thierry/AAH-4372-2020; Moreau, Guillaume/I-3153-2013
OI Peillard, Etienne/0000-0002-4429-670X; Moreau,
   Guillaume/0000-0003-2215-1865
FU National Research Agency
FX No Statement Available
NR 66
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2411
EP 2421
DI 10.1109/TVCG.2024.3372041
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400010
PM 38437074
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ibrahim, MT
   Gopi, M
   Majumder, A
AF Ibrahim, Muhammad Twaha
   Gopi, M.
   Majumder, Aditi
TI Real-Time Seamless Multi-Projector Displays on Deformable Surfaces
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cameras; Calibration; Three-dimensional displays; Shape; Surface
   reconstruction; Real-time systems; Surface fitting; Computing
   Methodologies; Artificial Intelligence; Computer Vision; Image and Video
   Acquisition
ID CALIBRATION
AB Prior works on multi-projector displays have focused primarily on static rigid objects, some focusing on dynamic rigid objects. However, works on projection based displays on deformable dynamic objects have focused only on small scale single projector displays. Tracking a deformable dynamic surface and updating projections precisely in real time on it is a significantly challenging task, even for a single projector system. In this paper, we present the first end-to-end solution for achieving a real-time, seamless display on deformable surfaces using mutliple unsychronized projectors without requiring any prior knowledge of the surface or device parameters. The system first accurately calibrates multiple RGB-D cameras and projectors using the deformable display surface itself, and then using those calibrated devices, tracks the continuous changes in the surface shape. Based on the deformation and projector calibration, the system warps and blends the image content in real-time to create a seamless display on a surface that continuously changes shape. Using multiple projectors and RGB-D cameras, we provide the much desired aspect of scale to the displays on deformable surfaces. Most prior dynamic multi-projector systems assume rigid objects and depend critically on the constancy of surface normals and non-existence of local shape deformations. These assumptions break in deformable surfaces making prior techniques inapplicable. Point-based correspondences become inadequate for calibration, exacerbated with no synchronization between the projectors. A few works address non-rigid objects with several restrictions like targeting semi-deformable surfaces (e.g. human face), or using single coaxial (optically aligned) projector-camera pairs, or temporally synchronized cameras. We break loose from such restrictions and handle multiple projector systems for dynamic deformable fabric-like objects using temporally unsynchronized devices. We devise novel methods using ray and plane-based constraints imposed by the pinhole camera model to address these issues and design new blending methods dependent on 3D distances suitable for deformable surfaces. Finally, unlike all prior work with rigid dynamic surfaces that use a single RGB-D camera, we devise a method that involve all RGB-D cameras for tracking since the surface is not seen completely by a single camera. These methods enable a seamless display at scale in the presence of continuous movements and deformations. This work has tremendous applications on mobile and expeditionary systems where environmentals (e.g. wind, vibrations, suction) cannot be avoided. One can create large displays on tent walls in remote, austere military or emergency operations in minutes to support large scale command and control, mission rehearsal or training operations. It can be used to create displays on mobile and inflatable objects for tradeshows/events and touring edutainment applications.
C1 [Ibrahim, Muhammad Twaha; Gopi, M.; Majumder, Aditi] Univ Calif Irvine, Irvine, CA 92697 USA.
C3 University of California System; University of California Irvine
RP Ibrahim, MT (corresponding author), Univ Calif Irvine, Irvine, CA 92697 USA.
EM muhammti@uci.edu; gopi@ics.uci.edu; majumder@ics.uci.edu
RI Ibrahim, Muhammad Twaha/ADD-8310-2022
OI Ibrahim, Muhammad Twaha/0000-0001-9286-6124
FU US Air Force
FX No Statement Available
NR 65
TC 0
Z9 1
U1 5
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2527
EP 2537
DI 10.1109/TVCG.2024.3372097
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400002
PM 38437087
DA 2025-03-07
ER

PT J
AU Jackson, B
   Lor, L
   Heggeseth, BC
AF Jackson, Bret
   Lor, Linda
   Heggeseth, Brianna C.
TI Workspace Guardian: Investigating Awareness of Personal Workspace
   Between Co-Located Augmented Reality Users
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Mixed reality; augmented reality; three-dimensional displays
AB As augmented reality (AR) systems proliferate and the technology gets smaller and less intrusive, we imagine a future where many AR users will interact in the same physical locations (e.g., in shared work places and public spaces). While previous research has explored AR collaboration in these spaces, our focus is on co-located but independent work. In this paper, we explore co-located AR user behavior and investigate techniques for promoting awareness of personal workspace boundaries. Specifically, we compare three techniques: showing all virtual content, visualizing bounding box outlines of content, and a self-defined workspace boundary. The findings suggest that a self-defined boundary led to significantly more personal workspace encroachments.
C1 [Jackson, Bret; Lor, Linda; Heggeseth, Brianna C.] Macalester Coll, St Paul, MN 55105 USA.
C3 Macalester College
RP Jackson, B (corresponding author), Macalester Coll, St Paul, MN 55105 USA.
EM bjackson@macalester.edu; llor@macalester.edu; bheggese@macalester.edu
FU Clare Booth Luce Foundation
FX No Statement Available
NR 15
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2724
EP 2733
DI 10.1109/TVCG.2024.3372073
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400056
PM 38437099
DA 2025-03-07
ER

PT J
AU Lenz, LS
   Fender, AR
   Chatain, J
   Holz, C
AF Lenz, Lara Sofie
   Fender, Andreas Rene
   Chatain, Julia
   Holz, Christian
TI Comparing Synchronous and Asynchronous Task Delivery in Mixed Reality
   Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Virtual reality; Mixed reality; Switches; Collaboration;
   Asynchronous communication; Training; Mixed Reality; Workspaces;
   Interruptions; Evaluation; Task focus
ID INTERRUPTIONS; WORK
AB Asynchronous digital communication is a widely applied and well-known form of information exchange. Most pieces of technology make use of some variation of asynchronous communication systems, be it messaging or email applications. This allows recipients to process digital messages immediately (synchronous) or whenever they have time (asynchronous), meaning that purely digital interruptions can be mitigated easily. Mixed Reality systems have the potential to not only handle digital interruptions but also interruptions in physical space, e.g., caused by co-workers in workspaces or learning environments. However, the benefits of such systems previously remained untested in the context of Mixed Reality. We conducted a user study (N=26) to investigate the impact that the timing of task delivery has on the participants' performance, workflow, and emotional state. Participants had to perform several cognitively demanding tasks in a Mixed Reality workspace. Inside the virtual workspace, we simulated in-person task delivery either during tasks (i.e., interrupting the participant) or between tasks (i.e., delaying the interruption). Our results show that delaying interruptions has a significant impact on subjective metrics like the perceived performance and workload.
C1 [Lenz, Lara Sofie; Fender, Andreas Rene; Holz, Christian] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
   [Chatain, Julia] Swiss Fed Inst Technol, Dept Educ Dev & Technol, Zurich, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich; Swiss Federal
   Institutes of Technology Domain; ETH Zurich
RP Fender, AR (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.
EM lara.lenz@gmx.ch; progga.af@gmail.com; jchatain@ethz.ch;
   christian.holz@inf.ethz.ch
RI Holz, Christian/AAV-4925-2020; Chatain, Julia/AEH-7938-2022
OI Holz, Christian/0000-0001-9655-9519; Fender, Andreas/0000-0002-5903-0736
FU Zurich Information Security and Privacy Center
FX No Statement Available
NR 66
TC 1
Z9 1
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2776
EP 2784
DI 10.1109/TVCG.2024.3372034
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400042
PM 38437079
DA 2025-03-07
ER

PT J
AU Ren, YL
   Zhang, Y
   Liu, ZT
   Xie, N
AF Ren, Yunlei
   Zhang, Yan
   Liu, Zhitao
   Xie, Ning
TI Eye-Hand Typing: Eye Gaze Assisted Finger Typing via Bayesian Processes
   in AR
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Keyboards; Production facilities; Bayes methods; Task analysis;
   Performance evaluation; Prediction algorithms; Visualization; Augmented
   reality; text entry; multi-modal interaction; eye-hand coordination;
   bayesian process; fitts' law; primacy effect
ID TEXT ENTRY; COORDINATION; INPUT
AB Nowadays, AR HMDs are widely used in scenarios such as intelligent manufacturing and digital factories. In a factory environment, fast and accurate text input is crucial for operators' efficiency and task completion quality. However, the traditional AR keyboard may not meet this requirement, and the noisy environment is unsuitable for voice input. In this article, we introduce Eye-Hand Typing, an intelligent AR keyboard. We leverage the speed advantage of eye gaze and use a Bayesian process based on the information of gaze points to infer users' text input intentions. We improve the underlying keyboard algorithm without changing user input habits, thereby improving factory users' text input speed and accuracy. In real-time applications, when the user's gaze point is on the keyboard, the Bayesian process can predict the most likely characters, vocabulary, or commands that the user will input based on the position and duration of the gaze point and input history. The system can enlarge and highlight recommended text input options based on the predicted results, thereby improving user input efficiency. A user study showed that compared with the current HoloLens 2 system keyboard, Eye-Hand Typing could reduce input error rates by 28.31 % and improve text input speed by 14.5%. It also outperformed a gaze-only technique, being 43.05% more accurate and 39.55% faster. And it was no significant compromise in eye fatigue. Users also showed positive preferences.
C1 [Ren, Yunlei; Zhang, Yan; Liu, Zhitao; Xie, Ning] Univ Elect Sci & Technol China, Ctr Future Media, Sch Comp Sci & Engn, Chengdu, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Xie, N (corresponding author), Univ Elect Sci & Technol China, Ctr Future Media, Sch Comp Sci & Engn, Chengdu, Peoples R China.
EM allenrens@qq.com; 1278425816@qq.com; zl425uestc@gmail.com;
   seanxiening@gmail.com
FU National Key R&D Program of China
FX No Statement Available
NR 75
TC 0
Z9 0
U1 3
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2496
EP 2506
DI 10.1109/TVCG.2024.3372106
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400049
PM 38498759
DA 2025-03-07
ER

PT J
AU Wang, JL
   Shi, RK
   Li, XD
   Wei, YS
   Liang, HN
AF Wang, Jialin
   Shi, Rongkai
   Li, Xiaodong
   Wei, Yushi
   Liang, Hai-Ning
TI Omnidirectional Virtual Visual Acuity: A User-Centric Visual Clarity
   Metric for Virtual Reality Head-Mounted Displays and Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Measurement; Resists; Rendering (computer graphics);
   Optical imaging; Image resolution; Optical sensors; Virtual reality;
   Head-mounted displays; Measurements; Visual clarity; Passthrough; Render
   resolution; Frame rate
ID PERFORMANCE; LOGMAR
AB Users' perceived image quality of virtual reality head-mounted displays (VR HMDs) is determined by multiple factors, including the HMD's structure, optical system, display and render resolution, and users' visual acuity (VA). Existing metrics such as pixels per degree (PPD) have limitations that prevent accurate comparison of different VR HMDs. One of the main limitations is that not all VR HMD manufacturers released the official PPD or details of their HMDs' optical systems. Without these details, developers and users cannot know the precise PPD or calculate it for a given HMD. The other issue is that the visual clarity varies with the VR environment. Our work has identified a gap in having a feasible metric that can measure the visual clarity of VR HMDs. To address this gap, we present an end-to-end and user-centric visual clarity metric, omnidirectional virtual visual acuity (OVVA), for VR HMDs. OVVA extends the physical visual acuity chart into a virtual format to measure the virtual visual acuity of an HMD's central focal area and its degradation in its noncentral area. OVVA provides a new perspective to measure visual clarity and can serve as an intuitive and accurate reference for VR applications sensitive to visual accuracy. Our results show that OVVA is a simple yet effective metric for comparing VR HMDs and environments.
C1 [Wang, Jialin; Shi, Rongkai; Li, Xiaodong; Wei, Yushi] Xian Jiaotong Liverpool Univ, Sch Adv Technol, Suzhou, Peoples R China.
   [Wang, Jialin; Shi, Rongkai; Wei, Yushi] Univ Liverpool, Dept Comp Sci, Liverpool, England.
   [Liang, Hai-Ning] Xian Jiaotong Liverpool Univ, Sch Adv Technol, Dept Comp, Suzhou, Peoples R China.
C3 Xi'an Jiaotong-Liverpool University; University of Liverpool; Xi'an
   Jiaotong-Liverpool University
RP Liang, HN (corresponding author), Xian Jiaotong Liverpool Univ, Sch Adv Technol, Dept Comp, Suzhou, Peoples R China.
EM Jialin.Wang16@student.xjtlu.edu.cn; rongkai.shi19@student.xjtlu.edu.cn;
   Xiaodong.Li22@student.xjtlu.edu.cn; Yushi.Wei21@student.xjtlu.edu.cn;
   haining.liang@xjtlu.edu.cn
RI Wang, Jialin/KFS-9745-2024
OI Wang, Jialin/0000-0002-1990-1293; Shi, Rongkai/0000-0001-8845-6034; Wei,
   Yushi/0000-0002-6003-0557; Liang, Hai-Ning/0000-0003-3600-8955
FU Suzhou Municipal Key Laboratory for Intelligent Virtual Engineering
FX No Statement Available
NR 47
TC 3
Z9 3
U1 3
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2033
EP 2043
DI 10.1109/TVCG.2024.3372127
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400066
PM 38437113
DA 2025-03-07
ER

PT J
AU Braun, D
   Borgo, R
   Sondag, M
   von Landesberger, T
AF Braun, Daniel
   Borgo, Rita
   Sondag, Max
   von Landesberger, Tatiana
TI Reclaiming the Horizon: Novel Visualization Designs for Time-Series Data
   with Large Value Ranges
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Task analysis; Image color analysis; Bars;
   Standards; Market research; Estimation; Visualization techniques;
   time-series; design study; orders of magnitude; logarithmic scale
AB introduce two novel visualization designs to support practitioners in performing identification and discrimination tasks on large value ranges (i.e., several orders of magnitude) in time-series data: (1) The order of magnitude horizon graph, which extends the classic horizon graph; and (2) the order of magnitude line chart, which adapts the log-line chart. These new visualization designs visualize large value ranges by explicitly splitting the mantissa m and exponent e of a value v = m <middle dot> 10(e). We evaluate our novel designs against the most relevant state-of-the-art visualizations in an empirical user study. It focuses on four main tasks commonly employed in the analysis of time-series and large value ranges visualization: identification, discrimination, estimation, and trend detection. For each task we analyze error, confidence, and response time. The new order of magnitude horizon graph performs better or equal to all other designs in identification, discrimination, and estimation tasks. Only for trend detection tasks, the more traditional horizon graphs reported better performance. Our results are domain-independent, only requiring time-series data with large value ranges.
C1 [Braun, Daniel; Sondag, Max; von Landesberger, Tatiana] Univ Cologne, Cologne, Germany.
   [Borgo, Rita] Kings Coll London, London, England.
C3 University of Cologne; University of London; King's College London
RP Braun, D (corresponding author), Univ Cologne, Cologne, Germany.
EM braun@cs.uni-koeln.de; rita.borgo@kcl.ac.uk; sondag@cs.uni-koeln.de;
   landesberger@cs.uni-koeln.de
RI Braun, Daniel/B-7696-2015; sondag, max/KIJ-0026-2024
OI sondag, max/0000-0003-3309-638X; Braun, Daniel/0000-0002-8824-7184; von
   Landesberger, Tatiana/0000-0002-5279-1444
FU BMBF
FX No Statement Available
NR 47
TC 1
Z9 1
U1 7
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 1161
EP 1171
DI 10.1109/TVCG.2023.3326576
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500096
PM 37871083
OA Green Published, Green Submitted
DA 2025-03-07
ER

PT J
AU Offenwanger, A
   Brehmer, M
   Chevalier, F
   Tsandilas, T
AF Offenwanger, Anna
   Brehmer, Matthew
   Chevalier, Fanny
   Tsandilas, Theophanis
TI TimeSplines: Sketch-Based Authoring of Flexible and Idiosyncratic
   Timelines
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Temporal Data; interaction design; communication / presentation;
   storytelling; sketch-based interface; lazy data binding
ID VISUALIZATION DESIGN; TIME-SERIES; IMAGES
AB Timelines are essential for visually communicating chronological narratives and reflecting on the personal and cultural significance of historical events. Existing visualization tools tend to support conventional linear representations, but fail to capture personal idiosyncratic conceptualizations of time. In response, we built TimeSplines, a visualization authoring tool that allows people to sketch multiple free-form temporal axes and populate them with heterogeneous, time-oriented data via incremental and lazy data binding. Authors can bend, compress, and expand temporal axes to emphasize or de-emphasize intervals based on their personal importance; they can also annotate the axes with text and figurative elements to convey contextual information. The results of two user studies show how people appropriate the concepts in TimeSplines to express their own conceptualization of time, while our curated gallery of images demonstrates the expressive potential of our approach.
C1 [Offenwanger, Anna; Tsandilas, Theophanis] Univ Paris Saclay, CRNS, Inria, LISN, Paris, France.
   [Brehmer, Matthew] Tableau Res, Amherst, MA USA.
   [Chevalier, Fanny] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.
   [Chevalier, Fanny] Univ Toronto, Dept Stat Sci, Toronto, ON, Canada.
C3 Universite Paris Saclay; Inria; University of Toronto; University of
   Toronto
RP Offenwanger, A (corresponding author), Univ Paris Saclay, CRNS, Inria, LISN, Paris, France.
EM anna.offenwanger@lisn.upsaclay.fr; mbrehmer@tableau.com;
   fanny@dgp.toronto.edu; theophanis.tsandilas@lisn.upsaclay.fr
FU CNRS
FX No Statement Available
NR 79
TC 2
Z9 2
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 34
EP 44
DI 10.1109/TVCG.2023.3326520
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500093
PM 37922183
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Shen, QM
   You, ZX
   Yan, X
   Zhang, CZ
   Xu, K
   Zeng, D
   Qin, JB
   Tang, B
AF Shen, Qiaomu
   You, Zhengxin
   Yan, Xiao
   Zhang, Chaozu
   Xu, Ke
   Zeng, Dan
   Qin, Jianbin
   Tang, Bo
TI QEVIS: Multi-Grained Visualization of Distributed Query Execution
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE visual analytics system; distributed query execution; performance
   analysis
AB Distributed query processing systems such as Apache Hive and Spark are widely-used in many organizations for large-scale data analytics. Analyzing and understanding the query execution process of these systems are daily routines for engineers and crucial for identifying performance problems, optimizing system configurations, and rectifying errors. However, existing visualization tools for distributed query execution are insufficient because (i) most of them (if not all) do not provide fine-grained visualization (i.e., the atomic task level), which can be crucial for understanding query performance and reasoning about the underlying execution anomalies, and (ii) they do not support proper linkages between system status and query execution, which makes it difficult to identify the causes of execution problems. To tackle these limitations, we propose QEVIS, which visualizes distributed query execution process with multiple views that focus on different granularities and complement each other. Specifically, we first devise a query logical plan layout algorithm to visualize the overall query execution progress compactly and clearly. We then propose two novel scoring methods to summarize the anomaly degrees of the jobs and machines during query execution, and visualize the anomaly scores intuitively, which allow users to easily identify the components that are worth paying attention to. Moreover, we devise a scatter plot-based task view to show a massive number of atomic tasks, where task distribution patterns are informative for execution problems. We also equip QEVIS with a suite of auxiliary views and interaction methods to support easy and effective cross-view exploration, which makes it convenient to track the causes of execution problems. QEVIS has been used in the production environment of our industry partner, and we present three use cases from real-world applications and user interview to demonstrate its effectiveness. QEVIS is open-source at https://github.com/DBGroup-SUSTech/QEVIS.
C1 [Shen, Qiaomu; Zeng, Dan; Tang, Bo] Southern Univ Sci & Technol, Res Inst Trustworthy Autonomous Syst, Shenzhen, Peoples R China.
   [You, Zhengxin] Southern Univ Sci & Technol, Dept Comp Sci & Engn, Shenzhen, Peoples R China.
   [Yan, Xiao; Xu, Ke] Huawei Technol Co Ltd, Shenzhen, Peoples R China.
   [Zhang, Chaozu; Qin, Jianbin] Shenzhen Univ, Shenzhen Inst Comp Sci, Shenzhen, Peoples R China.
C3 Southern University of Science & Technology; Southern University of
   Science & Technology; Huawei Technologies; Shenzhen University; Shenzhen
   Institute of Computing Sciences
RP Tang, B (corresponding author), Southern Univ Sci & Technol, Res Inst Trustworthy Autonomous Syst, Shenzhen, Peoples R China.
EM shenqm@sustech.edu.cn; 12250078@mail.sustech.edu.cn;
   yanx@sustech.edu.cn; 12132372@mail.sustech.edu.cn; xuke81@huawei.com;
   zengd@sustech.edu.cn; qinjianbin@szu.edu.cn; tangb3@sustech.edu.cn
OI Zeng, Dan/0000-0002-9036-7791; Shen, Qiaomu/0000-0002-6510-0964
FU National Key R&D program of China
FX No Statement Available
NR 55
TC 1
Z9 1
U1 4
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 153
EP 163
DI 10.1109/TVCG.2023.3326930
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500082
PM 37883261
DA 2025-03-07
ER

PT J
AU Wu, GD
   Guo, SN
   Hoffswell, J
   Chan, GYY
   Rossi, RA
   Koh, E
AF Wu, Guande
   Guo, Shunan
   Hoffswell, Jane
   Chan, Gromit Yeuk-Yin
   Rossi, Ryan A.
   Koh, Eunyee
TI Socrates: Data Story Generation via Adaptive Machine-Guided Elicitation
   of User Feedback
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Narrative visualization; visual storytelling; conversational agent
ID VISUALIZATIONS; GUIDANCE; MODEL
AB Visual data stories can effectively convey insights from data, yet their creation often necessitates intricate data exploration, insight discovery, narrative organization, and customization to meet the communication objectives of the storyteller. Existing automated data storytelling techniques, however, tend to overlook the importance of user customization during the data story authoring process, limiting the system's ability to create tailored narratives that reflect the user's intentions. We present a novel data story generation workflow that leverages adaptive machine-guided elicitation of user feedback to customize the story. Our approach employs an adaptive plug-in module for existing story generation systems, which incorporates user feedback through interactive questioning based on the conversation history and dataset. This adaptability refines the system's understanding of the user's intentions, ensuring the final narrative aligns with their goals. We demonstrate the feasibility of our approach through the implementation of an interactive prototype: Socrates. Through a quantitative user study with 18 participants that compares our method to a state-of-the-art data story generation algorithm, we show that Socrates produces more relevant stories with a larger overlap of insights compared to human-generated stories. We also demonstrate the usability of Socrates via interviews with three data analysts and highlight areas of future work.
C1 [Wu, Guande] NYU, New York, NY 10012 USA.
   [Guo, Shunan; Hoffswell, Jane; Chan, Gromit Yeuk-Yin; Rossi, Ryan A.; Koh, Eunyee] Adobe Res, San Jose, CA USA.
C3 New York University; Adobe Systems Inc.
RP Wu, GD (corresponding author), NYU, New York, NY 10012 USA.
EM guandewu@nyu.edu; sguo@adobe.com; jhoffs@adobe.com; ychan@adobe.com;
   rrossi@adobe.com; eunyee@adobe.com
RI Wu, Guande/JZE-5610-2024; Guo, Shunan/AAE-2616-2019; Rossi,
   Ryan/C-7974-2013
OI Hoffswell, Jane/0000-0002-9871-4575; Rossi, Ryan/0000-0001-9758-0635;
   Wu, Guande/0000-0002-9244-173X; Chan, Gromit
   Yeuk-Yin/0000-0003-1356-4406; Guo, Shunan/0000-0001-5355-8399
NR 79
TC 4
Z9 4
U1 3
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 131
EP 141
DI 10.1109/TVCG.2023.3327363
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500049
PM 37922178
DA 2025-03-07
ER

PT J
AU Xue, YM
   Paetzold, P
   Kehlbeck, R
   Chen, B
   Kwan, KC
   Wang, YH
   Deussen, O
AF Xue, Yumeng
   Paetzold, Patrick
   Kehlbeck, Rebecca
   Chen, Bin
   Kwan, Kin Chung
   Wang, Yunhai
   Deussen, Oliver
TI Reducing Ambiguities in Line-Based Density Plots by Image-Space
   Colorization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Trajectory data; times series; density-based visualization; clustering;
   coloring
ID SIMILARITY MEASURES; VISUALIZATION; EDGE; COLORS; UNCERTAINTY; MAPS;
   TREE
AB Line-based density plots are used to reduce visual clutter in line charts with a multitude of individual lines. However, these traditional density plots are often perceived ambiguously, which obstructs the user's identification of underlying trends in complex datasets. Thus, we propose a novel image space coloring method for line-based density plots that enhances their interpretability. Our method employs color not only to visually communicate data density but also to highlight similar regions in the plot, allowing users to identify and distinguish trends easily. We achieve this by performing hierarchical clustering based on the lines passing through each region and mapping the identified clusters to the hue circle using circular MDS. Additionally, we propose a heuristic approach to assign each line to the most probable cluster, enabling users to analyze density and individual lines. We motivate our method by conducting a small-scale user study, demonstrating the effectiveness of our method using synthetic and real-world datasets, and providing an interactive online tool for generating colored line-based density plots.
C1 [Xue, Yumeng] Germany & Shandong Univ, Univ Konstanz, Jinan, Peoples R China.
   [Paetzold, Patrick; Kehlbeck, Rebecca; Chen, Bin; Deussen, Oliver] Univ Konstanz, Constance, Germany.
   [Kwan, Kin Chung] Calif State Univ Sacramento, Sacramento, CA USA.
   [Wang, Yunhai] Shandong Univ, Jinan, Peoples R China.
C3 University of Konstanz; California State University System; California
   State University Sacramento; Shandong University
RP Deussen, O (corresponding author), Univ Konstanz, Constance, Germany.; Wang, YH (corresponding author), Shandong Univ, Jinan, Peoples R China.
EM yumeng.xue@uni-konstanz.de; patrick.paetzold@uni-konstanz.de;
   rebecca.kehlbeck@uni-konstanz.de; bin.chen@uni-konstanz.de;
   kwan@csus.edu; cloudseawang@gmail.com; oliver.deussen@uni-konstanz.de
RI Deussen, Oliver/HKF-2004-2023; Bin, Chen/AGZ-5833-2022
OI Paetzold, Patrick/0000-0002-1315-4602; Xue, Yumeng/0000-0002-8195-517X
FU Deutsche Forschungsgemeinschaft (DFG)
FX No Statement Available
NR 83
TC 3
Z9 3
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 825
EP 835
DI 10.1109/TVCG.2023.3327149
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500022
PM 37883272
OA Green Submitted, hybrid
DA 2025-03-07
ER

PT J
AU Zhou, YX
   Yang, WK
   Chen, JS
   Chen, CJ
   Shen, ZY
   Luo, XN
   Yu, LY
   Liu, SX
AF Zhou, Yuxing
   Yang, Weikai
   Chen, Jiashu
   Chen, Changjian
   Shen, Zhiyang
   Luo, Xiaonan
   Yu, Lingyun
   Liu, Shixia
TI Cluster-Aware Grid Layout
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Grid layout; similarity; convexity; compactness; optimization
ID CONVEXITY MEASURE; VISUALIZATION
AB Grid visualizations are widely used in many applications to visually explain a set of data and their proximity relationships. However, existing layout methods face difficulties when dealing with the inherent cluster structures within the data. To address this issue, we propose a cluster-aware grid layout method that aims to better preserve cluster structures by simultaneously considering proximity, compactness, and convexity in the optimization process. Our method utilizes a hybrid optimization strategy that consists of two phases. The global phase aims to balance proximity and compactness within each cluster, while the local phase ensures the convexity of cluster shapes. We evaluate the proposed grid layout method through a series of quantitative experiments and two use cases, demonstrating its effectiveness in preserving cluster structures and facilitating analysis tasks.
C1 [Zhou, Yuxing; Yang, Weikai; Chen, Jiashu; Shen, Zhiyang; Liu, Shixia] Tsinghua Univ, Sch Software, BNRist, Beijing, Peoples R China.
   [Chen, Changjian] Kuaishou Technol, Beijing, Peoples R China.
   [Luo, Xiaonan] Guilin Univ Elect Technol, Guilin, Peoples R China.
   [Yu, Lingyun] Xian Jiaotong Liverpool Univ, Suzhou, Peoples R China.
C3 Tsinghua University; Guilin University of Electronic Technology; Xi'an
   Jiaotong-Liverpool University
RP Liu, SX (corresponding author), Tsinghua Univ, Sch Software, BNRist, Beijing, Peoples R China.
EM yx-zhou19@mails.tsinghua.edu.cn; yangwk21@mails.tsinghua.edu.cn;
   cjs22@mails.tsinghua.edu.cn; chenchangjian@kuaishou.com;
   shenzhiy21@mails.tsinghua.edu.cn; luoxn@guet.edu.cn;
   lingyun.yu@xjtlu.edu.cn; shixia@tsinghua.edu.cn
RI Liu, Shi-Xia/C-5574-2016; Li, Zexi/KFA-6939-2024; Zhou,
   Yuxing/AAA-3958-2020; Chen, Changjian/KBA-9462-2024
OI Chen, Changjian/0000-0003-2715-8839
FU National Natural Science Foundation of China
FX No Statement Available
NR 61
TC 3
Z9 4
U1 1
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 240
EP 250
DI 10.1109/TVCG.2023.3326934
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500136
PM 37871055
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Li, GZ
   Yuan, XR
AF Li, Guozheng
   Yuan, Xiaoru
TI GoTreeScape: Navigate and Explore the Tree Visualization Design Space
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Data visualization; Layout; Space exploration; Grammar;
   Navigation; Shape; Tree visualization; design space exploration; deep
   learning
AB Declarative grammar is becoming an increasingly important technique for understanding visualization design spaces. The GoTreeScape system presented in the paper allows users to navigate and explore the vast design space implied by GoTree, a declarative grammar for visualizing tree structures. To provide an overview of the design space, GoTreeScape, which is based on an encoder-decoder architecture, projects the tree visualizations onto a 2D landscape. Significantly, this landscape takes the relationships between different design features into account. GoTreeScape also includes an exploratory framework that allows top-down, bottom-up, and hybrid modes of exploration to support the inherently undirected nature of exploratory searches. Two case studies demonstrate the diversity with which GoTreeScape expands the universe of designed tree visualizations for users. The source code associated with GoTreeScape is available at https://github.com/bitvis2021/gotreescape.
C1 [Li, Guozheng] Peking Univ, Sch AI, Beijing 100871, Peoples R China.
   [Li, Guozheng] Beijing Inst Technol, Sch Comp Sci & Technol, Beijing 100811, Peoples R China.
   [Yuan, Xiaoru] Peking Univ, Sch AI, Minist Educ, Lab Machine Percept, Beijing 100871, Peoples R China.
   [Yuan, Xiaoru] Peking Univ, Natl Engn Lab Big Data Anal & Applicat, Beijing 100871, Peoples R China.
C3 Peking University; Beijing Institute of Technology; Peking University;
   Peking University
RP Yuan, XR (corresponding author), Peking Univ, Sch AI, Minist Educ, Lab Machine Percept, Beijing 100871, Peoples R China.; Yuan, XR (corresponding author), Peking Univ, Natl Engn Lab Big Data Anal & Applicat, Beijing 100871, Peoples R China.
EM guozhg.li@gmail.com; xiaoru.yuan@pku.edu.cn
RI Yuan, Xiaoru/E-1798-2013; Li, Guo-Zheng/D-5744-2011
OI Yuan, Xiaoru/0000-0002-7233-980X; Li, Guozheng/0000-0001-6663-6712
FU National Key Research and Development Program of China [2021YFB3301502];
   NSFC [61872013]; Beijing Institute of Technology Research Fund Program
   for Young Scholars
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2021YFB3301502, in part by NSFC
   under Grant 61872013, and in part by the Beijing Institute of Technology
   Research Fund Program for Young Scholars.
NR 65
TC 6
Z9 7
U1 3
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5451
EP 5467
DI 10.1109/TVCG.2022.3215070
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300044
PM 36251894
DA 2025-03-07
ER

PT J
AU Fourrier, N
   Moreau, G
   Benaouicha, M
   Norm, JM
AF Fourrier, Nicolas
   Moreau, Guillaume
   Benaouicha, Mustapha
   Norm, Jean-Marie
TI Handwriting for Efficient Text Entry in Industrial VR Applications:
   Influence of Board Orientation and Sensory Feedback on Performance
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 17th International Conference on High Speed Machining (HSM)
CY OCT 25-28, 2023
CL Nanjing, PEOPLES R CHINA
DE Keyboards; Training; Handwriting recognition; Error analysis; Usability;
   Task analysis; Speech recognition; Virtual reality; handwriting; text
   entry; industry
AB Text entry in Virtual Reality (VR) is becoming an increasingly important task as the availability of hardware increases and the range of VR applications widens. This is especially true for VR industrial applications where users need to input data frequently. Large-scale industrial adoption of VR is still hampered by the productivity gap between entering data via a physical keyboard and VR data entry methods. Data entry needs to be efficient, easy-to-use and to learn and not frustrating. In this paper, we present a new data entry method based on handwriting recognition (HWR). Users can input text by simply writing on a virtual surface. We conduct a user study to determine the best writing conditions when it comes to surface orientation and sensory feedback. This feedback consists of visual, haptic, and auditory cues. We find that using a slanted board with sensory feedback is best to maximize writing speeds and minimize physical demand. We also evaluate the performance of our method in terms of text entry speed, error rate, usability and workload. The results show that handwriting in VR has high entry speed, usability with little training compared to other controller-based virtual text entry techniques. The system could be further improved by reducing high error rates through the use of more efficient handwriting recognition tools. In fact, the total error rate is 9.28% in the best condition. After 40 phrases of training, participants reach an average of 14.5 WPM, while a group with high VR familiarity reach 16.16 WPM after the same training. The highest observed textual data entry speed is 21.11 WPM.
C1 [Fourrier, Nicolas; Benaouicha, Mustapha] Segula Technol Naval & Energy Engn Res & Innovat U, Rueil Malmaison, France.
   [Fourrier, Nicolas] Ecole Cent Nantes, AAU CRENAU, UMR 1563, Nantes, France.
   [Fourrier, Nicolas] LS2N PACCE, UMR 6004, Nantes, France.
   [Moreau, Guillaume] STICC, IMT Atlantique Lab, UMR 6285, Brest, France.
   [Benaouicha, Mustapha] Univ Caen Normandy, Cherbourg Univ, Lab Appl Sci LUSAC, Caen, France.
   [Norm, Jean-Marie] Nantes Univ, Ecole Cent Nantes, AAU CRENAU, UMR 1563, F-44000 Nantes, France.
   [Norm, Jean-Marie] LS2N PACCE, UMR 6004, F-44000 Nantes, France.
C3 Centre National de la Recherche Scientifique (CNRS); CNRS - Institute
   for Humanities & Social Sciences (INSHS); Nantes Universite; Ecole
   Centrale de Nantes; Universite de Bretagne Occidentale; Universite de
   Caen Normandie; Nantes Universite; Ecole Centrale de Nantes; Centre
   National de la Recherche Scientifique (CNRS); CNRS - Institute for
   Humanities & Social Sciences (INSHS)
RP Fourrier, N (corresponding author), Segula Technol Naval & Energy Engn Res & Innovat U, Rueil Malmaison, France.; Fourrier, N (corresponding author), Ecole Cent Nantes, AAU CRENAU, UMR 1563, Nantes, France.; Fourrier, N (corresponding author), LS2N PACCE, UMR 6004, Nantes, France.
EM nicolas.fourrier@segula.fr; guillaume.moreau@imt-atlantique.fr;
   mustapha.benaouicha@segula.fr; jean-marie.normand@ec-nantes.fr
RI BENAOUICHA, Mustapha/AAA-2236-2019; Moreau, Guillaume/I-3153-2013
OI BENAOUICHA, Mustapha/0000-0002-5332-2439; Fourrier,
   Nicolas/0009-0000-7349-9752; Moreau, Guillaume/0000-0003-2215-1865;
   Normand, Jean-Marie/0000-0003-0557-4356
NR 52
TC 3
Z9 3
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2023
VL 29
IS 11
BP 4438
EP 4448
DI 10.1109/TVCG.2023.3320215
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA X6ZW5
UT WOS:001099919100011
PM 37782596
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Liu, HM
   Xue, H
   Zhao, LS
   Chen, DP
   Peng, Z
   Zhang, GF
AF Liu, Haomin
   Xue, Hua
   Zhao, Linsheng
   Chen, Danpeng
   Peng, Zhen
   Zhang, Guofeng
TI MagLoc-AR: Magnetic-Based Localization for Visual-Free Augmented Reality
   in Large-Scale Indoor Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 17th International Conference on High Speed Machining (HSM)
CY OCT 25-28, 2023
CL Nanjing, PEOPLES R CHINA
DE Location awareness; Magnetic field measurement; Navigation;
   Magnetometers; Magnetic resonance imaging; Robustness; Indoor
   environment; Indoor localization; Augmented reality; Inertial navigation
   system
ID MONOCULAR SLAM; FIELD
AB Accurate localization of a display device is essential for AR in large-scale environments. Visual-based localization is the most commonly used solution, but poses privacy risks, suffers from robustness issues and consumes high power. Wireless signal-based localization is a potential visual-free solution, but its accuracy is not enough for AR. In this paper, we present MagLoc-AR, a novel visual-free localization solution that achieves sufficient accuracy for some AR applications (e.g. AR navigation) in large-scale indoor environments. We exploit the location-dependent magnetic field interference that is ubiquitous indoors as a localization signal. Our method requires only a consumer-grade 9-axis IMU, with the gyroscope and acceleration measurements used to recover the motion trajectory, and the magnetic measurements used to register the trajectory to the global map. To meet the accuracy requirement of AR, we propose a mapping method to reconstruct a globally consistent magnetic field of the environment, and a localization method fusing the biased magnetic measurements with the network-predicted motion to improve localization accuracy. In addition, we provide the first dataset for both visual-based and geomagnetic-based localization in large-scale indoor environments. Evaluations on the dataset demonstrate that our proposed method is sufficiently accurate for AR navigation and has advantages over the visual-based methods in terms of power consumption and robustness.
C1 [Liu, Haomin] Peking Univ & Sensetime Res, Beijing, Peoples R China.
   [Xue, Hua; Zhao, Linsheng; Peng, Zhen] SenseTime Res, Beijing, Peoples R China.
   [Zhang, Guofeng] Zhejiang Univ, State Key Lab CAD&CG, Sensetime Res & Tetras AI, Hangzhou, Peoples R China.
C3 Zhejiang University
RP Zhang, GF (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Sensetime Res & Tetras AI, Hangzhou, Peoples R China.
EM liuhaomin@sensetime.com; xuehua@sensetime.com;
   zhaolinsheng@sensetime.com; chendanpeng@tetras.ai;
   pengzhen1@sensetime.com; zhangguofeng@zju.edu.cn
RI Zhang, Ge/K-9118-2019; Liu, Haomin/IXW-5373-2023
OI Zhang, Guofeng/0000-0001-5661-8430; Liu, Haomin/0000-0001-9511-2416
FU China Postdoctoral Science Foundation; NSF of China [61932003]
FX This work was partially supported by China Postdoctoral Science
   Foundation, and NSF of China (No. 61932003).
NR 51
TC 4
Z9 5
U1 7
U2 26
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2023
VL 29
IS 11
BP 4383
EP 4393
DI 10.1109/TVCG.2023.3321088
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA X6ZW5
UT WOS:001099919100007
PM 37782616
DA 2025-03-07
ER

PT J
AU Song, WF
   Jin, XL
   Li, S
   Chen, CLZ
   Hao, AM
   Hou, X
AF Song, Wenfeng
   Jin, Xingliang
   Li, Shuai
   Chen, Chenglizhao
   Hao, Aimin
   Hou, Xia
TI FineStyle: Semantic-Aware Fine-Grained Motion Style Transfer with Dual
   Interactive-Flow Fusion
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 17th International Conference on High Speed Machining (HSM)
CY OCT 25-28, 2023
CL Nanjing, PEOPLES R CHINA
DE Motion Style Transfer; Semantic-aware Motion Transfer; Dual
   Interactive-Flow Fusion
AB We present FineStyle, a novel framework for motion style transfer that generates expressive human animations with specific styles for virtual reality and vision fields. It incorporates semantic awareness, which improves motion representation and allows for precise and stylish animation generation. Existing methods for motion style transfer have all failed to consider the semantic meaning behind the motion, resulting in limited controls over the generated human animations. To improve, FineStyle introduces a new cross-modality fusion module called Dual Interactive-Flow Fusion (DIFF). As the first attempt, DIFF integrates motion style features and semantic flows, producing semantic-aware style codes for fine-grained motion style transfer. FineStyle uses an innovative two-stage semantic guidance approach that leverages semantic clues to enhance the discriminative power of both semantic and style features. At an early stage, a semantic-guided encoder introduces distinct semantic clues into the style flow. Then, at a fine stage, both flows are further fused interactively, selecting the matched and critical clues from both flows. Extensive experiments demonstrate that FineStyle outperforms state-of-the-art methods in visual quality and controllability. By considering the semantic meaning behind motion style patterns, FineStyle allows for more precise control over motion styles. Source code and model are available on https://github.com/XingliangJin/Fine-Style.git.
C1 [Song, Wenfeng; Jin, Xingliang; Hou, Xia] Beijing Informat Sci & Technol Univ, Comp Sch, Beijing, Peoples R China.
   [Li, Shuai] Beihang Univ, Zhongguancun Lab, Beijing, Peoples R China.
   [Chen, Chenglizhao] China Univ Petr East China, Coll Comp Sci & Technol, Dongying, Peoples R China.
   [Li, Shuai; Hao, Aimin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
   [Hou, Xia] Chinese Acad Med Sci, Res Unit Virtual Body & Virtual Surg 2019RU004, Beijing, Peoples R China.
C3 Beijing Information Science & Technology University; Zhongguancun
   Laboratory; Beihang University; China University of Petroleum; Beihang
   University; Chinese Academy of Medical Sciences - Peking Union Medical
   College
RP Chen, CLZ (corresponding author), China Univ Petr East China, Coll Comp Sci & Technol, Dongying, Peoples R China.
EM songwenfenga@gmail.com; xingliangjin276@gmail.com; lishuai@buaa.edu.cn;
   cclz123@163.com; ham@buaa.edu.cn; houxia@bistu.edu.cn
RI Zhao, Mingyu/HHS-0141-2022; Jin, XingLiang/GLT-3212-2022; HOU,
   Xia/JCD-6851-2023
OI jin, xingliang/0000-0001-9209-7804
FU National Natural Science Foundation of China [62102036, 62172246,
   62272021]; Beijing Natural Science Foundation [4222024]; R&D Program of
   Beijing Municipal Education Commission [KM202211232003]; Open Project
   Program of State Key Laboratory of Virtual Reality Technology and
   Systems, Beihang University [VRLAB2022A02]; Research Unit of Virtual
   Human and Virtual Surgery [2019RU004]; National Key R&D Program of China
   [2018YFB1700603]; Youth Innovation and Technology Support Plan of
   Colleges and Universities in Shandong Province [2021KJ062]
FX This paper is supported by National Natural Science Foundation of China
   (62102036, 62172246, 62272021), Beijing Natural Science Foundation
   (4222024), R&D Program of Beijing Municipal Education Commission
   (KM202211232003), Open Project Program of State Key Laboratory of
   Virtual Reality Technology and Systems, Beihang
   University(No.VRLAB2022A02), Research Unit of Virtual Human and Virtual
   Surgery (2019RU004), National Key R&D Program of China
   (No.2018YFB1700603), and the Youth Innovation and Technology Support
   Plan of Colleges and Universities in Shandong Province (2021KJ062).
NR 91
TC 1
Z9 1
U1 6
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2023
VL 29
IS 11
BP 4361
EP 4371
DI 10.1109/TVCG.2023.3320216
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA X6ZW5
UT WOS:001099919100005
PM 37788214
DA 2025-03-07
ER

PT J
AU Kusumastuti, SA
   Pollard, KA
   Oiknine, AH
   Dalangin, B
   Raber, TR
   Files, BT
AF Kusumastuti, Sarah A.
   Pollard, Kimberly A.
   Oiknine, Ashley H.
   Dalangin, Bianca
   Raber, Tiffany R.
   Files, Benjamin T.
TI Practice Improves Performance of a 2D Uncertainty Integration Task
   Within and Across Visualizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Decision making; training; visualization; uncertainty
ID LITERACY
AB Information uncertainty is ubiquitous in everyday life, including in domains as diverse as weather forecasts, investments, and health risks. Knowing how to interpret and integrate this uncertain information is vital for making good decisions, but this can be difficult for experts and novices alike. In this study, we examine whether brief, focused practice can improve people's ability to understand and integrate bivariate Gaussian uncertainty visualized via ensemble displays, summary displays, and distributional displays, and we examine whether this is influenced by the complexity of the displayed information. In two experiments (N=118 and 56), decision making was faster and more accurate after practice relative to before practice. Furthermore, the performance improvements transferred to use of display types that were not practiced. This suggests that practice with feedback may improve underlying skills in probabilistic reasoning and provides a promising approach to improve people's decision making under uncertainty.
C1 [Kusumastuti, Sarah A.] Univ Southern Calif, Dept Psychol, Los Angeles, CA 90089 USA.
   [Pollard, Kimberly A.; Raber, Tiffany R.; Files, Benjamin T.] DEVCOM Army Res Lab, Los Angeles, CA 90094 USA.
   [Oiknine, Ashley H.; Dalangin, Bianca] DCS Corp, Los Angeles, CA 90094 USA.
C3 University of Southern California
RP Files, BT (corresponding author), DEVCOM Army Res Lab, Los Angeles, CA 90094 USA.
EM kusumast@usc.edu; kimberly.a.pollard.civ@army.mil; aoiknine@dcscorp.com;
   bdalangin@dcscorp.com; benjamin.t.files.civ@army.mil;
   tiffany.raber.civ@army.mil
OI Files, Benjamin/0000-0002-1141-7886; Kusumastuti, Sarah
   AFR/0000-0001-6362-218X; Pollard, Kimberly/0000-0002-5849-1987
FU DEVCOM Army Research Laboratory's Human Sciences Campaign
FX This work was supported by the DEVCOM Army Research Laboratory's Human
   Sciences Campaign.
NR 42
TC 1
Z9 1
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2023
VL 29
IS 9
BP 3949
EP 3960
DI 10.1109/TVCG.2022.3173889
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA O2BA8
UT WOS:001041912300016
PM 35536797
DA 2025-03-07
ER

PT J
AU Song, SC
   Li, CH
   Sun, YJ
   Wang, CB
AF Song, Sicheng
   Li, Chenhui
   Sun, Yujing
   Wang, Changbo
TI VividGraph: Learning to Extract and Redesign Network Graphs From
   Visualization Images
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data mining; Semantics; Image color analysis; Image segmentation; Data
   visualization; Pipelines; Image edge detection; Information
   visualization; network graph; data extraction; chart recognition;
   semantic segmentation; redesign
AB Network graphs are common visualization charts. They often appear in the form of bitmaps in articles, web pages, magazine prints, and designer sketches. People often want to modify graphs because of their poor design, but it is difficult to obtain their underlying data. In this article, we present VividGraph, a pipeline for automatically extracting and redesigning graphs from static images. We propose using convolutional neural networks to solve the problem of graph data extraction. Our method is robust to hand-drawn graphs, blurred graph images, and large graph images. We also present a graph classification module to make it effective for directed graphs. We propose two evaluation methods to demonstrate the effectiveness of our approach. It can be used to quickly transform designer sketches, extract underlying data from existing graphs, and interactively redesign poorly designed graphs.
C1 [Song, Sicheng; Li, Chenhui; Sun, Yujing; Wang, Changbo] East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200050, Peoples R China.
C3 East China Normal University
RP Li, CH; Wang, CB (corresponding author), East China Normal Univ, Sch Comp Sci & Technol, Shanghai 200050, Peoples R China.
EM clare.song@foxmail.com; chli@cs.ecnu.edu.cn; syjing2017@163.com;
   cbwang@cs.ecnu.edu.cn
RI Lu, Xin/KHW-8570-2024; Wang, Zhen/KCL-5193-2024; Li,
   Chenhui/AAR-3682-2020
OI Wang, Changbo/0000-0001-8940-6418; Song, Sicheng/0000-0002-2158-0353;
   Li, Chenhui/0000-0001-9835-2650
FU NSFC [61802128, 62072183]
FX This work was supported by the NSFC under Grants 61802128 and 62072183.
NR 57
TC 5
Z9 6
U1 1
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2023
VL 29
IS 7
BP 3169
EP 3181
DI 10.1109/TVCG.2022.3153514
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA H4XO7
UT WOS:000996011900003
PM 35196240
DA 2025-03-07
ER

PT J
AU Wang, C
   Zhang, SH
   Zhang, YZ
   Zollmann, S
   Hu, SM
AF Wang, Chen
   Zhang, Song-Hai
   Zhang, Yizhuo
   Zollmann, Stefanie
   Hu, Shi-Min
TI On Rotation Gains Within and Beyond Perceptual Limitations for Seated VR
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Legged locomotion; Task analysis; Computer science; Tracking;
   Visualization; Head-mounted displays; User experience; Rotation gains;
   amplified head rotation; head-mounted displays
ID HAND; EXPLORATION; WALKING
AB Head tracking in head-mounted displays (HMDs) enables users to explore a 360-degree virtual scene with free head movements. However, for seated use of HMDs such as users sitting on a chair or a couch, physically turning around 360-degree is not possible. Redirection techniques decouple tracked physical motion and virtual motion, allowing users to explore virtual environments with more flexibility. In seated situations with only head movements available, the difference of stimulus might cause the detection thresholds of rotation gains to differ from that of redirected walking. Therefore we present an experiment with a two-alternative forced-choice (2AFC) design to compare the thresholds for seated and standing situations. Results indicate that users are unable to discriminate rotation gains between 0.89 and 1.28, a smaller range compared to the standing condition. We further treated head amplification as an interaction technique and found that a gain of 2.5, though not a hard threshold, was near the largest gain that users consider applicable. Overall, our work aims to better understand human perception of rotation gains in seated VR and the results provide guidance for future design choices of its applications.
C1 [Wang, Chen; Zhang, Song-Hai; Zhang, Yizhuo; Hu, Shi-Min] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100190, Peoples R China.
   [Zollmann, Stefanie] Univ Otago, Dept Comp Sci, Dunedin 9016, New Zealand.
C3 Tsinghua University; University of Otago
RP Wang, C (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100190, Peoples R China.
EM cw.chenwang@outlook.com; shz@tsinghua.edu.cn;
   yizhuo-z18@mails.tsinghua.edu.cn; stefanie.zollmann@otago.ac.nz;
   shimin@tsinghua.edu.cn
RI Hu, Shi-Min/AAW-1952-2020; Wang, Chen/JPK-7141-2023
OI Wang, Chen/0000-0002-9315-3780; Zollmann, Stefanie/0000-0002-4690-5409;
   Hu, Shi-Min/0000-0001-7507-6542
FU Natural Science Foundation of China [62132012]; Tsinghua-Tencent Joint
   Laboratory for Internet Innovation Technology
FX This work was supported by the Natural Science Foundation of China under
   Grant 62132012, and in part by the Tsinghua-Tencent Joint Laboratory for
   Internet Innovation Technology. This work involved human subjects or
   animals in its research. Approval of all ethical and experimental
   procedures and protocols was granted by Ethics Committee of Tsinghua
   University, and performed inline with the Declaration of Helsinki.
NR 68
TC 7
Z9 8
U1 2
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2023
VL 29
IS 7
BP 3380
EP 3391
DI 10.1109/TVCG.2022.3159799
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA H4XO7
UT WOS:000996011900017
PM 35294351
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Chen, G
   Dai, H
   Zhou, T
   Shen, JB
   Shao, L
AF Chen, Geng
   Dai, Hang
   Zhou, Tao
   Shen, Jianbing
   Shao, Ling
TI Automatic Schelling Point Detection From Meshes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Deep learning; Heating systems; Feature
   extraction; Shape; Point cloud compression; Image edge detection; Deep
   neural network; mesh schelling points; geometric deep learning; heat map
   regression
ID SALIENCY; PARAMETERIZATION
AB Mesh Schelling points explain how humans focus on specific regions of a 3D object. They have a large number of important applications in computer graphics and provide valuable information for perceptual psychology studies. However, detecting mesh Schelling points is time-consuming and expensive since the existing techniques are mostly based on participant observation studies. To overcome these limitations, we propose to employ powerful deep learning techniques to detect mesh Schelling points in an automatic manner, free from participant observation studies. Specifically, we utilize the mesh convolution and pooling operations to extract informative features from mesh objects, and then predict the 3D heat map of Schelling points in an end-to-end manner. In addition, we propose a Deep Schelling Network (DS-Net) to automatically detect the Schelling points, including a multi-scale fusion component and a novel region-specific loss function to improve our network for a better regression of heat maps. To the best of our knowledge, DS-Net is the first deep neural network for detecting Schelling points from 3D meshes. We evaluate DS-Net on a mesh Schelling point dataset obtained from participant observation studies. The experimental results demonstrate that DS-Net is capable of detecting mesh Schelling points effectively and outperforms various state-of-the-art mesh saliency methods and deep learning models, both qualitatively and quantitatively.
C1 [Chen, Geng] Northwestern Polytech Univ, Sch Comp Sci & Engn, Natl Engn Lab Integrated Aerosp Ground Ocean Big D, Xian 710060, Peoples R China.
   [Dai, Hang] Mohamed bin Zayed Univ Artificial Intelligence, Abu Dhabi, U Arab Emirates.
   [Zhou, Tao] Nanjing Univ Sci & Technol, Sch Comp Sci & Technol, Nanjing 210094, Peoples R China.
   [Shen, Jianbing] Univ Macau, Dept Comp & Informat Sci, State Key Lab Internet Things Smart City, Macau, Peoples R China.
   [Shao, Ling] Saudi Data & AI Author, Natl Ctr Artificial Intelligence, Riyadh, Saudi Arabia.
C3 Northwestern Polytechnical University; Mohamed bin Zayed University of
   Artificial Intelligence MBZUAI; Nanjing University of Science &
   Technology; University of Macau
RP Dai, H (corresponding author), Mohamed bin Zayed Univ Artificial Intelligence, Abu Dhabi, U Arab Emirates.; Shen, JB (corresponding author), Univ Macau, Dept Comp & Informat Sci, State Key Lab Internet Things Smart City, Macau, Peoples R China.
EM geng.chen.cs@gmail.com; hang.dai@mbzuai.ac.ae; taozhou.ai@gmail.com;
   shenjianbingcg@gmail.com; ling.shao@ieee.org
RI Shao, Ling/D-3535-2011; Shen, Jianbing/KPB-2753-2024
OI Zhou, Tao/0000-0002-3733-7286; Chen, Geng/0000-0001-8350-6581
FU MBZUAI [GR006]; TII funding [TII//ARRC/2072//2021]; National Natural
   Science Foundation of China [62172228]
FX This work was supported in part by MBZUAI start-up funding under Grant
   GR006, in part by TII funding under Grant TII//ARRC/2072//2021, and
   inpart by the National Natural Science Foundation of China under Grant
   62172228.
NR 81
TC 4
Z9 4
U1 4
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2023
VL 29
IS 6
BP 2926
EP 2939
DI 10.1109/TVCG.2022.3144143
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F4DZ2
UT WOS:000981880500008
PM 35044917
DA 2025-03-07
ER

PT J
AU Guerreiro, J
   Kim, Y
   Nogueira, R
   Chung, SA
   Rodrigues, A
   Oh, U
AF Guerreiro, Joao
   Kim, Yujin
   Nogueira, Rodrigo
   Chung, SeungA
   Rodrigues, Andre
   Oh, Uran
TI The Design Space of the Auditory Representation of Objects and Their
   Behaviours in Virtual Reality for Blind People
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Space exploration; Blindness; Visualization; Virtual reality; Virtual
   environments; Haptic interfaces; Games; Inclusive Virtual Reality;
   Nonvisual Interaction; Blind; Auditory Feedback; Design Space
ID HEALTH-BENEFITS; FEEDBACK
AB As virtual reality (VR) is typically designed in terms of visual experience, it poses major challenges for blind people to understand and interact with the environment. To address this, we propose a design space to explore how to augment objects and their behaviours in VR with a nonvisual audio representation. It intends to support designers in creating accessible experiences by explicitly considering alternative representations to visual feedback. To demonstrate its potential, we recruited 16 blind users and explored the design space under two scenarios in the context of boxing: understanding the location of objects (the opponent's defensive stance) and their movement (opponent's punches). We found that the design space enables the exploration of multiple engaging approaches for the auditory representation of virtual objects. Our findings depicted shared preferences but no one-size-fits-all solution, suggesting the need to understand the consequences of each design choice and their impact on the individual user experience.
C1 [Guerreiro, Joao; Nogueira, Rodrigo; Rodrigues, Andre] Univ Lisbon, Fac Ciencias, LASIGE, Lisbon, Portugal.
   [Kim, Yujin; Chung, SeungA; Oh, Uran] Ewha Womans Univ, Comp Sci & Engn, Seoul, South Korea.
C3 Universidade de Lisboa; Ewha Womans University
RP Guerreiro, J (corresponding author), Univ Lisbon, Fac Ciencias, LASIGE, Lisbon, Portugal.
EM jpguerreiro@fc.ul.pt; komgi0715@ewhain.net; fc55721@alunos.fc.ul.pt;
   ewhacsa@ewhain.net; afrodrigues@fc.ul.pt; uran.oh@ewha.ac.kr
RI rodrigues, andre/JWP-8432-2024; Nogueira, Rodrigo/AAX-1610-2020;
   Guerreiro, João/AAD-6404-2020; Rodrigues, Andre/N-2164-2015
OI Guerreiro, Joao/0000-0002-0952-8368; Rodrigues,
   Andre/0000-0002-0810-4619; Oh, Uran/0000-0002-7832-6313
FU FCT [UIDB/00408/2020, UIDP/00408/2020]; Access VR project
   [2022.08286.PTDC]; MSIT (Ministry of Science and ICT), Korea under ITRC
   (Information Technology Research Center) [IITP-2023-2020-0-01460)]
FX The authors wish to thank the participants of the two user studies. This
   work was supported in part by FCT through the LASIGE Research Unit
   (UIDB/00408/2020, UIDP/00408/2020) and the Access VR project
   (2022.08286.PTDC). It was also supported by the MSIT (Ministry of
   Science and ICT), Korea, under the ITRC (Information Technology Research
   Center) support program (IITP-2023-2020-0-01460) supervised by the IITP
   (Institute for Information Communications Technology Planning
   Evaluation).
NR 82
TC 11
Z9 11
U1 2
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2763
EP 2773
DI 10.1109/TVCG.2023.3247094
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D2PZ7
UT WOS:000967207200001
PM 37027696
DA 2025-03-07
ER

PT J
AU Qu, Q
   Chen, XM
   Chung, YY
   Cai, WD
AF Qu, Qiang
   Chen, Xiaoming
   Chung, Yuk Ying
   Cai, Weidong
TI LFACon: Introducing Anglewise Attention to No-Reference Quality
   Assessment in Light Field Space
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Kernel; Measurement; Feature extraction; Light fields; Image quality;
   Convolution; Computational efficiency; No-reference Image Quality
   Assessment; Quality of Experience; Light Field Imaging; Immersive Media;
   Attention Mechanism; Deep Learning
AB Light field imaging can capture both the intensity information and the direction information of light rays. It naturally enables a six-degrees-of-freedom viewing experience and deep user engagement in virtual reality. Compared to 2D image assessment, light field image quality assessment (LFIQA) needs to consider not only the image quality in the spatial domain but also the quality consistency in the angular domain. However, there is a lack of metrics to effectively reflect the angular consistency and thus the angular quality of a light field image (LFI). Furthermore, the existing LFIQA metrics suffer from high computational costs due to the excessive data volume of LFIs. In this paper, we propose a novel concept of "anglewise attention" by introducing a multihead self-attention mechanism to the angular domain of an LFl. This mechanism better reflects the LFI quality. In particular, we propose three new attention kernels, including anglewise self-attention, anglewise grid attention, and anglewise central attention. These attention kernels can realize angular self-attention, extract multiangled features globally or selectively, and reduce the computational cost of feature extraction. By effectively incorporating the proposed kernels, we further propose our light field attentional convolutional neural network (LFACon) as an LFIQA metric. Our experimental results show that the proposed LFACon metric significantly outperforms the state-of-the-art LFIQA metrics. For the majority of distortion types, LFACon attains the best performance with lower complexity and less computational time.
C1 [Qu, Qiang; Chung, Yuk Ying; Cai, Weidong] Univ Sydney, Sydney, Australia.
   [Chen, Xiaoming] Beijing Technol & Business Univ, Beijing, Peoples R China.
C3 University of Sydney; Beijing Technology & Business University
RP Chen, XM (corresponding author), Beijing Technol & Business Univ, Beijing, Peoples R China.
EM vincent.qu@sydney.edu.au; xiaoming.chen@btbu.edu.cn;
   vera.chung@sydney.edu.au; tom.cai@sydney.edu.au
RI Qu, Qiang/IXD-9845-2023; Cai, Tingwei/AAJ-8822-2020
OI Qu, Qiang/0000-0002-6648-5050; Cai, Weidong/0000-0003-3706-8896; Chen,
   Xiaoming/0000-0002-7503-3021
FU Beijing Natural Science Foundation [4222003]; National Natural Science
   Foundation of China [62177001]
FX This work was supported in part by Beijing Natural Science Foundation
   under Grant 4222003 and National Natural Science Foundation of China
   under Grant 62177001.
NR 47
TC 15
Z9 15
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2239
EP 2248
DI 10.1109/TVCG.2023.3247069
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D2SN8
UT WOS:000967273500001
PM 37027711
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ren, YM
   Zhao, CF
   He, YN
   Cong, PS
   Liang, H
   Yu, JY
   Xu, L
   Ma, YX
AF Ren, Yiming
   Zhao, Chengfeng
   He, Yannan
   Cong, Peishan
   Liang, Han
   Yu, Jingyi
   Xu, Lan
   Ma, Yuexin
TI LiDAR-aid Inertial Poser: Large-scale Human Motion Capture by Sparse
   Inertial and LiDAR Sensors
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Motion capture; Laser radar; Point cloud compression; Three-dimensional
   displays; Cameras; Trajectory; Optical sensors; Human motion capture;
   shape modeling; virtual reality; sensor fusion
ID VIDEO
AB We propose a multi-sensor fusion method for capturing challenging 3D human motions with accurate consecutive local poses and global trajectories in large-scale scenarios, only using single LiDAR and 4 IMUs, which are set up conveniently and worn lightly. Specifically, to fully utilize the global geometry information captured by LiDAR and local dynamic motions captured by IMUs, we design a two-stage pose estimator in a coarse-to-fine manner, where point clouds provide the coarse body shape and IMU measurements optimize the local actions. Furthermore, considering the translation deviation caused by the view-dependent partial point cloud, we propose a pose-guided translation corrector. It predicts the offset between captured points and the real root locations, which makes the consecutive movements and trajectories more precise and natural. Moreover, we collect a LiDAR-IMU multi-modal mocap dataset, LIPD, with diverse human actions in long-range scenarios. Extensive quantitative and qualitative experiments on LIPD and other open datasets all demonstrate the capability of our approach for compelling motion capture in large-scale scenarios, which outperforms other methods by an obvious margin. We will release our code and captured dataset to stimulate future research.
C1 [Ren, Yiming; Zhao, Chengfeng; He, Yannan; Cong, Peishan; Liang, Han; Yu, Jingyi; Xu, Lan; Ma, Yuexin] ShanghaiTech Univ, Shanghai, Peoples R China.
   [Yu, Jingyi] Shanghai Engn Res Ctr Intelligent Vis & Imaging, Shanghai, Peoples R China.
C3 ShanghaiTech University
RP Xu, L (corresponding author), ShanghaiTech Univ, Shanghai, Peoples R China.
EM renym2022@shanghaitech.edu.cn; zhaochf2022@shanghaitech.edu.cn;
   heyn@shanghaitech.edu.cn; congpsh@shanghaitech.edu.cn;
   lianghan@shanghaitech.edu.cn; yujingyi@shanghaitech.edu.cn;
   xulan1@shanghaitech.edu.cn; mayuexin@shanghaitech.edu.cn
RI He, Yannan/LTF-0087-2024; zhang, quan/KHY-9180-2024; Cong,
   Peishan/JED-3079-2023
OI Zhao, Chengfeng/0000-0002-8649-7470
FU NSFC [62206173]; Shanghai Sailing Program [22YF1428700]; Shanghai
   Frontiers Science Center of Human-centered Artificial Intelligence
   (ShangHAI)
FX This work was supported by NSFC (No.62206173), Shanghai Sailing Program
   (No.22YF1428700), and Shanghai Frontiers Science Center of
   Human-centered Artificial Intelligence (ShangHAI).
NR 82
TC 21
Z9 21
U1 11
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2337
EP 2347
DI 10.1109/TVCG.2023.3247088
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D2GP9
UT WOS:000966961100001
PM 37027736
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kouril, D
   Strnad, O
   Mindek, P
   Halladjian, S
   Isenberg, T
   Gröller, ME
   Viola, I
AF Kouril, David
   Strnad, Ondrej
   Mindek, Peter
   Halladjian, Sarkis
   Isenberg, Tobias
   Groller, M. Eduard
   Viola, Ivan
TI Molecumentary: Adaptable Narrated Documentaries Using Molecular
   Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Cameras; Three-dimensional displays;
   Animation; Real-time systems; Solid modeling; Virtual tour; audio;
   biological data; storytelling; illustrative visualization
ID EXPLORANATION; TOOL
AB We present a method for producing documentary-style content using real-time scientific visualization. We introduce molecumentaries, i.e., molecular documentaries featuring structural models from molecular biology, created through adaptable methods instead of the rigid traditional production pipeline. Our work is motivated by the rapid evolution of scientific visualization and it potential in science dissemination. Without some form of explanation or guidance, however, novices and lay-persons often find it difficult to gain insights from the visualization itself. We integrate such knowledge using the verbal channel and provide it along an engaging visual presentation. To realize the synthesis of a molecumentary, we provide technical solutions along two major production steps: (1) preparing a story structure and (2) turning the story into a concrete narrative. In the first step, we compile information about the model from heterogeneous sources into a story graph. We combine local knowledge with external sources to complete the story graph and enrich the final result. In the second step, we synthesize a narrative, i.e., story elements presented in sequence, using the story graph. We then traverse the story graph and generate a virtual tour, using automated camera and visualization transitions. We turn texts written by domain experts into verbal representations using text-to-speech functionality and provide them as a commentary. Using the described framework, we synthesize fly-throughs with descriptions: automatic ones that mimic a manually authored documentary or semi-automatic ones which guide the documentary narrative solely through curated textual input.
C1 [Kouril, David] Masaryk Univ, Brno 60177, Czech Republic.
   [Kouril, David] TU Wien, A-040 Vienna, Austria.
   [Strnad, Ondrej; Viola, Ivan] King Abdullah Univ Sci & Technol KAUST, Thuwal 23955, Saudi Arabia.
   [Mindek, Peter] TU Wien, A-1040 Vienna, Austria.
   [Mindek, Peter] Nanograph GmbH, A-1040 Vienna, Austria.
   [Halladjian, Sarkis; Isenberg, Tobias] Univ Paris Saclay, CNRS, Inria, LISN, F-91190 Gif Sur Yvette, France.
   [Groller, M. Eduard] TU Wien, A-1220 Vienna, Austria.
   [Groller, M. Eduard] VRVis Res Ctr, A-1220 Vienna, Austria.
C3 Masaryk University Brno; Technische Universitat Wien; King Abdullah
   University of Science & Technology; Technische Universitat Wien;
   Universite Paris Saclay; Centre National de la Recherche Scientifique
   (CNRS); Inria; Technische Universitat Wien
RP Kouril, D (corresponding author), Masaryk Univ, Brno 60177, Czech Republic.; Kouril, D (corresponding author), TU Wien, A-040 Vienna, Austria.
EM dvdkouril@cg.tuwien.ac.at; ondrej.strnad@kaust.edu.sa;
   mindek@cg.tuwien.ac.at; sarkis.halladjian@inria.fr;
   tobias.isenberg@inria.fr; groeller@cg.tuwien.ac.at;
   ivan.viola@kaust.edu.sa
RI Strnad, Ondřej/GXV-9172-2022; Viola, Ivan/O-8944-2014; Isenberg,
   Tobias/A-7575-2008
OI Viola, Ivan/0000-0003-4248-6574; Kouril, David/0000-0003-4043-3487;
   Isenberg, Tobias/0000-0001-7953-8644; Strnad, Ondrej/0000-0002-8077-4692
FU ILLUSTRARE grant; Austrian Science Fund (FWF) [I 2953-N31]; French
   National Research Agency (ANR) [ANR-16-CE91-0011-01]; King Abdullah
   University of Science and Technology [BAS/1/1680-01-01]; LLVISATION
   grant; WWTF [VRG11-010]; COMET [879730]; FFG; Agence Nationale de la
   Recherche (ANR) [ANR-16-CE91-0011] Funding Source: Agence Nationale de
   la Recherche (ANR)
FX This work was supported in part by the ILLUSTRARE grant by both the
   Austrian Science Fund (FWF): I 2953-N31 and the French National Research
   Agency (ANR): ANR-16-CE91-0011-01 in part by the King Abdullah
   University of Science and Technology under Grant BAS/1/1680-01-01 and
   the ILLVISATION grant by WWTF (VRG11-010). This paper was partly written
   in collaboration with VRVis funded in COMET under Grant 879730 a program
   managed by FFG.
NR 70
TC 11
Z9 11
U1 1
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2023
VL 29
IS 3
BP 1733
EP 1747
DI 10.1109/TVCG.2021.3130670
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8N3OW
UT WOS:000925059900010
PM 34822330
OA hybrid, Green Submitted
DA 2025-03-07
ER

PT J
AU Sicat, R
   Ibrahim, M
   Ageeli, A
   Mannuss, F
   Rautek, P
   Hadwiger, M
AF Sicat, Ronell
   Ibrahim, Mohamed
   Ageeli, Amani
   Mannuss, Florian
   Rautek, Peter
   Hadwiger, Markus
TI Real-Time Visualization of Large-Scale Geological Models With Nonlinear
   Feature-Preserving Levels of Detail
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Geological models; structured hexahedral meshes; multiresolution
   representations; interactive visualization
ID WAVELET; SIMULATION; MESHES
AB The rapidly growing size and complexity of 3D geological models has increased the need for level-of-detail techniques and compact encodings to facilitate interactive visualization. For large-scale hexahedral meshes, state-of-the-art approaches often employ wavelet schemes for level of detail as well as for data compression. Here, wavelet transforms serve two purposes: (1) they achieve substantial compression for data reduction; and (2) the multiresolution encoding provides levels of detail for visualization. However, in coarser detail levels, important geometric features, such as geological faults, often get too smoothed out or lost, due to linear translation-invariant filtering. The same is true for attribute features, such as discontinuities in porosity or permeability. We present a novel, integrated approach addressing both purposes above, while preserving critical data features of both model geometry and its attributes. Our first major contribution is that we completely decouple the computation of levels of detail from data compression, and perform nonlinear filtering in a high-dimensional data space jointly representing the geological model geometry with its attributes. Computing detail levels in this space enables us to jointly preserve features in both geometry and attributes. While designed in a general way, our framework specifically employs joint bilateral filters, computed efficiently on a high-dimensional permutohedral grid. For data compression, after the computation of all detail levels, each level is separately encoded with a standard wavelet transform. Our second major contribution is a compact GPU data structure for the encoded mesh and attributes that enables direct real-time GPU visualization without prior decoding.
C1 [Sicat, Ronell; Ibrahim, Mohamed; Ageeli, Amani; Rautek, Peter; Hadwiger, Markus] King Abdullah Univ Sci & Technol, Thuwal 23955, Saudi Arabia.
   [Mannuss, Florian] Saudi Aramco, Dhahran 31311, Saudi Arabia.
C3 King Abdullah University of Science & Technology
RP Sicat, R (corresponding author), King Abdullah Univ Sci & Technol, Thuwal 23955, Saudi Arabia.
EM ronell.sicat@kaust.edu.sa; moeizle@gmail.com; amani.ageeli@kaust.edu.sa;
   florian.mannuss@aramco.com; peter.rautek@gmail.com;
   markus.hadwiger@kaust.edu.sa
OI Ageeli, Amani/0000-0001-6627-503X; Ibrahim, Mohamed/0000-0002-3559-3761;
   Rautek, Peter/0000-0003-4821-7404
FU Saudi Aramco [3879]
FX This work was supported in part by a grant from Saudi Aramco 3879.
NR 48
TC 0
Z9 0
U1 3
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2023
VL 29
IS 2
BP 1491
EP 1505
DI 10.1109/TVCG.2021.3120372
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7M2HO
UT WOS:000906475100016
PM 34653000
OA Green Published
DA 2025-03-07
ER

PT J
AU Song, Y
   Tang, F
   Dong, WM
   Huang, FY
   Lee, TY
   Xu, CS
AF Song, Yu
   Tang, Fan
   Dong, Weiming
   Huang, Feiyue
   Lee, Tong-Yee
   Xu, Changsheng
TI Balance-Aware Grid Collage for Small Image Collections
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Grid collage; visual balance; reinforcement learning
ID GENETIC ALGORITHMS; PHOTO COLLAGE
AB Grid collages (GClg) of small image collections are popular and useful in many applications, such as personal album management, online photo posting, and graphic design. In this article, we focus on how visual effects influence individual preferences through various arrangements of multiple images under such scenarios. A novel balance-aware metric is proposed to bridge the gap between multi-image joint presentation and visual pleasure. The metric merges psychological achievements into the field of grid collage. To capture user preference, a bonus mechanism related to a user-specified special location in the grid and uniqueness values of the subimages is integrated into the metric. An end-to-end reinforcement learning mechanism empowers the model without tedious manual annotations. Experiments demonstrate that our metric can evaluate the GClg visual balance in line with human subjective perception, and the model can generate visually pleasant GClg results, which is comparable to manual designs.
C1 [Song, Yu; Dong, Weiming; Xu, Changsheng] Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.
   [Song, Yu; Dong, Weiming; Xu, Changsheng] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100040, Peoples R China.
   [Tang, Fan] Jinlin Univ, Sch Artificial Intelligence, Changchun 130012, Jilin, Peoples R China.
   [Huang, Feiyue] Tencent, Youtu Lab, Shanghai 200233, Peoples R China.
   [Lee, Tong-Yee] Natl Cheng Kung Univ, Tainan 701, Taiwan.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; University of Chinese Academy of Sciences, CAS;
   Jilin University; Tencent; National Cheng Kung University
RP Dong, WM (corresponding author), Chinese Acad Sci, Inst Automat, NLPR, Beijing 100190, Peoples R China.; Dong, WM (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing 100040, Peoples R China.
EM songyu2017@ia.ac.cn; tangfan@jlu.edu.cn; weiming.dong@ia.ac.cn;
   garyhuang@tencent.com; tonylee@mail.ncku.edu.tw; changsheng.xu@ia.ac.cn
RI DONG, Weiming/AAG-7678-2020; Tang, Fan/O-3923-2018; zhang,
   zhenyu/HCI-5576-2022
OI xu, chang sheng/0000-0001-8343-9665; Dong, Weiming/0000-0001-6502-145X;
   tang, fan/0000-0002-3975-2483
FU National Key R&D Program of China [2020AAA0106200]; National Natural
   Science Foundation of China [61832016, U20B2070, 6210070958]; Ministry
   of Science and Technology, Taiwan [110-2221-E-006-135-MY3]; Open
   Projects Program of National Laboratory of Pattern Recognition
FX This work was supported by National Key R&D Program of China under Grant
   2020AAA0106200, in part by National Natural Science Foundation of China
   under Grants 61832016, U20B2070, and 6210070958, in part by Ministry of
   Science and Technology under Grant 110-2221-E-006-135-MY3, Taiwan, and
   in part by Open Projects Program of National Laboratory of Pattern
   Recognition.
NR 50
TC 3
Z9 3
U1 3
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2023
VL 29
IS 2
BP 1330
EP 1344
DI 10.1109/TVCG.2021.3113031
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7M2HO
UT WOS:000906475100004
PM 34529567
DA 2025-03-07
ER

PT J
AU Dewe, H
   Gottwald, JM
   Bird, LA
   Brenton, H
   Dillies, M
   Cowie, D
AF Dewe, Hayley
   Gottwald, Janna M.
   Bird, Laura-Ashleigh
   Brenton, Harry
   Dillies, Marco
   Cowie, Dorothy
TI My Virtual Self: The Role of Movement in Children's Sense of Embodiment
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Rubber; Task analysis; Correlation; Robot sensing
   systems; Legged locomotion; Headphones; Agency; body ownership;
   embodiment; synchronous integration; psychology; user interaction;
   virtual reality
ID RUBBER-HAND ILLUSION; MULTISENSORY INTEGRATION; BODY REPRESENTATION;
   ILLUSORY OWNERSHIP; PERCEPTION; AGENCY; SKIN; RECALIBRATION;
   CONTINGENCY; EXPERIENCE
AB There are vast potential applications for children's entertainment and education with modern virtual reality (VR) experiences, yet we know very little about how the movement or form of such a virtual body can influence children's feelings of control (agency) or the sensation that they own the virtual body (ownership). In two experiments, we gave a total of 197 children aged 4-14 years a virtual hand which moved synchronously or asynchronously with their own movements and had them interact with a VR environment. We found that movement synchrony influenced feelings of control and ownership at all ages. In Experiment 1 only, participants additionally felt haptic feedback either congruently, delayed or not at all - this did not influence feelings of control or ownership. In Experiment 2 only, participants used either a virtual hand or non-human virtual block. Participants embodied both forms to some degree, provided visuomotor signals were synchronous (as indicated by ownership, agency, and location ratings). Yet, only the hand in the synchronous movement condition was described as feeling like part of the body, rather than like a tool (e.g., a mouse or controller). Collectively, these findings highlight the overall dominance of visuomotor synchrony for children's own-body representation; that children can embody non-human forms to some degree; and that embodiment is also somewhat constrained by prior expectations of body form.
C1 [Dewe, Hayley; Cowie, Dorothy] Univ Durham, Durham DH1 3LE, England.
   [Gottwald, Janna M.] Uppsala Univ, SE-75105 Uppsala, Sweden.
   [Bird, Laura-Ashleigh] UCL, London WC1E 6BT, England.
   [Brenton, Harry] Bespoke VR, London EC2A 4NE, England.
   [Dillies, Marco] Goldsmiths Univ London, London SE14 6NW, England.
C3 Durham University; Uppsala University; University of London; University
   College London; University of London; Goldsmiths University London
RP Dewe, H (corresponding author), Univ Durham, Durham DH1 3LE, England.; Gottwald, JM (corresponding author), Uppsala Univ, SE-75105 Uppsala, Sweden.
EM hayley.l.dewe@durham.ac.uk; janna.gottwald@psyk.uu.se; l.bird@ucl.ac.uk;
   harry@bespokevr.com; m.gillies@gold.ac.uk; dorothy.cowie@durham.ac.uk
RI Gottwald, Janna/I-1140-2019
OI Gillies, Marco/0000-0002-3100-9230; Gottwald, Janna/0000-0001-5497-4001;
   Bird, Laura-Ashleigh/0000-0002-1701-7912; Dewe,
   Hayley/0000-0003-1757-0432
FU Economic and Social Research Council [ES/P008798/1]; Swedish Research
   Council [VR-PG 2017-01504]; ESRC [ES/P008798/1] Funding Source: UKRI
FX The work of Hayley Dewe, Janna M. Gottwald and Dorothy Cowiewas
   supported by Economic and Social Research Council under Grant ESRC,
   ES/P008798/1. The work of Janna M. Gottwald was supported by Swedish
   Research Council under Grant VR-PG 2017-01504. The authors wish to thank
   Prof. Lynda Boothroyd for guidance on statistical analyses; the children
   who volunteered for these experiments; and the Life Science Centre in
   Newcastle, U.K. for hosting us. *Dr Dewe and Dr Gottwald contributed
   equally to the work and correspondence should be addressed to either.
NR 88
TC 11
Z9 11
U1 3
U2 35
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4061
EP 4072
DI 10.1109/TVCG.2021.3073906
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400008
PM 33872150
OA Green Published, Green Accepted
DA 2025-03-07
ER

PT J
AU Feng, WQ
   Zhang, JY
   Zhou, YF
   Xin, SQ
AF Feng, Wanquan
   Zhang, Juyong
   Zhou, Yuanfeng
   Xin, Shiqing
TI GDR-Net: A Geometric Detail Recovering Network for 3D Scanned Objects
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Shape; Surface reconstruction; Geometry;
   Computational modeling; Task analysis; Solid modeling; Surface
   representation; geometric detail recovery; 3D scanning; mesh
   reconstruction
ID SUBDIVISION SCHEME; SURFACES; CONSTRUCTION
AB This article addresses the problem of mesh super-resolution such that the geometry details which are not well represented in the low-resolution models can be recovered and well represented in the generated high-quality models. The main challenges of this problem are the nonregularity of 3D mesh representation and the high complexity of 3D shapes. We propose a deep neural network called GDR-Net to solve this ill-posed problem, which resolves the two challenges simultaneously. First, to overcome the nonregularity, we regress a displacement in radial basis function parameter space instead of the vertex-wise coordinates in the euclidean space. Second, to overcome the high complexity, we apply the detail recovery process to small surface patches extracted from the input surface and obtain the overall high-quality mesh by fusing the refined surface patches. To train the network, we constructed a dataset composed of both real-world and synthetic scanned models, including high/low-quality pairs. Our experimental results demonstrate that GDR-Net works well for general models and outperforms previous methods for recovering geometric details.
C1 [Feng, Wanquan; Zhang, Juyong] Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Peoples R China.
   [Zhou, Yuanfeng; Xin, Shiqing] Shandong Univ, Sch Software, Jinan 264209, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Shandong University
RP Zhang, JY (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Peoples R China.
EM lcfwq@mail.ustc.edu.cn; juyong@ustc.edu.cn; yfzhou@sdu.edu.cn;
   xinshiqing@sdu.edu.cn
RI Zhou, Yuanfeng/AAT-4670-2020
FU National Natural Science Foundation of China [62122071 61772016,
   62172257, 61772312]; Youth Innovation Promotion Association CAS
   [2018495]; Fundamental Research Funds for the Central Universities
   [WK3470000021]; Key Research Development program of Shandong Province
   [2019GGX101021]; NSFC-Zhejiang Joint Fund of the Integration of
   Informatization and Industrialization [U1909210]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62122071 61772016, 62172257, and
   61772312, the Youth Innovation Promotion Association CAS under Grant
   2018495, "the Fundamental Research Funds for the Central Universities"
   under Grant WK3470000021, the Key Research Development program of
   Shandong Province under Grant 2019GGX101021, and the NSFC-Zhejiang Joint
   Fund of the Integration of Informatization and Industrialization under
   Grant U1909210.
NR 74
TC 2
Z9 2
U1 2
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 3959
EP 3973
DI 10.1109/TVCG.2021.3110658
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400001
PM 34495834
DA 2025-03-07
ER

PT J
AU Genay, A
   Lécuyer, A
   Hachet, M
AF Genay, Adelaide
   Lecuyer, Anatole
   Hachet, Martin
TI Being an Avatar "for Real": A Survey on Virtual Embodiment in Augmented
   Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Avatars; Psychology; Visualization; Tools; Taxonomy; Augmented reality;
   Virtual environments; Augmented reality; avatar; sense of embodiment;
   psychology; social and behavioral sciences
ID BODY OWNERSHIP; RUBBER-HAND; SENSE; DISPLAYS; AGENCY; SKIN; CHALLENGES;
   EXPERIENCE; STIFFNESS; RESPONSES
AB Virtual self-avatars have been increasingly used in Augmented Reality (AR) where one can see virtual content embedded into physical space. However, little is known about the perception of self-avatars in such a context. The possibility that their embodiment could be achieved in a similar way as in Virtual Reality opens the door to numerous applications in education, communication, entertainment, or the medical field. This article aims to review the literature covering the embodiment of virtual self-avatars in AR. Our goal is (i) to guide readers through the different options and challenges linked to the implementation of AR embodiment systems, (ii) to provide a better understanding of AR embodiment perception by classifying the existing knowledge, and (iii) to offer insight on future research topics and trends for AR and avatar research. To do so, we introduce a taxonomy of virtual embodiment experiences by defining a "body avatarization" continuum. The presented knowledge suggests that the sense of embodiment evolves in the same way in AR as in other settings, but this possibility has yet to be fully investigated. We suggest that, whilst it is yet to be well understood, the embodiment of avatars has a promising future in AR and conclude by discussing possible directions for research.
C1 [Genay, Adelaide; Hachet, Martin] Inria, F-33405 Bordeaux, France.
   [Lecuyer, Anatole] Inria, F-35042 Rennes, France.
C3 Inria; Inria
RP Genay, A (corresponding author), Inria, F-33405 Bordeaux, France.
EM adelaide.genay@inria.fr; anatole.lecuyer@inria.fr;
   martin.hachet@inria.fr
OI Genay, Adelaide/0000-0003-3151-1164
FU Inria for the "Avatar Challenge" Inria Project Lab (IPL)
FX The research of this article was funded by Inria for the "Avatar
   Challenge" Inria Project Lab (IPL).
NR 169
TC 44
Z9 47
U1 8
U2 95
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 5071
EP 5090
DI 10.1109/TVCG.2021.3099290
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400078
PM 34310309
OA Green Published
DA 2025-03-07
ER

PT J
AU Li, Z
   Wang, XT
   Yang, WK
   Wu, J
   Zhang, ZY
   Liu, ZY
   Sun, MS
   Zhang, H
   Liu, SX
AF Li, Zhen
   Wang, Xiting
   Yang, Weikai
   Wu, Jing
   Zhang, Zhengyan
   Liu, Zhiyuan
   Sun, Maosong
   Zhang, Hui
   Liu, Shixia
TI A Unified Understanding of Deep NLP Models for Text Classification
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computational modeling; Analytical models; Visualization; Internet; Data
   models; Computer architecture; Adaptation models; Explainable AI; visual
   debugging; visual analytics; deep NLP model; information-based
   interpretation
AB The rapid development of deep natural language processing (NLP) models for text classification has led to an urgent need for a unified understanding of these models proposed individually. Existing methods cannot meet the need for understanding different models in one framework due to the lack of a unified measure for explaining both low-level (e.g., words) and high-level (e.g., phrases) features. We have developed a visual analysis tool, DeepNLPVis, to enable a unified understanding of NLP models for text classification. The key idea is a mutual information-based measure, which provides quantitative explanations on how each layer of a model maintains the information of input words in a sample. We model the intra- and inter-word information at each layer measuring the importance of a word to the final prediction as well as the relationships between words, such as the formation of phrases. A multi-level visualization, which consists of a corpus-level, a sample-level, and a word-level visualization, supports the analysis from the overall training set to individual samples. Two case studies on classification tasks and comparison between models demonstrate that DeepNLPVis can help users effectively identify potential problems caused by samples and model architectures and then make informed improvements.
C1 [Li, Zhen; Yang, Weikai; Zhang, Hui; Liu, Shixia] Tsinghua Univ, Sch Software, BNRist, Beijing 100084, Peoples R China.
   [Wang, Xiting] Microsoft Res Asia, Beijing 100080, Peoples R China.
   [Wu, Jing] Cardiff Univ, Cardiff CF10 3AT, Wales.
   [Zhang, Zhengyan; Liu, Zhiyuan; Sun, Maosong] Tsinghua Univ, Dept Comp Sci & Technol, BNRist, Beijing 100084, Peoples R China.
C3 Tsinghua University; Microsoft Research Asia; Microsoft; Microsoft
   China; Cardiff University; Tsinghua University
RP Zhang, H (corresponding author), Tsinghua Univ, Sch Software, BNRist, Beijing 100084, Peoples R China.
EM thu.lz@outlook.com; xitwan@microsoft.com;
   yangwk21@mails.tsinghua.edu.cn; wuj11@cardiff.ac.uk;
   zy-z19@mails.tsinghua.edu.cn; liuzy@tsinghua.edu.cn;
   sms@tsinghua.edu.cn; huizhang@tsinghua.edu.cn; shixia@tsinghua.edu.cn
RI Liu, Zhiyuan/I-2233-2014; Liu, Shi-Xia/C-5574-2016; wang,
   xiting/HGF-3827-2022; zhengyan, zhang/D-2029-2012
OI yang, wei kai/0000-0002-6520-1642; Wu, Jing/0000-0001-5123-9861; Liu,
   Zhiyuan/0000-0002-7709-2543
FU National Key R&D Program of China [2020YFB2104100]; National Natural
   Science Foundation of China [U21A20469, 61936002]; Institute Guo Qiang;
   THUIBCS; BLBCI; Tsinghua-Kuaishou Institute of Future Media Data
FX This work was supported in part by the National Key R&D Program of China
   under Grant 2020YFB2104100, in part by the National Natural Science
   Foundation of China under Grants U21A20469 and 61936002, grants from the
   Institute Guo Qiang, THUIBCS, and BLBCI, and in part by the
   Tsinghua-Kuaishou Institute of Future Media Data.
NR 49
TC 23
Z9 26
U1 3
U2 33
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4980
EP 4994
DI 10.1109/TVCG.2022.3184186
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400073
PM 35724276
OA Green Accepted, Green Submitted
DA 2025-03-07
ER

PT J
AU Thiel, KK
   Naumann, F
   Jundt, E
   Günnemann, S
   Klinker, G
AF Thiel, Kevin Kennard
   Naumann, Florian
   Jundt, Eduard
   Guennemann, Stephan
   Klinker, Gudrun
TI C.DOT-Convolutional Deep Object Tracker for Augmented Reality Based
   Purely on Synthetic Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cameras; Target tracking; Optical imaging; Training; Neural networks;
   Tuning; Task analysis; Object tracking; deep learning; industry 4; 0;
   neural network; synthetic data; augmented reality; computer vision
ID TOOL
AB Augmented reality applications use object tracking to estimate the pose of a camera and to superimpose virtual content onto the observed object. Today, a number of tracking systems are available, ready to be used in industrial applications. However, such systems are hard to handle for a service maintenance engineer, due to obscure configuration procedures. In this article, we investigate options towards replacing the manual configuration process with a machine learning approach based on automatically synthesized data. We present an automated process of creating object tracker facilities exclusively from synthetic data. The data is highly enhanced to train a convolutional neural network, while still being able to receive reliable and robust results during real world applications only from simple RGB cameras. Comparison against related work using the LINEMOD dataset showed that we are able to outperform similar approaches. For our intended industrial applications with high accuracy demands, its performance is still lower than common object tracking methods with manual configuration. Yet, it can greatly support those as an add-on during initialization, due to its higher reliability.
C1 [Thiel, Kevin Kennard; Naumann, Florian; Jundt, Eduard] Volkswagen Grp, D-38440 Wolfsburg, Germany.
   [Thiel, Kevin Kennard; Guennemann, Stephan; Klinker, Gudrun] Tech Univ Munich, D-80333 Munich, Germany.
C3 Volkswagen; Volkswagen Germany; Technical University of Munich
RP Thiel, KK (corresponding author), Volkswagen Grp, D-38440 Wolfsburg, Germany.
EM kevin.thiel@volkswagen.de; florian.naumann@volkswagen.de;
   eduard.jundt@volkswagen.de; guennemann@in.tum.de; klinker@in.tum.de
RI Klinker, Gudrun/JVP-3665-2024
OI Thiel, Kevin/0000-0003-2940-8061; Naumann, Florian/0000-0001-6236-5628;
   Klinker, Gudrun/0000-0003-0971-5726
NR 78
TC 8
Z9 8
U1 4
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4434
EP 4451
DI 10.1109/TVCG.2021.3089096
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400034
PM 34125682
OA hybrid
DA 2025-03-07
ER

PT J
AU Zhao, Y
   Shi, JC
   Liu, JW
   Zhao, J
   Zhou, FF
   Zhang, WZ
   Chen, KY
   Zhao, X
   Zhu, CY
   Chen, W
AF Zhao, Ying
   Shi, Jingcheng
   Liu, Jiawei
   Zhao, Jian
   Zhou, Fangfang
   Zhang, Wenzhi
   Chen, Kangyi
   Zhao, Xin
   Zhu, Chunyao
   Chen, Wei
TI Evaluating Effects of Background Stories on Graph Perception
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Visual perception; Layout; Shape; Psychology; Bridges;
   Semantics; Graph visualization; node-link diagram; storytelling;
   evaluation
ID LARGE-SCALE GRAPHS; INFORMATION VISUALIZATION; KNOWLEDGE; LAYOUT;
   MEMORABILITY; CONTEXT
AB A graph is an abstract model that represents relations among entities, for example, the interactions between characters in a novel. A background story endows entities and relations with real-world meanings and describes the semantics and context of the abstract model, for example, the actual story that the novel presents. Considering practical experience and prior research, human viewers who are familiar with the background story of a graph and those who do not know the background story may perceive the same graph differently. However, no previous research has adequately addressed this problem. This research article thus presents an evaluation that investigated the effects of background stories on graph perception. Three hypotheses that focused on the role of visual focus areas, graph structure identification, and mental model formation on graph perception were formulated and guided three controlled experiments that evaluated the hypotheses using real-world graphs with background stories. An analysis of the resulting experimental data, which compared the performance of participants who read and did not read the background stories, obtained a set of instructive findings. First, having knowledge about a graph's background story influences participants' focus areas during interactive graph explorations. Second, such knowledge significantly affects one's ability to identify community structures but not high degree and bridge structures. Third, this knowledge influences graph recognition under blurred visual conditions. These findings can bring new considerations to the design of storytelling visualizations and interactive graph explorations.
C1 [Zhao, Ying; Shi, Jingcheng; Liu, Jiawei; Zhou, Fangfang; Zhang, Wenzhi; Chen, Kangyi; Zhao, Xin; Zhu, Chunyao] Cent South Univ, Sch Comp Sci & Engn, Changsha 410083, Peoples R China.
   [Zhao, Jian] Univ Waterloo, Sch Comp Sci, Waterloo, ON N2L 3G1, Canada.
   [Chen, Wei] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Peoples R China.
C3 Central South University; University of Waterloo; Zhejiang University
RP Zhou, FF (corresponding author), Cent South Univ, Sch Comp Sci & Engn, Changsha 410083, Peoples R China.
EM zhaoying@csu.edu.cn; 429433693@qq.com; 870656034@qq.com;
   jianzhao@uwaterloo.ca; zff@csu.edu.cn; 1093894600@qq.com;
   2500150552@qq.com; 1064253658@qq.com; 240427611@qq.com;
   chenwei@cad.zju.edu.cn
RI Zhao, Liangyu/IAO-7294-2023; Chen, Wei/AAR-9817-2020; Liu,
   Jiawei/CAJ-1777-2022; Zhang, Wenzhi/S-3658-2018
OI Chen, Wei/0000-0002-8365-4741; Chen, Kangyi/0000-0003-3795-5444
FU National Key Research and Development Program of China [2018YFB1700403];
   National Natural Science Foundation of China [61872388, 62072470];
   National Natural Science Foundation of Hunan Province [2020JJ4758]
FX The authors would like to thank all the reviewers for fruitful
   suggestions. They wish to thank Yang You from Graph Visualization Lab in
   Mininglamp Technology Group. This work was supported in part by the
   National Key Research and Development Program of China under Grant
   2018YFB1700403, in part by the National Natural Science Foundation of
   China under Grants 61872388 and 62072470, and in part by the National
   Natural Science Foundation of Hunan Province under Grant 2020JJ4758.
NR 94
TC 41
Z9 44
U1 1
U2 59
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4839
EP 4854
DI 10.1109/TVCG.2021.3107297
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400063
PM 34437066
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Liu, JS
   Tversky, B
   Feiner, S
AF Liu, Jen-Shuo
   Tversky, Barbara
   Feiner, Steven
TI Precueing Object Placement and Orientation for Manual Tasks in Augmented
   Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Visualization; Copper; Training; Manuals; Computer
   science; Collaboration; Augmented reality; precueing; cueing; manual
   tasks
ID SEE
AB When a user is performing a manual task, AR or VR can provide information about the current subtask (cueing) and upcoming subtasks (precueing) that makes them easier and faster to complete. Previous research on cueing and precueing in AR and VR has focused on path-following tasks requiring simple actions at each of a series of locations, such as pushing a button or just visiting. We consider a more complex task, whose subtasks involve moving to and picking up an item, moving that item to a designated place while rotating it to a specific angle, and depositing it. We conducted two user studies to examine how people accomplish this task while wearing an AR headset, guided by different visualizations that cue and precue movement and rotation. Participants performed best when given movement information for two successive subtasks and rotation information for a single subtask. In addition, participants performed best when the rotation visualization was split across the manipulated object and its destination.
C1 [Liu, Jen-Shuo; Feiner, Steven] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
   [Tversky, Barbara] Columbia Univ, Teachers Coll, Dept Human Dev, New York, NY 10027 USA.
C3 Columbia University; Columbia University; Columbia University Teachers
   College
RP Liu, JS (corresponding author), Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.
EM jl5004@columbia.edu; bt2158@tc.columbia.edu; feiner@cs.columbia.edu
RI Tversky, Barbara/KWT-7222-2024
OI Liu, Jen-Shuo/0000-0002-4109-5769
FU National Science Foundation [CMMI-2037101]
FX This research was funded in part by National Science Foundation Grant
   CMMI-2037101. We thank Portia Wang for her assistance in making the
   video.
NR 45
TC 8
Z9 8
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2022
VL 28
IS 11
BP 3799
EP 3809
DI 10.1109/TVCG.2022.3203111
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5T5BH
UT WOS:000875881300001
PM 36049002
DA 2025-03-07
ER

PT J
AU Zhang, YJ
   Wang, R
   Huo, YC
   Hua, W
   Bao, HJ
AF Zhang, Yunjin
   Wang, Rui
   Huo, Yuchi
   Hua, Wei
   Bao, Hujun
TI PowerNet: Learning-Based Real-Time Power-Budget Rendering
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Rendering (computer graphics); Power demand; Real-time systems;
   Predictive models; Neural networks; Integrated circuit modeling;
   Computational modeling; Power-budget rendering; rendering system; neural
   network
AB With the prevalence of embedded GPUs on mobile devices, power-efficient rendering has become a widespread concern for graphics applications. Reducing the power consumption of rendering applications is critical for extending battery life. In this paper, we present a new real-time power-budget rendering system to meet this need by selecting the optimal rendering settings that maximize visual quality for each frame under a given power budget. Our method utilizes two independent neural networks trained entirely by synthesized datasets to predict power consumption and image quality under various workloads. This approach spares time-consuming precomputation or runtime periodic refitting and additional error computation. We evaluate the performance of the proposed framework on different platforms, two desktop PCs and two smartphones. Results show that compared to the previous state of the art, our system has less overhead and better flexibility. Existing rendering engines can integrate our system with negligible costs.
C1 [Zhang, Yunjin; Wang, Rui; Huo, Yuchi; Hua, Wei; Bao, Hujun] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Wang, R (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM zhangyunjin@zju.edu.cn; rwang@cad.zju.edu.cn; huo.yuchi.sc@gmail.com;
   huawei@cad.zju.edu.cn; bao@cad.zju.edu.cn
RI Zhang, Yunjin/MFI-7965-2025
OI Bao, Hujun/0000-0002-2662-0334; Zhang, Yunjin/0000-0003-3582-8900
FU NSFC [61872319]; Zhejiang Provincial NSFC [LR18F020002]; National Key
   R&D Programof China [2017YFB1002605]
FX The authors would like to thank all reviewers and editors for their
   insightful comments. This work was partially funded by NSFC under Grant
   No. 61872319, Zhejiang Provincial NSFC under Grant No. LR18F020002, and
   the National Key R&D Programof China under Grant No. 2017YFB1002605.
NR 50
TC 3
Z9 4
U1 1
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2022
VL 28
IS 10
BP 3486
EP 3498
DI 10.1109/TVCG.2021.3064367
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 4G5UL
UT WOS:000849261100013
PM 33684038
DA 2025-03-07
ER

PT J
AU Su, ZQ
   Wan, WL
   Yu, T
   Liu, LJ
   Fang, L
   Wang, WP
   Liu, YB
AF Su, Zhaoqi
   Wan, Weilin
   Yu, Tao
   Liu, Lingjie
   Fang, Lu
   Wang, Wenping
   Liu, Yebin
TI MulayCap: Multi-Layer Human Performance Capture Using a Monocular Video
   Camera
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Clothing; Geometry; Image reconstruction; Shape; Three-dimensional
   displays; Strain; Solid modeling; Human performance capture; 3D pose
   estimation; cloth animation; non-rigid deformation; intrinsic
   decomposition
ID INTRINSIC IMAGES; SHAPE ESTIMATION; MOTION CAPTURE; TRACKING
AB We introduce MulayCap, a novel human performance capture method using a monocular video camera without the need for pre-scanning. The method uses "multi-layer" representations for geometry reconstruction and texture rendering, respectively. For geometry reconstruction, we decompose the clothed human into multiple geometry layers, namely a body mesh layer and a garment piece layer. The key technique behind is a Garment-from-Video (GfV) method for optimizing the garment shape and reconstructing the dynamic cloth to fit the input video sequence, based on a cloth simulation model which is effectively solved with gradient descent. For texture rendering, we decompose each input image frame into a shading layer and an albedo layer, and propose a method for fusing a fixed albedo map and solving for detailed garment geometry using the shading layer. Compared with existing single view human performance capture systems, our "multi-layer" approach bypasses the tedious and time consuming scanning step for obtaining a human specific mesh template. Experimental results demonstrate that MulayCap produces realistic rendering of dynamically changing details that has not been achieved in any previous monocular video camera systems. Benefiting from its fully semantic modeling, MulayCap can be applied to various important editing applications, such as cloth editing, re-targeting, relighting, and AR applications.
C1 [Su, Zhaoqi; Yu, Tao; Fang, Lu; Liu, Yebin] Tsinghua Univ, Beijing 100084, Peoples R China.
   [Wan, Weilin; Liu, Lingjie; Wang, Wenping] Univ Hong Kong, Hong Kong, Peoples R China.
C3 Tsinghua University; University of Hong Kong
RP Liu, YB (corresponding author), Tsinghua Univ, Beijing 100084, Peoples R China.
EM suzq13@tsinghua.org.cn; weilinwan1223@gmail.com;
   ytrock@mail.tsinghua.edu.cn; liulingjie0206@gmail.com;
   fanglu@sz.tsinghua.edu.cn; Wenping@cs.hku.hk;
   liuyebin@mail.tsinghua.edu.cn
RI Li, Yan/JRW-0176-2023; Wan, Weilin/KVB-3834-2024; Liu,
   Lingjie/LIC-0932-2024
OI Su, Zhaoqi/0000-0003-3651-8373
FU Tsinghua University; University of Hong Kong
FX The authors would like to thank Tsinghua University and The University
   of Hong Kong for supporting this work.
NR 118
TC 13
Z9 14
U1 2
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2022
VL 28
IS 4
BP 1862
EP 1879
DI 10.1109/TVCG.2020.3027763
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZH9CR
UT WOS:000761227900012
PM 32991282
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Tuffaha, M
   Stavdahl, O
   Stensdotter, AK
AF Tuffaha, Mutaz
   Stavdahl, Oyvind
   Stensdotter, Ann-Katrin
TI Modeling Movement-Induced Errors in AC Electromagnetic Trackers
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Position measurement; Heuristic algorithms; Coils; Current measurement;
   Robot sensing systems; Time measurement; Tracking; Electromagnetic
   motion tracking system; motion capture; error analysis; polhemus systems
ID TRACKING; POSITION; MOTION
AB Error analysis of electromagnetic motion tracking systems is of growing interest to many researchers. Under sensor movement, it is logical to presume that the error in position and orientation measurements will increase due to the linearization used in the algorithms, among other reasons. In this article, we analyze theoretically the error, that results from linearization, in position measurement of the Polhemus tracking system for a moving sensor. We derive formulas to estimate this error in terms of the sensor position and speed. Then, we verify these formulas by numerical simulations.
C1 [Tuffaha, Mutaz] Indra Navia AS, NO-1383 Asker, Norway.
   [Stavdahl, Oyvind] Norwegian Univ Sci & Technol NTNU, Fac Informat Technol Math & Elect Engn, Dept Engn Cybernet, NO-7491 Trondheim, Norway.
   [Stensdotter, Ann-Katrin] Norwegian Univ Sci & Technol NTNU, Fac Med & Hlth Sci, Dept Neuromed & Movement Sci, NO-7491 Trondheim, Norway.
C3 Norwegian University of Science & Technology (NTNU); Norwegian
   University of Science & Technology (NTNU)
RP Tuffaha, M (corresponding author), Indra Navia AS, NO-1383 Asker, Norway.
EM mutaz.tuffaha@indra.no; oyvind.stavdahl@ntnu.no;
   ann-katrin.stensdotter@ntnu.no
RI Stavdahl, Øyvind/AAC-2875-2019
OI Stavdahl, Oyvind/0000-0003-0212-3146
FU Norwegian University of Science and Technology (NTNU)
FX The authors would like to thank the Norwegian University of Science and
   Technology (NTNU) for their financial support. They would also like to
   thank Dr. Frederick H. Raab for the valuable discussions. Further, They
   also thank Polhemus for their cooperation.
NR 16
TC 4
Z9 4
U1 1
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2022
VL 28
IS 3
BP 1597
EP 1607
DI 10.1109/TVCG.2020.3019700
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP1EJ
UT WOS:000748371200011
PM 32845841
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Elhamdadi, H
   Canavan, S
   Rosen, P
AF Elhamdadi, Hamza
   Canavan, Shaun
   Rosen, Paul
TI AffectiveTDA: Using Topological Data Analysis to Improve Analysis and
   Explainability in Affective Computing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Face recognition; Affective computing; Three-dimensional displays;
   Topology; Feature extraction; Data visualization; Data analysis;
   Affective computing; topological data analysis; explainability;
   visualization
ID FACIAL EXPRESSION RECOGNITION; PERSISTENCE; EMOTION; SHAPE
AB We present an approach utilizing Topological Data Analysis to study the structure of face poses used in affective computing, i.e., the process of recognizing human emotion. The approach uses a conditional comparison of different emotions, both respective and irrespective of time, with multiple topological distance metrics, dimension reduction techniques, and face subsections (e.g., eyes, nose, mouth, etc.). The results confirm that our topology-based approach captures known patterns, distinctions between emotions, and distinctions between individuals, which is an important step towards more robust and explainable emotion recognition by machines.
C1 [Elhamdadi, Hamza] Univ Massachusetts, Amherst, MA 01003 USA.
   [Canavan, Shaun; Rosen, Paul] Univ S Florida, Tampa, FL 33620 USA.
C3 University of Massachusetts System; University of Massachusetts Amherst;
   State University System of Florida; University of South Florida
RP Elhamdadi, H (corresponding author), Univ Massachusetts, Amherst, MA 01003 USA.
EM helhamdadi@umass.edu; scanavan@usf.edu; prosen@usf.edu
RI Rosen, Paul/AAN-1370-2021
OI Elhamdadi, Hamza/0009-0006-8767-0681
FU National Science Foundation [IIS-1845204]
FX We thank the anonymous reviewers for their feedback. This project was
   supported in part by the National Science Foundation (IIS-1845204).
NR 87
TC 13
Z9 13
U1 2
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 769
EP 779
DI 10.1109/TVCG.2021.3114784
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA XW3DW
UT WOS:000735505300010
PM 34587031
OA Bronze, Green Submitted
DA 2025-03-07
ER

PT J
AU He, WB
   Zou, LC
   Shekar, AK
   Gou, L
   Ren, L
AF He, Wenbin
   Zou, Lincan
   Shekar, Arvind Kumar
   Gou, Liang
   Ren, Liu
TI <i>Where Can We Help</i>? A Visual Analytics Approach to Diagnosing and
   Improving Semantic Segmentation of Movable Objects
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Semantics; Image segmentation; Autonomous vehicles; Analytical models;
   Robustness; Visual analytics; Data models; Model diagnosis; semantic
   segmentation; spatial representation learning; adversarial learning;
   autonomous driving
AB Semantic segmentation is a critical component in autonomous driving and has to be thoroughly evaluated due to safety concerns. Deep neural network (DNN) based semantic segmentation models are widely used in autonomous driving. However, it is challenging to evaluate DNN-based models due to their black-box-like nature, and it is even more difficult to assess model performance for crucial objects, such as lost cargos and pedestrians, in autonomous driving applications. In this work, we propose VASS, a Visual Analytics approach to diagnosing and improving the accuracy and robustness of Semantic Segmentation models, especially for critical objects moving in various driving scenes. The key component of our approach is a context-aware spatial representation learning that extracts important spatial information of objects, such as position, size, and aspect ratio, with respect to given scene contexts. Based on this spatial representation, we first use it to create visual summarization to analyze models' performance. We then use it to guide the generation of adversarial examples to evaluate models' spatial robustness and obtain actionable insights. We demonstrate the effectiveness of VASS via two case studies of lost cargo detection and pedestrian detection in autonomous driving. For both cases, we show quantitative evaluation on the improvement of models' performance with actionable insights obtained from VASS.
C1 [He, Wenbin; Zou, Lincan; Gou, Liang; Ren, Liu] Robert Bosch Res & Technol Ctr, Cambridge, MA 02139 USA.
   [Shekar, Arvind Kumar] Robert Bosch GmbH, Gerlingen, Germany.
C3 Bosch; Bosch
RP He, WB (corresponding author), Robert Bosch Res & Technol Ctr, Cambridge, MA 02139 USA.
EM wenbin.he2@us.bosch.com; lincan.zou@us.bosch.com;
   arvindkumar.shekar@de.bosch.com; liang.gou@us.bosch.com;
   liu.ren@us.bosch.com
RI Shekar, Arvind/AAA-3366-2019
OI Shekar, Arvind Kumar/0000-0002-5853-5310
NR 52
TC 23
Z9 25
U1 2
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 1040
EP 1050
DI 10.1109/TVCG.2021.3114855
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000100
PM 34587077
DA 2025-03-07
ER

PT J
AU Jeon, H
   Ko, HK
   Jo, J
   Kim, Y
   Seo, J
AF Jeon, Hyeon
   Ko, Hyung-Kwon
   Jo, Jaemin
   Kim, Youngtaek
   Seo, Jinwook
TI Measuring and Explaining the Inter-Cluster Reliability of
   Multidimensional Projections
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Distortion; Reliability; Measurement; Distortion measurement; Task
   analysis; Data visualization; Extraterrestrial measurements;
   Multidimensional projections; MDP distortions; Inter-cluster tasks;
   Inter-cluster reliability; Distortion metrics
ID DIMENSIONALITY REDUCTION; VISUAL ANALYSIS; EXPLORATION; SHIFT; TOOL
AB We propose Steadiness and Cohesiveness, two novel metrics to measure the inter-cluster reliability of multidimensional projection (MDP), specifically how well the inter-cluster structures are preserved between the original high-dimensional space and the low-dimensional projection space. Measuring inter-cluster reliability is crucial as it directly affects how well inter-cluster tasks (e.g., identifying cluster relationships in the original space from a projected view) can be conducted; however, despite the importance of inter-cluster tasks, we found that previous metrics, such as Trustworthiness and Continuity, fail to measure inter-cluster reliability. Our metrics consider two aspects of the inter-cluster reliability: Steadiness measures the extent to which clusters in the projected space form clusters in the original space, and Cohesiveness measures the opposite. They extract random clusters with arbitrary shapes and positions in one space and evaluate how much the clusters are stretched or dispersed in the other space. Furthermore, our metrics can quantify pointwise distortions, allowing for the visualization of inter-cluster reliability in a projection, which we call a reliability map. Through quantitative experiments, we verify that our metrics precisely capture the distortions that harm inter-cluster reliability while previous metrics have difficulty capturing the distortions. A case study also demonstrates that our metrics and the reliability map 1) support users in selecting the proper projection techniques or hyperparameters and 2) prevent misinterpretation while performing inter-cluster tasks, thus allow an adequate identification of inter-cluster structure.
C1 [Jeon, Hyeon; Ko, Hyung-Kwon; Kim, Youngtaek; Seo, Jinwook] Seoul Natl Univ, Seoul, South Korea.
   [Jo, Jaemin] Sungkyunkwan Univ, Seoul, South Korea.
C3 Seoul National University (SNU); Sungkyunkwan University (SKKU)
RP Jeon, H (corresponding author), Seoul Natl Univ, Seoul, South Korea.
EM hj@hcil.snu.ac.kr; hkko@hcil.snu.ac.kr; jmjo@skku.edu;
   ytaek.kim@hcil.snu.ac.kr; jseo@snu.ac.kr
OI Ko, Hyungkwon/0000-0002-6488-607X
FU National Research Foundation of Korea (NRF) - Korea government (MSIT)
   [NRF2019R1A2C208906213]
FX Thanks to Yoo-Min Jung and Aeri Cho for their valuable feedback. This
   work was supported by the National Research Foundation of Korea (NRF)
   grant funded by the Korea government (MSIT) (No. NRF2019R1A2C208906213).
NR 72
TC 11
Z9 11
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 551
EP 561
DI 10.1109/TVCG.2021.3114833
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000063
PM 34587063
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhang, AJ
   Zhao, Y
   Wang, SG
AF Zhang, Aijia
   Zhao, Yan
   Wang, Shigang
TI An Improved Augmented-Reality Framework for Differential Rendering
   Beyond the Lambertian-World Assumption
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Lighting; Estimation; Rendering (computer graphics); Light sources;
   Augmented reality; Visualization; Cameras; Augmented reality; specular
   and transparent objects; global illumination; light estimation; material
   estimation; joint optimization
ID ILLUMINATION ESTIMATION; REFLECTANCE; OBJECTS; SHADOW
AB In augmented reality, it is important to achieve visual consistency between inserted virtual objects and the real scene. As specular and transparent objects can produce caustics, which affect the appearance of inserted virtual objects, we herein propose a framework for differential rendering beyond the Lambertian-world assumption. Our key idea is to jointly optimize illumination and parameters of specular and transparent objects. To estimate the parameters of transparent objects efficiently, the psychophysical scaling method is introduced while considering visual characteristics of the human eye to obtain the step size for estimating the refractive index. We verify our technique on multiple real scenes, and the experimental results show that the fusion effects are visually consistent.
C1 [Zhang, Aijia; Zhao, Yan; Wang, Shigang] Jilin Univ, Coll Commun Engn, Changchun 130000, Peoples R China.
C3 Jilin University
RP Zhao, Y (corresponding author), Jilin Univ, Coll Commun Engn, Changchun 130000, Peoples R China.
EM 296529503@qq.com; zhao_y@jlu.edu.cn; wangshigang@vip.sina.com
RI wang, shigang/F-3718-2011; Zhao, Yan/AAF-8988-2019
OI Aijia, Zhang/0000-0003-2260-3331
FU National Key RD Plan [2017YFB1002900]; National Natural Science
   Foundation of China [61771220, 61631009]
FX The authors would like to thank the authors of [7] and Zhenxing Li for
   the help and suggestions. This work was supported by the National Key
   R&D Plan (2017YFB1002900) and the National Natural Science Foundation of
   China (No.61771220 andNo. 61631009).
NR 38
TC 5
Z9 6
U1 1
U2 27
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2021
VL 27
IS 12
BP 4374
EP 4386
DI 10.1109/TVCG.2020.3004195
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WN2ZU
UT WOS:000711642800004
PM 32746268
DA 2025-03-07
ER

PT J
AU Dong, XH
   Gao, Y
   Dong, JY
   Chantler, MJ
AF Dong, Xinghui
   Gao, Ying
   Dong, Junyu
   Chantler, Mike J.
TI The Importance of Phase to Texture Discrimination and Similarity
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Observers; Feature extraction; Visualization;
   Convolutional neural networks; Computer science; Fourier phase; Fourier
   magnitude; texture features; texture discrimination; texture similarity
ID PERCEPTION; FEATURES; SENSITIVITY; STATISTICS; AMPLITUDE; CONTOUR;
   VISION
AB In this article, we investigate the importance of phase for texture discrimination and similarity estimation tasks. We first use two psychophysical experiments to investigate the relative importance of phase and magnitude spectra for human texture discrimination and similarity estimation. The results show that phase is more important to humans for both tasks. We further examine the ability of 51 computational feature sets to perform these two tasks. In contrast with the psychophysical experiments, it is observed that the magnitude data is more important to these computational feature sets than the phase data. We hypothesise that this inconsistency is due to the difference between the abilities of humans and the computational feature sets to utilise phase data. This motivates us to investigate the application of the 51 feature sets to phase-only images in addition to their use on the original data set. This investigation is extended to exploit Convolutional Neural Network (CNN) features. The results show that our feature fusion scheme improves the average performance of those feature sets for estimating humans' perceptual texture similarity. The superior performance should be attributed to the importance of phase to texture similarity.
C1 [Dong, Xinghui; Gao, Ying; Dong, Junyu] Ocean Univ China, Dept Comp Sci, Qingdao 266071, Shandong, Peoples R China.
   [Chantler, Mike J.] Heriot Watt Univ, Sch Math & Comp Sci, Texture Lab, Edinburgh EH14 4AS, Midlothian, Scotland.
C3 Ocean University of China; Heriot Watt University
RP Dong, XH; Dong, JY (corresponding author), Ocean Univ China, Dept Comp Sci, Qingdao 266071, Shandong, Peoples R China.
EM dongxinghui@gmail.com; gaoying@stu.ouc.edu.cn; dongjunyu@ouc.edu.cn;
   m.j.chantler@hw.ac.uk
OI Dong, Junyu/0000-0001-7012-2087; Chantler, Mike/0000-0002-8381-1751
FU Young Taishan Scholars Program; National Natural Science Foundation of
   China (NSFC) [61271405, 41576011]; Program Foundation of Ministry of
   Education of China [20120132110018]
FX We would like to thank the editor and anonymous reviewers for their
   extensive comments and help in improving this paper. X. Dong is
   supported by the Young Taishan Scholars Program. J. Dong is supported by
   the National Natural Science Foundation of China (NSFC) (No. 61271405,
   41576011) and the Ph.D. Program Foundation of Ministry of Education of
   China (No. 20120132110018).
NR 61
TC 0
Z9 0
U1 1
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2021
VL 27
IS 9
BP 3755
EP 3768
DI 10.1109/TVCG.2020.2981063
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TS3CP
UT WOS:000679532000012
PM 32191889
DA 2025-03-07
ER

PT J
AU Graciano, A
   Rueda, AJ
   Pospisil, A
   Bittner, J
   Benes, B
AF Graciano, Alejandro
   Rueda, Antonio J.
   Pospisil, Adam
   Bittner, Jiri
   Benes, Bedrich
TI QuadStack: An Efficient Representation and Direct Rendering of Layered
   Datasets
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Rendering (computer graphics); Data visualization; Graphics processing
   units; Data models; Geology; Data structures; Ray tracing; Computer
   graphics; object hierarchies; graphics data structures and data types
ID OF-THE-ART; VOLUME; VISUALIZATION; SUBSURFACE; FRAMEWORK
AB We introduce QuadStack, a novel algorithm for volumetric data compression and direct rendering. Our algorithm exploits the data redundancy often found in layered datasets which are common in science and engineering fields such as geology, biology, mechanical engineering, medicine, etc. QuadStack first compresses the volumetric data into vertical stacks which are then compressed into a quadtree that identifies and represents the layered structures at the internal nodes. The associated data (color, material, density, etc.) and shape of these layer structures are decoupled and encoded independently, leading to high compression rates (4x to 54x of the original voxel model memory footprint in our experiments). We also introduce an algorithm for value retrieving from the QuadStack representation and we show that the access has logarithmic complexity. Because of the fast access, QuadStack is suitable for efficient data representation and direct rendering. We show that our GPU implementation performs comparably in speed with the state-of-the-art algorithms (18-79 MRays/s in our implementation), while maintaining a significantly smaller memory footprint.
C1 [Graciano, Alejandro] Univ Jaen, Jaen 23071, Spain.
   [Rueda, Antonio J.] Univ Jaen, Escuela Politecn Super, Comp Sci, Jaen 23071, Spain.
   [Pospisil, Adam; Bittner, Jiri] Czech Tech Univ, Prague 16636 6, Czech Republic.
   [Benes, Bedrich] Purdue Univ, Technol, W Lafayette, IN 47907 USA.
   [Benes, Bedrich] Purdue Univ, Comp Sci, W Lafayette, IN 47907 USA.
C3 Universidad de Jaen; Universidad de Jaen; Czech Technical University
   Prague; Purdue University System; Purdue University; Purdue University
   System; Purdue University
RP Benes, B (corresponding author), Purdue Univ, Technol, W Lafayette, IN 47907 USA.; Benes, B (corresponding author), Purdue Univ, Comp Sci, W Lafayette, IN 47907 USA.
EM graciano@ujaen.es; ajrueda@ujaen.es; pospiad2@fel.cvut.cz;
   bittner@fel.cvut.cz; bbenes@purdue.edu
RI Graciano, Alejandro/AAA-1533-2020; Rueda-Ruiz, Antonio
   Jesús/AAY-5298-2021; Bittner, Jiri/B-1677-2010; Benes,
   Bedrich/A-8150-2016
OI Benes, Bedrich/0000-0002-5293-2112; Graciano Segura,
   Alejandro/0000-0002-9230-5113; Rueda Ruiz, Antonio
   Jesus/0000-0001-7692-454X
FU National Science Foundation [10001387]; Functional Proceduralization of
   3D Geometric Models, the Research Center for Informatics
   [CZ.02.1.01/0.0/0.0/16_019/0000765]; Czech Science Foundation
   [GA18-20374S]; Ministry of Science and Innovation of Spain
   [TIN2017-84968-R, RTI2018-099638-B-I00]; University of Jaen
FX This research was funded in part by National Science Foundation Grant
   #10001387, Functional Proceduralization of 3D Geometric Models, the
   Research Center for Informatics No. CZ.02.1.01/0.0/0.0/16_019/0000765,
   the Czech Science Foundation under Project GA18-20374S, the Ministry of
   Science and Innovation of Spain under Projects TIN2017-84968-R and
   RTI2018-099638-B-I00 and the University of Jaen. The authors would like
   to thank Niels Aage for providing the Wing model and Daniel and Carmen
   Benes-Magana as well as Colin Gray for proofreading the paper.
NR 60
TC 7
Z9 7
U1 1
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2021
VL 27
IS 9
BP 3733
EP 3744
DI 10.1109/TVCG.2020.2981565
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TS3CP
UT WOS:000679532000010
PM 32191892
OA Bronze
DA 2025-03-07
ER

PT J
AU Romat, H
   Appert, C
   Pietriga, E
AF Romat, Hugo
   Appert, Caroline
   Pietriga, Emmanuel
TI Expressive Authoring of Node-Link Diagrams With Graphies
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Tools; Data visualization; Layout; Data structures; User
   centered design; Expressive design; node-link diagram; multivariate
   networks
ID OF-THE-ART; VISUAL ANALYSIS; VISUALIZATION; EXPLORATION; NETWORKS;
   DESIGN
AB Expressive design environments enable visualization designers not only to specify chart types and visual mappings, but also to customize individual graphical marks, as they would in a vector graphics drawing tool. Prior work has mainly investigated how to support the expressive design of a wide range of charts generated from tabular data: bar charts, scatterplots, maps, etc. We focus here on an expressive design environment for node-link diagrams generated from multivariate networks. Such data structures raise specific challenges and opportunities in terms of visual design and interactive authoring. We discuss those specificities and describe the user-centered design process that led to Graphies, a prototype environment for expressive node-link diagram authoring. We then report on a study in which participants successfully reproduced several expressive designs, and created their own designs as well.
C1 [Romat, Hugo; Appert, Caroline; Pietriga, Emmanuel] Univ Paris Saclay, CNRS, INRIA, F-91400 Orsay, France.
   [Romat, Hugo] TecKnowMetrix, F-38500 Voiron, France.
C3 Microsoft; Centre National de la Recherche Scientifique (CNRS); Inria;
   Universite Paris Saclay
RP Romat, H (corresponding author), Univ Paris Saclay, CNRS, INRIA, F-91400 Orsay, France.
EM hugo.romat@inria.fr; appert@lri.fr; emmanuel.pietriga@inria.fr
OI Appert, Caroline/0000-0002-3050-9284; Pietriga,
   Emmanuel/0000-0002-9762-0462
NR 50
TC 7
Z9 8
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2021
VL 27
IS 4
BP 2329
EP 2340
DI 10.1109/TVCG.2019.2950932
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO8XL
UT WOS:000623420400006
PM 31689194
OA Green Published
DA 2025-03-07
ER

PT J
AU Mao, AH
   Zhang, H
   Xie, ZF
   Yu, MJ
   Liu, YJ
   He, Y
AF Mao, Aihua
   Zhang, Hong
   Xie, Zhenfeng
   Yu, Minjing
   Liu, Yong-Jin
   He, Ying
TI Automatic Sitting Pose Generation for Ergonomic Ratings of Chairs
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Ergonomics; Strain; Avatars; Shape; Computational modeling; Skeleton;
   Geometry; Sitting pose generation; alignment; human-chair interaction;
   ergonomic rating
ID VIRTUAL TRY-ON
AB Human poses play a critical role in human-centric product design. Despite considerable researches on pose synthesis and pose-driven product design, most of them adopt the simple stick figure model that captures only skeletons rather than real body geometries and do not link human poses to the environment (e.g., chairs for sitting). This paper focuses on user-tailored ergonomic design and rating of chairs using scanned human geometries. Fully utilizing the anthropometric information of the human models, our method considers more ergonomic guidelines of chair design (such as pressure distribution and support intensity) and links the geometry of 3D chair models and human-to-chair interactions into the pose deformation constraints of the human avatars. The core of our method is a pose generation algorithm which rigs the user's successive poses through coarse- and fine-level pose deformations. We define a non-linear energy function with contact, collision, and joint limit terms, and solve it using a hill-climbing algorithm. The fitting results allow us to quantitatively evaluate the chair model in terms of various ergonomic criteria. Our method is flexible and effective and can be applied to users with varying body shapes and a wide range of chairs. Moreover, the proposed technique can be easily extended to other furniture, such as desk, bed, and cabinet. Extensive evaluations and a user study demonstrate the efficiency and advantages of the proposed virtual fitting method. Given that our method avoids tedious on-site trying, facilitates the exploration/evaluation of various chair products, and provides valuable feedback for the designers and manufacturers to deliver customized products, it is ideal for online shopping of chairs.
C1 [Mao, Aihua; Zhang, Hong; Xie, Zhenfeng] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510006, Peoples R China.
   [Yu, Minjing] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
   [Liu, Yong-Jin] Tsinghua Univ, Dept Comp Sci & Technol, MOE Key Lab Pervas Comp, BNRist, Beijing 100084, Peoples R China.
   [He, Ying] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
C3 South China University of Technology; Tianjin University; Tsinghua
   University; Nanyang Technological University
RP Yu, MJ (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.; Liu, YJ (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, MOE Key Lab Pervas Comp, BNRist, Beijing 100084, Peoples R China.
EM ahmao@scut.edu.cn; 201620131291@scut.edu.cn; 201720140063@scut.edu.cn;
   minjingyu@tju.edu.cn; liuyongjin@tsinghua.edu.cn; yhe@ntu.edu.sg
RI Xie, zhenfeng/KLB-9287-2024; He, Ying/A-3708-2011
OI He, Ying/0000-0002-6749-4485
FU Natural Science Foundation (NSF) of China [61725204, 61521002]; NSF of
   Guangdong Province [2016A030313499]; Science and Technology Planning
   Project (STPP) of Guangdong Province [2015A030401030]; STPP of Guangzhou
   City [201804010362]
FX We thank the anonymous reviewers for their constructive comments that
   help us to improve the paper. This work was supported by the Natural
   Science Foundation (NSF) of China (61725204, 61521002), NSF of Guangdong
   Province (2016A030313499), The Science and Technology Planning Project
   (STPP) of Guangdong Province (2015A030401030) and STPP of Guangzhou City
   (201804010362).
NR 55
TC 4
Z9 4
U1 12
U2 87
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2021
VL 27
IS 3
BP 1890
EP 1903
DI 10.1109/TVCG.2019.2938746
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA QA9EF
UT WOS:000613744500002
PM 31502980
DA 2025-03-07
ER

PT J
AU Feng, ZZ
   Li, HT
   Zeng, W
   Yang, SH
   Qu, HM
AF Feng, Zezheng
   Li, Haotian
   Zeng, Wei
   Yang, Shuang-Hua
   Qu, Huamin
TI Topology Density Map for Urban Data Visualization and Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Density map; network topology; urban data
ID MOBILITY; GRAPH
AB Density map is an effective visualization technique for depicting the scalar field distribution in 2D space. Conventional methods for constructing density maps are mainly based on Euclidean distance, limiting their applicability in urban analysis that shall consider road network and urban traffic. In this work, we propose a new method named Topology Density Map, targeting for accurate and intuitive density maps in the context of urban environment. Based on the various constraints of road connections and traffic conditions, the method first constructs a directed acyclic graph (DAG) that propagates nonlinear scalar fields along 1D road networks. Next, the method extends the scalar fields to a 2D space by identifying key intersecting points in the DAG and calculating the scalar fields for every point, yielding a weighted Voronoi diagram like effect of space division. Two case studies demonstrate that the Topology Density Map supplies accurate information to users and provides an intuitive visualization for decision making. An interview with domain experts demonstrates the feasibility, usability, and effectiveness of our method.
C1 [Feng, Zezheng; Li, Haotian; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Zeng, Wei] Chinese Acad Sci, Shenzhen Inst Adv Technol, Beijing, Peoples R China.
   [Yang, Shuang-Hua] Southern Univ Sci & Technol, Shenzhen, Peoples R China.
C3 Hong Kong University of Science & Technology; Chinese Academy of
   Sciences; Shenzhen Institute of Advanced Technology, CAS; Southern
   University of Science & Technology
RP Zeng, W (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Beijing, Peoples R China.
EM zfengak@connect.ust.hk; haotian.li@connect.ust.hk; wei.zeng@siat.ac.cn;
   yangsh@sustech.edu.cn; huamin@cse.ust.hk
RI Yang, Shuang-Hua/GZA-7839-2022; Li, Haotian/LXV-6051-2024
OI Yang, Shuang-Hua/0000-0003-0717-5009; Li, Haotian/0000-0001-9547-3449;
   Zeng, Wei/0000-0002-5600-8824; Feng, Zezheng/0000-0003-4874-8133
FU National Key Research and Development Plans of P. R. China
   [2019YFC0810705, 2018YFC0807002]; National Natural Science Foundation of
   China [61802388]; HK RGC GRF [16208514]; HKUST SSC [F0707]
FX We thank all the domain experts interviewed in this research. We also
   thank the reviewers for their comments and suggestions. The work is
   supported in part by the National Key Research and Development Plans
   (2019YFC0810705 and 2018YFC0807002) of P. R. China, National Natural
   Science Foundation of China (No. 61802388), HK RGC GRF grant 16208514
   and HKUST SSC grant F0707.
NR 52
TC 27
Z9 27
U1 4
U2 47
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 828
EP 838
DI 10.1109/TVCG.2020.3030469
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100068
PM 33048749
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Nguyen, N
   Strnad, O
   Klein, T
   Luo, D
   Alharbi, R
   Wonka, P
   Maritan, M
   Mindek, P
   Autin, L
   Goodsell, DS
   Viola, I
AF Nguyen, Ngan
   Strnad, Ondrej
   Klein, Tobias
   Luo, Deng
   Alharbi, Ruwayda
   Wonka, Peter
   Maritan, Martina
   Mindek, Peter
   Autin, Ludovic
   Goodsell, David S.
   Viola, Ivan
TI <i>Modeling in the Time of COVID</i>-<i>19</i>: Statistical and
   Rule-based Mesoscale Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE molecular visualization; mesoscale modeling
ID SUPRAMOLECULAR ARCHITECTURE; VISUALIZATION; PROTEIN; GENERATION;
   SURFACE; PREDICTION
AB We present a new technique for the rapid modeling and construction of scientifically accurate mesoscale biological models. The resulting 3D models are based on a few 2D microscopy scans and the latest knowledge available about the biological entity, represented as a set of geometric relationships. Our new visual-programming technique is based on statistical and rule-based modeling approaches that are rapid to author, fast to construct, and easy to revise. From a few 2D microscopy scans, we determine the statistical properties of various structural aspects, such as the outer membrane shape, the spatial properties, and the distribution characteristics of the macromolecular elements on the membrane. This information is utilized in the construction of the 3D model. Once all the imaging evidence is incorporated into the model, additional information can be incorporated by interactively defining the rules that spatially characterize the rest of the biological entity, such as mutual interactions among macromolecules, and their distances and orientations relative to other structures. These rules are defined through an intuitive 3D interactive visualization as a visual-programming feedback loop. We demonstrate the applicability of our approach on a use case of the modeling procedure of the SARS-CoV-2 virion ultrastructure. This atomistic model, which we present here, can steer biological research to new promising directions in our efforts to fight the spread of the virus.
C1 [Nguyen, Ngan; Strnad, Ondrej; Luo, Deng; Alharbi, Ruwayda; Wonka, Peter; Viola, Ivan] King Abdullah Univ Sci & Technol KAUST, Thuwal, Saudi Arabia.
   [Klein, Tobias; Mindek, Peter] TU Wien, Vienna, Austria.
   [Klein, Tobias; Mindek, Peter] Nanographics GmbH, Vienna, Austria.
   [Maritan, Martina; Autin, Ludovic; Goodsell, David S.] Scripps Res Inst, San Diego, CA USA.
C3 King Abdullah University of Science & Technology; Technische Universitat
   Wien; Scripps Research Institute
RP Nguyen, N (corresponding author), King Abdullah Univ Sci & Technol KAUST, Thuwal, Saudi Arabia.
EM ngan.nguyen@kaust.edu.sa; ondrej.strnad@kaust.edu.sa;
   tklein@cg.tuwien.ac.at; deng.luo@kaust.edu.sa;
   ruwayda.alharbi@kaust.edu.sa; peter.wonka@kaust.edu.sa;
   mmaritan@scripps.edu; mindek@cg.tuwien.ac.at; autin@scripps.edu;
   goodsell@scripps.edu; ivan.viola@kaust.edu.sa
RI Strnad, Ondřej/GXV-9172-2022; Nguyen, Ngan/HPG-4304-2023; Viola,
   Ivan/O-8944-2014
OI Nguyen, Hoang Ngan/0000-0003-0054-801X; Luo, Deng/0000-0003-4610-8730;
   Viola, Ivan/0000-0003-4248-6574; Strnad, Ondrej/0000-0002-8077-4692;
   Autin, Ludovic/0000-0002-2197-191X
FU King Abdullah University of Science and Technology (KAUST) Office of
   Sponsored Research (OSR) [OSR-2019-CPF-4108, BAS/1/1680-01-01]; US
   National Institutes of Health [R01-GM120604]
FX The research was supported by the King Abdullah University of Science
   and Technology (KAUST) Office of Sponsored Research (OSR), under award
   numbers OSR-2019-CPF-4108 and BAS/1/1680-01-01, and grant R01-GM120604
   from the US National Institutes of Health (DSG). We thank
   nanographics.at for providing the Marion software, Michael Cusack from
   Publication Services at KAUST for proofreading, and anonymous reviewers
   for their constructive comments.
NR 82
TC 19
Z9 20
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 722
EP 732
DI 10.1109/TVCG.2020.3030415
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100058
PM 33055034
OA hybrid, Green Submitted, Green Published
DA 2025-03-07
ER

PT J
AU Reda, K
   Szafir, DA
AF Reda, Khairi
   Szafir, Danielle Albers
TI Rainbows Revisited: Modeling Effective Colormap Design for Graphical
   Inference
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Color; perception; graphical inference; scalar data
ID EXPLORATORY DATA-ANALYSIS; SCHEMES; MAPS; VISUALIZATION; PALETTES
AB Color mapping is a foundational technique for visualizing scalar data. Prior literature offers guidelines for effective colormap design, such as emphasizing luminance variation while limiting changes in hue. However, empirical studies of color are largely focused on perceptual tasks. This narrow focus inhibits our understanding of how generalizable these guidelines are. particularly to tasks like visual inference that require synthesis and judgement across multiple percepts. Furthermore, the emphasis on traditional ramp designs (e.g., sequential or diverging) may sideline other key metrics or design strategies. We study how a cognitive metric color name variation impacts people's ability to make model-based judgments. In two graphical inference experiments, participants saw a series of color-coded scalar fields sampled from different models and assessed the relationships between these models. Contrary to conventional guidelines, participants were more accurate when viewing colormaps that cross a variety of uniquely nameable colors. We modeled participants' performance using this metric and found that it provides a better fit to the experimental data than do existing design principles. Our findings indicate cognitive advantages for colorful maps like rainbow: which exhibit high color categorization, despite their traditionally undesirable perceptual properties. We also found no evidence that color categorization would lead observers to infer false data features. Our results provide empirically grounded metrics for predicting a colormap's performance and suggest alternative guidelines for designing new quantitative colormaps to support inference.
C1 [Reda, Khairi] Indiana Univ Purdue Univ, Indianapolis, IN 46202 USA.
   [Szafir, Danielle Albers] Univ Colorado, Boulder, CO 80309 USA.
C3 Purdue University System; Purdue University; University of Colorado
   System; University of Colorado Boulder
RP Reda, K (corresponding author), Indiana Univ Purdue Univ, Indianapolis, IN 46202 USA.
EM redak@iu.edu; danielle.szajir@colorado.edu
OI Reda, Khairi/0000-0002-8096-658X
FU National Science Foundation [1942429, 1764092, 1657599]; Argonne
   Leadership Computing Facility, a U.S. Department of Energy Office of
   Science User Facility [DE-AC02-06CH11357]; Div Of Information &
   Intelligent Systems; Direct For Computer & Info Scie & Enginr [1764092,
   1942429] Funding Source: National Science Foundation; Div Of Information
   & Intelligent Systems; Direct For Computer & Info Scie & Enginr
   [1657599] Funding Source: National Science Foundation
FX This paper is based upon research supported by National Science
   Foundation awards 1942429, 1764092, & 1657599. The work was also
   supported in part by the Argonne Leadership Computing Facility, a U.S.
   Department of Energy Office of Science User Facility operated under
   contract DE-AC02-06CH11357.
NR 68
TC 26
Z9 28
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1032
EP 1042
DI 10.1109/TVCG.2020.3030439
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA WF5FO
UT WOS:000706330100087
PM 33048735
DA 2025-03-07
ER

PT J
AU Wang, QW
   Alexander, W
   Pegg, J
   Qu, HM
   Chen, M
AF Wang, Qianwen
   Alexander, William
   Pegg, Jack
   Qu, Huamin
   Chen, Min
TI HypoML: Visual Analysis for Hypothesis-based Evaluation of Machine
   Learning Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Analytical models; Testing; Data models; Computational modeling; Tools;
   Visualization; Neurons; Visual analytics; model-developmental
   visualization; machine learning; neural network; hypothesis test; HypoML
ID ANALYTICS
AB In this paper, we present a visual analytics tool for enabling hypothesis-based evaluation of machine learning (ML) models. We describe a novel ML-testing framework that combines the traditional statistical hypothesis testing (commonly used in empirical research) with logical reasoning about the conclusions of multiple hypotheses. The framework defines a controlled configuration for testing a number of hypotheses as to whether and how some extra information about a concept or feature may benefit or hinder an ML model. Because reasoning multiple hypotheses is not always straightforward, we provide HypoML as a visual analysis tool, with which, the multi-thread testing results are first transformed to analytical results using statistical and logical inferences, and then to a visual representation for rapid observation of the conclusions and the logical flow between the testing results and hypotheses. We have applied HypoML to a number of hypothesized concepts, demonstrating the intuitive and explainable nature of the visual analysis.
C1 [Wang, Qianwen; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Alexander, William; Pegg, Jack; Chen, Min] Univ Oxford, Oxford OX1 2JD, England.
C3 Hong Kong University of Science & Technology; University of Oxford
RP Wang, QW (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
EM qwangbb@connect.ust.hk; walexander1997@gmail.com; jack.p3gg@gmail.com;
   huamin@cse.ust.hk; min.chen@oerc.ox.ac.uk
RI Wang, Qianwen/GRJ-9435-2022
FU European Union [822214]; Marie Curie Actions (MSCA) [822214] Funding
   Source: Marie Curie Actions (MSCA)
FX A part of this work has received funding from the European Union's
   Horizon 2020 research and innovation programme under the Marie
   Sklodowska-Curie grant agreement No 822214.
NR 43
TC 6
Z9 7
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1417
EP 1426
DI 10.1109/TVCG.2020.3030449
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100121
PM 33048739
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Weng, D
   Zheng, CB
   Deng, ZK
   Ma, MZ
   Bao, J
   Zheng, Y
   Xu, ML
   Wu, YC
AF Weng, Di
   Zheng, Chengbo
   Deng, Zikun
   Ma, Mingze
   Bao, Jie
   Zheng, Yu
   Xu, Mingliang
   Wu, Yingcai
TI Towards Better Bus Networks: A Visual Analytics Approach
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Bus route planning; spatial decision -making; urban data visual
   analytics
ID TRANSIT ROUTE NETWORK; VISUALIZATION DESIGN; TRANSPORTATION;
   OPTIMIZATION; MOBILITY; MODEL; ALGORITHM; PATTERNS
AB Bus routes are typically updated every 3-5 years to meet constantly changing travel demands. However, identifying deficient bus routes and finding their optimal replacements remain challenging due to the difficulties in analyzing a complex bus network and the large solution space comprising alternative routes. Most of the automated approaches cannot produce satisfactory results in real -world settings without laborious inspection and evaluation of the candidates. The limitations observed in these approaches motivate us to collaborate with domain experts and propose a visual analytics solution for the performance analysis and incremental planning of bus routes based on an existing bus network. Developing such a solution involves three major challenges namely, a) the in-depth analysis of complex bus route networks, b) the interactive generation of improved route candidates. and c) the effective evaluation of alternative bus routes. For challenge a, we employ an overview-to-detail approach by dividing the analysis of a complex bus network into three levels to facilitate the efficient identification of deficient routes. For challenge b, we improve a route generation model and interpret the performance of the generation with tailored visualizations. For challenge c, we incorporate a conflict resolution strategy in the progressive decision -making process to assist users in evaluating the alternative routes and finding the most optimal one. The proposed system is evaluated with two usage scenarios based on real -world data and received positive feedback from the experts.
C1 [Weng, Di; Deng, Zikun; Wu, Yingcai] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.
   [Weng, Di; Zheng, Chengbo; Deng, Zikun; Ma, Mingze; Wu, Yingcai] Zhejiang Lab, Hangzhou, Peoples R China.
   [Bao, Jie; Zheng, Yu] JD Intelligent Cities Res, Beijing, Peoples R China.
   [Bao, Jie; Zheng, Yu] JD Intelligent Cities Business Unit, JD Digits, Beijing, Peoples R China.
   [Xu, Mingliang] Zhengzhou Univ, Sch Informat Engn, Zhengzhou, Peoples R China.
   [Xu, Mingliang] Zhengzhou Univ, Henan inst Adv Technol, Zhengzhou, Peoples R China.
C3 Zhejiang University; Zhejiang Laboratory; Zhengzhou University;
   Zhengzhou University
RP Wu, YC (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.; Wu, YC (corresponding author), Zhejiang Lab, Hangzhou, Peoples R China.
EM dweng@zju.edu.cn; chbozheng@gmail.com; zikun-rain@zju.edu.cn;
   mamzaug@foxmail.com; baojie@jd.com; msyuzheng@outlook.com;
   iexumingliang@zzu.edu.cn; ycwu@zju.edu.cn
RI Weng, Di/ABG-7408-2020; Ma, Mingze/KRP-3096-2024; Zheng,
   Yu/GRJ-5808-2022; Deng, Zikun/IQT-3106-2023
OI Weng, Di/0000-0003-2712-7274; Zheng, Chengbo/0000-0003-0226-9399
FU NSFC-Zhejiang Joint Fund for the Integration of Industrialization and
   Informatization [U1609217]; National Key R&D Program of China
   [2018YFB1004300]; NSFC [61761136020]; Zhejiang Provincial Natural
   Science Foundation [LR18F020001]; 100 Talents Program of Zhejiang
   University
FX The work was supported by NSFC-Zhejiang Joint Fund for the Integration
   of Industrialization and Informatization (U1609217), National Key R&D
   Program of China (2018YFB1004300), NSFC (61761136020), Zhejiang
   Provincial Natural Science Foundation (LR18F020001) and the 100 Talents
   Program of Zhejiang University.
NR 82
TC 48
Z9 50
U1 5
U2 38
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 817
EP 827
DI 10.1109/TVCG.2020.3030458
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100067
PM 33048743
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhou, ZG
   Shi, C
   Shen, XL
   Cai, LH
   Wang, HX
   Liu, YH
   Zhao, Y
   Chen, W
AF Zhou, Zhiguang
   Shi, Chen
   Shen, Xilong
   Cai, Lihong
   Wang, Haoxuan
   Liu, Yuhua
   Zhao, Ying
   Chen, Wei
TI Context-aware Sampling of Large Networks via Graph Representation
   Learning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Measurement; Sampling methods; Context modeling; Task
   analysis; Clutter; Layout; Graph sampling; Graph representation
   learning; Blue noise sampling; Graph evaluation
ID VISUAL ANALYTICS; VISUALIZATION; LAYOUT
AB Numerous sampling strategies have been proposed to simplify large-scale networks for highly readable visualizations. It is of great challenge to preserve contextual structures formed by nodes and edges with tight relationships in a sampled graph, because they are easily overlooked during the process of sampling due to their irregular distribution and immunity to scale. In this paper, a new graph sampling method is proposed oriented to the preservation of contextual structures. We first utilize a graph representation learning (GRL) model to transform nodes into vectors so that the contextual structures in a network can be effectively extracted and organized. Then, we propose a multi-objective blue noise sampling model to select a subset of nodes in the vectorized space to preserve contextual structures with the retention of relative data and cluster densities in addition to those features of significance, such as bridging nodes and graph connections. We also design a set of visual interfaces enabling users to interactively conduct context-aware sampling, visually compare results with various sampling strategies, and deeply explore large networks. Case studies and quantitative comparisons based on real-world datasets have demonstrated the effectiveness of our method in the abstraction and exploration of large networks.
C1 [Zhou, Zhiguang; Shi, Chen; Shen, Xilong; Cai, Lihong; Wang, Haoxuan; Liu, Yuhua] Zhejiang Univ Finance & Econ, Sch Informat, Hangzhou, Peoples R China.
   [Zhao, Ying] Cent South Univ, Changsha, Peoples R China.
   [Chen, Wei] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.
C3 Zhejiang University of Finance & Economics; Central South University;
   Zhejiang University
RP Zhao, Y (corresponding author), Cent South Univ, Changsha, Peoples R China.; Chen, W (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Peoples R China.
EM zhgzhou1983@zufe.edu.cn; shichen@zufe.edu.cn; 180110910420@zufe.edu.cn;
   cailihong@zufe.edu.cn; wanghaoxuan@zufe.edu.cn; liuyuhua@zufe.edu.cn;
   zhaoying@csu.edu.cn; chenwei@cad.zju.edu.cn
RI Zhao, Liangyu/IAO-7294-2023; Chen, Wei/AAR-9817-2020
OI Shi, Chen/0000-0002-4615-4123
FU National Natural Science Foundation of China [61872314, 61802339,
   41901363, 61872388, 61772456, 61761136]; Humanities and Social Sciences
   Foundation of Ministry of Education in China [18YJC910017]; Natural
   Science Foundation of Zhejiang Province [LGF20G010003, LY18F020024];
   Major Humanities and Social Sciences Research Projects in Colleges of
   Zhejiang Province [2018QN021]; Open Project Program of the State Key Lab
   of CAD&CG of Zhejiang University [A2001]; First Class Discipline of
   Zhejiang-A (Zhejiang University of Finance and Economics-Statistics)
FX We would like to thank the reviewers for their thoughtful comments. The
   work is supported in part by the National Natural Science Foundation of
   China (No.61872314, 61802339, 41901363, 61872388, 61772456 and
   61761136), the Humanities and Social Sciences Foundation of Ministry of
   Education in China (No.18YJC910017), the Natural Science Foundation of
   Zhejiang Province (No.LY18F020024 and LGF20G010003), the Major
   Humanities and Social Sciences Research Projects in Colleges of Zhejiang
   Province (No.2018QN021), the Open Project Program of the State Key Lab
   of CAD&CG of Zhejiang University (No.A2001) and the First Class
   Discipline of Zhejiang-A (Zhejiang University of Finance and
   Economics-Statistics).
NR 58
TC 24
Z9 29
U1 3
U2 30
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1709
EP 1719
DI 10.1109/TVCG.2020.3030440
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100147
PM 33052861
DA 2025-03-07
ER

PT J
AU Jiang, BY
   Zhang, JY
   Cai, JF
   Zheng, JN
AF Jiang, Boyi
   Zhang, Juyong
   Cai, Jianfei
   Zheng, Jianmin
TI Disentangled Human Body Embedding Based on Deep Hierarchical Neural
   Network
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shape; Strain; Three-dimensional displays; Biological system modeling;
   Computational modeling; Solid modeling; Face; 3D body shape; 3D human
   articulated body model; variational autoencoder; deformation
   representation; hierarchical structure
ID HANDS; MODEL
AB Human bodies exhibit various shapes for different identities or poses, but the body shape has certain similarities in structure and thus can be embedded in a low-dimensional space. This article presents an autoencoder-like network architecture to learn disentangled shape and pose embedding specifically for the 3D human body. This is inspired by recent progress of deformation-based latent representation learning. To improve the reconstruction accuracy, we propose a hierarchical reconstruction pipeline for the disentangling process and construct a large dataset of human body models with consistent connectivity for the learning of the neural network. Our learned embedding can not only achieve superior reconstruction accuracy but also provide great flexibility in 3D human body generation via interpolation, bilinear interpolation, and latent space sampling. The results from extensive experiments demonstrate the powerfulness of our learned 3D human body embedding in various applications.
C1 [Jiang, Boyi; Zhang, Juyong] Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Anhui, Peoples R China.
   [Cai, Jianfei] Monash Univ, Fac IT, Clayton, Vic 3800, Australia.
   [Zheng, Jianmin] Nanyang Technol Univ, Sch Engn & Comp Sci, Singapore 639798, Singapore.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Monash University; Nanyang Technological University
RP Zhang, JY (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Anhui, Peoples R China.
EM jby1993@mail.ustc.edu.cn; juyong@ustc.edu.cn; asjfcai@ntu.edu.sg;
   asjmzheng@ntu.edu.sg
RI Zheng, Jianmin/A-3717-2011; Cai, Jianfei/A-3691-2011
OI Zheng, Jianmin/0000-0002-5062-6226; Cai, Jianfei/0000-0002-9444-3763
FU National Natural Science Foundation of China [61672481]; Youth
   Innovation PromotionAssociation CAS [2018495]; NTU Data Science and
   Artificial Intelligence Research Center (DSAIR) [M4082285]; MoE Tier-2
   Grant of Singapore [2016-T2-2-065, 2017-T2-1-076]; National Research
   Foundation, Singapore under its International Research Centres in
   Singapore Funding Initiative
FX The authors would like to thank VRC Inc. (Japan) for sharing the scanned
   human shapemodelswith us in Fig. 6 and Table 3. This research was
   partially supported by National Natural Science Foundation of China (No.
   61672481), Youth Innovation PromotionAssociation CAS (No. 2018495), NTU
   Data Science and Artificial Intelligence Research Center (DSAIR) (No.
   M4082285), MoE Tier-2 Grant (2016-T2-2-065, 2017-T2-1-076) of Singapore,
   and the National Research Foundation, Singapore under its International
   Research Centres in Singapore Funding Initiative. Any opinions,
   findings, and conclusions or recommendations expressed in this material
   are those of the authors and do not reflect the views of the National
   Research Foundation, Singapore.
NR 57
TC 19
Z9 21
U1 0
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG. 1
PY 2020
VL 26
IS 8
BP 2560
EP 2575
DI 10.1109/TVCG.2020.2988476
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MG6BL
UT WOS:000546115000003
PM 32324557
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Bi, HK
   Mao, TL
   Wang, ZQ
   Deng, ZG
AF Bi, Huikun
   Mao, Tianlu
   Wang, Zhaoqi
   Deng, Zhigang
TI A Deep Learning-Based Framework for Intersectional Traffic Simulation
   and Editing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Trajectory; Solid modeling; Computational modeling; Vehicle dynamics;
   Traffic control; Data models; Deep learning; Traffic simulation; crowd
   simulation; data-driven; deep learning; intersectional traffic
ID MODEL; CLASSIFICATION; ANIMATION; ATTENTION; DYNAMICS; BEHAVIOR; LSTM;
   FLOW
AB Most of existing traffic simulation methods have been focused on simulating vehicles on freeways or city-scale urban networks. However, relatively little research has been done to simulate intersectional traffic to date despite its broad potential applications. In this paper, we propose a novel deep learning-based framework to simulate and edit intersectional traffic. Specifically, based on an in-house collected intersectional traffic dataset, we employ the combination of convolution network (CNN) and recurrent network (RNN) to learn the patterns of vehicle trajectories in intersectional traffic. Besides simulating novel intersectional traffic, our method can be used to edit existing intersectional traffic. Through many experiments as well as comparative user studies, we demonstrate that the results by our method are visually indistinguishable from ground truth, and our method can outperform existing methods.
C1 [Bi, Huikun] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Bi, Huikun; Mao, Tianlu; Wang, Zhaoqi] Chinese Acad Sci, Beijing Key Lab Mobile Comp & Pervas Device, Inst Comp Technol, Beijing 100190, Peoples R China.
   [Bi, Huikun] Univ Houston, Comp Graph & Interact Media Lab, Houston, TX 77204 USA.
   [Deng, Zhigang] Univ Houston, Comp Sci Dept, Houston, TX 77004 USA.
C3 Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; Chinese Academy of Sciences; Institute of Computing Technology,
   CAS; University of Houston System; University of Houston; University of
   Houston System; University of Houston
RP Deng, ZG (corresponding author), Univ Houston, Comp Sci Dept, Houston, TX 77004 USA.
EM xiaobi361@gmail.com; ltm@ict.ac.cn; zqwang@ict.ac.cn; zdeng4@uh.edu
RI Deng, Zhigang/MBH-0755-2025
OI Deng, Zhigang/0000-0003-2571-5865; Bi, Huikun/0000-0002-0690-4663; Deng,
   Zhigang/0000-0002-0452-8676
FU National Key Research and Development Program of China [2017YFC0804900];
   National Natural Science Foundation of China [61532002]; 13th Five-Year
   Common Technology pre Research Program [41402050301-170441402065];
   Science and Technology Mobilization Program of Dongguan [KZ2017-06]; US
   NSF [IIS 1524782]; CSC Fellowship
FX This work is in part supported by the National Key Research and
   Development Program of China (2017YFC0804900), the National Natural
   Science Foundation of China (61532002), the 13th Five-Year Common
   Technology pre Research Program (41402050301-170441402065), the Science
   and Technology Mobilization Program of Dongguan (KZ2017-06), and US NSF
   IIS 1524782. Huikun Bi is also supported by a CSC Fellowship.
NR 62
TC 12
Z9 12
U1 1
U2 27
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2020
VL 26
IS 7
BP 2335
EP 2348
DI 10.1109/TVCG.2018.2889834
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MB9QV
UT WOS:000542933100001
PM 30605102
DA 2025-03-07
ER

PT J
AU Ma, YX
   Tung, AKH
   Wang, W
   Gao, X
   Pan, ZG
   Chen, W
AF Ma, Yuxin
   Tung, Anthony K. H.
   Wang, Wei
   Gao, Xiang
   Pan, Zhigeng
   Chen, Wei
TI ScatterNet: A Deep Subjective Similarity Model for Visual Analysis of
   Scatterplots
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Feature extraction; Measurement; Neural networks;
   Personal area networks; Visual perception; Computational modeling;
   Scatterplot; similarity measuring; deep learning; visualization; visual
   exploration
ID RANKING VISUALIZATIONS; IMAGE SIMILARITY; EXPLORATION; SCAGNOSTICS;
   PERCEPTION; FEEDBACK; FEATURES; METRICS; VIEWS
AB Similarity measuring methods are widely adopted in a broad range of visualization applications. In this work, we address the challenge of representing human perception in the visual analysis of scatterplots by introducing a novel deep-learning-based approach, ScatterNet, captures perception-driven similarities of such plots. The approach exploits deep neural networks to extract semantic features of scatterplot images for similarity calculation. We create a large labeled dataset consisting of similar and dissimilar images of scatterplots to train the deep neural network. We conduct a set of evaluations including performance experiments and a user study to demonstrate the effectiveness and efficiency of our approach. The evaluations confirm that the learned features capture the human perception of scatterplot similarity effectively. We describe two scenarios to show how ScatterNet can be applied in visual analysis applications.
C1 [Ma, Yuxin; Chen, Wei] Zhejiang Univ, Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
   [Tung, Anthony K. H.; Wang, Wei] Natl Univ Singapore, Singapore 119077, Singapore.
   [Gao, Xiang; Pan, Zhigeng] Hangzhou Normal Univ, Yuhang 311121, Peoples R China.
C3 Zhejiang University; National University of Singapore; Hangzhou Normal
   University
RP Chen, W (corresponding author), Zhejiang Univ, Key Lab CAD&CG, Hangzhou 310058, Peoples R China.
EM mayuxin@zju.edu.cn; atung@comp.nus.edu.sg; wangwei@comp.nus.edu.sg;
   geekplux@gmail.com; zgpan@cad.zju.edu.cn; chenwei@cad.zju.edu.cn
RI MA, Yuxin/AAG-8630-2020; Chen, Wei/AAR-9817-2020
OI Gao, Xiang/0000-0003-1816-6327; Pan, Zhi-geng/0000-0003-0717-5850; Chen,
   Wei/0000-0002-8365-4741
FU National 973 Program of China [2015CB352503]; National Natural Science
   Foundation of China [61772456, 61761136020]; Key National Natural
   Science Foundation of China [61332017]; National Key R&D Program of
   China [2017YFB1002803, 2018YFB0904503]; National Research Foundation,
   Prime Ministers Office, Singapore under its Strategic Capability
   Research Centres Funding Initiative (N-CRiPT); International Research
   Centre Funding Initiative (SeSaMe Center); Singapore Ministry of
   Education Academic Research Fund Tier 1
FX This work is supported by National 973 Program of China (2015CB352503),
   National Natural Science Foundation of China (61772456 and 61761136020),
   Key National Natural Science Foundation of China (61332017), National
   Key R&D Program of China (2017YFB1002803 and 2018YFB0904503), the
   National Research Foundation, Prime Ministers Office, Singapore under
   its Strategic Capability Research Centres Funding Initiative (N-CRiPT)
   and International Research Centre Funding Initiative (SeSaMe Center),
   and Singapore Ministry of Education Academic Research Fund Tier 1. We
   thank the authors of [8] for sharing the test dataset. The first author
   would like to thank a fawn for her helpful inspiration.
NR 62
TC 56
Z9 63
U1 1
U2 28
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2020
VL 26
IS 3
BP 1562
EP 1576
DI 10.1109/TVCG.2018.2875702
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KH5VW
UT WOS:000510719400010
PM 30334762
DA 2025-03-07
ER

PT J
AU Chen, ZT
   Zeng, W
   Yang, ZG
   Yu, LY
   Fu, CW
   Qu, HM
AF Chen, Zhutian
   Zeng, Wei
   Yang, Zhiguang
   Yu, Lingyun
   Fu, Chi-Wing
   Qu, Huamin
TI LassoNet: Deep Lasso-Selection of 3D Point Clouds
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Point Clouds; Lasso Selection; Deep Learning
AB Selection is a fundamental task in exploratory analysis and visualization of 3D point clouds. Prior researches on selection methods were developed mainly based on heuristics such as local point density, thus limiting their applicability in general data. Specific challenges root in the great variabilities implied by point clouds (e.g., dense vs. sparse), viewpoint (e.g., occluded vs. non-occluded), and lasso (e.g., small vs. large). In this work, we introduce LassoNet, a new deep neural network for lasso selection of 3D point clouds, attempting to learn a latent mapping from viewpoint and lasso to point cloud regions. To achieve this, we couple user-target points with viewpoint and lasso information through 3D coordinate transform and naive selection, and improve the method scalability via an intention filtering and farthest point sampling. A hierarchical network is trained using a dataset with over 30K lasso-selection records on two different point cloud data. We conduct a formal user study to compare LassoNet with two state-of-the-art lasso-selection methods. The evaluations confirm that our approach improves the selection effectiveness and efficiency across different combinations of 3D point clouds, viewpoints, and lasso selections. Project Website: https://lassonet.github.io
C1 [Chen, Zhutian; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Zeng, Wei; Yang, Zhiguang] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.
   [Yu, Lingyun] Univ Groningen, Groningen, Netherlands.
   [Fu, Chi-Wing] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology; Chinese Academy of
   Sciences; Shenzhen Institute of Advanced Technology, CAS; University of
   Groningen; Chinese University of Hong Kong
RP Zeng, W (corresponding author), Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen, Peoples R China.
EM zhutian.chen@connect.ust.hk; wei.zeng@siat.ac.cn; zg.yang@siat.ac.cn;
   lingyun.yu@rug.nl; cwfu@cse.cuhk.edu.hk; huamin@cse.ust.hk
RI Fu, Chi-Wing/X-4703-2019
OI Fu, Chi Wing/0000-0002-5238-593X; Zeng, Wei/0000-0002-5600-8824
FU National Natural Science Foundation of China [61802388, 61602139]; MSRA
   [MRA19EG02]
FX The authors wish to thank the anonymous reviewers for their valuable
   comments. This work was supported in part by National Natural Science
   Foundation of China (No.61802388 and No.61602139). This work is
   partially supported by a grant from MSRA (code: MRA19EG02).
NR 43
TC 33
Z9 40
U1 4
U2 24
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 195
EP 204
DI 10.1109/TVCG.2019.2934332
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100018
PM 31425100
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Fujiwara, T
   Chou, JK
   Shilpika
   Xu, PP
   Ren, L
   Ma, KL
AF Fujiwara, Takanori
   Chou, Jia-Kai
   Shilpika
   Xu, Panpan
   Ren, Liu
   Ma, Kwan-Liu
TI An Incremental Dimensionality Reduction Method for Visualizing Streaming
   Multidimensional Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Dimensionality reduction; principal component analysis; streaming data;
   uncertainty; visual analytics
ID ANOMALY DETECTION; ANALYTICS; TIME; SYSTEM
AB Dimensionality reduction (DR) methods are commonly used for analyzing and visualizing multidimensional data. However, when data is a live streaming feed, conventional DR methods cannot be directly used because of their computational complexity and inability to preserve the projected data positions at previous time points. In addition, the problem becomes even more challenging when the dynamic data records have a varying number of dimensions as often found in real-world applications. This paper presents an DR solution. We enhance an existing incremental PCA method in several ways to ensure its usability for visualizing streaming multidimensional data. First, we use geometric transformation and animation methods to help preserve a viewer mental map when visualizing the incremental results. Second, to handle data dimension variants, we use an optimization method to estimate the projected data positions, and also convey the resulting uncertainty in the visualization. We demonstrate the effectiveness of our design with two case studies using real-world datasets.
C1 [Fujiwara, Takanori; Chou, Jia-Kai; Shilpika; Ma, Kwan-Liu] Univ Calif Davis, Davis, CA 95616 USA.
   [Xu, Panpan; Ren, Liu] Bosch Res North Amer, Palo Alto, CA USA.
C3 University of California System; University of California Davis
RP Fujiwara, T (corresponding author), Univ Calif Davis, Davis, CA 95616 USA.
EM tfujiwara@ucdavis.edu; jkchou@ucdavis.edu; fshilpika@ucdavis.edu;
   panpan.xu@us.bosch.com; liu.ren@us.bosch.com; klma@ucdavis.edu
RI Fujiwara, Takanori/AAY-5045-2020
OI Fujiwara, Takanori/0000-0002-6382-2752
FU Bosch Research; U.S. National Science Foundation [IIS-1528203,
   IIS-1741536]
FX This research is sponsored in part by Bosch Research and the U.S.
   National Science Foundation through grants IIS-1528203 and IIS-1741536.
NR 70
TC 46
Z9 57
U1 1
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 418
EP 428
DI 10.1109/TVCG.2019.2934433
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100039
PM 31449024
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kwon, OH
   Ma, KL
AF Kwon, Oh-Hyun
   Ma, Kwan-Liu
TI A Deep Generative Model for Graph Layout
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Graph; network; visualization; layout; machine learning; deep learning;
   neural network; generative model; autoencoder
ID DRAWING GRAPHS; COMMUNITY STRUCTURE
AB Different layouts can characterize different aspects of the same graph. Finding a good layout of a graph is thus an important task for graph visualization. In practice, users often visualize a graph in multiple layouts by using different methods and varying parameter settings until they find a layout that best suits the purpose of the visualization. However, this trial-and-error process is often haphazard and time-consuming. To provide users with an intuitive way to navigate the layout design space, we present a technique to systematically visualize a graph in diverse layouts using deep generative models. We design an encoder-decoder architecture to learn a model from a collection of example layouts, where the encoder represents training examples in a latent space and the decoder produces layouts from the latent space. In particular, we train the model to construct a two-dimensional latent space for users to easily explore and generate various layouts. We demonstrate our approach through quantitative and qualitative evaluations of the generated layouts. The results of our evaluations show that our model is capable of learning and generalizing abstract concepts of graph layouts, not just memorizing the training examples. In summary, this paper presents a fundamentally new approach to graph visualization where a machine learning model learns to visualize a graph from examples without manually-defined heuristics.
C1 [Kwon, Oh-Hyun; Ma, Kwan-Liu] Univ Calif Davis, Davis, CA 95616 USA.
C3 University of California System; University of California Davis
RP Kwon, OH (corresponding author), Univ Calif Davis, Davis, CA 95616 USA.
EM kw@ucdavis.edu; ma@cs.ucdavis.edu
RI Kwon, Oh-Hyun/V-8085-2019
FU U.S. National Science Foundation [IIS-1741536]
FX This research has been sponsored in part by the U.S. National Science
   Foundation through grant IIS-1741536.
NR 85
TC 40
Z9 43
U1 2
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 665
EP 675
DI 10.1109/TVCG.2019.2934396
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100062
PM 31425108
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Pezzotti, N
   Thijssen, J
   Mordvintsev, A
   Höllt, T
   van Lew, B
   Lelieveldt, BPF
   Eisemann, E
   Vilanova, A
AF Pezzotti, Nicola
   Thijssen, Julian
   Mordvintsev, Alexander
   Hollt, Thomas
   van Lew, Baldur
   Lelieveldt, Boudewijn P. F.
   Eisemann, Elmar
   Vilanova, Anna
TI GPGPU Linear Complexity t-SNE Optimization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE High Dimensional Data; Dimensionality Reduction; Progressive Visual
   Analytics; Approximate Computation; GPGPU
ID PROGRESSIVE VISUAL ANALYTICS; DIMENSIONALITY REDUCTION; VISUALIZATION;
   EXPLORATION
AB In recent years the t-distributed Stochastic Neighbor Embedding (t-SNE) algorithm has become one of the most used and insightful techniques for exploratory data analysis of high-dimensional data. It reveals clusters of high-dimensional data points at different scales while only requiring minimal tuning of its parameters. However, the computational complexity of the algorithm limits its application to relatively small datasets. To address this problem, several evolutions of t-SNE have been developed in recent years, mainly focusing on the scalability of the similarity computations between data points. However, these contributions are insufficient to achieve interactive rates when visualizing the evolution of the t-SNE embedding for large datasets. In this work, we present a novel approach to the minimization of the t-SNE objective function that heavily relies on graphics hardware and has linear computational complexity. Our technique decreases the computational cost of running t-SNE on datasets by orders of magnitude and retains or improves on the accuracy of past approximated techniques. We propose to approximate the repulsive forces between data points by splatting kernel textures for each data point. This approximation allows us to reformulate the t-SNE minimization problem as a series of tensor operations that can be efficiently executed on the graphics card. An efficient implementation of our technique is integrated and available for use in the widely used Google TensorFlow.js, and an open-source C++ library.
C1 [Pezzotti, Nicola; Mordvintsev, Alexander] Google AI, Zurich, Switzerland.
   [Pezzotti, Nicola; Thijssen, Julian; Hollt, Thomas; Lelieveldt, Boudewijn P. F.; Eisemann, Elmar; Vilanova, Anna] Delft Univ Technol, Delft, Netherlands.
   [Hollt, Thomas; van Lew, Baldur; Lelieveldt, Boudewijn P. F.] Leiden Univ, Med Ctr, Leiden, Netherlands.
C3 Delft University of Technology; Leiden University - Excl LUMC; Leiden
   University; Leiden University Medical Center (LUMC)
RP Pezzotti, N (corresponding author), Google AI, Zurich, Switzerland.; Pezzotti, N (corresponding author), Delft Univ Technol, Delft, Netherlands.
RI Lelieveldt, Boudewijn/B-6501-2008
FU Google AI team PAIR; STW Project, VAnPIRe [12720]
FX The authors wish to thank the Google AI team PAIR for supporting the
   development of the TensorFlow.js implementation. This work received
   funding through the STW Project 12720, VAnPIRe.
NR 45
TC 44
Z9 45
U1 0
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 1172
EP 1181
DI 10.1109/TVCG.2019.2934307
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100108
PM 31449023
OA Green Submitted, Green Published
DA 2025-03-07
ER

PT J
AU Rojo, IB
   Günther, T
AF Rojo, Irene Baeza
   Guenther, Tobias
TI Vector Field Topology of Time-Dependent Flows in a Steady Reference
   Frame
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Scientific visualization; unsteady flow; vector field topology;
   reference frame optimization
ID LAGRANGIAN COHERENT STRUCTURES; OF-THE-ART; VISUALIZATION; DEFINITION;
   VORTICES; MOTIONS
AB The topological analysis of unsteady vector fields remains to this day one of the largest challenges in flow visualization. We build up on recent work on vortex extraction to define a time-dependent vector field topology for 2D and 3D flows. In our work, we split the vector field into two components: a vector field in which the flow becomes steady, and the remaining ambient flow that describes the motion of topological elements (such as sinks, sources and saddles) and feature curves (vortex corelines and bifurcation lines). To this end, we expand on recent local optimization approaches by modeling spatially-varying deformations through displacement transformations from continuum mechanics. We compare and discuss the relationships with existing local and integration-based topology extraction methods, showing for instance that separatrices seeded from saddles in the optimal frame align with the integration-based streakline vector field topology. In contrast to the streakline-based approach, our method gives a complete picture of the topology for every time slice, including the steps near the temporal domain boundaries. With our work it now becomes possible to extract topological information even when only few time slices are available. We demonstrate the method in several analytical and numerically-simulated flows and discuss practical aspects, limitations and opportunities for future work.
C1 [Rojo, Irene Baeza; Guenther, Tobias] Swiss Fed Inst Technol, Comp Graph Lab, Zurich, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich
RP Rojo, IB (corresponding author), Swiss Fed Inst Technol, Comp Graph Lab, Zurich, Switzerland.
EM irene.baeza@inf.ethz.ch; tobias.guenther@inf.ethz.ch
OI Gunther, Tobias/0000-0002-3020-0930
FU Swiss National Science Foundation (SNSF) Ambizione grant
   [PZ00P2_180114]; Swiss National Science Foundation (SNF) [PZ00P2_180114]
   Funding Source: Swiss National Science Foundation (SNF)
FX This work was supported by the Swiss National Science Foundation (SNSF)
   Ambizione grant no. PZ00P2_180114.
NR 65
TC 36
Z9 37
U1 1
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 280
EP 290
DI 10.1109/TVCG.2019.2934375
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100026
PM 31425107
DA 2025-03-07
ER

PT J
AU Satyanarayan, A
   Lee, B
   Ren, D
   Heer, J
   Stasko, J
   Thompson, J
   Brehmer, M
   Liu, ZC
AF Satyanarayan, Arvind
   Lee, Bongshin
   Ren, Donghao
   Heer, Jeffrey
   Stasko, John
   Thompson, John
   Brehmer, Matthew
   Liu, Zhicheng
TI Critical Reflections on Visualization Authoring Systems
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Critical reflection; visualization authoring; expressivity;
   learnability; reusability
ID DESIGN; VEGA
AB An emerging generation of visualization authoring systems support expressive information visualization without textual programming. As they vary in their visualization models, system architectures, and user interfaces, it is challenging to directly compare these systems using traditional evaluative methods. Recognizing the value of contextualizing our decisions in the broader design space, we present critical reflections on three systems we developed Lyra, Data Illustrator, and Charticulator. This paper surfaces knowledge that would have been daunting within the constituent papers of these three systems. We compare and contrast their (previously unmentioned) limitations and trade-offs between expressivity and learnability. We also reflect on common assumptions that we made during the development of our systems, thereby informing future research directions in visualization authoring systems.
C1 [Satyanarayan, Arvind] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Lee, Bongshin; Brehmer, Matthew] Microsoft Res, Redmond, WA USA.
   [Ren, Donghao] Univ Calif Santa Barbara, Santa Barbara, CA USA.
   [Heer, Jeffrey] Univ Washington, Seattle, WA 98195 USA.
   [Stasko, John; Thompson, John] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Liu, Zhicheng] Adobe Res, San Jose, CA USA.
C3 Massachusetts Institute of Technology (MIT); Microsoft; University of
   California System; University of California Santa Barbara; University of
   Washington; University of Washington Seattle; University System of
   Georgia; Georgia Institute of Technology; Adobe Systems Inc.
RP Satyanarayan, A (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
EM arvindsatya@mit.edu; bongshin@microsoft.com; donghaoren@cs.ucsb.edu;
   jheer@uw.edu; john.stasko@cc.gatech.edu; jrthompson@gatech.edu;
   mb@mattbrehmer.ca; leoli@adobe.com
NR 47
TC 57
Z9 66
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 461
EP 471
DI 10.1109/TVCG.2019.2934281
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100043
PM 31442976
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Yang, Y
   Wybrow, M
   Li, YF
   Czauderna, T
   He, YQ
AF Yang, Ying
   Wybrow, Michael
   Li, Yuan-Fang
   Czauderna, Tobias
   He, Yongqun
TI OntoPlot: A Novel Visualisation for Non-hierarchical Associations in
   Large Ontologies
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Ontology visualisation; visual compression; interactive exploration;
   ontology associations
ID BIOMEDICAL ONTOLOGIES; BIOLOGICAL KNOWLEDGE; PREDICTION; EVENTS; MODEL
AB Ontologies are formal representations of concepts and complex relationships among them. They have been widely used to capture comprehensive domain knowledge in areas such as biology and medicine, where large and complex ontologies can contain hundreds of thousands of concepts. Especially due to the large size of ontologies, visualisation is useful for authoring, exploring and understanding their underlying data. Existing ontology visualisation tools generally focus on the hierarchical structure, giving much less emphasis to non-hierarchical associations. In this paper we present OntoPlot, a novel visualisation specifically designed to facilitate the exploration of all concept associations whilst still showing an ontology's large hierarchical structure. This hybrid visualisation combines icicle plots, visual compression techniques and interactivity, improving space-efficiency and reducing visual structural complexity. We conducted a user study with domain experts to evaluate the usability of OntoPlot, comparing it with the de facto ontology editor Protege. The results confirm that OntoPlot attains our design goals for association-related tasks and is strongly favoured by domain experts.
C1 [Yang, Ying; Wybrow, Michael; Li, Yuan-Fang; Czauderna, Tobias] Monash Univ, Clayton, Vic, Australia.
   [He, Yongqun] Univ Michigan, Sch Med, Ann Arbor, MI 48109 USA.
C3 Monash University; University of Michigan System; University of Michigan
RP Yang, Y (corresponding author), Monash Univ, Clayton, Vic, Australia.
EM ying.yang@monash.edu; michael.wybrow@monash.edu; yuanfang.li@monash.edu;
   tobias.czauderna@monash.edu; yongqunh@med.umich.edu
RI Li, Yuan-Fang/T-7532-2019
OI Czauderna, Tobias/0000-0002-1788-9593; Wybrow,
   Michael/0000-0001-5536-7780; Li, Yuan-Fang/0000-0003-4651-2821; He,
   Yongqun/0000-0001-9189-9661; Yang, Ying/0000-0001-9630-687X
NR 71
TC 5
Z9 5
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 1140
EP 1150
DI 10.1109/TVCG.2019.2934557
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100105
PM 31442991
OA Green Submitted, Green Published
DA 2025-03-07
ER

PT J
AU Wang, JP
   Hazarika, S
   Li, C
   Shen, HW
AF Wang, Junpeng
   Hazarika, Subhashis
   Li, Cheng
   Shen, Han-Wei
TI Visualization and Visual Analysis of Ensemble Data: A Survey
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Ensemble data; visualization and visual analysis; literature analysis;
   taxonomy
ID UNCERTAINTY VISUALIZATION; NONPARAMETRIC MODELS; SCIENTIFIC-DATA;
   VARIABILITY; FLOW; SETS; SENSITIVITY; EXPLORATION; SIMILARITY; ANALYTICS
AB Over the last decade, ensemble visualization has witnessed a significant development due to the wide availability of ensemble data, and the increasing visualization needs from a variety of disciplines. From the data analysis point of view, it can be observed that many ensemble visualization works focus on the same facet of ensemble data, use similar data aggregation or uncertainty modeling methods. However, the lack of reflections on those essential commonalities and a systematic overview of those works prevents visualization researchers from effectively identifying new or unsolved problems and planning for further developments. In this paper, we take a holistic perspective and provide a survey of ensemble visualization. Specifically, we study ensemble visualization works in the recent decade, and categorize them from two perspectives: (1) their proposed visualization techniques; and (2) their involved analytic tasks. For the first perspective, we focus on elaborating how conventional visualization techniques (e.g., surface, volume visualization techniques) have been adapted to ensemble data; for the second perspective, we emphasize how analytic tasks (e.g., comparison, clustering) have been performed differently for ensemble data. From the study of ensemble visualization literature, we have also identified several research trends, as well as some future research opportunities.
C1 [Wang, Junpeng; Hazarika, Subhashis; Li, Cheng; Shen, Han-Wei] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
C3 University System of Ohio; Ohio State University
RP Wang, JP (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
EM Wang.7665@osu.edu; hazarika.3@osu.edu; li.4076@osu.edu; shen.94@osu.edu
RI Hazarika, Subhashis/GNP-3087-2022; Shen, Han-wei/A-4710-2012
OI Wang, Junpeng/0000-0002-1130-9914; Hazarika,
   Subhashis/0000-0003-0575-9318
FU US Department of Energy Los Alamos National Laboratory [47145];
   UT-Battelle LLC [4000159447]; NSF [IIS-1250752, IIS-1065025]; US
   Department of Energy [DE-SC0007444, DE-DC0012495]
FX This work was supported in part by US Department of Energy Los Alamos
   National Laboratory contract 47145, UT-Battelle LLC contract 4000159447,
   NSF grants IIS-1250752, IIS-1065025, and US Department of Energy grants
   DE-SC0007444, DE-DC0012495, program manager Lucy Nowell.
NR 118
TC 92
Z9 105
U1 2
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2019
VL 25
IS 9
BP 2853
EP 2872
DI 10.1109/TVCG.2018.2853721
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IN8OV
UT WOS:000478940300012
PM 29994615
OA Bronze
DA 2025-03-07
ER

PT J
AU Hohman, F
   Kahng, M
   Pienta, R
   Chau, DH
AF Hohman, Fred
   Kahng, Minsuk
   Pienta, Robert
   Chau, Duen Horng
TI Visual Analytics in Deep Learning: An Interrogative Survey for the Next
   Frontiers
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Deep learning; visual analytics; information visualization; neural
   networks
ID CONVOLUTIONAL NEURAL-NETWORKS; MACHINE; HUMANS; TOOL
AB Deep learning has recently seen rapid development and received significant attention due to its state-of-the-art performance on previously-thought hard problems. However, because of the internal complexity and nonlinear structure of deep neural networks, the underlying decision making processes for why these models are achieving such performance are challenging and sometimes mystifying to interpret. As deep learning spreads across domains, it is of paramount importance that we equip users of deep learning with tools for understanding when a model works correctly, when it fails, and ultimately how to improve its performance. Standardized toolkits for building neural networks have helped democratize deep learning; visual analytics systems have now been developed to support model explanation, interpretation, debugging, and improvement. We present a survey of the role of visual analytics in deep learning research, which highlights its short yet impactful history and thoroughly summarizes the state-of-the-art using a human-centered interrogative framework, focusing on the Five W's and How (Why, Who, What, How, When, and Where). We conclude by highlighting research directions and open research problems. This survey helps researchers and practitioners in both visual analytics and deep learning to quickly learn key aspects of this young and rapidly growing body of research, whose impact spans a diverse range of domains.
C1 [Hohman, Fred; Kahng, Minsuk; Pienta, Robert; Chau, Duen Horng] Georgia Tech, Coll Comp, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology
RP Hohman, F (corresponding author), Georgia Tech, Coll Comp, Atlanta, GA 30332 USA.
EM fredhohman@gatech.edu; kahng@gatech.edu; pientars@gatech.edu;
   polo@gatech.edu
OI Hohman, Frederick/0000-0002-4164-844X; Chau, Polo/0000-0001-9824-3323;
   Kahng, Minsuk/0000-0002-0291-6026
FU NSF [IIS-1563816, CNS-1704701, TWC-1526254]; NIBIB grant [U54EB020404];
   NASA Space Technology Research Fellowship; NSF GRFP [DGE-1650044]
FX This work was supported by NSF grants IIS-1563816, CNS-1704701, and
   TWC-1526254; NIBIB grant U54EB020404; NSF GRFP DGE-1650044; NASA Space
   Technology Research Fellowship; and gifts from Intel, Google, Symantec.
NR 124
TC 291
Z9 329
U1 6
U2 24
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2019
VL 25
IS 8
BP 2674
EP 2693
DI 10.1109/TVCG.2018.2843369
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IG2AT
UT WOS:000473597800012
PM 29993551
OA Green Accepted, Bronze, Green Submitted
HC Y
HP N
DA 2025-03-07
ER

PT J
AU Wang, JP
   Gou, L
   Zhang, W
   Yang, H
   Shen, HW
AF Wang, Junpeng
   Gou, Liang
   Zhang, Wei
   Yang, Hao
   Shen, Han-Wei
TI <i>DeepVID</i>: Deep Visual Interpretation and Diagnosis for Image
   Classifiers via Knowledge Distillation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 12th IEEE Pacific Visualization Symposium (IEEE PacificVis)
CY APR 23-26, 2019
CL Chulalongkorn Univ, Bangkok, THAILAND
SP IEEE, IEEE Comp Soc, IEEE Comp Soc, Visualizat & Graph Tech Comm
HO Chulalongkorn Univ
DE Deep neural networks; model interpretation; knowledge distillation;
   generative model; visual analytics
AB Deep Neural Networks (DNNs) have been extensively used in multiple disciplines due to their superior performance. However, in most cases, DNNs are considered as black-boxes and the interpretation of their internal working mechanism is usually challenging. Given that model trust is often built on the understanding of how a model works, the interpretation of DNNs becomes more important, especially in safety-critical applications (e.g., medical diagnosis, autonomous driving). In this paper, we propose DeepVID, a Deep learning approach to Visually Interpret and Diagnose DNN models, especially image classifiers. In detail, we train a small locally-faithful model to mimic the behavior of an original cumbersome DNN around a particular data instance of interest, and the local model is sufficiently simple such that it can be visually interpreted (e.g., a linear model). Knowledge distillation is used to transfer the knowledge from the cumbersome DNN to the small model, and a deep generative model (i.e., variational auto-encoder) is used to generate neighbors around the instance of interest. Those neighbors, which come with small feature variances and semantic meanings, can effectively probe the DNN's behaviors around the interested instance and help the small model to learn those behaviors. Through comprehensive evaluations, as well as case studies conducted together with deep learning experts, we validate the effectiveness of DeepVID.
C1 [Wang, Junpeng; Shen, Han-Wei] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
   [Gou, Liang; Zhang, Wei] Visa Res, Data Analyt Team, Palo Alto, CA 94306 USA.
   [Yang, Hao] Visa Res, Data Analyt, Palo Alto, CA 94306 USA.
C3 University System of Ohio; Ohio State University
RP Wang, JP (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
EM wang.7665@osu.edu; ligou@visa.com; wzhan@visa.com; haoyang@visa.com;
   shen.94@osu.edu
RI Shen, Han-wei/A-4710-2012
OI Wang, Junpeng/0000-0002-1130-9914
FU UT-Battelle LLC [4000159557]; Los Alamos National Laboratory [471415];
   NSF [SBE-1738502]
FX This work was supported in part by UT-Battelle LLC 4000159557, Los
   Alamos National Laboratory Contract 471415, and NSF grant SBE-1738502.
NR 33
TC 83
Z9 96
U1 0
U2 28
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2019
VL 25
IS 6
BP 2168
EP 2180
DI 10.1109/TVCG.2019.2903943
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HW8AC
UT WOS:000466910200003
PM 30892211
DA 2025-03-07
ER

PT J
AU Liang, W
   Liu, JJ
   Lang, YN
   Ning, B
   Yu, LF
AF Liang, Wei
   Liu, Jingjing
   Lang, Yining
   Ning, Bing
   Yu, Lap-Fai
TI Functional Workspace Optimization via Learning Personal Preferences from
   Virtual Experiences
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Affordance; Human-centered Design; Virtual Environments; Workspace
   Design; Remodeling
ID PRACTICAL METHOD; EXPOSURE
AB The functionality of a workspace is one of the most important considerations in both virtual world design and interior design. To offer appropriate functionality to the user, designers usually take some general rules into account, e.g., general workflow and average stature of users, which are summarized from the population statistics. Yet, such general rules cannot reflect the personal preferences of a single individual, which vary from person to person. In this paper, we intend to optimize a functional workspace according to the personal preferences of the specific individual who will use it. We come up with an approach to learn the individual's personal preferences from his activities while using a virtual version of the workspace via virtual reality devices. Then, we construct a cost function, which incorporates personal preferences, spatial constraints, pose assessments, and visual field. At last, the cost function is optimized to achieve an optimal layout. To evaluate the approach, we experimented with different settings. The results of the user study show that the workspaces updated in this way better fit the users.
C1 [Liang, Wei; Liu, Jingjing; Lang, Yining; Ning, Bing] Beijing Inst Technol, Beijing, Peoples R China.
   [Yu, Lap-Fai] George Mason Univ, Fairfax, VA 22030 USA.
C3 Beijing Institute of Technology; George Mason University
RP Liang, W (corresponding author), Beijing Inst Technol, Beijing, Peoples R China.
EM liangwei@bit.edu.cn; 3120181009@bit.edu.cn; lucky@langyining.com;
   ningbing@bift.edu.cn; craigyu@gmu.edu
FU Natural Science Foundation of China (NSFC) [61876020]; National Science
   Foundation [1565978]; Div Of Information & Intelligent Systems; Direct
   For Computer & Info Scie & Enginr [1565978] Funding Source: National
   Science Foundation
FX This research is supported by the Natural Science Foundation of China
   (NSFC) under award number 61876020 and the National Science Foundation
   under award number 1565978.
NR 50
TC 24
Z9 26
U1 1
U2 41
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2019
VL 25
IS 5
BP 1836
EP 1845
DI 10.1109/TVCG.2019.2898721
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA HR3EI
UT WOS:000463019100003
PM 30762550
DA 2025-03-07
ER

PT J
AU Serrano, A
   Kim, I
   Chen, ZL
   DiVerdi, S
   Gutierrez, D
   Hertzmann, A
   Masia, B
AF Serrano, Ana
   Kim, Incheol
   Chen, Zhili
   DiVerdi, Stephen
   Gutierrez, Diego
   Hertzmann, Aaron
   Masia, Belen
TI Motion parallax for 360° RGBD video
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Immersive environments; Virtual Reality video
AB We present a method for adding parallax and real-time playback of 360 degrees videos in Virtual Reality headsets. In current video players, the playback does not respond to translational head movement, which reduces the feeling of immersion, and causes motion sickness for some viewers. Given a 360 degrees video and its corresponding depth (provided by current stereo 360 degrees stitching algorithms), a naive image-based rendering approach would use the depth to generate a 3D mesh around the viewer, then translate it appropriately as the viewer moves their head. However, this approach breaks at depth discontinuities, showing visible distortions, whereas cutting the mesh at such discontinuities leads to ragged silhouettes and holes at disocclusions. We address these issues by improving the given initial depth map to yield cleaner, more natural silhouettes. We rely on a three-layer scene representation, made up of a foreground layer and two static background layers, to handle disocclusions by propagating information from multiple frames for the first background layer, and then inpainting for the second one. Our system works with input from many of today's most popular 360 degrees stereo capture devices (e.g., Yi Halo or GoPro Odyssey), and works well even if the original video does not provide depth information. Our user studies confirm that our method provides a more compelling viewing experience than without parallax, increasing immersion while reducing discomfort and nausea.
C1 [Serrano, Ana; Kim, Incheol; Gutierrez, Diego; Masia, Belen] Univ Zaragoza, I3A, Zaragoza, Spain.
   [Chen, Zhili; DiVerdi, Stephen; Hertzmann, Aaron] Adobe Res, Cambridge, MA USA.
C3 University of Zaragoza; Adobe Systems Inc.
RP Serrano, A (corresponding author), Univ Zaragoza, I3A, Zaragoza, Spain.
RI Serrano Pacheu, Ana Belen/ABC-3358-2021
OI Serrano Pacheu, Ana Belen/0000-0002-7796-3177; Hertzmann,
   Aaron/0000-0001-9667-0292; Gutierrez Perez, Diego/0000-0002-7503-7022;
   Masia, Belen/0000-0003-0060-7278
FU ERC; Spanish Ministry of Economy and Competitiveness [TIN2016-78753-P,
   TIN2016-79710-P]; FPI grant from the Spanish Ministry of Economy and
   Competitiveness; Adobe Research Fellowship; Nvidia Graduate Fellowship
FX The authors would like to thank Miguel Crespo for support with
   experiments, the participants for their colaboration, and Adobe Research
   for the 360 degrees videos provided (in particular the creators and
   performers). This research has been partially funded by an ERC
   Consolidator Grant (project CHAMELEON), and the Spanish Ministry of
   Economy and Competitiveness (projects TIN2016-78753-P, and
   TIN2016-79710-P). Ana Serrano was supported by an FPI grant from the
   Spanish Ministry of Economy and Competitiveness, an Adobe Research
   Fellowship, and a Nvidia Graduate Fellowship.
NR 67
TC 52
Z9 55
U1 2
U2 28
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2019
VL 25
IS 5
BP 1817
EP 1827
DI 10.1109/TVCG.2019.2898757
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HR3EI
UT WOS:000463019100001
PM 30843842
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Zhang, JH
   Fan, ZC
   Sun, DW
   Liao, HE
AF Zhang, Jiahui
   Fan, Zhencheng
   Sun, Dawei
   Liao, Hongen
TI Unified Mathematical Model for Multilayer-Multiframe Compressive Light
   Field Displays Using LCDs
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Compressive light field display; multilayer-multiframe LCD display;
   optimization methods; polarization-based display
AB We propose a unified mathematical model for multilayer-multiframe compressive light field displays that supports both attenuation-based and polarization-based architectures. We show that the light field decomposition of such a display can be cast as a bound constrained nonlinear matrix optimization problem. Efficient light field decomposition algorithms are developed using the limited-memory BFGS (L-BFGS) method for automultiscopic displays with high resolution and high image fidelity. In addition, this framework is the first to support multilayer polarization-based compressive light field displays with time multiplexing. This new architecture significantly reduces artifacts compared with attenuation-based multilayer-multiframe displays; thus, it can allow the requirements regarding the number of layers or the refresh rate to be relaxed. We verify the proposed methods by constructing two 3-layer prototypes using high-speed LCDs, one based on the attenuation architecture and one based on the polarization architecture. Moreover, an efficient CUDA-based program is implemented. Our displays can produce images with higher spatial resolution with thinner form factors compared with traditional automultiscopic displays in both simulations and experiments.
C1 [Zhang, Jiahui; Fan, Zhencheng; Liao, Hongen] Tsinghua Univ, Sch Med, Dept Biomed Engn, Beijing 100084, Peoples R China.
   [Sun, Dawei] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University
RP Liao, HE (corresponding author), Tsinghua Univ, Sch Med, Dept Biomed Engn, Beijing 100084, Peoples R China.
EM jiahui-z15@mails.tsinghua.edu.cn; fanzc13@mails.tsinghua.edu.cn;
   sdw14@mails.tsinghua.edu.cn; liao@tsinghua.edu.cn
RI Liao, Hongen/C-3097-2009
OI Zhang, Jiahui/0000-0003-4541-5894; Liao, Hongen/0000-0003-3847-9347
FU National Key Research and Development Program of China [2017YFC0108000];
   National Natural Science Foundation of China [81427803, 81771940];
   Beijing Municipal Science & Technology Commission [Z151100003915079];
   Beijing Municipal Natural Science Foundation [7172122]; Soochow-Tsinghua
   Innovation Project [2016SZ0206]
FX This work was supported in part by National Key Research and Development
   Program of China (2017YFC0108000), National Natural Science Foundation
   of China (81427803, 81771940), Beijing Municipal Science & Technology
   Commission (Z151100003915079), Beijing Municipal Natural Science
   Foundation (7172122), and Soochow-Tsinghua Innovation Project
   (2016SZ0206). We thank the reviewers for their suggestion. Helpful
   advice was provided by Guowen Chen, Xinran Zhang, Tianqi Huang, and Cong
   Ma with Department of Biomedical Engineering, School of Medicine,
   Tsinghua University.
NR 38
TC 13
Z9 19
U1 0
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2019
VL 25
IS 3
BP 1603
EP 1614
DI 10.1109/TVCG.2018.2810279
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HK3NZ
UT WOS:000457824500013
PM 29994155
DA 2025-03-07
ER

PT J
AU Wyman, C
   McGuire, M
AF Wyman, Chris
   McGuire, Morgan
TI Improved Alpha Testing Using Hashed Sampling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Anisotropy; alpha map; alpha test; hash; hashed alpha test; stable
   shading; stochastic sampling
AB We further describe and analyze the idea of hashed alpha testing from Wyman and McGuire [1], which builds on stochastic alpha testing and simplifies stochastic transparency. Typically, alpha testing provides a simple mechanism to mask out complex silhouettes using simple proxy geometry with applied alpha textures. While widely used, alpha testing has a long-standing problem: geometry can disappear entirely as alpha mapped polygons recede with distance. As foveated rendering for virtual reality spreads, this problem worsens as peripheral minification and prefiltering introduce this problem on nearby objects. We first introduce the notion of stochastic alpha testing, which replaces a fixed alpha threshold of alpha(tau) = 0.5 with a randomly chosen at alpha(tau) is an element of [0..1). This entirely avoids the problem of disappearing alpha-tested geometry, but introduces temporal noise. Hashed alpha testing uses a hash function to choose alpha(tau) procedurally. With a good hash function and inputs, hashed alpha testing maintains distant geometry without introducing more temporal flicker than traditional alpha testing. We also describe how hashed alpha interacts with temporal antialiasing and applies to alpha-to-coverage and screen-door transparency. Because hashed alpha testing addresses alpha test aliasing by introducing stable sampling, it has implications in other domains where increased sample stability is desirable. We show how our hashed sampling might apply to other stochastic effects.
C1 [Wyman, Chris] NVIDIA Corp, Redmond, WA 98052 USA.
   [McGuire, Morgan] NVIDIA Corp, Williamstown, MA 01267 USA.
   [McGuire, Morgan] Williams Coll, Williamstown, MA 01267 USA.
C3 Nvidia Corporation; Nvidia Corporation; Williams College
RP Wyman, C (corresponding author), NVIDIA Corp, Redmond, WA 98052 USA.
EM chris.wyman@acm.org; morgan@cs.williams.edu
NR 24
TC 3
Z9 3
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2019
VL 25
IS 2
BP 1309
EP 1320
DI 10.1109/TVCG.2017.2739149
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG5ZY
UT WOS:000455062000006
PM 28809702
DA 2025-03-07
ER

PT J
AU Muthumanickam, PK
   Vrotsou, K
   Nordman, A
   Johansson, J
   Cooper, M
AF Muthumanickam, Prithiviraj K.
   Vrotsou, Katerina
   Nordman, Aida
   Johansson, Jimmy
   Cooper, Matthew
TI Identification of Temporally Varying Areas of Interest in Long-Duration
   Eye-Tracking Data Sets
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Terms Eye-tracking data. areas of interest; clustering. minimum spanning
   tree; temporal data; spatio-temporal data
ID VISUAL ANALYTICS; GAZE; VISUALIZATION; EXPLORATION; MOVEMENTS; REGIONS
AB Eye-tracking has become an invaluable tool for the analysis of working practices in many technological fields of activity. Typically studies focus on short tasks and use static expected areas of interest (Aol) in the display to explore subjects' behaviour, making the analyst's task quite straightforward. In long-duration studies, where the observations may last several hours over a complete work session, the Aols may change over time in response to altering workload, emergencies or other variables making the analysis more difficult. This work puts forward a novel method to automatically identify spatial Aols changing over time through a combination of clustering and cluster merging in the temporal domain. A visual analysis system based on the proposed methods is also presented. Finally, we illustrate our approach within the domain of air traffic control, a complex task sensitive to prevailing conditions over long durations, though it is applicable to other domains such as monitoring of complex systems.
C1 [Muthumanickam, Prithiviraj K.; Vrotsou, Katerina; Nordman, Aida; Johansson, Jimmy; Cooper, Matthew] Linkoping Univ, Linkoping, Sweden.
C3 Linkoping University
RP Muthumanickam, PK (corresponding author), Linkoping Univ, Linkoping, Sweden.
EM prithiviraj.muthumanickam@liu.se; katerina.vrotsou@liu.se;
   aida.vitoria@liu.se; jimmy.johansson@liu.se; matthew.cooper@liu.se
OI Nordman, Aida/0000-0002-9601-5981
FU Swedish Research Council [2013-4939]; RESKILL project - Swedish
   Transport Administration; Swedish Maritime Administration; Swedish Air
   Navigation Service Provider LFV
FX We thank Asa Svensson for providing the dataset. This work is supported
   by the Swedish Research Council, grant number 2013-4939 and partly by
   RESKILL project funded by the Swedish Transport Administration, Swedish
   Maritime Administration and the Swedish Air Navigation Service Provider
   LFV.
NR 49
TC 9
Z9 12
U1 1
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 87
EP 97
DI 10.1109/TVCG.2018.2865042
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000009
PM 30183636
DA 2025-03-07
ER

PT J
AU Ren, DH
   Lee, B
   Brehmer, M
AF Ren, Donghao
   Lee, Bongshin
   Brehmer, Matthew
TI Charticulator: Interactive Construction of Bespoke Chart Layouts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Interactive visualization authoring; Chart layout design; Glyph design;
   Constraint-based design; Reusable chart layout
ID VISUALIZATION; DESIGN; VEGA
AB We present Charticulator, an interactive authoring tool that enables the creation of bespoke and reusable chart layouts. Charticulator is our response to most existing chart construction interfaces that require authors to choose from predefined chart layouts, thereby precluding the construction of novel charts. In contrast, Charticulator transforms a chart specification into mathematical layout constraints and automatically computes a set of layout attributes using a constraint-solving algorithm to realize the chart. It allows for the articulation of compound marks or glyphs as well as links between these glyphs, all without requiring any coding or knowledge of constraint satisfaction. Furthermore, thanks to the constraint-based layout approach, Charticulator can export chart designs into reusable templates that can be imported into other visualization tools. In addition to describing Charticulator's conceptual framework and design, we present three forms of evaluation: a gallery to illustrate its expressiveness, a user study to verify its usability, and a click-count comparison between Charticulator and three existing tools. Finally, we discuss the limitations and potentials of Charticulator as well as directions for future research. Charticulator is available with its source code at https : //charticulator.com.
C1 [Ren, Donghao] Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.
   [Ren, Donghao; Lee, Bongshin; Brehmer, Matthew] Microsoft Res, Redmond, WA 98052 USA.
C3 University of California System; University of California Santa Barbara;
   Microsoft
RP Ren, DH (corresponding author), Univ Calif Santa Barbara, Santa Barbara, CA 93106 USA.; Ren, DH (corresponding author), Microsoft Res, Redmond, WA 98052 USA.
EM donghaoren@cs.ucsb.edu; bongshin@microsoft.com; mabrehme@microsoft.com
NR 48
TC 83
Z9 93
U1 1
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 789
EP 799
DI 10.1109/TVCG.2018.2865158
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000075
PM 30136992
DA 2025-03-07
ER

PT J
AU Song, H
   Szafir, DA
AF Song, Hayeong
   Szafir, Danielle Albers
TI Where's My Data? Evaluating Visualizations with Missing Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Information Visualization; Graphical Perception; Time Series Data; Data
   Wrangling; Imputation
ID VISUAL ANALYSIS; UNCERTAINTY; ENCODINGS; TRUST; BARS
AB Many real-world datasets are incomplete due to factors such as data collection failures or misalignments between fused datasets. Visualizations of incomplete datasets should allow analysts to draw conclusions from their data while effectively reasoning about the quality of the data and resulting conclusions. We conducted a pair of crowdsourced studies to measure how the methods used to impute and visualize missing data may influence analysts' perceptions of data quality and their confidence in their conclusions. Our experiments used different design choices for line graphs and bar charts to estimate averages and trends in incomplete time series datasets. Our results provide preliminary guidance for visualization designers to consider when working with incomplete data in different domains and scenarios.
C1 [Song, Hayeong; Szafir, Danielle Albers] Univ Colorado, Boulder, CO 80309 USA.
C3 University of Colorado System; University of Colorado Boulder
RP Song, H (corresponding author), Univ Colorado, Boulder, CO 80309 USA.
EM hayeong.song@colorado.edu; danielle.szafir@colorado.edu
FU NSF CRII: CHS [1657599]
FX The authors wish to thank reviewers and other members of VisuaLab at
   University of Colorado Boulder. This research was funded by NSF CRII:
   CHS #1657599.
NR 57
TC 47
Z9 60
U1 1
U2 24
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 914
EP 924
DI 10.1109/TVCG.2018.2864914
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000087
PM 30136964
DA 2025-03-07
ER

PT J
AU Traoré, M
   Hurter, C
   Telea, A
AF Traore, Michael
   Hurter, Christophe
   Telea, Alexandru
TI Interactive obstruction-free lensing for volumetric data visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Interaction techniques; focus plus context; volume visualization; volume
   rendering; raycasting
ID ATTRIBUTE; VIEWS
AB Occlusion is an issue in volumetric visualization as it prevents direct visualization of the region of interest. While many techniques such as transfer functions, volume segmentation or view distortion have been developed to address this, there is still room for improvement to better support the understanding of objects' vicinity. However, most existing Focus+Context fail to solve partial occlusion in datasets where the target and the occluder are very similar density-wise. For these reasons, we investigate a new technique which maintains the general structure of the investigated volumetric dataset while addressing occlusion issues. With our technique, the user interactively defines an area of interest where an occluded region or object is partially visible. Then our lens starts pushing at its border occluding objects, thus revealing hidden volumetric data. Next, the lens is modified with an extended field of view (fish-eye deformation) to better see the vicinity of the selected region. Finally, the user can freely explore the surroundings of the area under investigation within the lens. To provide real-time exploration, we implemented our lens using a GPU accelerated ray-casting framework to handle ray deformations, local lighting, and local viewpoint manipulation. We illustrate our technique with five application scenarios in baggage inspection, 3D fluid flow visualization, chest radiology, air traffic planning, and DTI fiber exploration.
C1 [Traore, Michael; Hurter, Christophe] French Civil Aviat Univ, ENAC, Toulouse, France.
   [Telea, Alexandru] Univ Groningen, Inst Johan Bernoulli, Groningen, Netherlands.
C3 Universite Federale Toulouse Midi-Pyrenees (ComUE); Universite de
   Toulouse; Ecole Nationale de l'Aviation Civile (ENAC); University of
   Groningen
RP Traoré, M (corresponding author), French Civil Aviat Univ, ENAC, Toulouse, France.
EM traore.s.michael@enac.fr; christophe.hurter@enac.fr; a.c.telea@rug.nl
RI Hurter, Christophe/AHB-0811-2022
FU French National Agency for Research (Agence Nationale de la Recherche
   ANR) [ANR-14-CE24-0006-01]; SESAR Research and Innovation Action Horizon
   2020 under project MOTO (The embodied reMOte TOwer); Agence Nationale de
   la Recherche (ANR) [ANR-14-CE24-0006] Funding Source: Agence Nationale
   de la Recherche (ANR)
FX The authors acknowledge the support of the French National Agency for
   Research (Agence Nationale de la Recherche ANR) under the grant
   ANR-14-CE24-0006-01 project TERANOVA and the SESAR Research and
   Innovation Action Horizon 2020 under project MOTO (The embodied reMOte
   TOwer). We also thank dr. Van de Perre and dr. Brozici from the
   pulmonology and radiology departments at the Heilig Hart Ziekenhuis,
   Mol, Belgium for their invaluable help with the chest tumor use-case
   described in Sec. 5.3.
NR 51
TC 6
Z9 8
U1 1
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 1029
EP 1039
DI 10.1109/TVCG.2018.2864690
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000098
PM 30235132
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhang, H
   Xu, F
AF Zhang, Hao
   Xu, Feng
TI MixedFusion: Real-Time Reconstruction of an Indoor Scene with Dynamic
   Objects
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Scene reconstruction; dynamic reconstruction; single view
ID CAPTURE
AB Real-time indoor scene reconstruction aims to recover the 3D geometry of an indoor scene in real time with a sensor scanning the scene. Previous works of this topic consider pure static scenes, but in this paper, we focus on more challenging cases that the scene contains dynamic objects, for example, moving people and floating curtains, which are quite common in reality and thus are eagerly required to be handled. We develop an end-to-end system using a depth sensor to scan a scene on the fly. By proposing a Sigmoid-based Iterative Closest Point (S-ICP) method, we decouple the camera motion and the scene motion from the input sequence and segment the scene into static and dynamic parts accordingly. The static part is used to estimate the camera rigid motion, while for the dynamic part, graph node-based motion representation and model-to-depth fitting are applied to reconstruct the scene motions. With the camera and scene motions reconstructed, we further propose a novel mixed voxel allocation scheme to handle static and dynamic scene parts with different mechanisms, which helps to gradually fuse a large scene with both static and dynamic objects. Experiments show that our technique successfully fuses the geometry of both the static and dynamic objects in a scene in real time, which extends the usage of the current techniques for indoor scene reconstruction.
C1 [Zhang, Hao; Xu, Feng] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
   [Zhang, Hao; Xu, Feng] Minist Educ China, Key Lab Informat Syst Secur, Beijing 100084, Peoples R China.
   [Zhang, Hao; Xu, Feng] Tsinghua Natl Lab Informat Sci & Technol, Beijing 100084, Peoples R China.
C3 Tsinghua University; Tsinghua University
RP Xu, F (corresponding author), Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.; Xu, F (corresponding author), Minist Educ China, Key Lab Informat Syst Secur, Beijing 100084, Peoples R China.
EM zhanghao16@mails.tsinghua.edu.cn; xufeng2003@gmail.com
RI Zhang, Hao/P-6220-2017
FU NSFC [61671268, 61727808]
FX This work was supported by the NSFC (No. 61671268, 61727808). We wish to
   express our thanks to the reviewers for their insightful comments. We
   also would like to thank Kaiwen Guo and Tao Yu for their helpful
   discussion in this work. Feng Xu is the corresponding author.
NR 29
TC 19
Z9 21
U1 2
U2 31
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2018
VL 24
IS 12
BP 3137
EP 3146
DI 10.1109/TVCG.2017.2786233
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GZ0TW
UT WOS:000449079000011
PM 29990141
DA 2025-03-07
ER

PT J
AU Hristova, H
   Le Meur, O
   Cozot, R
   Bouatouch, K
AF Hristova, Hristina
   Le Meur, Olivier
   Cozot, Remi
   Bouatouch, Kadi
TI Transformation of the Multivariate Generalized Gaussian Distribution for
   Image Editing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
ID SCREENED POISSON EQUATION; COLOR TRANSFER; SEGMENTATION; MIXTURE
AB Multivariate generalized Gaussian distributions (MGGDs) have aroused a great interest in the image processing community thanks to their ability to describe accurately various image features, such as image gradient fields. However, so far their applicability has been limited by the lack of a transformation between two of these parametric distributions. In this paper, we propose a novel transformation between MGGDs, consisting of an optimal transportation of the second-order statistics and a stochastic-based shape parameter transformation. We employ the proposed transformation between MGGDs for a color transfer and a gradient transfer between images. We also propose a new simultaneous transfer of color and gradient, which we apply for image color correction.
C1 [Hristova, Hristina; Le Meur, Olivier; Cozot, Remi; Bouatouch, Kadi] Univ Rennes 1, F-35000 Rennes, France.
C3 Universite de Rennes
RP Hristova, H (corresponding author), Univ Rennes 1, F-35000 Rennes, France.
EM hristina.hristova@irisa.fr; olivier.lemeur@irisa.fr;
   remi.cozot@irisa.fr; kadi.bouatouch@irisa.fr
OI Le Meur, Olivier/0000-0001-9883-0296; Hristova,
   Hristina/0000-0003-2894-6933
NR 47
TC 9
Z9 10
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2018
VL 24
IS 10
BP 2813
EP 2826
DI 10.1109/TVCG.2017.2769050
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GS7PN
UT WOS:000443894900012
PM 29990084
DA 2025-03-07
ER

PT J
AU Saha, PK
   Jin, DK
   Liu, YX
   Christensen, GE
   Chen, C
AF Saha, Punam K.
   Jin, Dakai
   Liu, Yinxiao
   Christensen, Gary E.
   Chen, Cheng
TI Fuzzy Object Skeletonization: Theory, Algorithms, and Applications
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Skeletonization; fuzzy set; grassfire propagation; collision-impact;
   distance transform; digital topology and geometry
ID PARALLEL THINNING ALGORITHM; TRABECULAR BONE; DISTANCE TRANSFORMATIONS;
   CURVE SKELETONIZATION; TOPOLOGICAL ANALYSIS; SHAPE-DESCRIPTION;
   DIGITAL-TOPOLOGY; SURFACE; THICKNESS; ROBUST
AB Skeletonization offers a compact representation of an object while preserving important topological and geometrical features. Literature on skeletonization of binary objects is quite mature. However, challenges involved with skeletonization of fuzzy objects are mostly unanswered. This paper presents a new theory and algorithm of skeletonization for fuzzy objects, evaluates its performance, and demonstrates its applications. A formulation of fuzzy grassfire propagation is introduced; its relationships with fuzzy distance functions, level sets, and geodesics are discussed; and several new theoretical results are presented in the continuous space. A notion of collision-impact of fire-fronts at skeletal points is introduced, and its role in filtering noisy skeletal points is demonstrated. A fuzzy object skeletonization algorithm is developed using new notions of surface-and curve-skeletal voxels, digital collision-impact, filtering of noisy skeletal voxels, and continuity of skeletal surfaces. A skeletal noise pruning algorithm is presented using branch-level significance. Accuracy and robustness of the new algorithm are examined on computer-generated phantoms and micro-and conventional CT imaging of trabecular bone specimens. An application of fuzzy object skeletonization to compute structure-width at a low image resolution is demonstrated, and its ability to predict bone strength is examined. Finally, the performance of the new fuzzy object skeletonization algorithm is compared with two binary object skeletonization methods.
C1 [Saha, Punam K.; Jin, Dakai; Liu, Yinxiao; Chen, Cheng] Univ Iowa, Dept Elect & Comp Engn, Iowa City, IA 52246 USA.
   [Saha, Punam K.] Univ Iowa, Dept Radiol, Iowa City, IA 52246 USA.
   [Christensen, Gary E.] Univ Iowa, Dept Elect & Comp Engn & Radiat Oncol, Iowa City, IA 52246 USA.
C3 University of Iowa; University of Iowa; University of Iowa
RP Saha, PK (corresponding author), Univ Iowa, Dept Elect & Comp Engn, Iowa City, IA 52246 USA.; Saha, PK (corresponding author), Univ Iowa, Dept Radiol, Iowa City, IA 52246 USA.
EM pksaha@engineering.uiowa.edu; dakai-jin@uiowa.edu;
   yinxiao-liu@uiowa.edu; gary-christensen@uiowa.edu; cheng-chen@uiowa.edu
RI Chen, Cheng/B-3119-2019; Saha, Punam/F-8833-2011
OI Saha, Punam/0000-0003-1576-118X
FU NIH [R01-AR054439]
FX This work was supported in part by the NIH grant R01-AR054439.
NR 89
TC 15
Z9 18
U1 3
U2 24
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2018
VL 24
IS 8
BP 2298
EP 2314
DI 10.1109/TVCG.2017.2738023
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GL6DJ
UT WOS:000437269000004
PM 28809701
DA 2025-03-07
ER

PT J
AU Zhao, LY
   Hansard, M
   Cavallaro, A
AF Zhao, Lingyun
   Hansard, Miles
   Cavallaro, Andrea
TI Layered Scene Models from Single Hazy Images
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Scene modelling; single view reconstruction; image dehazing
ID COLOR
AB This paper describes the construction of a layered scene model, based on a single hazy image that has sufficient depth variation. A depth map and radiance image are estimated by standard dehazing methods. The radiance image is then segmented into a small number of clusters, and a corresponding scene plane is estimated for each. This provides the basic structure of a layered scene model, without the need for multiple views, or image correspondences. We show that problems of gap filling and depth blending can be addressed systematically, with respect to the layered depth structure. The final models, which resemble cardboard 'pop-ups', are visually convincing. An implementation is described, and subjective depth preferences are tested in a psychophysical experiment.
C1 [Zhao, Lingyun; Hansard, Miles; Cavallaro, Andrea] Queen Mary Univ London, Ctr Intelligent Sensing, Mile End Rd, London E1 4NS, England.
C3 University of London; Queen Mary University London
RP Zhao, LY (corresponding author), Queen Mary Univ London, Ctr Intelligent Sensing, Mile End Rd, London E1 4NS, England.
EM lingyun.zhao@qmul.ac.uk; miles.hansard@qmul.ac.uk;
   a.cavallaro@qmul.ac.uk
OI Zhao, Lingyun/0000-0002-8246-8242
FU China Scholarship Council
FX The authors would like to thank all participants in the subjective
   evaluation experiment. The first author is grateful to the China
   Scholarship Council for financial support.
NR 38
TC 2
Z9 2
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2018
VL 24
IS 7
BP 2167
EP 2179
DI 10.1109/TVCG.2017.2708108
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GH3SG
UT WOS:000433321900009
PM 29813020
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Han, DT
   Suhail, M
   Ragan, ED
AF Han, Dustin T.
   Suhail, Mohamed
   Ragan, Eric D.
TI Evaluating Remapped Physical Reach for Hand Interactions with Passive
   Haptics in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 25th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 18-22, 2018
CL Reutlingen, GERMANY
SP IEEE
DE Virtual reality; 3D interaction; passive haptics; hand interaction;
   remapped reach; 3D object selection
AB Virtual reality often uses motion tracking to incorporate physical hand movements into interaction techniques for selection and manipulation of virtual objects. To increase realism and allow direct hand interaction, real-world physical objects can be aligned with virtual objects to provide tactile feedback and physical grasping. However, unless a physical space is custom configured to match a specific virtual reality experience, the ability to perfectly match the physical and virtual objects is limited. Our research addresses this challenge by studying methods that allow one physical object to be mapped to multiple virtual objects that can exist at different virtual locations in an egocentric reference frame. We study two such techniques: one that introduces a static translational offset between the virtual and physical hand before a reaching action, and one that dynamically interpolates the position of the virtual hand during a reaching motion. We conducted two experiments to assess how the two methods affect reaching effectiveness, comfort, and ability to adapt to the remapping techniques when reaching for objects with different types of mismatches between physical and virtual locations. We also present a case study to demonstrate how the hand remapping techniques could be used in an immersive game application to support realistic hand interaction while optimizing usability. Overall, the translational technique performed better than the interpolated reach technique and was more robust for situations with larger mismatches between virtual and physical objects.
C1 [Han, Dustin T.; Suhail, Mohamed; Ragan, Eric D.] Texas A&M Univ, College Stn, TX 77843 USA.
C3 Texas A&M University System; Texas A&M University College Station
RP Han, DT (corresponding author), Texas A&M Univ, College Stn, TX 77843 USA.
EM dthan@tamu.edu; mohamedsuhail@tamu.edu; eragan@tamu.edu
NR 26
TC 53
Z9 58
U1 3
U2 33
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2018
VL 24
IS 4
BP 1467
EP 1476
DI 10.1109/TVCG.2018.2794659
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FZ6BS
UT WOS:000427682500009
PM 29543165
OA Green Published
DA 2025-03-07
ER

PT J
AU Padmanaban, N
   Ruban, T
   Sitzmann, V
   Norcia, AM
   Wetzstein, G
AF Padmanaban, Nitish
   Ruban, Timon
   Sitzmann, Vincent
   Norcia, Anthony M.
   Wetzstein, Gordon
TI Towards a Machine-learning Approach for Sickness Prediction in 360°
   Stereoscopic Videos
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 25th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 18-22, 2018
CL Reutlingen, GERMANY
SP IEEE, IEEE Comp Soc, IEEE Comp Soc Visualizat & Graph Tech Comm, VICON, Digital Project, ART, Haption, MiddleVR, VR ON, VISCON, BARCO, WorldViz, Disney Res, Chinese Acad Sci, Comp Network Informat Ctr, KUKA
DE Virtual reality; simulator sickness; vection; machine learning
ID MOTION SICKNESS; PERIPHERAL-VISION; CIRCULAR VECTION; PERCEPTION; ROLL
AB Virtual reality systems are widely believed to be the next major computing platform. There are, however, some barriers to adoption that must be addressed, such as that of motion sickness - which can lead to undesirable symptoms including postural instability, headaches, and nausea. Motion sickness in virtual reality occurs as a result of moving visual stimuli that cause users to perceive self-motion while they remain stationary in the real world. There are several contributing factors to both this perception of motion and the subsequent onset of sickness, including field of view, motion velocity, and stimulus depth. We verify first that differences in vection due to relative stimulus depth remain correlated with sickness. Then, we build a dataset of stereoscopic 3D videos and their corresponding sickness ratings in order to quantify their nauseogenicity, which we make available for future use. Using this dataset, we train a machine learning algorithm on hand-crafted features (quantifying speed, direction, and depth as functions of time) from each video, learning the contributions of these various features to the sickness ratings. Our predictor generally outperforms a naive estimate, but is ultimately limited by the size of the dataset. However, our result is promising and opens the door to future work with more extensive datasets. This and further advances in this space have the potential to alleviate developer and end user concerns about motion sickness in the increasingly commonplace virtual world.
C1 [Padmanaban, Nitish; Ruban, Timon; Sitzmann, Vincent; Wetzstein, Gordon] Stanford Elect Engn Dept, Stanford, CA 94305 USA.
   [Norcia, Anthony M.] Stanford Psychol Dept, Stanford, CA USA.
RP Wetzstein, G (corresponding author), Stanford Elect Engn Dept, Stanford, CA 94305 USA.
EM gordon.wetzstein@stanford.edu
OI Padmanaban, Nitish/0000-0002-1643-7413; Wetzstein,
   Gordon/0000-0002-9243-6885
NR 40
TC 76
Z9 82
U1 1
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2018
VL 24
IS 4
BP 1594
EP 1603
DI 10.1109/TVCG.2018.2793560
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FZ6BS
UT WOS:000427682500022
PM 29553929
DA 2025-03-07
ER

PT J
AU Xiao, R
   Schwarz, J
   Throm, N
   Wilson, AD
   Benko, H
AF Xiao, Robert
   Schwarz, Julia
   Throm, Nick
   Wilson, Andrew D.
   Benko, Hrvoje
TI MRTouch: Adding Touch Input to Head-Mounted Mixed Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 25th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 18-22, 2018
CL Reutlingen, GERMANY
SP IEEE
DE Augmented reality; touch interaction; depth sensing; sensor fusion;
   on-world interaction
AB We present MRTouch, a novel multitouch input solution for head-mounted mixed reality systems. Our system enables users to reach out and directly manipulate virtual interfaces affixed to surfaces in their environment, as though they were touchscreens. Touch input offers precise, tactile and comfortable user input, and naturally complements existing popular modalities, such as voice and hand gesture. Our research prototype combines both depth and infrared camera streams together with real-time detection and tracking of surface planes to enable robust finger-tracking even when both the hand and head are in motion. Our technique is implemented on a commercial Microsoft HoloLens without requiring any additional hardware nor any user or environmental calibration. Through our performance evaluation, we demonstrate high input accuracy with an average positional error of 5.4 mm and 95% button size of 16 mm, across 17 participants, 2 surface orientations and 4 surface materials. Finally, we demonstrate the potential of our technique to enable on-world touch interactions through 5 example applications.
C1 [Xiao, Robert; Wilson, Andrew D.; Benko, Hrvoje] Microsoft Res, Redmond, WA 98052 USA.
   [Xiao, Robert] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
   [Schwarz, Julia; Throm, Nick] Microsoft, Redmond, WA USA.
C3 Microsoft; Carnegie Mellon University; Microsoft
RP Xiao, R (corresponding author), Microsoft Res, Redmond, WA 98052 USA.; Xiao, R (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
EM brx@cs.cmu.edu; jschwarz@microsoft.com; nithrom@microsoft.com;
   awilson@microsoft.com; benko@microsoft.com
RI Xiao, Robert/MHR-2554-2025
NR 61
TC 96
Z9 109
U1 2
U2 39
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2018
VL 24
IS 4
BP 1653
EP 1660
DI 10.1109/TVCG.2018.2794222
PG 8
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FZ6BS
UT WOS:000427682500028
PM 29543181
DA 2025-03-07
ER

PT J
AU Nguyen, H
   Rosen, P
AF Hoa Nguyen
   Rosen, Paul
TI DSPCP: A Data Scalable Approach for Identifying Relationships in
   Parallel Coordinates
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Correlation; parallel coordinates plot; large data visualization
ID VISUALIZATIONS
AB Parallel coordinates plots (PCPs) are a well-studied technique for exploring multi-attribute datasets. In many situations, users find them a flexible method to analyze and interact with data. Unfortunately, using PCPs becomes challenging as the number of data items grows large or multiple trends within the data mix in the visualization. The resulting overdraw can obscure important features. A number of modifications to PCPs have been proposed, including using color, opacity, smooth curves, frequency, density, and animation to mitigate this problem. However, these modified PCPs tend to have their own limitations in the kinds of relationships they emphasize. We propose a new data scalable design for representing and exploring data relationships in PCPs. The approach exploits the point/ line duality property of PCPs and a local linear assumption of data to extract and represent relationship summarizations. This approach simultaneously shows relationships in the data and the consistency of those relationships. Our approach supports various visualization tasks, including mixed linear and nonlinear pattern identification, noise detection, and outlier detection, all in large data. We demonstrate these tasks on multiple synthetic and real-world datasets.
C1 [Hoa Nguyen] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
   [Rosen, Paul] Univ S Florida, Dept Comp Sci & Engn, Tampa, FL 33620 USA.
C3 Utah System of Higher Education; University of Utah; State University
   System of Florida; University of South Florida
RP Nguyen, H (corresponding author), Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
EM hoanguyen@sci.utah.edu; prosen@usf.edu
RI Rosen, Paul/AAN-1370-2021
OI Rosen, Paul/0000-0002-0873-9518
FU US National Science Foundation CIF21 DIBBs [ACI-1443046]; US National
   Science Foundation Core Program [IIS-1513616]; Lawrence Livermore
   National Laboratory; Lawrence Berkeley National Laboratory; Div Of
   Information & Intelligent Systems; Direct For Computer & Info Scie &
   Enginr [1513616] Funding Source: National Science Foundation
FX We would like to thank our reviewers and colleagues who provided
   valuable feedback in our design process. We would also like to thank our
   funding agents, US National Science Foundation CIF21 DIBBs
   (ACI-1443046), US National Science Foundation Core Program
   (IIS-1513616), Lawrence Livermore National Laboratory, and Lawrence
   Berkeley National Laboratory.
NR 62
TC 11
Z9 13
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2018
VL 24
IS 3
BP 1301
EP 1315
DI 10.1109/TVCG.2017.2661309
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FU0LA
UT WOS:000423541200007
OA Bronze
DA 2025-03-07
ER

PT J
AU Edge, D
   Riche, NH
   Larson, J
   White, C
AF Edge, Darren
   Riche, Nathalie Henry
   Larson, Jonathan
   White, Christopher
TI Beyond Tasks: An Activity Typology for Visual Analytics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Activity theory; visual analytics; activity-centered design; literature
   review; human-computer interaction
ID VISUALIZATION; DESIGN; COLLABORATION; FRAMEWORK; SENSE
AB As Visual Analytics (VA) research grows and diversifies to encompass new systems, techniques, and use contexts, gaining a holistic view of analytic practices is becoming ever more challenging. However, such a view is essential for researchers and practitioners seeking to develop systems for broad audiences that span multiple domains. In this paper, we interpret VA research through the lens of Activity Theory (AT) a framework for modelling human activities that has been influential in the field of Human Computer Interaction. We first provide an overview of Activity Theory, showing its potential for thinking beyond tasks, representations, and interactions to the broader systems of activity in which interactive tools are embedded and used. Next, we describe how Activity Theory can be used as an organizing framework in the construction of activity typologies, building and expanding upon the tradition of abstract task taxonomies in the field of Information Visualization. We then apply the resulting process to create an activity typology for Visual Analytics, synthesizing a wide range of systems and activity concepts from the literature. Finally, we use this typology as the foundation of an activity-centered design process, highlighting both tensions and opportunities in the design space of VA systems.
C1 [Edge, Darren; Riche, Nathalie Henry; Larson, Jonathan; White, Christopher] Microsoft Res, Redmond, WA 98052 USA.
C3 Microsoft
RP Edge, D (corresponding author), Microsoft Res, Redmond, WA 98052 USA.
EM darren.edge@microsoft.com; nath@microsoft.com; jolarso@microsoft.com;
   chwh@microsoft.com
NR 72
TC 4
Z9 6
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 267
EP 277
DI 10.1109/TVCG.2017.2745180
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400028
PM 28866577
DA 2025-03-07
ER

PT J
AU Hulman, J
   Kay, M
   Kim, YS
   Shrestha, S
AF Hulman, Jessica
   Kay, Matthew
   Kim, Yea-Seul
   Shrestha, Samana
TI Imagining Replications: Graphical Prediction & Discrete Visualizations
   Improve Recall & Estimation of Effect Uncertainty
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Graphical prediction; interactive uncertainty visualization; replication
   crisis; probability distribution
ID DIAGNOSTIC INFERENCES; CONFIDENCE-INTERVALS; INFORMATION; STATISTICS;
   REPRESENTATION; RETRIEVAL; DECISIONS; ERROR; BARS
AB People often have erroneous intuitions about the results of uncertain processes, such as scientific experiments. Many uncertainty visualizations assume considerable statistical knowledge, but have been shown to prompt erroneous conclusions even when users possess this knowledge. Active learning approaches been shown to improve statistical reasoning, but are rarely applied in visualizing uncertainty in scientific reports. We present a controlled study to evaluate the impact of an interactive, graphical uncertainty prediction technique for communicating uncertainty in experiment results. Using our technique, users sketch their prediction of the uncertainty in experimental effects prior to viewing the true sampling distribution from an experiment. We find that having a user graphically predict the possible effects from experiment replications is an effective way to improve one's ability to make predictions about replications of new experiments. Additionally, visualizing uncertainty as a set of discrete outcomes, as opposed to a continuous probability distribution, can improve recall of a sampling distribution from a single experiment. Our work has implications for various applications where it is important to elicit peoples' estimates of probability distributions and to communicate uncertainty effectively.
C1 [Hulman, Jessica; Kim, Yea-Seul] Univ Washington, Seattle, WA 98195 USA.
   [Kay, Matthew] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Shrestha, Samana] Vassar Coll, Poughkeepsie, NY 12601 USA.
C3 University of Washington; University of Washington Seattle; University
   of Michigan System; University of Michigan; Vassar College
RP Hulman, J (corresponding author), Univ Washington, Seattle, WA 98195 USA.
EM jhullman@uw.edu; mjskay@umich.edu; yeaseull@uw.edu;
   sashrestha@vassar.edu
RI Hullman, Jessica/P-7130-2018; Kay, Matthew/AAZ-8197-2020
OI Kay, Matthew/0000-0001-9446-0419; Kim, Yea-Seul/0000-0003-1854-1537
NR 67
TC 32
Z9 46
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 446
EP 456
DI 10.1109/TVCG.2017.2743898
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400045
PM 28866501
DA 2025-03-07
ER

PT J
AU Kahng, M
   Andrews, PY
   Kalro, A
   Chau, DH
AF Kahng, Minsuk
   Andrews, Pierre Y.
   Kalro, Aditya
   Chau, Duen Horng (Polo)
TI AcTiVis: Visual Exploration of Industry-Scale Deep Neural Network Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Visual analytics; deep learning; machine learning; information
   visualization
AB While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ACTIVIS, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance- and subset-level. ACTIVIS has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ACTIVIS may work with different models.
C1 [Kahng, Minsuk; Chau, Duen Horng (Polo)] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Kahng, Minsuk; Andrews, Pierre Y.; Kalro, Aditya] Facebook, Menlo Pk, CA 94025 USA.
C3 University System of Georgia; Georgia Institute of Technology; Facebook
   Inc
RP Kahng, M (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.; Kahng, M (corresponding author), Facebook, Menlo Pk, CA 94025 USA.
EM kahng@gatech.edu; mortimer@fb.com; aclityakalro@fb.com; polo@gatech.edu
OI Chau, Polo/0000-0001-9824-3323
FU NSF Graduate Research Fellowship Program [DGE-1650044]
FX We thank Facebook Applied Machine Learning Group, especially Yangqing
   Jia, Andrew Tulloch, Liang Xiong, and Zhao Tan for their advice and
   feedback. This work is partly supported by the NSF Graduate Research
   Fellowship Program under Grant No. DGE-1650044.
NR 37
TC 208
Z9 248
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 88
EP 97
DI 10.1109/TVCG.2017.2744718
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400011
PM 28866557
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Morgand, A
   Tamaazousti, M
   Bartoli, A
AF Morgand, Alexandre
   Tamaazousti, Mohamed
   Bartoli, Adrien
TI A Multiple-View Geometric Model of Specularities on Non-Planar Shapes
   with Application to Dynamic Retexturing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY SEP 09-13, 2017
CL Nantes, FRANCE
SP IEEE
DE Specularity Prediction; Augmented Reality; Retexturing; Quadric;
   Multiple Light Sources
AB Predicting specularities in images, given the camera pose and scene geometry from SLAM, forms a challenging and open problem. It is nonetheless essential in several applications such as retexturing. A recent geometric model called JOLIMAS partially answers this problem, under the assumptions that the specularities are elliptical and the scene is planar. JOLIMAS models a moving specularity as the image of a fixed 3D quadric. We propose dual JOLIMAS, a new model which raises the planarity assumption. It uses the fact that specularities remain elliptical on convex surfaces and that every surface can be divided in convex parts. The geometry of dual JOLIMAS then uses a 3D quadric per convex surface part and light source, and predicts the specularities by a means of virtual cameras, allowing it to cope with surface's unflatness. We assessed the efficiency and precision of dual JOLIMAS on multiple synthetic and real videos with various objects and lighting conditions. We give results of a retexturing application. Further results are presented as supplementary video material.
C1 [Morgand, Alexandre; Tamaazousti, Mohamed] CEA, LIST, Gif Sur Yvette, France.
   [Morgand, Alexandre] CNRS UCA CHU, IP UMR 6602, Clermont Ferrand, France.
   [Bartoli, Adrien] IP, Clermont Ferrand, France.
C3 Universite Paris Saclay; CEA; Centre National de la Recherche
   Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences
   (INSIS)
RP Morgand, A (corresponding author), CEA, LIST, Gif Sur Yvette, France.; Morgand, A (corresponding author), CNRS UCA CHU, IP UMR 6602, Clermont Ferrand, France.
EM alexandre.morgand@cea.fr; mohamed.tamaazousti@cea.fr;
   adrien.bartoli@gmail.com
OI Morgand, Alexandre/0000-0003-1986-6894
FU EU's FP7 [307483 FLEXABLE]
FX This research has received funding from the EU's FP7 through the
   research grant 307483 FLEXABLE.
NR 43
TC 2
Z9 2
U1 1
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2017
VL 23
IS 11
BP 2485
EP 2493
DI 10.1109/TVCG.2017.2734538
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FI7XT
UT WOS:000412214000015
PM 28809694
DA 2025-03-07
ER

PT J
AU von Landesberger, T
   Fellner, DW
   Ruddle, RA
AF von Landesberger, Tatiana
   Fellner, Dieter W.
   Ruddle, Roy A.
TI Visualization System Requirements for Data Processing Pipeline Design
   and Optimization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization systems; requirement analysis; data processing pipelines
ID INTERACTIVE VISUAL ANALYSIS; MEDICAL IMAGE SEGMENTATION; PARAMETER
   SPACE; BLACK-BOX; EXPLORATION; ANALYTICS; MODEL; UNCERTAINTY;
   SENSITIVITY; VALIDATION
AB The rising quantity and complexity of data creates a need to design and optimize data processing pipelines-the set of data processing steps, parameters and algorithms that perform operations on the data. Visualization can support this process but, although there are many examples of systems for visual parameter analysis, there remains a need to systematically assess users' requirements and match those requirements to exemplar visualization methods. This article presents a new characterization of the requirements for pipeline design and optimization. This characterization is based on both a review of the literature and first-hand assessment of eight application case studies. We also match these requirements with exemplar functionality provided by existing visualization tools. Thus, we provide end-users and visualization developers with a way of identifying functionality that addresses data processing problems in an application. We also identify seven future challenges for visualization research that are not met by the capabilities of today's systems.
C1 [von Landesberger, Tatiana] Tech Univ Darmstadt, Interact Graph Syst Grp, D-64289 Darmstadt, Germany.
   [Fellner, Dieter W.] Tech Univ Darmstadt, CGV Inst, D-64289 Darmstadt, Germany.
   [Ruddle, Roy A.] Univ Leeds, Leeds LS2 9JT, W Yorkshire, England.
C3 Technical University of Darmstadt; Technical University of Darmstadt;
   University of Leeds
RP von Landesberger, T (corresponding author), Tech Univ Darmstadt, Interact Graph Syst Grp, D-64289 Darmstadt, Germany.
EM tatiana.landesberger@gris.tu-darmstadt.de;
   dieter.fellner@gris.tu-darmstadt.de; R.A.Ruddle@leeds.ac.uk
OI Fellner, Dieter W./0000-0001-7756-0901
FU DFG
FX The work has been partially supported by DFG. The authors are grateful
   to Hans-Jorg Schulz, Johannes A. Pretorius and Arjan Kuijper for their
   helpful suggestions to the paper. We also thank the interviewed experts
   for their insights. All images are reused with authors' permission.
NR 81
TC 8
Z9 8
U1 0
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2017
VL 23
IS 8
BP 2028
EP 2041
DI 10.1109/TVCG.2016.2603178
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EZ8LR
UT WOS:000404977900012
PM 28113376
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Yeh, CK
   Huang, SY
   Jayaraman, PK
   Fu, CW
   Lee, TY
AF Yeh, Chih-Kuo
   Huang, Shi-Yang
   Jayaraman, Pradeep Kumar
   Fu, Chi-Wing
   Lee, Tong-Yee
TI Interactive High-Relief Reconstruction for Organic and Double-Sided
   Objects from a Photo
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Reconstruction; high-relief; lenticular posters; single image; folded;
   double-sided; object modeling; depth cues; completion; inflation
ID IRREGULAR MESHES; FORM
AB We introduce an interactive user-driven method to reconstruct high-relief 3D geometry from a single photo. Particularly, we consider two novel but challenging reconstruction issues: i) common non-rigid objects whose shapes are organic rather than polyhedral/symmetric, and ii) double-sided structures, where front and back sides of some curvy object parts are revealed simultaneously on image. To address these issues, we develop a three-stage computational pipeline. First, we construct a 2.5D model from the input image by user-driven segmentation, automatic layering, and region completion, handling three common types of occlusion. Second, users can interactively mark-up slope and curvature cues on the image to guide our constrained optimization model to inflate and lift up the image layers. We provide real-time preview of the inflated geometry to allow interactive editing. Third, we stitch and optimize the inflated layers to produce a high-relief 3D model. Compared to previous work, we can generate high-relief geometry with large viewing angles, handle complex organic objects with multiple occluded regions and varying shape profiles, and reconstruct objects with double-sided structures. Lastly, we demonstrate the applicability of our method on a wide variety of input images with human, animals, flowers, etc.
C1 [Yeh, Chih-Kuo; Huang, Shi-Yang; Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
   [Jayaraman, Pradeep Kumar] Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.
   [Fu, Chi-Wing] Chinese Univ Hong Kong, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
C3 National Cheng Kung University; Nanyang Technological University;
   Chinese University of Hong Kong
RP Yeh, CK (corresponding author), Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
EM simpson.ycg@gmail.com; t8590338@gmail.com; pradeep.pyro@gmail.com;
   philip.chiwing.fu@gmail.com; tonylee@mail.ncku.edu.tw
RI Jayaraman, Pradeep/AAV-8965-2021; Yeh, Chih-Kuo/JBS-2228-2023; Fu,
   Chi-Wing/X-4703-2019
OI Fu, Chi Wing/0000-0002-5238-593X
FU Ministry of Science and Technology, Taiwan [MOST-104-2221-E-006-044-MY3,
   MOST-103-2221-E-006-106-MY3]; Chinese University of Hong Kong [4055061]
FX We would like to thank the reviewers for the many constructive comments
   that help improve the paper. We are grateful to Daniel Sykora and Olga
   Sorkine-Hornung for their help in generating the comparison results, and
   the authors of [4] for releasing their dataset and results online. This
   work was supported in part by the Ministry of Science and Technology
   (contracts MOST-104-2221-E-006-044-MY3 and MOST-103-2221-E-006-106-MY3),
   Taiwan, and the Chinese University of Hong Kong strategic recruitment
   fund and direct grant (4055061).
NR 36
TC 15
Z9 18
U1 1
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2017
VL 23
IS 7
BP 1796
EP 1808
DI 10.1109/TVCG.2016.2574705
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EW7OV
UT WOS:000402705000006
PM 27254869
DA 2025-03-07
ER

PT J
AU Ye, T
   Qi, SY
   Kubricht, J
   Zhu, YX
   Lu, HJ
   Zhu, SC
AF Ye, Tian
   Qi, Siyuan
   Kubricht, James
   Zhu, Yixin
   Lu, Hongjing
   Zhu, Song-Chun
TI The Martian: Examining Human Physical Judgments Across Virtual Gravity
   Fields
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 19th IEEE Virtual Reality Conference (VR)
CY MAR 18-22, 2017
CL Los Angeles, CA
SP IEEE, IEEE Comp Soc, IEEE Comp Soc, Visualizat & Graph Tech Comm
DE Virtual reality; intuitive physics; mental simulation
ID INTUITIVE PHYSICS; VISUAL-PERCEPTION; MENTAL ANIMATION; MOTION; ABSENCE
AB This paper examines how humans adapt to novel physical situations with unknown gravitational acceleration in immersive virtual environments. We designed four virtual reality experiments with different tasks for participants to complete: strike a ball to hit a target. trigger a ball to hit a target. predict the landing location of a projectile. and estimate the flight duration of a projectile. The first two experiments compared human behavior in the virtual environment with real-world performance reported in the literature. The last two experiments aimed to test the human ability to adapt to novel gravity fields by measuring their performance in trajectory prediction and time estimation tasks. The experiment results show that: 1) based on brief observation of a projectile's initial trajectory. humans are accurate at predicting the landing location even under novel gravity fields. and 2) humans' time estimation in a familiar earth environment fluctuates around the ground truth flight duration, although the time estimation in unknown gravity fields indicates a bias toward earth's gravity.
C1 [Ye, Tian; Qi, Siyuan; Zhu, Yixin; Zhu, Song-Chun] Univ Calif Los Angeles, Ctr Vis Cognit Learning & Auton VCLA, Los Angeles, CA 90024 USA.
   [Kubricht, James; Lu, Hongjing] Univ Calif Los Angeles, Computat Vis & Learning Lab CVL, Los Angeles, CA 90024 USA.
C3 University of California System; University of California Los Angeles;
   University of California System; University of California Los Angeles
RP Ye, T (corresponding author), Univ Calif Los Angeles, Ctr Vis Cognit Learning & Auton VCLA, Los Angeles, CA 90024 USA.
EM etalice@g.ucla.edu; syqi@cs.ucla.edu; kubricht@ucla.edu;
   yixin.zhu@ucla.edu; hongjing@ucla.edu; sczhu@stat.ucla.edu
RI Qi, Siyuan/MFI-2587-2025
OI Zhu, Yixin/0000-0001-7024-1545
FU DARPA SIMPLEX grant [N66001-15-C-4035]; ONR MURI grant
   [N00014-16-1-2007]; NSF [BCS-1353391]; NSF Graduate Research Fellowship
FX We thank Dr. Chenfanfu Jiang of the UCLA Computer Science and Math
   Department for useful discussions. The work reported herein was
   supported by DARPA SIMPLEX grant N66001-15-C-4035, ONR MURI grant
   N00014-16-1-2007, NSF BCS-1353391 and an NSF Graduate Research
   Fellowship.
NR 43
TC 10
Z9 13
U1 1
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2017
VL 23
IS 4
BP 1369
EP 1378
DI 10.1109/TVCG.2017.2657235
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EO0QU
UT WOS:000396403800013
PM 28129174
DA 2025-03-07
ER

PT J
AU Kolesár, I
   Bruckner, S
   Viola, I
   Hauser, H
AF Kolesar, Ivan
   Bruckner, Stefan
   Viola, Ivan
   Hauser, Helwig
TI A Fractional Cartesian Composition Model for Semi-spatial Comparative
   Visualization Design
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Visualization Models; Integrating Spatial and Non-Spatial Data
   Visualization; Design Methodologies
ID INTERACTIVE VISUAL ANALYSIS; SPACE; REFORMATION; INFORMATION; COHORT;
   FLOW
AB The study of spatial data ensembles leads to substantial visualization challenges in a variety of applications. In this paper, we present a model for comparative visualization that supports the design of according ensemble visualization solutions by partial automation. We focus on applications, where the user is interested in preserving selected spatial data characteristics of the data as much as possible-even when many ensemble members should be jointly studied using comparative visualization. In our model, we separate the design challenge into a minimal set of user-specified parameters and an optimization component for the automatic configuration of the remaining design variables. We provide an illustrated formal description of our model and exemplify our approach in the context of several application examples from different domains in order to demonstrate its generality within the class of comparative visualization problems for spatial data ensembles.
C1 [Kolesar, Ivan; Bruckner, Stefan; Hauser, Helwig] Univ Bergen, Dept Informat, N-5008 Bergen, Norway.
   [Viola, Ivan] TU Wien, Vienna, Austria.
C3 University of Bergen; Technische Universitat Wien
RP Viola, I (corresponding author), TU Wien, Vienna, Austria.
EM Viola@cg.tuwien.ac.at
RI Bruckner, Stefan/AAA-2625-2019; Viola, Ivan/O-8944-2014
OI Viola, Ivan/0000-0003-4248-6574; Bruckner, Stefan/0000-0002-0885-8402
FU Norwegian Research Council [218021]
FX This work has been carried out within the PhysioIllustration project
   (#218021), which is funded by the Norwegian Research Council. We thank
   our application domain expert in the field of molecular biology, Mathias
   Ziegler, for his useful ideas and feedback.
NR 41
TC 7
Z9 7
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 851
EP 860
DI 10.1109/TVCG.2016.2598870
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600088
PM 27875199
DA 2025-03-07
ER

PT J
AU Netzel, R
   Hlawatsch, M
   Burch, M
   Balakrishnan, S
   Schmauder, H
   Weiskopf, D
AF Netzel, Rudolf
   Hlawatsch, Marcel
   Burch, Michael
   Balakrishnan, Sanjeev
   Schmauder, Hansjrg
   Weiskopf, Daniel
TI An Evaluation of Visual Search Support in Maps
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Visual search; laboratory study; eye tracking; map visualization
AB Visual search can be time-consuming, especially if the scene contains a large number of possibly relevant objects. An instance of this problem is present when using geographic or schematic maps with many different elements representing cities, streets, sights, and the like. Unless the map is well-known to the reader, the full map or at least large parts of it must be scanned to find the elements of interest. In this paper, we present a controlled eye-tracking study (30 participants) to compare four variants of map annotation with labels: within-image annotations, grid reference annotation, directional annotation, and miniature annotation. Within-image annotation places labels directly within the map without any further search support. Grid reference annotation corresponds to the traditional approach known from atlases. Directional annotation utilizes a label in combination with an arrow pointing in the direction of the label within the map. Miniature annotation shows a miniature grid to guide the reader to the area of the map in which the label is located. The study results show that within-image annotation is outperformed by all other annotation approaches. Best task completion times are achieved with miniature annotation. The analysis of eye-movement data reveals that participants applied significantly different visual task solution strategies for the different visual annotations.
C1 [Netzel, Rudolf; Hlawatsch, Marcel; Burch, Michael; Balakrishnan, Sanjeev; Schmauder, Hansjrg; Weiskopf, Daniel] Univ Stuttgart, VISUS, Stuttgart, Germany.
C3 University of Stuttgart
RP Netzel, R (corresponding author), Univ Stuttgart, VISUS, Stuttgart, Germany.
EM rudolf.netzel@visus.uni-stuttgart.de;
   marcel.hlawatsch@visus.uni-stuttgart.de;
   michael.burch@visus.uni-stuttgart.de;
   daniel.weiskopf@visus.uni-stuttgart.de
RI Weiskopf, Daniel/KWT-7459-2024
OI Weiskopf, Daniel/0000-0003-1174-1026; Hlawatsch,
   Marcel/0000-0003-2839-7030
FU German Research Foundation (DFG) [SFB/Transregio 161]
FX We would like to thank the German Research Foundation (DFG) for
   financial support within project B01 of SFB/Transregio 161. The used
   maps are courtesy of OpenStreetMap Foundation and are licensed under the
   Creative Commons Attribution-ShareAlike 2.0 license (please see
   http://www.openstreetmap.org/copyright).
NR 37
TC 15
Z9 18
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 421
EP 430
DI 10.1109/TVCG.2016.2598898
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600045
PM 27875158
DA 2025-03-07
ER

PT J
AU Liu, SX
   Yin, JL
   Wang, XT
   Cui, WW
   Cao, KL
   Pei, J
AF Liu, Shixia
   Yin, Jialun
   Wang, Xiting
   Cui, Weiwei
   Cao, Kelei
   Pei, Jian
TI Online Visual Analytics of Text Streams
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Streaming text data; evolutionary tree clustering; streaming tree cut;
   streaming topic visualization
ID COLLECTIONS; TOPICS
AB We present an online visual analytics approach to helping users explore and understand hierarchical topic evolution in high-volume text streams. The key idea behind this approach is to identify representative topics in incoming documents and align them with the existing representative topics that they immediately follow (in time). To this end, we learn a set of streaming tree cuts from topic trees based on user-selected focus nodes. A dynamic Bayesian network model has been developed to derive the tree cuts in the incoming topic trees to balance the fitness of each tree cut and the smoothness between adjacent tree cuts. By connecting the corresponding topics at different times, we are able to provide an overview of the evolving hierarchical topics. A sedimentation-based visualization has been designed to enable the interactive analysis of streaming text data from global patterns to local details. We evaluated our method on real-world datasets and the results are generally favorable.
C1 [Liu, Shixia] Tsinghua Univ, Sch Software, State Key lab Intell Tech & Syst, TNList Lab, Beijing, Peoples R China.
   [Yin, Jialun; Wang, Xiting; Cao, Kelei] Tsinghua Univ, Beijing, Peoples R China.
   [Cui, Weiwei] Microsoft Res, Redmond, WA USA.
   [Pei, Jian] Simon Fraser Univ, Burnaby, BC, Canada.
C3 Tsinghua University; Tsinghua University; Microsoft; Simon Fraser
   University
RP Liu, SX (corresponding author), Tsinghua Univ, Sch Software, State Key lab Intell Tech & Syst, TNList Lab, Beijing, Peoples R China.
EM shixia@tsinghua.edu.cn; yinjl14@mails.tsinghua.edu.cn;
   wang-xt11@mails.tsinghua.edu.cn; weiwei.cui@microsoft.com;
   ckl13@mails.tsinghua.edu.cn; jpei@cs.sfu.ca
RI wang, xiting/HGF-3827-2022; Liu, Shi-Xia/C-5574-2016
OI Pei, Jian/0000-0002-2200-8711; Liu, Shi-Xia/0000-0001-6104-4320
FU National Key Technologies RAMP;D Program of China [2015BAF23B03];
   National Natural Science Foundation of China [61373070, 61272225,
   61572274]; Microsoft Research Fund [FY15-RES-OPP-112]
FX This research was supported by National Key Technologies R&D Program of
   China (No. 2015BAF23B03), the National Natural Science Foundation of
   China (Nos. 61373070, 61272225, 61572274), and a Microsoft Research Fund
   (No. FY15-RES-OPP-112).
NR 43
TC 60
Z9 70
U1 0
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2016
VL 22
IS 11
BP 2451
EP 2466
DI 10.1109/TVCG.2015.2509990
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DX7OV
UT WOS:000384578800010
PM 26701787
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Chen, MC
   Shadden, SC
   Hart, JC
AF Chen, Mingcheng
   Shadden, Shawn C.
   Hart, John C.
TI Fast Coherent Particle Advection through Time-Varying Unstructured Flow
   Datasets
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Unsteady flow; unstructured mesh; GPU; parallel algorithms
ID VISUALIZATION
AB Tracing the paths of collections of particles through a flow field is a key step for many flow visualization and analysis methods. When a flow field is interpolated from the nodes of an unstructured mesh, the process of advecting a particle must first find which cell in the unstructured mesh contains the particle. Since the paths of nearby particles often diverge, the parallelization of particle advection quickly leads to incoherent memory accesses of the unstructured mesh. We have developed a new block advection GPU approach that reorganizes particles into spatially coherent bundles as they follow their advection paths, which greatly improves memory coherence and thus shared-memory GPU performance. This approach works best for flows that meet the CFL criterion on unstructured meshes of uniformly sized elements, small enough to fit at least two timesteps in GPU memory.
C1 [Chen, Mingcheng; Hart, John C.] Univ Illinois, Dept Comp Sci, Champaign, IL 61820 USA.
   [Shadden, Shawn C.] Univ Calif Berkeley, Dept Mech Engn, Berkeley, CA 94720 USA.
C3 University of Illinois System; University of Illinois Urbana-Champaign;
   University of California System; University of California Berkeley
RP Chen, MC (corresponding author), Univ Illinois, Dept Comp Sci, Champaign, IL 61820 USA.
EM mchen50@illinois.edu; shadden@berkeley.edu; jch@illinois.edu
FU US National Science Foundation (NSF) [OCI-1047963, OCI-1047764]; NVIDIA
FX This work was supported in part by US National Science Foundation (NSF)
   Grants OCI-1047963 and OCI-1047764, and NVIDIA. The authors thank
   Siavash Ameli, Yogin Desai, and Anil Hirani for their help with this
   project.
NR 42
TC 2
Z9 2
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2016
VL 22
IS 8
BP 1959
EP 1972
DI 10.1109/TVCG.2015.2476795
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DR6UC
UT WOS:000380035800003
PM 26353375
OA Bronze
DA 2025-03-07
ER

PT J
AU Liao, J
   Nehab, D
   Hoppe, H
   Sander, PV
AF Liao, Jing
   Nehab, Diego
   Hoppe, Hugues
   Sander, Pedro V.
TI New Controls for Combining Images in Correspondence
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image interpolation; morphing; edge-aware decomposition; design
   galleries
ID PHOTOGRAPHY; FLASH
AB When interpolating images, for instance in the context of morphing, there are myriad approaches for defining correspondence maps that align structurally similar elements. However, the actual interpolation usually involves simple functions for both geometric paths and color blending. In this paper we explore new types of controls for combining two images related by a correspondence map. Our insight is to apply recent edge-aware decomposition techniques, not just to the image content but to the map itself. Our framework establishes an intuitive low-dimensional parameter space for merging the shape and color from the two source images at both low and high frequencies. A gallery-based user interface enables interactive traversal of this rich space, to either define a morph path or synthesize new hybrid images. Extrapolation of the shape parameters achieves compelling effects. Finally we demonstrate an extension of the framework to videos.
C1 [Liao, Jing; Sander, Pedro V.] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
   [Nehab, Diego] IMPA, Visgraf, Rio De Janeiro, Brazil.
   [Hoppe, Hugues] Microsoft Res, Comp Graph Grp, Redmond, WA 98052 USA.
C3 Hong Kong University of Science & Technology; Instituto Nacional de
   Matematica Pura e Aplicada (IMPA); Microsoft
RP Liao, J (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn, Hong Kong, Hong Kong, Peoples R China.
EM liaojing8871@gmail.com; diego@impa.br; hhoppe@microsoft.com;
   psander@cse.ust.hk
RI Nehab, Diego/AAV-2002-2020
OI LIAO, Jing/0000-0001-7014-5377; Nehab, Diego/0000-0001-9536-0794
FU Hong Kong GRF [619509, 618513]
FX This work was partly supported by Hong Kong GRF grants #619509 and
   #618513.
NR 37
TC 0
Z9 0
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2016
VL 22
IS 7
BP 1875
EP 1885
DI 10.1109/TVCG.2015.2476794
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DO0NI
UT WOS:000377475200009
PM 26353374
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Schatzschneider, C
   Bruder, G
   Steinicke, F
AF Schatzschneider, Christian
   Bruder, Gerd
   Steinicke, Frank
TI Who turned the clock? Effects of Manipulated Zeitgebers, Cognitive Load
   and Immersion on Time Estimation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE Virtual Reality Conference (IEEE VR)
CY MAR 19-23, 2016
CL Greenville, SC
SP IEEE, IEEE Comp Soc, IEEE Comp Soc Visualizat & Graph Tech Comm, Clemson Univ
DE Time perception; cognitive load; virtual environments
ID CIRCADIAN-RHYTHMS; PERCEPTION; LIGHT; ATTENTION; MODELS; GAIT
AB Current virtual reality (VR) technologies have enormous potential to allow humans to experience computer-generated immersive virtual environments (IVEs). Many of these IVEs support near-natural audiovisual stimuli similar to the stimuli generated in our physical world. However, decades of VR research have been devoted to exploring and understand differences between perception and action in such IVEs compared to real-world perception and action. Although, significant differences have been revealed for spatiotemporal perception between IVEs and the physical world such as distance underestimation, there is still a scarcity of knowledge about the reasons for such perceptual discrepancies, in particular regarding the perception of temporal durations in IVEs. In this article, we explore the effects of manipulated zeitgebers, cognitive load and immersion on time estimation as yet unexplored factors of spatiotemporal perception in IVEs. We present an experiment in which we analyze human sensitivity to temporal durations while experiencing an immersive head-mounted display (HMD) environment. We found that manipulations of external zeitgebers caused by a natural or unnatural movement of the virtual sun had a significant effect on time judgments. Moreover, using the dual task paradigm the results show that increased spatial and verbal cognitive load resulted in a significant shortening of judged time as well as an interaction with the external zeitgebers. Finally, we discuss the implications for the design of near-natural computer generated virtual worlds.
C1 [Schatzschneider, Christian; Bruder, Gerd; Steinicke, Frank] Univ Hamburg, HCI Res Grp, Hamburg, Germany.
C3 University of Hamburg
RP Schatzschneider, C; Bruder, G; Steinicke, F (corresponding author), Univ Hamburg, HCI Res Grp, Hamburg, Germany.
EM christian.schatzschneider@studium.uni-hamburg.de;
   gerd.bruder@uni-hamburg.de; frank.steinicke@uni-hamburg.de
RI Steinicke, Frank/AAC-2976-2020
OI Steinicke, Frank/0000-0001-9879-7414
FU German Research Foundation
FX This work was partly supported by the German Research Foundation. We
   thank the participants of our experiment as well as the reviewers for
   their helpful comments.
NR 45
TC 45
Z9 48
U1 3
U2 42
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2016
VL 22
IS 4
BP 1387
EP 1395
DI 10.1109/TVCG.2016.2518137
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DH5RN
UT WOS:000372849600008
PM 26780806
DA 2025-03-07
ER

PT J
AU Schissler, C
   Nicholls, A
   Mehra, R
AF Schissler, Carl
   Nicholls, Aaron
   Mehra, Ravish
TI Efficient HRTF-based Spatial Audio for Area and Volumetric Sources
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE Virtual Reality Conference (IEEE VR)
CY MAR 19-23, 2016
CL Greenville, SC
SP IEEE, IEEE Comp Soc, IEEE Comp Soc Visualizat & Graph Tech Comm, Clemson Univ
DE Spatial audio; HRTF; area sources; volumetric sources; spherical
   harmonics
ID SOUND; REAL
AB We present a novel spatial audio rendering technique to handle sound sources that can be represented by either an area or a volume in VR environments. As opposed to point-sampled sound sources, our approach projects the area-volumetric source to the spherical domain centered at the listener and represents this projection area compactly using the spherical harmonic (SH) basis functions. By representing the head-related transfer function (HRTF) in the same basis, we demonstrate that spatial audio which corresponds to an area-volumetric source can be efficiently computed as a dot product of the SH coefficients of the projection area and the HRTF. This results in an efficient technique whose computational complexity and memory requirements are independent of the complexity of the sound source. Our approach can support dynamic area-volumetric sound sources at interactive rates. We evaluate the performance of our technique in large complex VR environments and demonstrate significant improvement over the naive point-sampling technique. We also present results of a user evaluation, conducted to quantify the subjective preference of the user for our approach over the point-sampling approach in VR environments.
C1 [Schissler, Carl; Nicholls, Aaron; Mehra, Ravish] Oculus & Facebook, Irvine, CA USA.
   [Schissler, Carl] Univ N Carolina, Chapel Hill, NC USA.
C3 Facebook Inc; University of North Carolina; University of North Carolina
   Chapel Hill
RP Schissler, C; Nicholls, A; Mehra, R (corresponding author), Oculus & Facebook, Irvine, CA USA.; Schissler, C (corresponding author), Univ N Carolina, Chapel Hill, NC USA.
EM carl.schissler@gmail.com; aaron.nicholls@oculus.com;
   ravish.mehra@oculus.com
FU Oculus Facebook
FX The authors wish to thank the anonymous reviewers for their constructive
   feedback. We would like to thank Amy Hong for her help with the demos,
   Tanya Micheletti, Erika Evans, and Lauri Kanerva for their help with the
   IRB paperwork, Tina Reinl and Lindsey Neby for their help with the user
   study, and Marina Zannoli for giving us feedback on the data analysis.
   We would also like to thank the anonymous participants of the user
   study. This research was fully supported by Oculus & Facebook.
NR 38
TC 44
Z9 64
U1 0
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2016
VL 22
IS 4
BP 1356
EP 1366
DI 10.1109/TVCG.2016.2518134
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DH5RN
UT WOS:000372849600005
PM 26780803
DA 2025-03-07
ER

PT J
AU Liu, XT
   Shen, HW
AF Liu, Xiaotong
   Shen, Han-Wei
TI Association Analysis for Visual Exploration of Multivariate Scientific
   Data Sets
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Multivariate data; association analysis; visual exploration; multiple
   views
AB The heterogeneity and complexity of multivariate characteristics poses a unique challenge to visual exploration of multivariate scientific data sets. as it requires investigating the usually hidden associations between different variables and specific scalar values to understand the data's multi-faceted properties. In this paper, we present a novel association analysis method that guides visual exploration of scalar-level associations in the multivariate context. We model the directional interactions between scalars of different variables as information flows based on association rules. We introduce the concepts of informativeness and uniqueness to describe how information flows between scalars of different variables and how they are associated with each other in the multivariate domain. Based on scalar-level associations represented by a probabilistic association graph, we propose the Multi-Scalar Informativeness-Uniqueness (MSIU) algorithm to evaluate the informativeness and uniqueness of scalars. We present an exploration framework with multiple interactive views to explore the scalars of interest with confident associations in the multivariate spatial domain, and provide guidelines for visual exploration using our framework. We demonstrate the effectiveness and usefulness of our approach through case studies using three representative multivariate scientific data sets.
C1 [Liu, Xiaotong; Shen, Han-Wei] Ohio State Univ, Columbus, OH 43210 USA.
C3 University System of Ohio; Ohio State University
RP Liu, XT (corresponding author), Ohio State Univ, Columbus, OH 43210 USA.
EM liu.1952@osu.edu; shen.94@osu.edu
RI Shen, Han-wei/A-4710-2012; Liu, Xiaotong/AAB-9458-2019
NR 34
TC 33
Z9 39
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 955
EP 964
DI 10.1109/TVCG.2015.2467431
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400101
PM 26529739
DA 2025-03-07
ER

PT J
AU Wongsuphasawat, K
   Moritz, D
   Anand, A
   Mackinlay, J
   Howe, B
   Heer, J
AF Wongsuphasawat, Kanit
   Moritz, Dominik
   Anand, Anushka
   Mackinlay, Jock
   Howe, Bill
   Heer, Jeffrey
TI Voyager: Exploratory Analysis via Faceted Browsing of Visualization
   Recommendations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE User interfaces; information visualization; exploratory analysis;
   visualization recommendation; mixed-initiative systems
ID DESIGN; QUERY
AB General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.
C1 [Wongsuphasawat, Kanit; Moritz, Dominik; Howe, Bill; Heer, Jeffrey] Univ Washington, Seattle, WA 98195 USA.
C3 University of Washington; University of Washington Seattle
RP Wongsuphasawat, K (corresponding author), Univ Washington, Seattle, WA 98195 USA.
EM kanitw@cs.washington.edu; domoritz@cs.washington.edu;
   aanand@tableau.com; jmackinlay@tableau.com; billhowe@cs.washington.edu;
   jheer@cs.washington.edu
OI Moritz, Dominik/0000-0002-3110-1053; Howe, Bill/0000-0001-8588-8472
FU Intel Big Data ISTC; DARPA XDATA; Gordon & Betty Moore Foundation;
   University of Washington eScience Institute; Direct For Computer & Info
   Scie & Enginr; Div Of Information & Intelligent Systems [1064505,
   1247469] Funding Source: National Science Foundation; Div Of Information
   & Intelligent Systems; Direct For Computer & Info Scie & Enginr
   [1064685] Funding Source: National Science Foundation
FX We thank the anonymous reviewers, Magda Balazinska, Daniel Halperin,
   Hanchuan Li, Matthew Kay, and members of the Interactive Data Lab and
   Tableau Research for their comments in improving this paper. This work
   was supported in part by the Intel Big Data ISTC, DARPA XDATA, the
   Gordon & Betty Moore Foundation, and the University of Washington
   eScience Institute. Part of this work was developed during the first
   author's internship at Tableau Research in 2013. We also thank the Noun
   Project and Dmitry Baranovskiy for the "person" icon used in Figure 8.
NR 47
TC 289
Z9 329
U1 0
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 649
EP 658
DI 10.1109/TVCG.2015.2467191
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400070
PM 26390469
DA 2025-03-07
ER

PT J
AU Li, B
   Wei, XL
   Liu, B
   He, ZF
   Cao, JJ
   Lai, YK
AF Li, Bo
   Wei, Xiaolin
   Liu, Bin
   He, Zhifen
   Cao, Junjie
   Lai, Yu-Kun
TI Pose-Aware 3D Talking Face Synthesis Using Geometry-Guided
   Audio-Vertices Attention
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Faces; Facial animation; Feature extraction;
   Head; Semantics; Mouth; Audio-driven; 3D facial animation; pose-aware;
   hierarchical features; audio-vertices attention
AB Most of the existing 3D talking face synthesis methods suffer from the lack of detailed facial expressions and realistic head poses, resulting in unsatisfactory experiences for users. In this article, we propose a novel pose-aware 3D talking face synthesis method with a novel geometry-guided audio-vertices attention. To capture more detailed expression, such as the subtle nuances of mouth shape and eye movement, we propose to build hierarchical audio features including a global attribute feature and a series of vertex-wise local latent movement features. Then, in order to fully exploit the topology of facial models, we further propose a novel geometry-guided audio-vertices attention module to predict the displacement of each vertex by using vertex connectivity relations to take full advantage of the corresponding hierarchical audio features. Finally, to accomplish pose-aware animation, we expand the existing database with an additional pose attribute, and a novel pose estimation module is proposed by paying attention to the whole head model. Numerical experiments demonstrate the effectiveness of the proposed method on realistic expression and head movements against state-of-the-art methods.
C1 [Li, Bo; Wei, Xiaolin; Liu, Bin; He, Zhifen] Nanchang Hangkong Univ, Sch Math & Informat Sci, Nanchang 330103, Peoples R China.
   [Cao, Junjie] Dalian Univ Technol, Sch Math, Dalian 116024, Peoples R China.
   [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF10 3AT, Wales.
C3 Nanchang Hangkong University; Dalian University of Technology; Cardiff
   University
RP Liu, B (corresponding author), Nanchang Hangkong Univ, Sch Math & Informat Sci, Nanchang 330103, Peoples R China.
EM nyliubin@outlook.com
RI He, Zhi-Fen/T-2490-2019; Lai, Yu-Kun/D-2343-2010
OI Lai, Yukun/0000-0002-2094-5680
FU Natural Science Foundation of China (NSFC) [62172198, 61762064,
   62362051]; Key Project of Jiangxi Natural Science Foundation
   [20224ACB202008]; Key R&D Plan of Jiangxi Province [20232BBE50022];
   Project of Academic and Technical Leaders in Major Disciplines in
   Jiangxi Province [20232BCJ22001]; Jiangxi Provincial Natural Science
   Foundation [20232BAB212013]; Opening Project of Nanchang Innovation
   Institute, Peking University
FX The work was funded in part by the Natural Science Foundation of China
   (NSFC) under Grant 62172198, under Grant 61762064, and under Grant
   62362051, in part by the Key Project of Jiangxi Natural Science
   Foundation under Grant 20224ACB202008, in part by the Key R&D Plan of
   Jiangxi Province under Grant 20232BBE50022, in part by the Project of
   Academic and Technical Leaders in Major Disciplines in Jiangxi Province
   under Grant 20232BCJ22001, in part by Jiangxi Provincial Natural Science
   Foundation under Grant 20232BAB212013, and in part by the Opening
   Project of Nanchang Innovation Institute, Peking University. Recommended
   for acceptance by F. Xu.
NR 37
TC 0
Z9 0
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2025
VL 31
IS 3
BP 1758
EP 1771
DI 10.1109/TVCG.2024.3371064
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA U7J0S
UT WOS:001413499200006
PM 38416616
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Palamarchuk, D
   Williams, L
   Mayer, B
   Danielson, T
   Faust, R
   Deschaine, L
   North, C
AF Palamarchuk, Daniel
   Williams, Lemara
   Mayer, Brian
   Danielson, Thomas
   Faust, Rebecca
   Deschaine, Larry
   North, Chris
TI Visualizing Temporal Topic Embeddings with a Compass
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Compass; Vectors; Streams; Electronic mail; Clustering
   algorithms; Training; High dimensional data; Dynamic topic modeling;
   Cluster analysis
AB Dynamic topic modeling is useful at discovering the development and change in latent topics over time. However, present methodology relies on algorithms that separate document and word representations. This prevents the creation of a meaningful embedding space where changes in word usage and documents can be directly analyzed in a temporal context. This paper proposes an expansion of the compass-aligned temporal Word2Vec methodology into dynamic topic modeling. Such a method allows for the direct comparison of word and document embeddings across time in dynamic topics. This enables the creation of visualizations that incorporate temporal word embeddings within the context of documents into topic visualizations. In experiments against the current state-of-the-art, our proposed method demonstrates overall competitive performance in topic relevancy and diversity across temporal datasets of varying size. Simultaneously, it provides insightful visualizations focused on temporal word embeddings while maintaining the insights provided by global topic evolution, advancing our understanding of how topics evolve over time.
C1 [Palamarchuk, Daniel; Williams, Lemara; Mayer, Brian; North, Chris] Virginia Polytech Inst & State Univ, Blacksburg, VA 24060 USA.
   [Danielson, Thomas; Deschaine, Larry] Savannah River Natl Lab, Savannah, GA USA.
   [Faust, Rebecca] Tulane Univ, Tulane, LA USA.
C3 Virginia Polytechnic Institute & State University; United States
   Department of Energy (DOE); Savannah River National Laboratory; Tulane
   University
RP Palamarchuk, D (corresponding author), Virginia Polytech Inst & State Univ, Blacksburg, VA 24060 USA.
EM d4n1elp@vt.edu; lemaraw@vt.edu; bmayer@cs.vt.edu;
   Thomas.Danielson@srnl.doe.gov; rfaust1@tulane.edu;
   larry.deschaine@srnl.doe.gov; north@cs.vt.edu
RI North, Chris/T-6465-2019
OI Mayer, Brian/0000-0002-9925-8276
FU National Science Foundation [2127309]
FX This material is based upon work supported by the National Science
   Foundation under Grant # 2127309 to the Computing Research Association
   for the CIFellows 2021 Project.
NR 39
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2025
VL 31
IS 1
BP 272
EP 282
DI 10.1109/TVCG.2024.3456143
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA N9Y7G
UT WOS:001367808800011
PM 39255128
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Yang, D
   Kang, J
   Kim, T
   Lee, SH
AF Yang, Dongseok
   Kang, Jiho
   Kim, Taehei
   Lee, Sung-Hee
TI Visual Guidance for User Placement in Avatar-Mediated Telepresence
   Between Dissimilar Spaces
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Avatars; Telepresence; Visualization; Three-dimensional displays; Shape;
   Image reconstruction; Collaboration; Mixed reality; telepresence;
   virtual avatar; visualization
ID REALITY; COLLABORATION; DESIGN
AB Rapid advances in technology gradually realize immersive mixed-reality (MR) telepresence between distant spaces. This paper presents a novel visual guidance system for avatar-mediated telepresence, directing users to optimal placements that facilitate the clear transfer of gaze and pointing contexts through remote avatars in dissimilar spaces, where the spatial relationship between the remote avatar and the interaction targets may differ from that of the local user. Representing the spatial relationship between the user/avatar and interaction targets with angle-based interaction features, we assign recommendation scores of sampled local placements as their maximum feature similarity with remote placements. These scores are visualized as color-coded 2D sectors to inform the users of better placements for interaction with selected targets. In addition, virtual objects of the remote space are overlapped with the local space for the user to better understand the recommendations. We examine whether the proposed score measure agrees with the actual user perception of the partner's interaction context and find a score threshold for recommendation through user experiments in virtual reality (VR). A subsequent user study in VR investigates the effectiveness and perceptual overload of different combinations of visualizations. Finally, we conduct a user study in an MR telepresence scenario to evaluate the effectiveness of our method in real-world applications.
C1 [Yang, Dongseok; Kang, Jiho; Kim, Taehei; Lee, Sung-Hee] Korea Adv Inst Sci & Technol KAIST, Daejeon 34141, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST)
RP Lee, SH (corresponding author), Korea Adv Inst Sci & Technol KAIST, Daejeon 34141, South Korea.
EM dsyang@kaist.ac.kr; jhkang0408@kaist.ac.kr; hayleyy321@kaist.ac.kr;
   sunghee.lee@kaist.ac.kr
RI Kim, Taehei/GQA-8213-2022
OI Lee, Sung-Hee/0000-0001-6604-4709; Yang, Dongseok/0000-0002-4696-3465;
   gang, jiho/0000-0001-9766-939X; Kim, Taehei/0000-0002-4357-0420
FU IITP, MSIT, Korea [2022-0-00566]; NRF, Korea [2022R1A4A5033689]
FX This work was supported in part by IITP, MSIT, Korea under Grant
   2022-0-00566 and in part by NRF, Korea under Grant 2022R1A4A5033689.
NR 37
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2024
VL 30
IS 12
BP 7558
EP 7570
DI 10.1109/TVCG.2024.3354256
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K8D1L
UT WOS:001346124800013
PM 38227413
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Han, J
   Zheng, H
   Bi, CK
AF Han, Jun
   Zheng, Hao
   Bi, Chongke
TI KD-INR: Time-Varying Volumetric Data Compression via Knowledge
   Distillation-Based Implicit Neural Representation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Time-varying data compression; implicit neural representation; knowledge
   distillation; volume visualization; Time-varying data compression;
   implicit neural representation; knowledge distillation; volume
   visualization
ID MULTILEVEL TECHNIQUES; SUPERRESOLUTION; REDUCTION
AB Traditional deep learning algorithms assume that all data is available during training, which presents challenges when handling large-scale time-varying data. To address this issue, we propose a data reduction pipeline called knowledge distillation-based implicit neural representation (KD-INR) for compressing large-scale time-varying data. The approach consists of two stages: spatial compression and model aggregation. In the first stage, each time step is compressed using an implicit neural representation with bottleneck layers and features of interest preservation-based sampling. In the second stage, we utilize an offline knowledge distillation algorithm to extract knowledge from the trained models and aggregate it into a single model. We evaluated our approach on a variety of time-varying volumetric data sets. Both quantitative and qualitative results, such as PSNR, LPIPS, and rendered images, demonstrate that KD-INR surpasses the state-of-the-art approaches, including learning-based (i.e., CoordNet, NeurComp, and SIREN) and lossy compression (i.e., SZ3, ZFP, and TTHRESH) methods, at various compression ratios ranging from hundreds to ten thousand.
C1 [Han, Jun] Chinese Univ Hong Kong, Sch Data Sci, Shenzhen 518172, Peoples R China.
   [Han, Jun] Hong Kong Univ Sci & Technol, Hong Kong 999077, Peoples R China.
   [Zheng, Hao] Univ Notre Dame, Notre Dame, IN 46556 USA.
   [Zheng, Hao] Univ Louisiana Lafayette, Sch Comp & Informat, Lafayette, LA 70504 USA.
   [Bi, Chongke] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300072, Peoples R China.
C3 The Chinese University of Hong Kong, Shenzhen; Hong Kong University of
   Science & Technology; University of Notre Dame; University of Louisiana
   Lafayette; Tianjin University
RP Han, J (corresponding author), Chinese Univ Hong Kong, Sch Data Sci, Shenzhen 518172, Peoples R China.; Han, J (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong 999077, Peoples R China.
EM hanjun@cuhk.edu.cn; haozheng9428@gmail.com; bichongke@tju.edu.cn
RI zheng, hao/HHM-6949-2022
OI Han, Jun/0000-0002-7286-062X; Zheng, Hao/0000-0002-9790-7607
FU Chinese University of Hong Kong, Shenzhen [UDF01002679]; Shenzhen
   Science and Technology Program [ZDSYS20211021111415025]; National
   Natural Science Foundation of China [62302422, 62172294]
FX This research was supported in part by the start-up fund UDF01002679 of
   the Chinese University of Hong Kong, Shenzhen, Shenzhen Science and
   Technology Program under Grant ZDSYS20211021111415025,and in part by the
   National Natural Science Foundation of China under Grant 62302422 and
   under Grant 62172294. Recommended for acceptance by C.Garth
NR 48
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6826
EP 6838
DI 10.1109/TVCG.2945
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600010
PM 38127599
DA 2025-03-07
ER

PT J
AU Chaves-de-Plaza, NF
   Mody, P
   Staring, M
   van Egmond, R
   Vilanova, A
   Hildebrandt, K
AF Chaves-de-Plaza, Nicolas F.
   Mody, Prerak
   Staring, Marius
   van Egmond, Rene
   Vilanova, Anna
   Hildebrandt, Klaus
TI Inclusion Depth for Contour Ensembles
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Uncertainty; Feature extraction; Data
   models; Computational modeling; Semantic segmentation; Uncertainty
   visualization; contours; ensemble summarization; depth statistics
ID NONPARAMETRIC MODELS; VISUAL ANALYSIS; UNCERTAINTY; VISUALIZATION;
   VARIABILITY
AB Ensembles of contours arise in various applications like simulation, computer-aided design, and semantic segmentation. Uncovering ensemble patterns and analyzing individual members is a challenging task that suffers from clutter. Ensemble statistical summarization can alleviate this issue by permitting analyzing ensembles' distributional components like the mean and median, confidence intervals, and outliers. Contour boxplots, powered by Contour Band Depth (CBD), are a popular non-parametric ensemble summarization method that benefits from CBD's generality, robustness, and theoretical properties. In this work, we introduce Inclusion Depth (ID), a new notion of contour depth with three defining characteristics. First, ID is a generalization of functional Half-Region Depth, which offers several theoretical guarantees. Second, ID relies on a simple principle: the inside/outside relationships between contours. This facilitates implementing ID and understanding its results. Third, the computational complexity of ID scales quadratically in the number of members of the ensemble, improving CBD's cubic complexity. This also in practice speeds up the computation enabling the use of ID for exploring large contour ensembles or in contexts requiring multiple depth evaluations like clustering. In a series of experiments on synthetic data and case studies with meteorological and segmentation data, we evaluate ID's performance and demonstrate its capabilities for the visual analysis of contour ensembles.
C1 [Chaves-de-Plaza, Nicolas F.; van Egmond, Rene; Hildebrandt, Klaus] Delft Univ Technol, NL-2628 CD Delft, Netherlands.
   [Chaves-de-Plaza, Nicolas F.; Mody, Prerak] Holland PTC, NL-2629 JH Delft, Netherlands.
   [Mody, Prerak] Leiden Univ, Med Ctr, Dept Radiol, NL-2333 ZA Leiden, Netherlands.
   [Staring, Marius] Leiden Univ, Med Ctr, Dept Radiol, Dept Radiat Oncol, NL-2333 ZA Leiden, Netherlands.
   [Vilanova, Anna] TU Eindhoven, NL-5612 AZ Eindhoven, Netherlands.
C3 Delft University of Technology; Leiden University - Excl LUMC; Leiden
   University; Leiden University Medical Center (LUMC); Leiden University -
   Excl LUMC; Leiden University; Leiden University Medical Center (LUMC);
   Eindhoven University of Technology
RP Chaves-de-Plaza, NF (corresponding author), Delft Univ Technol, NL-2628 CD Delft, Netherlands.
EM n.f.chavesdeplaza@tudelft.nl; p.p.mody@lumc.nl; m.staring@lumc.nl;
   R.vanEgmond@tudelft.nl; a.vilanova@tue.nl; k.a.hildebrandt@tudelft.nl
RI Staring, Marius/A-9517-2009; Mody, Prerak/HGC-2285-2022; VanEgmond,
   Rene/KOC-9788-2024
OI Vilanova, Anna/0000-0002-1034-737X; Van Egmond,
   Rene/0000-0001-5015-0670; Mody, Prerak/0000-0001-9697-2258; Hildebrandt,
   Klaus/0000-0002-9196-3923; Chaves-de-Plaza, Nicolas
   F./0000-0003-4971-3151; Staring, Marius/0000-0003-2885-5812
FU Varian, a Siemens Healthineers Company, through the HollandPTC-Varian
   Consortium [2019022]; Surcharge for Top Consortia for Knowledge and
   Innovation (TKIs) from the Ministry of Economic Affairs and Climate
FX This work was supported in part by Varian, a Siemens Healthineers
   Company, through the HollandPTC-Varian Consortium under Grant 2019022,
   and in part by the Surcharge for Top Consortia for Knowledge and
   Innovation (TKIs) from the Ministry of Economic Affairs and Climate. We
   also thank Rudiger Westermann and Keving Holhein for sharing with us
   ECMWF's meteorological forecasting data.
NR 29
TC 0
Z9 0
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6560
EP 6571
DI 10.1109/TVCG.2024.3350076
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000047
PM 38194372
DA 2025-03-07
ER

PT J
AU Yu, D
   Lau, M
   Gao, L
   Fu, HB
AF Yu, Deng
   Lau, Manfred
   Gao, Lin
   Fu, Hongbo
TI Sketch Beautification: Learning Part Beautification and Structure
   Refinement for Sketches of Man-Made Objects
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Training; Interpolation; Manifolds; Layout; Image
   reconstruction; Geometry; Sketch assembly; sketch beautification; sketch
   implicit representation
AB We present a novel freehand sketch beautification method, which takes as input a freely drawn sketch of a man-made object and automatically beautifies it both geometrically and structurally. Beautifying a sketch is challenging because of its highly abstract and heavily diverse drawing manner. Existing methods are usually confined to their limited training samples and thus cannot beautify freely drawn sketches with both geometric and structural variations. To address this challenge, we adopt a divide-and-combine strategy. Specifically, we first parse an input sketch into semantic components, beautify individual components by a learned part beautification module based on part-level implicit manifolds, and then reassemble the beautified components through a structure beautification module. With this strategy, our method can go beyond the training samples and handle novel freehand sketches. We demonstrate the effectiveness of our system with extensive experiments and a perceptual study.
C1 [Yu, Deng; Lau, Manfred; Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
   [Gao, Lin] Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100045, Peoples R China.
   [Gao, Lin] Univ Chinese Acad Sci, Beijing 101408, Peoples R China.
C3 City University of Hong Kong; Chinese Academy of Sciences; Institute of
   Computing Technology, CAS; Chinese Academy of Sciences; University of
   Chinese Academy of Sciences, CAS
RP Lau, M; Fu, HB (corresponding author), City Univ Hong Kong, Sch Creat Media, Hong Kong, Peoples R China.
EM dengyucn@gmail.com; manfred.lau@gmail.com; gaolin@ict.ac.cn;
   fuplus@gmail.com
OI FU, Hongbo/0000-0002-0284-726X; YU, Deng/0000-0003-2343-3688
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China under Grants CityU [11212119, 11205420, 11206319]; Chow Sang Sang
   Group Research Fund/Donation; Centre for Applied Computing and
   Interactive Media (ACIM) of the School of Creative Media, CityU
FX This work was supported in part by the Research Grants Council of the
   Hong Kong Special Administrative Region, China under Grants CityU
   11212119, 11206319, and 11205420, in part by Chow Sang Sang Group
   Research Fund/Donation, and in part by the Centre for Applied Computing
   and Interactive Media (ACIM) of the School of Creative Media, CityU.
   Recommended for acceptance by T. Weyrich.
NR 64
TC 1
Z9 1
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6533
EP 6546
DI 10.1109/TVCG.2023.3346995
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000041
PM 38145518
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Wang, XQ
   Yen, K
   Hu, YF
   Shen, HW
AF Wang, Xiaoqi
   Yen, Kevin
   Hu, Yifan
   Shen, Han-Wei
TI SmartGD: A GAN-Based Graph Drawing Framework for Diverse Aesthetic Goals
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Layout; Graph drawing; Deep learning; Generative adversarial networks;
   Stress; Generators; Training data; Deep learning for visualization;
   generative adversarial networks; graph visualization
ID LAYOUTS
AB While a multitude of studies have been conducted on graph drawing, many existing methods only focus on optimizing a single aesthetic aspect of graph layouts, which can lead to sub-optimal results. There are a few existing methods that have attempted to develop a flexible solution for optimizing different aesthetic aspects measured by different aesthetic criteria. Furthermore, thanks to the significant advance in deep learning techniques, several deep learning-based layout methods were proposed recently. These methods have demonstrated the advantages of deep learning approaches for graph drawing. However, none of these existing methods can be directly applied to optimizing non-differentiable criteria without special accommodation. In this work, we propose a novel Generative Adversarial Network (GAN) based deep learning framework for graph drawing, called SmartGD, which can optimize different quantitative aesthetic goals, regardless of their differentiability. To demonstrate the effectiveness and efficiency of SmartGD, we conducted experiments on minimizing stress, minimizing edge crossing, maximizing crossing angle, maximizing shape-based metrics, and a combination of multiple aesthetics. Compared with several popular graph drawing algorithms, the experimental results show that SmartGD achieves good performance both quantitatively and qualitatively.
C1 [Wang, Xiaoqi; Shen, Han-Wei] Ohio State Univ, Columbus, OH 43210 USA.
   [Yen, Kevin; Hu, Yifan] Yahoo Res, New York, NY 10003 USA.
C3 University System of Ohio; Ohio State University; Yahoo! Inc
RP Wang, XQ (corresponding author), Ohio State Univ, Columbus, OH 43210 USA.
EM wang.5502@osu.edu; kevinyen@yahooinc.com; yifanh@gmail.com;
   shen.94@osu.edu
RI Shen, Han-wei/A-4710-2012
OI Yen, Kevin/0000-0002-4338-9680; Shen, Han-Wei/0000-0002-1211-2320
FU NSF-funded AI Institute [OAC2112606]
FX No Statement Available
NR 38
TC 0
Z9 0
U1 5
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5666
EP 5678
DI 10.1109/TVCG.2023.3306356
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400061
PM 37594870
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Diller, F
   Scheuermann, G
   Wiebel, A
AF Diller, Florian
   Scheuermann, Gerik
   Wiebel, Alexander
TI Visual Cue Based Corrective Feedback for Motor Skill Training in Mixed
   Reality: A Survey
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Mixed reality; Augmented reality; Training; Task
   analysis; Sports; Feedback loop; Human-centered computing;
   visualization; visualization techniques and methodologies; interaction
   techniques; virtual and augmented reality
AB When learning a motor skill it is helpful to get corrective feedback from an instructor. This will support the learner to execute the movement correctly. With modern technology, it is possible to provide this feedback via mixed reality. In most cases, this involves visual cues to help the user understand the corrective feedback. We analyzed recent research approaches utilizing visual cues for feedback in mixed reality. The scope of this article is visual feedback for motor skill learning, which involves physical therapy, exercise, rehabilitation etc. While some of the surveyed literature discusses therapeutic effects of the training, this article focuses on visualization techniques. We categorized the literature from a visualization standpoint, including visual cues, technology and characteristics of the feedback. This provided insights into how visual feedback in mixed reality is applied in the literature and how different aspects of the feedback are related. The insights obtained can help to better adjust future feedback systems to the target group and their needs. This article also provides a deeper understanding of the characteristics of the visual cues in general and promotes future, more detailed research on this topic.
C1 [Diller, Florian; Wiebel, Alexander] Hsch Worms Univ Appl Sci, UX Vis Grp, D-67549 Worms, Germany.
   [Scheuermann, Gerik] Univ Leipzig, BSV Grp, D-04109 Leipzig, Germany.
C3 Leipzig University
RP Diller, F (corresponding author), Hsch Worms Univ Appl Sci, UX Vis Grp, D-67549 Worms, Germany.
EM diller@hs-worms.de; scheuermann@informatik.uni-leipzig.de;
   wiebel@hs-worms.de
OI Scheuermann, Gerik/0000-0001-5200-8870; Diller,
   Florian/0000-0001-7421-750X; Wiebel, Alexander/0000-0002-6583-3092
FU ProFIL -Programm zur Forderung des Forschungspersonals, Infrastruktur
   und forschendem Lernenof HSWorms; German Federal Ministry for Economic
   Affairs and Energy [16KN087122]
FX This work was supported in part by ProFIL -Programm zur Forderung des
   Forschungspersonals, Infrastruktur und forschendem Lernenof HSWorms. All
   other work was supported in part by ZIM under Grant 16KN087122 from the
   German Federal Ministry for Economic Affairs and Energy.
NR 79
TC 3
Z9 3
U1 9
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3121
EP 3134
DI 10.1109/TVCG.2022.3227999
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700064
PM 37015488
OA hybrid
DA 2025-03-07
ER

PT J
AU Liu, JY
   Saquib, N
   Zhutian, C
   Kazi, RH
   Wei, LY
   Fu, HB
   Tai, CL
AF Liu, Jingyuan
   Saquib, Nazmus
   Zhutian, Chen
   Kazi, Rubaiat Habib
   Wei, Li-Yi
   Fu, Hongbo
   Tai, Chiew-Lan
TI PoseCoach: A Customizable Analysis and Visualization System for
   Video-Based Running Coaching
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Videos; Sports; Three-dimensional displays; Data visualization; Data
   analysis; Analytical models; Solid modeling; Human pose; video
   processing; sports data analysis
ID BIOMECHANICS; RELIABILITY
AB Videos are an accessible form of media for analyzing sports postures and providing feedback to athletes. Existing sport-specific systems embed bespoke human pose attributes and thus can be hard to scale for new attributes, especially for users without programming experiences. Some systems retain scalability by directly showing the differences between two poses, but they might not clearly visualize the key differences that viewers would like to pursue. Besides, video-based coaching systems often present feedback on the correctness of poses by augmenting videos with visual markers or reference poses. However, previewing and augmenting videos limit the analysis and visualization of human poses due to the fixed viewpoints in videos, which confine the observation of captured human movements and cause ambiguity in the augmented feedback. To address these issues, we study customizable human pose data analysis and visualization in the context of running pose attributes, such as joint angles and step distances. Based on existing literature and a formative study, we have designed and implemented a system, PoseCoach, to provide feedback on running poses for amateurs by comparing the running poses between a novice and an expert. PoseCoach adopts a customizable data analysis model to allow users' controllability in defining pose attributes of their interests through our interface. To avoid the influence of viewpoint differences and provide intuitive feedback, PoseCoach visualizes the pose differences as part-based 3D animations on a human model to imitate the demonstration of a human coach. We conduct a user study to verify our design components and conduct expert interviews to evaluate the usefulness of the system.
C1 [Liu, Jingyuan; Tai, Chiew-Lan] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Saquib, Nazmus] Tero Labs, Santa Clara, CA 95051 USA.
   [Zhutian, Chen] Harvard Univ, Cambridge, MA 02138 USA.
   [Kazi, Rubaiat Habib; Wei, Li-Yi] Adobe Res, San Jose, CA 95110 USA.
   [Fu, Hongbo] City Univ Hong Kong, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology; Harvard University; Adobe
   Systems Inc.; City University of Hong Kong
RP Liu, JY (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
EM jliucb@connect.ust.hk; nzm.saquib@gmail.com; ztchen@seas.harvard.edu;
   rhabib@adobe.com; review@liyiwei.org; fuplus@gmail.com; taicl@cse.ust.hk
RI Wei, Li-Yi/F-4469-2011; Saquib, Nazmus/GWM-5475-2022
OI Liu, Jingyuan/0000-0002-4648-5555; FU, Hongbo/0000-0002-0284-726X; Wei,
   Li-Yi/0000-0002-1076-6339
FU Research Grants Council of HKSAR [HKUST16206722]
FX This work was supported by the Research Grants Council of HKSAR under
   Grant HKUST16206722. We thank the anonymous reviewers for the valuable
   feedback and the userstudy participants for their great help.
NR 63
TC 4
Z9 4
U1 3
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3180
EP 3195
DI 10.1109/TVCG.2022.3230855
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700045
PM 37015423
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Lyu, Y
   Lu, HX
   Lee, MK
   Schmitt, G
   Lim, BY
AF Lyu, Yan
   Lu, Hangxin
   Lee, Min Kyung
   Schmitt, Gerhard
   Lim, Brian Y.
TI IF-City: Intelligible Fair City Planning to Measure, Explain and
   Mitigate Inequality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Resource management; Visualization; Urban planning; Indexes; Data
   visualization; Buildings; Sociology; Fairness; intelligibility;
   explainable artificial intelligence; resource allocation; urban planning
ID LAND-USE; URBAN; ACCESSIBILITY; EQUITY; NETWORKS; DESIGN
AB With the increasing pervasiveness of Artificial Intelligence (AI), many visual analytics tools have been proposed to examine fairness, but they mostly focus on data scientist users. Instead, tackling fairness must be inclusive and involve domain experts with specialized tools and workflows. Thus, domain-specific visualizations are needed for algorithmic fairness. Furthermore, while much work on AI fairness has focused on predictive decisions, less has been done for fair allocation and planning, which require human expertise and iterative design to integrate myriad constraints. We propose the Intelligible Fair Allocation (IF-Alloc) Framework that leverages explanations of causal attribution (Why), contrastive (Why Not) and counterfactual reasoning (What If, How To) to aid domain experts to assess and alleviate unfairness in allocation problems. We apply the framework to fair urban planning for designing cities that provide equal access to amenities and benefits for diverse resident types. Specifically, we propose an interactive visual tool, Intelligible Fair City Planner (IF-City), to help urban planners to perceive inequality across groups, identify and attribute sources of inequality, and mitigate inequality with automatic allocation simulations and constraint-satisfying recommendations (IF-Plan). We demonstrate and evaluate the usage and usefulness of IF-City on a real neighborhood in New York City, US, with practicing urban planners from multiple countries, and discuss generalizing our findings, application, and framework to other use cases and applications of fair allocation.
C1 [Lyu, Yan] Southeast Univ, Sch Comp Sci & Engn, Nanjing 211189, Peoples R China.
   [Lu, Hangxin; Schmitt, Gerhard] Singapore ETH Ctr, Future Cities Lab, Singapore 138602, Singapore.
   [Lu, Hangxin; Schmitt, Gerhard] Swiss Fed Inst Technol, Dept Architecture, CH-8092 Zurich, Switzerland.
   [Lee, Min Kyung] Univ Texas Austin, Sch Informat, Austin, TX 78712 USA.
   [Lim, Brian Y.] Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
C3 Southeast University - China; Swiss Federal Institutes of Technology
   Domain; ETH Zurich; University of Texas System; University of Texas
   Austin; National University of Singapore
RP Lim, BY (corresponding author), Natl Univ Singapore, Sch Comp, Singapore 119077, Singapore.
EM lvyanly@gmail.com; luhangxin@gmail.com; minkyung.lee@austin.utexas.edu;
   schmitt@arch.ethz.ch; brianlim@comp.nus.edu.sg
OI LYU, Yan/0000-0001-9959-9217; Lee, Min Kyung/0000-0002-2696-6546; Lim,
   Brian/0000-0002-0543-2414
FU National Key Research and Development Program of China [2019YFB2102200];
   Singapore Ministry of Education (MOE) Academic Research Fund Tier 2
   [T2EP20121-0040]; Natural Science Foundation of China [62102082];
   Jiangsu Natural Science Foundation of China [BK20210203]; National
   Science Foundation [CNS-1952085, IIS-1939606, DGE-2125858]
FX This work was supported in part by the National Key Research and
   Development Program of China under Grant 2019YFB2102200; in part by the
   Singapore Ministry of Education (MOE) Academic Research Fund Tier 2
   under Grant T2EP20121-0040; in part by the Natural Science Foundation of
   China under Grant 62102082; in part by the Jiangsu Natural Science
   Foundation of China under Grant BK20210203; and in part by National
   Science Foundation under Grants CNS-1952085, IIS-1939606, and
   DGE-2125858.
NR 110
TC 4
Z9 4
U1 19
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3749
EP 3766
DI 10.1109/TVCG.2023.3239909
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700038
PM 37022033
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Song, XH
   Liu, C
   Zheng, YY
   Feng, ZL
   Li, LC
   Zhou, K
   Yu, X
AF Song, Xinhui
   Liu, Chen
   Zheng, Youyi
   Feng, Zunlei
   Li, Lincheng
   Zhou, Kun
   Yu, Xin
TI HairStyle Editing via Parametric Controllable Strokes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Hair; Image color analysis; Shape; Stroke (medical condition); Rendering
   (computer graphics); Faces; Pipelines; Hairstyle editing; hairstyle
   transfer; parameterized hair strokes; stroke-controllable
ID IMAGE SYNTHESIS; CAPTURE
AB In this work, we propose a stroke-based hairstyle editing network, dubbed HairstyleNet, allowing users to conveniently change the hairstyles of an image in an interactive fashion. Different from previous works, we simplify the hairstyle editing process where users can manipulate local or entire hairstyles by adjusting the parameterized hair regions. Our HairstyleNet consists of two stages: a stroke parameterization stage and a stroke-to-hair generation stage. In the stroke parameterization stage, we first introduce parametric strokes to approximate the hair wisps, where the stroke shape is controlled by a quadratic B & eacute;zier curve and a thickness parameter. Since rendering strokes with thickness to an image is not differentiable, we opt to leverage a neural renderer to construct the mapping from stroke parameters to a stroke image. Thus, the stroke parameters can be directly estimated from hair regions in a differentiable way, enabling us to flexibly edit the hairstyles of input images. In the stroke-to-hair generation stage, we design a hairstyle refinement network that first encodes coarsely composed images of hair strokes, face, and background into latent representations and then generates high-fidelity face images with desirable new hairstyles from the latent codes. Extensive experiments demonstrate that our HairstyleNet achieves state-of-the-art performance and allows flexible hairstyle manipulation.
C1 [Song, Xinhui; Li, Lincheng] Fuxi AI Lab Netease Inc, Hangzhou 310052, Zhejiang, Peoples R China.
   [Zheng, Youyi; Feng, Zunlei; Zhou, Kun] Zhejiang Univ, Hangzhou 310027, Zhejiang, Peoples R China.
   [Liu, Chen; Yu, Xin] Univ Queensland, Brisbane, Qld 4072, Australia.
C3 Zhejiang University; University of Queensland
RP Li, LC (corresponding author), Fuxi AI Lab Netease Inc, Hangzhou 310052, Zhejiang, Peoples R China.
EM songxinhui@corp.netease.com; yenanliu36@gmail.com;
   youyizheng@zju.edu.cn; zunleifeng@zju.edu.cn;
   lilincheng@corp.netease.com; kunzhou@acm.org; xin.yu@uts.edu.au
RI Li, Lincheng/GPX-7771-2022
OI LIU, CHEN/0000-0003-3159-0034; Feng, Zunlei/0000-0001-8640-8434; Yu,
   Xin/0000-0002-0269-5649
FU ARC-Discovery [DP220100800]; ARC-DECRA [DE230100477]
FX This work was supported in part by the ARC-Discovery under Grant
   DP220100800 and in part by the ARC-DECRA under Grant DE230100477.
NR 76
TC 3
Z9 3
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3857
EP 3870
DI 10.1109/TVCG.2023.3241894
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700067
PM 37022457
DA 2025-03-07
ER

PT J
AU Cai, ZJ
   Ma, YH
   Lu, F
AF Cai, Zhuojiang
   Ma, Yuhan
   Lu, Feng
TI Robust Dual-Modal Speech Keyword Spotting for XR Headsets
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Speech interaction; extended reality; keyword spotting; multimodal
   interaction
ID RECOGNITION; GESTURE
AB While speech interaction finds widespread utility within the Extended Reality (XR) domain, conventional vocal speech keyword spotting systems continue to grapple with formidable challenges, including suboptimal performance in noisy environments, impracticality in situations requiring silence, and susceptibility to inadvertent activations when others speak nearby. These challenges, however, can potentially be surmounted through the cost-effective fusion of voice and lip movement information. Consequently, we propose a novel vocal-echoic dual-modal keyword spotting system designed for XR headsets. We devise two different modal fusion approches and conduct experiments to test the system's performance across diverse scenarios. The results show that our dual-modal system not only consistently outperforms its single-modal counterparts, demonstrating higher precision in both typical and noisy environments, but also excels in accurately identifying silent utterances. Furthermore, we have successfully applied the system in real-time demonstrations, achieving promising results. The code is available at https://github.com/caizhuojiang/VE-KWS.
C1 [Cai, Zhuojiang; Ma, Yuhan; Lu, Feng] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
C3 Beihang University
RP Lu, F (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing, Peoples R China.
EM caizhuojiang@buaa.edu.cn; raphael.mayuhan@buaa.edu.cn;
   lufeng@buaa.edu.cn
RI Ma, Yuhan/HMD-8720-2023
OI Lu, Feng/0000-0001-9064-7964; Cai, Zhuojiang/0009-0005-3404-118X
NR 57
TC 1
Z9 1
U1 7
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2507
EP 2516
DI 10.1109/TVCG.2024.3372092
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400057
PM 38437114
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Liao, SQ
   Byrd, V
   Popescu, V
AF Liao, Shuqi
   Byrd, Vetria
   Popescu, Voicu
TI PreVR: Variable-Distance Previews for Higher-Order Disocclusion in VR
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Navigation; Task analysis; Virtual environments;
   Three-dimensional displays; Teleportation; Measurement; Disocclusion;
   visualization; navigation; virtual reality
AB The paper introduces PreVR, a method for allowing the user of a VR application to preview a virtual environment (VE) around any number of corners. This way the user can gain line of sight to any part of the VE, no matter how distant or how heavily occluded it is. PreVR relies on a multiperspective visualization that implements a higher-order disocclusion effect with piecewise linear rays that bend multiple times as needed to reach the visualization target. PreVR was evaluated in a user study ($\mathrm{N}=88$) that investigates four points on the VR interface design continuum defined by the maximum disocclusion order $\delta$. In a first control condition (CC0), $\delta=0$, corresponds to conventional VR exploration with no preview capability. In a second control condition (CC1), $\delta=1$, corresponds to the prior art approach of giving the user a preview around the first corner. In a first experimental condition (EC3), $\delta=3$, so PreVR provided up to third-order disocclusion. In a second experimental condition (ECN), $\delta$ was not capped, so PreVR could provide a disocclusion effect of any order, as needed to reach any location in the VE. Participants searched for a stationary target, for a dynamic target moving on a random continuous trajectory, and for a transient dynamic target that appeared at random locations in the maze and disappeared 5s later. The study quantified VE exploration efficiency with four metrics: viewpoint translation, view direction rotation, number of teleportations, and task completion time. Results show that the previews afforded by PreVR bring a significant VE exploration efficiency advantage. ECN outperforms EC3, CC1, and CC0 for all metrics and all tasks, and EC3 frequently outperforms CC1 and CC0.
C1 [Liao, Shuqi; Byrd, Vetria; Popescu, Voicu] Purdue Univ, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University
RP Liao, SQ (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.
EM liao201@purdue.edu; vlbyrd@purdue.edu; popescu@purdue.edu
RI Liao, Shuqi/KRP-3611-2024
OI Liao, Shuqi/0000-0002-9134-6845
FU National Science Foundation
FX No Statement Available
NR 44
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2454
EP 2463
DI 10.1109/TVCG.2024.3372068
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OT7U2
UT WOS:001209605200005
PM 38437137
DA 2025-03-07
ER

PT J
AU Wilson, E
   Ibragimov, A
   Proulx, MJ
   Tetali, SD
   Butler, K
   Jain, E
AF Wilson, Ethan
   Ibragimov, Azim
   Proulx, Michael J.
   Tetali, Sai Deep
   Butler, Kevin
   Jain, Eakta
TI Privacy-Preserving Gaze Data Streaming in Immersive Interactive Virtual
   Reality: Robustness and User Experience
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual reality; privacy; eye tracking
ID EYE-TRACKING; VR; PREDICTION; MOVEMENTS
AB Eye tracking is routinely being incorporated into virtual reality (VR) systems. Prior research has shown that eye tracking data, if exposed, can be used for re-identification attacks [14]. The state of our knowledge about currently existing privacy mechanisms is limited to privacy-utility trade-off curves based on data-centric metrics of utility, such as prediction error, and black-box threat models. We propose that for interactive VR applications, it is essential to consider user-centric notions of utility and a variety of threat models. We develop a methodology to evaluate real-time privacy mechanisms for interactive VR applications that incorporate subjective user experience and task performance metrics. We evaluate selected privacy mechanisms using this methodology and find that re-identification accuracy can be decreased to as low as 14% while maintaining a high usability score and reasonable task performance. Finally, we elucidate three threat scenarios (black-box, black-box with exemplars, and white-box) and assess how well the different privacy mechanisms hold up to these adversarial scenarios. This work advances the state of the art in VR privacy by providing a methodology for end-to-end assessment of the risk of re-identification attacks and potential mitigating solutions. f
C1 [Wilson, Ethan; Ibragimov, Azim; Butler, Kevin; Jain, Eakta] Univ Florida, Comp & Informat Sci & Engn Dept, Gainesville, FL 32611 USA.
   [Proulx, Michael J.] Meta Real Labs Res, Redmond, WA USA.
   [Tetali, Sai Deep] Meta Real Labs, Burlingame, CA USA.
C3 State University System of Florida; University of Florida
RP Wilson, E (corresponding author), Univ Florida, Comp & Informat Sci & Engn Dept, Gainesville, FL 32611 USA.
EM ethanwilson@ufl.edu; a.ibragimov@ufl.edu; michaelproulx@meta.com;
   saideept@meta.com; butler@ufl.edu; ejain@ufl.edu
RI Wilson, Ethan/JMB-1659-2023
OI Wilson, Ethan/0000-0003-0944-2641
FU NSF
FX No Statement Available
NR 83
TC 2
Z9 2
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2257
EP 2268
DI 10.1109/TVCG.2024.3372032
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400016
PM 38457326
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Jin, YZ
   de Jong, TJA
   Tennekes, M
   Chen, M
AF Jin, Yuanzhe
   de Jong, Tim J. A.
   Tennekes, Martijn
   Chen, Min
TI Radial Icicle Tree (RIT): Node Separation and Area Constancy
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Tree visualization; icicle tree; sunburst tree; size encoding; area
   constancy; node separation; radial icicle tree; RIT
ID PLOTS
AB Icicles and sunbursts are two commonly-used visual representations of trees. While icicle trees can map data values faithfully to rectangles of different sizes, often some rectangles are too narrow to be noticed easily. When an icicle tree is transformed into a sunburst tree, the width of each rectangle becomes the length of an annular sector that is usually longer than the original width. While sunburst trees alleviate the problem of narrow rectangles in icicle trees, it no longer maintains the consistency of size encoding. At different tree depths, nodes of the same data values are displayed in annular sections of different sizes in a sunburst tree, though they are represented by rectangles of the same size in an icicle tree. Furthermore, two nodes from different subtrees could sometimes appear as a single node in both icicle trees and sunburst trees. In this paper, we propose a new visual representation, referred to as radial icicle tree (RIT), which transforms the rectangular bounding box of an icicle tree into a circle, circular sector, or annular sector while introducing gaps between nodes and maintaining area constancy for nodes of the same size. We applied the new visual design to several datasets. Both the analytical design process and user-centered evaluation have confirmed that this new design has improved the design of icicles and sunburst trees without introducing any relative demerit.
C1 [Jin, Yuanzhe; Chen, Min] Univ Oxford, Oxford, England.
   [de Jong, Tim J. A.; Tennekes, Martijn] Stat Netherlands, The Hague, Netherlands.
C3 University of Oxford
RP Jin, YZ (corresponding author), Univ Oxford, Oxford, England.
EM yuanzhe.jin@eng.ox.ac.uk; tja.dejong@cbs.nl; m.tennekes@cbs.nl;
   min.chen@eng.ox.ac.uk
RI Jin, Yuan-Zhe/P-9666-2019
FU Network of European Data Scientists (NeEDS)
FX No Statement Available
NR 54
TC 0
Z9 0
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 251
EP 261
DI 10.1109/TVCG.2023.3327178
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500076
PM 37883266
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Piochowiak, M
   Dachsbacher, C
AF Piochowiak, Max
   Dachsbacher, Carsten
TI Fast Compressed Segmentation Volumes for Scientific Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Segmentation volumes; lossless compression; volume rendering
ID OF-THE-ART; EXPLORATION; BRAIN
AB Voxel-based segmentation volumes often store a large number of labels and voxels, and the resulting amount of data can make storage, transfer, and interactive visualization difficult. We present a lossless compression technique which addresses these challenges. It processes individual small bricks of a segmentation volume and compactly encodes the labelled regions and their boundaries by an iterative refinement scheme. The result for each brick is a list of labels, and a sequence of operations to reconstruct the brick which is further compressed using rANS-entropy coding. As the relative frequencies of operations are very similar across bricks, the entropy coding can use global frequency tables for an entire data set which enables efficient and effective parallel (de)compression. Our technique achieves high throughput (up to gigabytes per second both for compression and decompression) and strong compression ratios of about 1% to 3% of the original data set size while being applicable to GPU-based rendering. We evaluate our method for various data sets from different fields and demonstrate GPU-based volume visualization with on-the-fly decompression, level-of-detail rendering (with optional on-demand streaming of detail coefficients to the GPU), and a caching strategy for decompressed bricks for further performance improvement.
C1 [Piochowiak, Max; Dachsbacher, Carsten] Karlsruhe Inst Technol, Karlsruhe, Germany.
C3 Helmholtz Association; Karlsruhe Institute of Technology
RP Piochowiak, M (corresponding author), Karlsruhe Inst Technol, Karlsruhe, Germany.
EM max.piochowiak@kit.edu; dachsbacher@kit.edu
OI Piochowiak, Max/0000-0003-1980-6146
FU Helmholtz Association (HGF)
FX No Statement Available
NR 50
TC 0
Z9 0
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 12
EP 22
DI 10.1109/TVCG.2023.3326573
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500073
PM 37871064
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Bernal-Berdun, E
   Martin, D
   Malpica, S
   Perez, PJ
   Gutierrez, D
   Masia, B
   Serrano, A
AF Bernal-Berdun, Edurne
   Martin, Daniel
   Malpica, Sandra
   Perez, Pedro J.
   Gutierrez, Diego
   Masia, Belen
   Serrano, Ana
TI D-SAV360: A Dataset of Gaze Scanpaths on 360<SUP>°</SUP> Ambisonic
   Videos
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 17th International Conference on High Speed Machining (HSM)
CY OCT 25-28, 2023
CL Nanjing, PEOPLES R CHINA
DE Videos; Behavioral sciences; Visualization; Stereo image processing;
   Estimation; Solid modeling; Predictive models; Gaze; Saliency;
   Fixations; Ambisonics; 360(degrees) Videos; Dataset
ID SALIENCY PREDICTION; EYE
AB Understanding human visual behavior within virtual reality environments is crucial to fully leverage their potential. While previous research has provided rich visual data from human observers, existing gaze datasets often suffer from the absence of multimodal stimuli. Moreover, no dataset has yet gathered eye gaze trajectories (i.e., scanpaths) for dynamic content with directional ambisonic sound, which is a critical aspect of sound perception by humans. To address this gap, we introduce D-SAV360, a dataset of 4,609 head and eye scanpaths for 360(degrees) videos with first-order ambisonics. This dataset enables a more comprehensive study of multimodal interaction on visual behavior in virtual reality environments. We analyze our collected scanpaths from a total of 87 participants viewing 85 different videos and show that various factors such as viewing mode, content type, and gender significantly impact eye movement statistics. We demonstrate the potential of D-SAV360 as a benchmarking resource for state-of-the-art attention prediction models and discuss its possible applications in further research. By providing a comprehensive dataset of eye movement data for dynamic, multimodal virtual environments, our work can facilitate future investigations of visual behavior and attention in virtual reality.
C1 [Bernal-Berdun, Edurne; Martin, Daniel; Malpica, Sandra; Perez, Pedro J.; Gutierrez, Diego; Masia, Belen; Serrano, Ana] Univ Zaragoza, I3A, Zaragoza, Spain.
C3 University of Zaragoza
RP Bernal-Berdun, E (corresponding author), Univ Zaragoza, I3A, Zaragoza, Spain.
EM edurnebernal@unizar.es; danims@unizar.es; smalpica@unizar.es;
   756642@unizar.es; diegog@unizar.es; bmasia@unizar.es; anase@unizar.es
RI Martin, Daniel/KLZ-9356-2024; Malpica, Sandra/LBH-5196-2024; Serrano
   Pacheu, Ana Belen/ABC-3358-2021
OI Bernal-Berdun, Edurne/0000-0002-5275-8652; Serrano Pacheu, Ana
   Belen/0000-0002-7796-3177
FU European Union [682080, 956585]; Spanish Agencia Estatal de
   Investigacion [PID2019-105004GB-I00, PID2022-141539NB-I00]; Gobierno de
   Aragon
FX We extend our gratitude to the members of the Graphics and Imaging Lab
   for their support and collaboration in the video recordings, especially
   to Maria Plaza for her valuable assistance during the capture pro-cess.
   We would also like to thank the anonymous reviewers for their insightful
   comments and the participants in the experiment. Our work has received
   funding from the European Union's Horizon 2020 research and innovation
   programme (ERC project CHAMELEON, Grant No 682080, and Marie
   Sklodowska-Curie project PRIME, Grant No 956585). This project was also
   funded by the Spanish Agencia Estatal de Investigacion (projects
   PID2019-105004GB-I00 and PID2022-141539NB-I00). Additionally, Sandra
   Malpica, Daniel Martin, and Edurne Bernal-Berdun were supported by a
   Gobierno de Aragon predoctoral grant (2018-2022, 2020-2024, and
   2021-2025, respectively).
NR 61
TC 0
Z9 0
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2023
VL 29
IS 11
BP 4350
EP 4360
DI 10.1109/TVCG.2023.3320237
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA X6ZW5
UT WOS:001099919100004
PM 37782595
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Bruder, V
   Larsen, M
   Ertl, T
   Childs, H
   Frey, S
AF Bruder, Valentin
   Larsen, Matthew
   Ertl, Thomas
   Childs, Hank
   Frey, Steffen
TI A Hybrid in Situ Approach for Cost Efficient Image Database Generation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; High performance computing; In situ
ID VISUALIZATION
AB The visualization of results while the simulation is running is increasingly common in extreme scale computing environments. We present a novel approach for in situ generation of image databases to achieve cost savings on supercomputers. Our approach, a hybrid between traditional inline and in transit techniques, dynamically distributes visualization tasks between simulation nodes and visualization nodes, using probing as a basis to estimate rendering cost. Our hybrid design differs from previous works in that it creates opportunities to minimize idle time from four fundamental types of inefficiency: variability, limited scalability, overhead, and rightsizing. We demonstrate our results by comparing our method against both inline and in transit methods for a variety of configurations, including two simulation codes and a scaling study that goes above 19 K cores. Our findings show that our approach is superior in many configurations. As in situ visualization becomes increasingly ubiquitous, we believe our technique could lead to significant amounts of reclaimed cycles on supercomputers.
C1 [Bruder, Valentin] Daimler Truck AG, D-70327 Stuttgart, Germany.
   [Larsen, Matthew] Luminary Cloud Inc, Palo Alto, CA 97403 USA.
   [Ertl, Thomas] Univ Stuttgart, D-70174 Stuttgart, Germany.
   [Childs, Hank] Univ Oregon, Eugene, OR 97403 USA.
   [Frey, Steffen] Univ Groningen, NL-9712 Groningen, Netherlands.
C3 University of Stuttgart; University of Oregon; University of Groningen
RP Bruder, V (corresponding author), Daimler Truck AG, D-70327 Stuttgart, Germany.
EM valentin.bruder@visus.uni-stuttgart.de; larsen30@llnl.gov;
   thomas.ertl@vis.uni-stuttgart.de; hank@uoregon.edu; s.d.frey@rug.nl
OI Ertl, Thomas/0000-0003-4019-2505; Bruder, Valentin/0000-0001-5063-4894;
   Frey, Steffen/0000-0002-1872-6905
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation)
   [251654672]; Intel Graphics and Visualization Institutes of XeLLENCE
   program [35512501]; Exascale Computing Project [17-SC-20-SC]; National
   Nuclear Security Administration
FX <STRONG>???????</STRONG>This work was supported in part by Deutsche
   Forschungsgemeinschaft (DFG,German Research Foundation) with in Project
   A02 of the SFB/Transregio 161under Grant 251654672, in part by the Intel
   Graphics and Visualization Institutes of XeLLENCE program under Grant CG
   #35512501, in part by Exascale Computing Project under Grant
   17-SC-20-SC, and in part by a collaborative effort of the U.S.
   Department of Energy Office of Science
   (https://doi.org/10.13039/100000015) and the National Nuclear Security
   Administration.
NR 41
TC 1
Z9 2
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2023
VL 29
IS 9
BP 3788
EP 3798
DI 10.1109/TVCG.2022.3169590
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA O2BA8
UT WOS:001041912300005
PM 35486551
OA Green Published
DA 2025-03-07
ER

PT J
AU Radu, I
   Schneider, B
AF Radu, Iulian
   Schneider, Bertrand
TI How Augmented Reality (AR) Can Help and Hinder Collaborative Learning: A
   Study of AR in Electromagnetism Education
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Augmented reality; collaboration; education; makerspaces
ID MATHEMATICS
AB Learning physics is often difficult for students because concepts such as electricity, magnetism and sound, cannot be seen with the naked eye. Emerging technologies such as Augmented Reality (AR) can transform education by making challenging concepts visible and accessible to novices. We present a Hololens-based augmented reality system where collaborators learn about the invisible electromagnetism phenomena involved in audio speakers, and we measure the benefits of AR technology through quantitative and qualitative methods. Specifically, we measure learning (knowledge gains and transfer) and collaborative knowledge exchange behaviors. Our results indicate that, while AR generally provides a novelty effect, specific educational AR visualizations can be both beneficial and detrimental to learning - they helped students to learn spatial content and structural relationships, but hindered their understanding of kinesthetic content. Furthermore, AR facilitated learning in collaborations by providing representational common ground, which improved communication and peer teaching. We discuss these effects, as well as identify factors that have positive impact (e.g., co-located representations, easier access to resources, better grounding) or negative impact (e.g., tunnel vision, overlooking kinesthetic feedback) on student collaborative learning with augmented reality applications.
C1 [Radu, Iulian; Schneider, Bertrand] Harvard Univ, Grad Sch Educ, Cambridge, MA 02138 USA.
C3 Harvard University
RP Radu, I (corresponding author), Harvard Univ, Grad Sch Educ, Cambridge, MA 02138 USA.
EM iulian_radu@gse.harvard.edu; bertrand_schneider@gse.harvard.edu
OI RADU, IULIAN/0000-0002-0184-2969
FU National Science Foundation [1748093]
FX This work was supported by the National Science Foundation under Grant
   1748093.
NR 46
TC 9
Z9 10
U1 10
U2 48
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2023
VL 29
IS 9
BP 3734
EP 3745
DI 10.1109/TVCG.2022.3169980
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA O2BA8
UT WOS:001041912300001
PM 35500084
DA 2025-03-07
ER

PT J
AU Reimann, D
   Ram, N
   Gaschler, R
AF Reimann, Daniel
   Ram, Nilam
   Gaschler, Robert
TI Lollipops Help Align Visual and Statistical Fit Estimates in
   Scatterplots With Nonlinear Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data models; Estimation; Computational modeling; Visualization; Data
   visualization; Psychology; Task analysis; Information visualization;
   perception and psychophysics; theory and models
ID PERCEPTUAL ORGANIZATION; ATTENTION; SLOPE; LAW
AB Scatterplots overlayed with a nonlinear model enable visual estimation of model-data fit. Although statistical fit is calculated using vertical distances, viewers' subjective fit is often based on shortest distances. Our results suggest that adding vertical lines ("lollipops") supports more accurate fit estimation in the steep area of model curves (https://osf.io/fybx5/).
C1 [Reimann, Daniel; Gaschler, Robert] Fernuniv, Dept Psychol, D-58097 Hagen, Germany.
   [Ram, Nilam] Stanford Univ, Dept Psychol & Commun, Stanford, CA 94305 USA.
C3 Fern University Hagen; Stanford University
RP Reimann, D (corresponding author), Fernuniv, Dept Psychol, D-58097 Hagen, Germany.
EM daniel.reimann@fernuni-hagen.de; nilamram@stanford.edu;
   robert.gaschler@fernuni-hagen.de
OI Gaschler, Robert/0000-0002-8576-5330; Ram, Nilam/0000-0003-1671-5257
NR 33
TC 2
Z9 2
U1 3
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2023
VL 29
IS 7
BP 3436
EP 3440
DI 10.1109/TVCG.2022.3158093
PG 5
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA H4XO7
UT WOS:000996011900021
PM 35263256
DA 2025-03-07
ER

PT J
AU Zytko, D
   Chan, J
AF Zytko, Douglas
   Chan, Jonathan
TI The Dating Metaverse: Why We Need to Design for Consent in Social VR
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Metaverse; Mobile applications; Social factors; Safety; Avatars; X
   reality; Planets; Consent; dating; social VR; social virtual reality;
   metaverse; harm; harassment; participatory design
AB This paper presents a participatory design study about how consent to interaction and observation of other users can be supported in social VR. We use emerging VR dating applications, colloquially called the dating metaverse, as context for study of harm-mitigative design structures in social VR given the evidence of harms that occur through dating apps and general social VR applications individually, and the harms that may occur through their convergence. Through design workshops with potential dating metaverse users in the Midwest United States (n=18) we elucidate nonconsensual experiences that should be prevented and participant-created designs for informing and exchanging consent in VR. We position consent as a valuable lens for which to design preventative solutions to harm in social VR by reframing harm as unwanted experiences that happen because of the absence of mechanics to support users in giving and denying agreement to a virtual experience before it occurs.
C1 [Zytko, Douglas; Chan, Jonathan] Oakland Univ, Rochester, MI 48309 USA.
C3 Oakland University
RP Zytko, D (corresponding author), Oakland Univ, Rochester, MI 48309 USA.
EM zytko@oakland.edu
FU U.S. National Science Foundation [IIS-2211896]
FX The authors wish to thank Jesse Brown and Rachel Yang for their
   contributions to data collection. This work is partially supported by
   the U.S. National Science Foundation under Grant No. IIS-2211896.
NR 72
TC 16
Z9 17
U1 3
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2489
EP 2498
DI 10.1109/TVCG.2023.3247065
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D0KD9
UT WOS:000965693300001
PM 37027706
DA 2025-03-07
ER

PT J
AU Abramson, MA
   Kent, GD
   Smith, GW
AF Abramson, Mark A.
   Kent, Griffin D.
   Smith, Gavin W.
TI Penetration Depth Between Two Convex Polyhedra: An Efficient Stochastic
   Global Optimization Approach
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Optimization; Computational modeling; Stochastic processes; Linear
   programming; Atmospheric modeling; Mathematical model; Interference;
   Constrained optimization; global optimization; nonlinear optimization;
   penetration depth; geometric modelling
ID ALGORITHM; SEARCH; DISTANCES
AB During the detailed design phase of an aerospace program, one of the most important consistency checks is to ensure that no two distinct objects occupy the same physical space. Since exact geometrical modeling is usually intractable, geometry models are discretized, which often introduces small interferences not present in the fully detailed model. In this paper, we focus on computing the depth of the interference, so that these false positive interferences can be removed, and attention can be properly focused on the actual design. Specifically, we focus on efficiently computing the penetration depth between two polyhedra, which is a well-studied problem in the computer graphics community. We formulate the problem as a constrained five-variable global optimization problem, and then derive an equivalent unconstrained, two-variable nonsmooth problem. To solve the optimization problem, we apply a popular stochastic multistart optimization algorithm in a novel way, which exploits the advantages of each problem formulation simultaneously. Numerical results for the algorithm, applied to 14 randomly generated pairs of penetrating polytopes, illustrate both the effectiveness and efficiency of the method.
C1 [Abramson, Mark A.; Kent, Griffin D.] Utah Valley Univ, Orem, UT 84058 USA.
   [Kent, Griffin D.] Lehigh Univ, Bethlehem, PA 18015 USA.
   [Smith, Gavin W.] Intel Corp, Hillsboro, OR 97124 USA.
C3 Utah System of Higher Education; Utah Valley University; Lehigh
   University; Intel Corporation; Intel USA
RP Abramson, MA (corresponding author), Utah Valley Univ, Orem, UT 84058 USA.
EM mark.bramson@uvu.edu; 10764442@my.uvu.edu; gavin.w.smith@intel.com
NR 33
TC 2
Z9 2
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4267
EP 4273
DI 10.1109/TVCG.2021.3085703
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400023
PM 34061747
DA 2025-03-07
ER

PT J
AU Ye, H
   Kwan, KC
   Fu, HB
AF Ye, Hui
   Kwan, Kin Chung
   Fu, Hongbo
TI 3D Curve Creation on and Around Physical Objects With Mobile AR
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Mobile handsets; Two dimensional displays;
   Tracking; Visualization; Cameras; Tools; Mobile augmented reality;
   In-situ 3D drawing; interactive system; curve refinement; optimization
ID SLAM
AB The recent advance in motion tracking (e.g., Visual Inertial Odometry) allows the use of a mobile phone as a 3D pen, thus significantly benefiting various mobile Augmented Reality (AR) applications based on 3D curve creation. However, when creating 3D curves on and around physical objects with mobile AR, tracking might be less robust or even lost due to camera occlusion or textureless scenes. This motivates us to study how to achieve natural interaction with minimum tracking errors during close interaction between a mobile phone and physical objects. To this end, we contribute an elicitation study on input point and phone grip, and a quantitative study on tracking errors. Based on the results, we present a system for direct 3D drawing with an AR-enabled mobile phone as a 3D pen, and interactive correction of 3D curves with tracking errors in mobile AR. We demonstrate the usefulness and effectiveness of our system for two applications: in-situ 3D drawing, and direct 3D measurement.
C1 [Ye, Hui; Kwan, Kin Chung; Fu, Hongbo] City Univ Hong Kong, Sch Creat Media, Kowloon Tong, Hong Kong, Peoples R China.
   [Kwan, Kin Chung] Univ Konstanz, D-78464 Constance, Germany.
C3 City University of Hong Kong; University of Konstanz
RP Fu, HB (corresponding author), City Univ Hong Kong, Sch Creat Media, Kowloon Tong, Hong Kong, Peoples R China.
EM huiye4-c@my.cityu.edu.hk; kinckwan@cityu.edu.hk; hongbofu@cityu.edu.hk
OI FU, Hongbo/0000-0002-0284-726X; Ye, Hui/0000-0001-9539-9920
FU Research Grants Council of the Hong Kong Special Administrative Region,
   China [CityU 11212119]; City University of Hong Kong [7005590, 7005176];
   Centre for Applied Computing and Interactive Media (ACIM) of School of
   Creative Media, CityU
FX The authors would like to thank the anonymous reviewers for the
   constructive comments. This work was supported by an unrestricted gift
   from Adobe and grants from the Research Grants Council of the Hong Kong
   Special Administrative Region, China (Project No. CityU 11212119), City
   University of Hong Kong under Grants No. 7005590 and No. 7005176, and
   the Centre for Applied Computing and Interactive Media (ACIM) of School
   of Creative Media, CityU.
NR 53
TC 8
Z9 8
U1 1
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2022
VL 28
IS 8
BP 2809
EP 2821
DI 10.1109/TVCG.2020.3049006
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2P6BI
UT WOS:000819823600001
PM 33400650
DA 2025-03-07
ER

PT J
AU Wu, E
AF Wu, Eugene
TI View Composition Algebra for Ad Hoc Comparison
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; algebra; comparison; databases
ID VISUALIZATION; QUERY
AB Comparison is a core task in visual analysis. Although there are numerous guidelines to help users design effective visualizations to aid known comparison tasks, there are few techniques available when users want to make ad hoc comparisons between marks, trends, or charts during data exploration and visual analysis. For instance, to compare voting count maps from different years, two stock trends in a line chart, or a scatterplot of country GDPs with a textual summary of the average GDP. Ideally, users can directly select the comparison targets and compare them, however what elements of a visualization should be candidate targets, which combinations of targets are safe to compare, and what comparison operations make sense? This article proposes a conceptual model that lets users compose combinations of values, marks, legend elements, and charts using a set of composition operators that summarize, compute differences, merge, and model their operands. We further define a View Composition Algebra (VCA) that is compatible with datacube-based visualizations, derive an interaction design based on this algebra that supports ad hoc visual comparisons, and illustrate its utility through several use cases.
C1 [Wu, Eugene] Columbia Univ, New York, NY 10027 USA.
C3 Columbia University
RP Wu, E (corresponding author), Columbia Univ, New York, NY 10027 USA.
EM ewu@cs.columbia.edu
NR 65
TC 1
Z9 1
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2022
VL 28
IS 6
BP 2470
EP 2485
DI 10.1109/TVCG.2022.3152515
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 0Z1CH
UT WOS:000790817100017
PM 35180082
OA Green Submitted, Bronze
DA 2025-03-07
ER

PT J
AU Chatzimparmpas, A
   Martins, RM
   Kucher, K
   Kerren, A
AF Chatzimparmpas, Angelos
   Martins, Rafael M.
   Kucher, Kostiantyn
   Kerren, Andreas
TI FeatureEnVi: Visual Analytics for Feature Engineering Using Stepwise
   Selection and Semi-Automatic Extraction Approaches
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Feature extraction; Visual analytics; Data visualization; Transforms;
   Prediction algorithms; Interviews; Speech recognition; Feature
   selection; feature extraction; feature engineering; machine learning;
   visual analytics; visualization
ID CANCER CLASSIFICATION; DISCRIMINANT-ANALYSIS; GENE SELECTION; SVM-RFE;
   MODELS; EXPLORATION; ALGORITHMS; FRAMEWORK
AB The machine learning (ML) life cycle involves a series of iterative steps, from the effective gathering and preparation of the data-including complex feature engineering processes-to the presentation and improvement of results, with various algorithms to choose from in every step. Feature engineering in particular can be very beneficial for ML, leading to numerous improvements such as boosting the predictive results, decreasing computational times, reducing excessive noise, and increasing the transparency behind the decisions taken during the training. Despite that, while several visual analytics tools exist to monitor and control the different stages of the ML life cycle (especially those related to data and algorithms), feature engineering support remains inadequate. In this paper, we present FeatureEnVi, a visual analytics system specifically designed to assist with the feature engineering process. Our proposed system helps users to choose the most important feature, to transform the original features into powerful alternatives, and to experiment with different feature generation combinations. Additionally, data space slicing allows users to explore the impact of features on both local and global scales. FeatureEnVi utilizes multiple automatic feature selection techniques; furthermore, it visually guides users with statistical evidence about the influence of each feature (or subsets of features). The final outcome is the extraction of heavily engineered features, evaluated by multiple validation metrics. The usefulness and applicability of FeatureEnVi are demonstrated with two use cases and a case study. We also report feedback from interviews with two ML experts and a visualization researcher who assessed the effectiveness of our system.
C1 [Chatzimparmpas, Angelos; Martins, Rafael M.; Kucher, Kostiantyn; Kerren, Andreas] Linnaeus Univ, Dept Comp Sci & Media Technol, S-35195 Vaxjo, Sweden.
   [Kucher, Kostiantyn; Kerren, Andreas] Linkoping Univ, Dept Sci & Technol, S-60233 Norrkoping, Sweden.
C3 Linnaeus University; Linkoping University
RP Chatzimparmpas, A (corresponding author), Linnaeus Univ, Dept Comp Sci & Media Technol, S-35195 Vaxjo, Sweden.
EM angelos.chatzimparmpas@lnu.se; rafael.martins@lnu.se;
   kostiantyn.kucher@lnu.se; andreas.kerren@lnu.se
RI Kerren, Andreas/AAV-9187-2020; Martins, Rafael/H-9192-2019
OI Chatzimparmpas, Angelos/0000-0002-9079-2376
NR 107
TC 16
Z9 16
U1 1
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2022
VL 28
IS 4
BP 1773
EP 1791
DI 10.1109/TVCG.2022.3141040
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZH9CR
UT WOS:000761227900006
PM 34990365
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Lyu, Q
   Chai, ML
   Chen, X
   Zhou, K
AF Lyu, Qing
   Chai, Menglei
   Chen, Xiang
   Zhou, Kun
TI Real-Time Hair Simulation With Neural Interpolation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Hair; Computational modeling; Interpolation; Data models; Generators;
   Shape; Neural networks; Real-time hair simulation; neural interpolator;
   generative models; computer animation; CNN; GAN
ID DYNAMICS
AB Traditionally, reduced hair simulation methods are either restricted to heuristic approximations or bound to specific hairstyles. We introduce the first CNN-integrated framework for simulating various hairstyles. The approach produces visually realistic hairs with an interactive speed. To address the technical challenges, our hair simulation pipeline is designed as a two-stage process. First, we present a fully-convolutional neural interpolator as the backbone generator to compute dynamic weights for guide hair interpolation. Then, we adopt a second generator to produce fine-scale displacements to enhance the hair details. We train the neural interpolator with a dedicated loss function and the displacement generator with an adversarial discriminator. Experimental results demonstrate that our method is effective, efficient, and superior to the state-of-the-art on a wide variety of hairstyles. We further propose a performance-driven digital avatar system and an interactive hairstyle editing tool to illustrate the practical applications.
C1 [Lyu, Qing; Chen, Xiang; Zhou, Kun] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
   [Chai, Menglei] Snap Res, Venice, CA 90291 USA.
C3 Zhejiang University
RP Chen, X (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM lyuqing@zju.edu.cn; cmlatsim@gmail.com; xchen.cs@gmail.com;
   kunzhou@acm.org
RI Zhou, Kun/ITT-3967-2023
OI Chen, Xiang/0000-0002-6955-8729
FU National Natural Science Foundation of China [61772024, 61732016,
   61890954]
FX The authors would like to thank the anonymous reviewers for their
   constructive comments. This work was supported in part by the National
   Natural Science Foundation of China (under Grant Nos. 61772024,
   61732016, and 61890954).
NR 47
TC 4
Z9 4
U1 0
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2022
VL 28
IS 4
BP 1894
EP 1905
DI 10.1109/TVCG.2020.3029823
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZH9CR
UT WOS:000761227900014
PM 33044934
DA 2025-03-07
ER

PT J
AU Ferrari, V
   Cattari, N
   Fontana, U
   Cutolo, F
AF Ferrari, Vincenzo
   Cattari, Nadia
   Fontana, Umberto
   Cutolo, Fabrizio
TI Parallax Free Registration for Augmented Reality Optical See-Through
   Displays in the Peripersonal Space
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Augmented reality; optical see-through; registration
AB Egocentric augmented reality (AR) interfaces are quickly becoming a key asset for assisting high precision activities in the peripersonal space in several application fields. In these applications, accurate and robust registration of computer-generated information to the real scene is hard to achieve with traditional Optical See-Through (OST) displays given that it relies on the accurate calibration of the combined eye-display projection model. The calibration is required to efficiently estimate the projection parameters of the pinhole model that encapsulate the optical features of the display and whose values vary according to the position of the user's eye. In this article, we describe an approach that prevents any parallax-related AR misregistration at a pre-defined working distance in OST displays with infinity focus; our strategy relies on the use of a magnifier placed in front of the OST display, and features a proper parameterization of the virtual rendering camera achieved through a dedicated calibration procedure that accounts for the contribution of the magnifier. We model the registration error due to the viewpoint parallax outside the ideal working distance. Finally, we validate our strategy on a OST display, and we show that sub-millimetric registration accuracy can be achieved for working distances of +/- 100mm around the focal length of the magnifier.
C1 [Ferrari, Vincenzo; Fontana, Umberto; Cutolo, Fabrizio] Univ Pisa, Informat Engn Dept, Via G Caruso 16, I-56122 Pisa, Italy.
   [Ferrari, Vincenzo; Cutolo, Fabrizio] Univ Pisa, EndoCAS Ctr, Dept Translat Res & New Technol Med & Surg, I-56124 Pisa, Italy.
   [Cattari, Nadia] Univ Pisa, Dept Translat Res & New Technol Med & Surg, I-56124 Pisa, Italy.
C3 University of Pisa; University of Pisa; University of Pisa
RP Ferrari, V (corresponding author), Univ Pisa, Informat Engn Dept, Via G Caruso 16, I-56122 Pisa, Italy.; Ferrari, V (corresponding author), Univ Pisa, EndoCAS Ctr, Dept Translat Res & New Technol Med & Surg, I-56124 Pisa, Italy.
EM vincenzo.ferrari@unipi.it; nadia.cattari@endocas.unipi.it;
   umbertofontana93@gmail.com; fabrizio.cutolo@endocas.unipi.it
RI Cattari, Nadia/AAC-3748-2022; Ferrari, Vincenzo/H-9908-2015
OI Fontana, Umberto/0000-0002-8353-7051; Ferrari,
   Vincenzo/0000-0001-9294-2828; Cutolo, Fabrizio/0000-0001-6773-3741;
   Cattari, Nadia/0000-0002-7091-2908
FU HORIZON2020 Project VOSTARS (Video-Optical See Through AR surgical
   System) [731974, ICT-29-2016 Photonics KET 2016]; Italian Ministry of
   Education and Research (MIUR) within CrossLab project (Departments of
   Excellence) of the University of Pisa, laboratory of Augmented Reality
FX This research was supported in part by the HORIZON2020 Project VOSTARS
   (Video-Optical See Through AR surgical System), Project ID: 731974.
   Call: ICT-29-2016 Photonics KET 2016. Thisworkwas also supported in part
   by the Italian Ministry of Education and Research (MIUR) within the
   framework of the CrossLab project (Departments of Excellence) of the
   University of Pisa, laboratory of Augmented Reality.
NR 36
TC 15
Z9 16
U1 0
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2022
VL 28
IS 3
BP 1608
EP 1618
DI 10.1109/TVCG.2020.3021534
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP1EJ
UT WOS:000748371200012
PM 32881688
DA 2025-03-07
ER

PT J
AU Kale, A
   Wu, YF
   Hultman, J
AF Kale, Alex
   Wu, Yifan
   Hultman, Jessica
TI Causal Support: Modeling Causal Inferences with Visualizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Data models; Diseases; Cognition; Bars; Analytical
   models; Benchmark testing; Causal inference; visualization; contingency
   tables; data cognition
ID EXPLORATORY DATA-ANALYSIS; REPRESENTATION; PROBABILITY; STRATEGIES;
   JUDGMENTS
AB Analysts often make visual causal inferences about possible data-generating models. However, visual analytics (VA) software tends to leave these models implicit in the mind of the analyst, which casts doubt on the statistical validity of informal visual "insights". We formally evaluate the quality of causal inferences from visualizations by adopting causal support-a Bayesian cognition model that learns the probability of alternative causal explanations given some data-as a normative benchmark for causal inferences. We contribute two experiments assessing how well crowdworkers can detect (1) a treatment effect and (2) a confounding relationship. We find that chart users' causal inferences tend to be insensitive to sample size such that they deviate from our normative benchmark. While interactively cross-filtering data in visualizations can improve sensitivity, on average users do not perform reliably better with common visualizations than they do with textual contingency tables. These experiments demonstrate the utility of causal support as an evaluation framework for inferences in VA and point to opportunities to make analysts' mental models more explicit in VA software.
C1 [Kale, Alex] Univ Washington, Seattle, WA 98195 USA.
   [Wu, Yifan] Univ Calif Berkeley, Berkeley, CA 94720 USA.
   [Hultman, Jessica] Northwestern Univ, Evanston, IL 60208 USA.
C3 University of Washington; University of Washington Seattle; University
   of California System; University of California Berkeley; Northwestern
   University
RP Kale, A (corresponding author), Univ Washington, Seattle, WA 98195 USA.
EM kalea@uw.edu; yifanwu@berkeley.edu; jhullman@northwestern.edu
FU NSF [1930642]; Div Of Information & Intelligent Systems; Direct For
   Computer & Info Scie & Enginr [1930642] Funding Source: National Science
   Foundation
FX We thank the UW IDL and the NU MU Collective for their feedback. We
   thank NSF (#1930642) for funding this work.
NR 61
TC 10
Z9 11
U1 2
U2 29
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 1150
EP 1160
DI 10.1109/TVCG.2021.3114824
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA XW3DW
UT WOS:000735505300016
PM 34587057
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Narechania, A
   Coscia, A
   Wall, E
   Endert, A
AF Narechania, Arpit
   Coscia, Adam
   Wall, Emily
   Endert, Alex
TI Lumos: Increasing Awareness of Analytic Behavior during Visual Data
   Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Data analysis; Measurement; History;
   Hidden Markov models; Data models; visual data analysis; interaction
   traces; analytic provenance; awareness; human bias
ID VISUALIZATION; INFORMATION
AB Visual data analysis tools provide people with the agency and flexibility to explore data using a variety of interactive functionalities. However, this flexibility may introduce potential consequences in situations where users unknowingly overemphasize or underemphasize specific subsets of the data or attribute space they are analyzing. For example, users may overemphasize specific attributes and/or their values (e.g., Gender is always encoded on the X axis), underemphasize others (e.g., Religion is never encoded), ignore a subset of the data (e.g., older people are filtered out), etc. In response, we present Lumos, a visual data analysis tool that captures and shows the interaction history with data to increase awareness of such analytic behaviors. Using in-situ (at the place of interaction) and ex-situ (in an external view) visualization techniques, Lumos provides real-time feedback to users for them to reflect on their activities. For example, Lumos highlights datapoints that have been previously examined in the same visualization (in-situ) and also overlays them on the underlying data distribution (i.e., baseline distribution) in a separate visualization (ex-situ). Through a user study with 24 participants, we investigate how Lumos helps users' data exploration and decision-making processes. We found that Lumos increases users' awareness of visual data analysis practices in real-time, promoting reflection upon and acknowledgement of their intentions and potentially influencing subsequent interactions.
C1 [Narechania, Arpit; Coscia, Adam; Endert, Alex] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Wall, Emily] Emory Univ, Atlanta, GA 30322 USA.
   Northwestern Univ, Evanston, IL 60208 USA.
C3 University System of Georgia; Georgia Institute of Technology; Emory
   University; Northwestern University
RP Narechania, A (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM arpitnarechania@gatech.edu; acoscia6@gatech.edu; emily.wall@emory.edu;
   endert@gatech.edu
RI ; Coscia, Adam/GUO-9221-2022
OI Narechania, Arpit Ajay/0000-0001-6980-3686; Coscia,
   Adam/0000-0002-0429-9295
FU National Science Foundation [IIS-1813281]; Siemens FutureMaker
   Fellowship
FX This work was supported in part by the National Science Foundation grant
   IIS-1813281 and the Siemens FutureMaker Fellowship. We thank the
   reviewers for their constructive feedback during the review phase. We
   also thank the Georgia Tech Visualization Lab for their feedback.
NR 38
TC 8
Z9 10
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 1009
EP 1018
DI 10.1109/TVCG.2021.3114827
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA XW3DW
UT WOS:000735505300013
PM 34587059
OA Green Submitted, Bronze
DA 2025-03-07
ER

PT J
AU Zhao, Y
   Zhang, J
   Fu, CW
   Xu, ML
   Moritz, D
   Wang, YH
AF Zhao, Yue
   Zhang, Jian
   Fu, Chi-Wing
   Xu, Mingliang
   Moritz, Dominik
   Wang, Yunhai
TI KD-Box: Line-segment-based KD-tree for Interactive Exploration of
   Large-scale Time-Series Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Time series analysis; Visualization; Rendering
   (computer graphics); Market research; Clutter; Three-dimensional
   displays; Many time series; density-based visualization; interactive
   visualization for large-scale data
ID VISUAL ANALYSIS; MEAN SHIFT; VISUALIZATION; EDGE; PLOTS
AB Time-series data-usually presented in the form of lines-plays an important role in many domains such as finance, meteorology, health, and urban informatics. Yet, little has been done to support interactive exploration of large-scale time-series data, which requires a clutter-free visual representation with low-latency interactions. In this paper, we contribute a novel line-segment-based KD-tree method to enable interactive analysis of many time series. Our method enables not only fast queries over time series in selected regions of interest but also a line splatting method for efficient computation of the density field and selection of representative lines. Further, we develop KD-Box, an interactive system that provides rich interactions, e.g., timebox, attribute filtering, and coordinated multiple views. We demonstrate the effectiveness of KD-Box in supporting efficient line query and density field computation through a quantitative comparison and show its usefulness for interactive visual analysis on several real-world datasets.
C1 [Zhao, Yue; Wang, Yunhai] Shandong Univ, Qingdao, Peoples R China.
   [Zhang, Jian] Chinese Acad Sci, CNIC, Beijing, Peoples R China.
   [Fu, Chi-Wing] Chinese Univ Hong Kong, Hong Kong, Peoples R China.
   [Xu, Mingliang] Zhengzhou Univ, Zhengzhou, Peoples R China.
   [Moritz, Dominik] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.
C3 Shandong University; Chinese Academy of Sciences; Chinese University of
   Hong Kong; Zhengzhou University; Carnegie Mellon University
RP Wang, YH (corresponding author), Shandong Univ, Qingdao, Peoples R China.
EM jack.zhao9802@gmail.com; zhangjian@sccas.cn; cwfu@cse.cuhk.edu.hk;
   iexumingliang@zzu.edu.cn; domoritz@cmu.edu; cloudseawang@gmail.com
RI Fu, Chi-Wing/X-4703-2019
OI Zhao, Yue/0000-0003-0365-5291; Moritz, Dominik/0000-0002-3110-1053
NR 58
TC 22
Z9 23
U1 7
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 890
EP 900
DI 10.1109/TVCG.2021.3114865
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000091
PM 34587082
DA 2025-03-07
ER

PT J
AU Chakravarthula, P
   Zhang, Z
   Tursun, O
   Didyk, P
   Sun, Q
   Fuchs, H
AF Chakravarthula, Praneeth
   Zhang, Zhan
   Tursun, Okan
   Didyk, Piotr
   Sun, Qi
   Fuchs, Henry
TI Gaze-Contingent Retinal Speckle Suppression for Perceptually-Matched
   Foveated Holographic Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Speckle; Retina; Image quality; Holography; Phase modulation; Image
   reconstruction; Visualization; Holograms; foveated rendering; near-eye
   immersive displays
ID PHASE-RETRIEVAL ALGORITHMS; GANGLION-CELLS; RESOLUTION; PROJECTION;
   RANGE; IMAGE
AB Computer-generated holographic (CGH) displays show great potential and are emerging as the next-generation displays for augmented and virtual reality, and automotive heads-up displays. One of the critical problems harming the wide adoption of such displays is the presence of speckle noise inherent to holography, that compromises its quality by introducing perceptible artifacts. Although speckle noise suppression has been an active research area, the previous works have not considered the perceptual characteristics of the Human Visual System (HVS), which receives the final displayed imagery. However, it is well studied that the sensitivity of the HVS is not uniform across the visual field, which has led to gaze-contingent rendering schemes for maximizing the perceptual quality in various computer-generated imagery. Inspired by this, we present the first method that reduces the "perceived speckle noise" by integrating foveal and peripheral vision characteristics of the HVS, along with the retinal point spread function, into the phase hologram computation. Specifically, we introduce the anatomical and statistical retinal receptor distribution into our computational hologram optimization, which places a higher priority on reducing the perceived foveal speckle noise while being adaptable to any individual's optical aberration on the retina. Our method demonstrates superior perceptual quality on our emulated holographic display. Our evaluations with objective measurements and subjective studies demonstrate a significant reduction of the human perceived noise.
C1 [Chakravarthula, Praneeth; Fuchs, Henry] Univ N Carolina, Chapel Hill, NC 27515 USA.
   [Zhang, Zhan] Univ Sci & Technol China, Hefei, Peoples R China.
   [Tursun, Okan; Didyk, Piotr] Univ Svizzera Italiana, Lugano, Switzerland.
   [Sun, Qi] NYU, New York, NY 10003 USA.
C3 University of North Carolina; University of North Carolina Chapel Hill;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Universita della Svizzera Italiana; New York University
RP Chakravarthula, P (corresponding author), Univ N Carolina, Chapel Hill, NC 27515 USA.
EM cpk@cs.unc.edu; whirlwind@mail.ustc.edu.cn; tursuo@usi.ch;
   piotr.didyk@usi.ch; qisun@nyu.edu; fuchs@cs.unc.edu
RI Sun, Qi/AGY-5791-2022; Chakravarthula, Praneeth Kumar/AFZ-2211-2022
OI Didyk, Piotr/0000-0003-0768-8939; /0000-0002-9729-5460; Zhang,
   Zhan/0009-0006-2611-4809
FU European Research Council (ERC) under the European Union's Horizon 2020
   research and innovation program [804226 - PERDY]; NSF [1840131,
   1405847]; Div Of Civil, Mechanical, & Manufact Inn; Directorate For
   Engineering [1840131] Funding Source: National Science Foundation
FX This work is supported by the European Research Council (ERC) under the
   European Union's Horizon 2020 research and innovation program (grant
   agreement No 804226 -PERDY), NSF grants 1840131 and 1405847, and a
   generous gift from Intel.
NR 63
TC 23
Z9 23
U1 5
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2021
VL 27
IS 11
BP 4194
EP 4203
DI 10.1109/TVCG.2021.3106433
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WN2ZT
UT WOS:000711642700013
PM 34449368
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Lv, XQ
   Wang, X
   Wang, Q
   Yu, JY
AF Lv, Xianqiang
   Wang, Xue
   Wang, Qing
   Yu, Jingyi
TI 4D Light Field Segmentation From Light Field Super-Pixel Hypergraph
   Representation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image segmentation; Optimization; Light fields; Estimation; Cameras; Two
   dimensional displays; Task analysis; Hypergraph representation; light
   field segmentation; light field super-pixel; graph-cut optimization
ID ENERGY MINIMIZATION; IMAGE
AB Efficient and accurate segmentation of full 4D light fields is an important task in computer vision and computer graphics. The massive volume and the redundancy of light fields make it an open challenge. In this article, we propose a novel light field hypergraph (LFHG) representation using the light field super-pixel (LFSP) for interactive light field segmentation. The LFSPs not only maintain the light field spatio-angular consistency, but also greatly contribute to the hypergraph coarsening. These advantages make LFSPs useful to improve segmentation performance. Based on the LFHG representation, we present an efficient light field segmentation algorithm via graph-cut optimization. Experimental results on both synthetic and real scene data demonstrate that our method outperforms state-of-the-art methods on the light field segmentation task with respect to both accuracy and efficiency.
C1 [Lv, Xianqiang; Wang, Xue; Wang, Qing] Northwestern Potytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
   [Yu, Jingyi] Shanghai Tech Univ, Sch Informat Sci & Technol, Virtual Real & Visual Comp Ctr, Shanghai 200031, Peoples R China.
C3 ShanghaiTech University
RP Wang, Q (corresponding author), Northwestern Potytech Univ, Sch Comp Sci, Xian 710072, Peoples R China.
EM xianqianglv@mail.nwpu.edu.cn; xwang@nwpu.edu.cn; qwang@nwpu.edu.cn;
   jingyi.udel@gmail.com
OI Wang, Qing/0000-0003-3439-0644; Wang, Xue/0009-0003-5224-906X
FU NSFC [61531014, 61801396]
FX The work was supported by NSFC under Grants 61531014 and 61801396. The
   authors would like to thank H. Mihara and T. Funatomi for their help on
   real scene light field segmentation comparisons. We thank M. Rizkallah's
   team for their help in preprocessing real data using the method proposed
   in [17].
NR 38
TC 23
Z9 24
U1 12
U2 55
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2021
VL 27
IS 9
BP 3597
EP 3610
DI 10.1109/TVCG.2020.2982158
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TS3CP
UT WOS:000679532000001
PM 32203022
DA 2025-03-07
ER

PT J
AU Li, W
   Liu, DM
   Desbrun, M
   Huang, J
   Liu, XP
AF Li, Wei
   Liu, Daoming
   Desbrun, Mathieu
   Huang, Jin
   Liu, Xiaopei
TI Kinetic-Based Multiphase Flow Simulation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computational modeling; Kinetic theory; Mathematical model; Atmospheric
   modeling; Numerical models; Solid modeling; Visualization; Multiphase
   flow; kinetic theory; phase-field lattice Boltzmann model; interface
   phenomena
ID LATTICE-BOLTZMANN METHOD; ANIMATION; MODEL; WATER
AB Multiphase flows exhibit a large realm of complex behaviors such as bubbling, glugging, wetting, and splashing which emerge from air-water and water-solid interactions. Current fluid solvers in graphics have demonstrated remarkable success in reproducing each of these visual effects, but none have offered a model general enough to capture all of them concurrently. In contrast, computational fluid dynamics have developed very general approaches to multiphase flows, typically based on kinetic models. Yet, in both communities, there is dearth of methods that can simulate density ratios and Reynolds numbers required for the type of challenging real-life simulations that movie productions strive to digitally create, such as air-water flows. In this article, we propose a kinetic model of the coupling of the Navier-Stokes equations with a conservative phase-field equation, and provide a series of numerical improvements over existing kinetic-based approaches to offer a general multiphase flow solver. The resulting algorithm is embarrassingly parallel, conservative, far more stable than current solvers even for real-life conditions, and general enough to capture the typical multiphase flow behaviors. Various simulation results are presented, including comparisons to both previous work and real footage, to highlight the advantages of our new method.
C1 [Li, Wei; Liu, Daoming; Desbrun, Mathieu; Liu, Xiaopei] ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China.
   [Li, Wei] Chinese Acad Sci, Shanghai Inst Microsyst & Informat Technol, Beijing 100049, Peoples R China.
   [Li, Wei] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Huang, Jin] Zhejiang Univ, Coll Comp Sci, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
C3 ShanghaiTech University; Chinese Academy of Sciences; Shanghai Institute
   of Microsystem & Information Technology, CAS; Chinese Academy of
   Sciences; University of Chinese Academy of Sciences, CAS; Zhejiang
   University
RP Liu, XP (corresponding author), ShanghaiTech Univ, Sch Informat Sci & Technol, Shanghai 201210, Peoples R China.
EM liwei@shanghaitech.edu.cn; liudm@shanghaitech.edu.cn;
   mathieu@cms.caltech.edu; hj@cad.zju.edu.cn; liuxp@shanghaitech.edu.cn
RI Liu, Daoming/KXR-2662-2024; Desbrun, Mathieu/AAG-9974-2021
OI Desbrun, Mathieu/0000-0003-3424-6079; Liu, Daoming/0000-0002-2091-7081
FU National Natural Science Foundation of China [61502305]; ShanghaiTech
   University
FX The authors would like to thank Dr. Ryoichi Ando from National Institute
   of Informatics, Tokyo, Japan, for sharing his fluid simulation codes for
   comparison, Jinglei Yang from the University of California, Santa
   Barbara, for her early help on rendering, as well as Yiran Sun and
   Chaoyang Lyu from ShanghaiTech University for rendering and figures.
   This work was supported by the Young Scientists Fund of the National
   Natural Science Foundation of China (Grant No. 61502305), as well as
   startup funds from ShanghaiTech University. They also thank the
   reviewers for their comments.
NR 100
TC 12
Z9 13
U1 3
U2 47
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2021
VL 27
IS 7
BP 3318
EP 3334
DI 10.1109/TVCG.2020.2972357
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SK0PE
UT WOS:000655924400017
PM 32054580
DA 2025-03-07
ER

PT J
AU Park, JH
   Nadeem, S
   Boorboor, S
   Marino, J
   Kaufman, A
AF Park, Ji Hwan
   Nadeem, Saad
   Boorboor, Saeed
   Marino, Joseph
   Kaufman, Arie
TI CMed: Crowd Analytics for Medical Imaging Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Crowdsourcing; Biomedical imaging; Data visualization; Task analysis;
   Visual analytics; Lung; Computed tomography; Crowdsourcing; medical
   imaging; virtual colonoscopy; lung nodules; visual analytics
ID VIDEO
AB We present a visual analytics framework, CMed, for exploring medical image data annotations acquired from crowdsourcing. CMed can be used to visualize, classify, and filter crowdsourced clinical data based on a number of different metrics such as detection rate, logged events, and clustering of the annotations. CMed provides several interactive linked visualization components to analyze the crowd annotation results for a particular video and the associated workers. Additionally, all results of an individual worker can be inspected using multiple linked views in our CMed framework. We allow a crowdsourcing application analyst to observe patterns and gather insights into the crowdsourced medical data, helping him/her design future crowdsourcing applications for optimal output from the workers. We demonstrate the efficacy of our framework with two medical crowdsourcing studies: polyp detection in virtual colonoscopy videos and lung nodule detection in CT thin-slab maximum intensity projection videos. We also provide experts' feedback to show the effectiveness of our framework. Lastly, we share the lessons we learned from our framework with suggestions for integrating our framework into a clinical workflow.
C1 [Park, Ji Hwan] Brookhaven Natl Lab, Upton, NY 11973 USA.
   [Nadeem, Saad] Mem Sloan Kettering Canc Ctr, 1275 York Ave, New York, NY 10021 USA.
   [Boorboor, Saeed; Marino, Joseph; Kaufman, Arie] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 United States Department of Energy (DOE); Brookhaven National
   Laboratory; Memorial Sloan Kettering Cancer Center; State University of
   New York (SUNY) System; Stony Brook University
RP Park, JH (corresponding author), Brookhaven Natl Lab, Upton, NY 11973 USA.
EM parkj@bnl.gov; nadeems@mskcc.org; sboorboor@cs.stonybrook.edu;
   jmarino@cs.stonybrook.edu; ari@cs.stonybrook.edu
RI Boorboor, Saeed/ABI-7739-2020
OI Park, Ji Hwan/0000-0002-7971-2419; Boorboor, Saeed/0000-0001-6644-5983;
   Kaufman, Arie/0000-0002-0796-6196
FU US National Science Foundation [NRT1633299, CNS1650499, OAC1919752];
   Marcus Foundation
FX The VC datasets are courtesy of Stony Brook University Hospital (SBUH)
   and Dr. Richard Choi, Walter Reed Army Medical Center. The lung datasets
   are courtesy of Lung Image Database Consortium. We would like to thank
   Dr. Matthew Barish and Dr. Kevin Baker of SBUH for their help in this
   project. This work has been partially supported by the US National
   Science Foundation Grants NRT1633299, CNS1650499, and OAC1919752, and
   the Marcus Foundation.
NR 42
TC 6
Z9 9
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2021
VL 27
IS 6
BP 2869
EP 2880
DI 10.1109/TVCG.2019.2953026
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SA9KC
UT WOS:000649620700008
PM 31751242
OA Green Accepted, Bronze, Green Submitted
DA 2025-03-07
ER

PT J
AU Jung, S
   Li, R
   McKee, R
   Whitton, MC
   Lindeman, RW
AF Jung, Sungchul
   Li, Richard
   McKee, Ryan
   Whitton, Mary C.
   Lindeman, Robert W.
TI Floor-vibration VR: Mitigating Cybersickness Using Whole-body Tactile
   Stimuli in Highly Realistic Vehicle Driving Experiences
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Cybersickness; motion sickness; simulator sickness; immersive virtual
   reality; floor-vibration; whole-body tactile; tactile; vibration; floor;
   reducing cybersickness; mitigating cybersickness
ID MOTION SICKNESS; VIRTUAL-REALITY; SIMULATOR SICKNESS
AB This work addresses cybersickness, a major barrier to successful long-exposure immersive virtual reality (VR) experiences since user discomfort frequently leads to prematurely ending such experiences. Starting from sensory conflict theory, we posit that if a vibrating floor delivers vestibular stimuli that minimally match the vibration characteristics of a scenario, the size of the conflict between the visual and vestibular senses will be reduced and, thus, the incidence and/or severity of cybersickness will also be reduced. We integrated a custom-built, computer-controlled vibrating floor in our VR system. To evaluate the system, we implemented a realistic off-road vehicle driving simulator in which participants rode multiple laps as passengers on an off-road course. We programmed the floor to generate vertical vibrations similar to those experienced in real off-road vehicle travel. The scenario and driving conditions were designed to be cybersickness-inducing for users in both the Vibration and No-vibration conditions. We collected subjective and objective data for variables previously shown to be related to levels of cybersickness or presence. These included presence and simulator sickness questionnaires (SSQ), self-rated discomfort levels, and the physiological signals of heart rate, galvanic skin response (GSR), and pupil size. Comparing data between participants in the Vibration group (N=11) to the No-Vibration group (N=11), we found that Delta-SSQ Oculomotor response and the GSR physiological signal, both known to be positively correlated with cybersickness, were significantly lower (with large effect sizes) for the Vibration group. Other variables differed between groups in the same direction, but with trivial or small effect sizes. The results indicate that the floor vibration significantly reduced some measures of cybersickness.
C1 [Jung, Sungchul; Li, Richard; McKee, Ryan; Lindeman, Robert W.] Univ Canterbury, HIT Lab NZ, Christchurch, New Zealand.
   [Whitton, Mary C.] Univ N Carolina, Chapel Hill, NC 27515 USA.
C3 University of Canterbury; University of North Carolina; University of
   North Carolina Chapel Hill
RP Jung, S (corresponding author), Univ Canterbury, HIT Lab NZ, Christchurch, New Zealand.
EM sungchul.jung@canterbury.ac.nz; richard.li@canterbury.ac.nz;
   ryan.mckee@canterbury.ac.nz; whitton@cs.unc.edu; gogo@hitlabnz.org
RI Lindeman, Rob/AAG-8857-2020; Li, Richard/ITV-0170-2023; Whitton,
   Mary/AAN-2378-2021
OI Li, Chen/0000-0002-3782-0737; Jung, Sungchul/0000-0003-3633-7767
NR 58
TC 34
Z9 36
U1 0
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2021
VL 27
IS 5
BP 2669
EP 2680
DI 10.1109/TVCG.2021.3067773
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA SR5YU
UT WOS:000661120200016
PM 33760736
OA hybrid
DA 2025-03-07
ER

PT J
AU Garcia, G
   Silveira, J
   Poco, J
   Paiva, A
   Nery, MB
   Silva, CT
   Adorno, S
   Nonato, LG
AF Garcia, Germain
   Silveira, Jaqueline
   Poco, Jorge
   Paiva, Afonso
   Nery, Marcelo Batista
   Silva, Claudio T.
   Adorno, Sergio
   Nonato, Luis Gustavo
TI CrimAnalyzer: Understanding Crime Patterns in Sao Paulo
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Crime data; spatio-temporal data; visual analytics; non-negative matrix
   factorization
AB Sao Paulo is the largest city in South America, with crime rates that reflect its size. The number and type of crimes vary considerably around the city, assuming different patterns depending on urban and social characteristics of each particular location. Previous works have mostly focused on the analysis of crimes with the intent of uncovering patterns associated to social factors, seasonality, and urban routine activities. Therefore, those studies and tools are more global in the sense that they are not designed to investigate specific regions of the city such as particular neighborhoods, avenues, or public areas. Tools able to explore specific locations of the city are essential for domain experts to accomplish their analysis in a bottom-up fashion, revealing how urban features related to mobility, passersby behavior, and presence of public infrastructures (e.g., terminals of public transportation and schools) can influence the quantity and type of crimes. In this paper, we present CrimAnalyzer, a visual analytic tool that allows users to study the behavior of crimes in specific regions of a city. The system allows users to identify local hotspots and the pattern of crimes associated to them, while still showing how hotspots and corresponding crime patterns change over time. CrimAnalyzer has been developed from the needs of a team of experts in criminology and deals with three major challenges: i) flexibility to explore local regions and understand their crime patterns, ii) identification of spatial crime hotspots that might not be the most prevalent ones in terms of the number of crimes but that are important enough to be investigated, and iii) understand the dynamic of crime patterns over time. The effectiveness and usefulness of the proposed system are demonstrated by qualitative and quantitative comparisons as well as by case studies run by domain experts involving real data. The experiments show the capability of CrimAnalyzer in identifying crime-related phenomena.
C1 [Garcia, Germain; Silveira, Jaqueline; Paiva, Afonso; Nonato, Luis Gustavo] Univ Sao Paulo, Inst Ciencias Matemat & Comp, BR-13566590 Sao Carlos, Brazil.
   [Poco, Jorge] Fundacao Getulio Vargas, Sch Appl Math, Sao Paulo, SP, Brazil.
   [Poco, Jorge] Univ Catolica San Pablo, Arequipa 04001, Peru.
   [Nery, Marcelo Batista] RIDC FAPESP, Ctr Study Violence, Sao Paulo, SP, Brazil.
   [Nery, Marcelo Batista] Inst Adv Studies, Global Cities Program, Sao Paulo, SP, Brazil.
   [Silva, Claudio T.] NYU, Comp Sci & Engn & Data Sci, New York, NY 10003 USA.
   [Adorno, Sergio] Univ Sao Paulo, Nucleo Estudos Violencia, BR-05508900 Sao Paulo, Brazil.
   [Nonato, Luis Gustavo] NYU, Ctr Data Sci, New York, NY 10003 USA.
C3 Universidade de Sao Paulo; Getulio Vargas Foundation; Universidad
   Catolica San Pablo; New York University; Universidade de Sao Paulo; New
   York University
RP Garcia, G (corresponding author), Univ Sao Paulo, Inst Ciencias Matemat & Comp, BR-13566590 Sao Carlos, Brazil.
EM germaingarcia@usp.br; alva.jaque@usp.br; jorge.poco@fgv.br;
   apneto@icmc.usp.br; mbnery@gmail.com; csilva@nyu.edu; marsadorno@usp.br;
   gnonato@icmc.usp.br
RI Nery, Marcelo/JAN-6213-2023; Adorno, Sergio/J-6106-2015; GARCIA,
   GERMAIN/MDT-1071-2025; Nonato, Luis/D-5782-2011; Batista Nery,
   Marcelo/J-7323-2015; Poco, Jorge/F-3344-2016; Paiva, Afonso/E-2593-2011;
   Alvarenga Silveira, Jaqueline/A-8225-2017
OI Batista Nery, Marcelo/0000-0003-0299-8479; Silva,
   Claudio/0000-0003-2452-2295; GARCIA-ZANABRIA,
   GERMAIN/0000-0003-3266-9043; Poco, Jorge/0000-0001-9096-6287; Paiva,
   Afonso/0000-0001-8229-3385; Alvarenga Silveira,
   Jaqueline/0000-0002-3106-7027
FU CNPq-Brazil [302643/2013-3, 301642/2017-63]; CAPES-Brazil [10242771];
   Sao Paulo Research Foundation (FAPESP)Brazil [2014/12236-1,
   2016/04391-2, 2017/05416-1]; NASA; NSF [CNS-1229185, CCF1533564,
   CNS-1544753, CNS-1730396, CNS-1828576]; DARPA; Moore-Sloan Data Science
   Environment at NYU; Fundacao de Amparo a Pesquisa do Estado de Sao Paulo
   (FAPESP) [16/04391-2, 14/12236-1, 17/05416-1] Funding Source: FAPESP;
   Swedish Research Council [2017-05416] Funding Source: Swedish Research
   Council; Vinnova [2017-05416] Funding Source: Vinnova
FX This work was supported by CNPq-Brazil (Grants #302643/2013-3 and
   #301642/2017-63), CAPES-Brazil (grants #10242771), and Sao Paulo
   Research Foundation (FAPESP)Brazil (Grants#2014/12236-1, #2016/04391-2
   and #2017/054161). The views expressed are those of the authors and do
   not reflect the official policy or position of the Sao Paulo Research
   Foundation. We also thanks Intel for making available part of the
   computational resources we use in the development of this work. Silva is
   funded in part by: theMoore-Sloan Data Science Environment atNYU; NASA;
   NSF awards CNS-1229185, CCF1533564, CNS-1544753, CNS-1730396,
   CNS-1828576, and DARPA. Any opinions, findings, and conclusions or
   recommendations expressed in this material are those of the authors and
   do not necessarily reflect the views of DARPA.
NR 56
TC 21
Z9 22
U1 3
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2021
VL 27
IS 4
BP 2313
EP 2328
DI 10.1109/TVCG.2019.2947515
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA QO8XL
UT WOS:000623420400005
PM 31634135
OA Bronze, Green Submitted
DA 2025-03-07
ER

PT J
AU Marques, R
   Bouville, C
   Bouatouch, K
   Blat, J
AF Marques, Ricardo
   Bouville, Christian
   Bouatouch, Kadi
   Blat, Josep
TI Extensible Spherical Fibonacci Grids
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Lattices; Rendering (computer graphics); Vector quantization;
   Measurement; Azimuth; Indexes; Spherical quasi-Monte Carlo; low
   discrepancy spherical point sets; adaptive sampling; rendering equation
AB Spherical Fibonacci grids (SFG) yield extremely uniform point set distributions on the sphere. This feature makes SFGs particularly well-suited to a wide range of computer graphics applications, from numerical integration, to vector quantization, among others. However, the application of SFGs to problems in which further refinement of an initial point set is required is currently not possible. This is because there is currently no solution to the problem of adding new points to an existing SFG while maintaining the point set properties. In this work, we fill this gap by proposing the extensible spherical Fibonacci grids (E-SFG). We start by carrying out a formal analysis of SFGs to identify the properties which make these point sets exhibit a nearly-optimal uniform spherical distribution. Then, we propose an algorithm (E-SFG) to extend the original point set while preserving these properties. Finally, we compare the E-SFG with a other extensible spherical point sets. Our results show that the E-SFG outperforms spherical point sets based on a low discrepancy sequence both in terms of spherical cap discrepancy and in terms of root mean squared error for evaluating the rendering integral.
C1 [Marques, Ricardo; Blat, Josep] Univ Pompeu Fabra, Grp Recerca Tecnol Interact, Barcelona 08002, Spain.
   [Bouville, Christian; Bouatouch, Kadi] Univ Rennes 1, IRISA, F-35000 Rennes, France.
C3 Pompeu Fabra University; Universite de Rennes
RP Marques, R (corresponding author), Univ Pompeu Fabra, Grp Recerca Tecnol Interact, Barcelona 08002, Spain.
EM ricardo.marques@upf.edu; christian.bouville@irisa.fr;
   kadi.bouatouch@irisa.fr; josep.blat@upf.edu
RI ; Blat, Josep/J-2178-2015; Marques, Ricardo/M-5451-2015
OI Bouville, Christian/0000-0001-5884-9849; Blat,
   Josep/0000-0002-5308-475X; Marques, Ricardo/0000-0001-8261-4409
FU European Union's Horizon 2020 research programme through a Marie
   Sklodowska-Curie Individual Fellowship [707027]; Marie Curie Actions
   (MSCA) [707027] Funding Source: Marie Curie Actions (MSCA)
FX Ricardo Marques was supported by the European Union's Horizon 2020
   research programme through a Marie Sklodowska-Curie Individual
   Fellowship (Grant number 707027).
NR 29
TC 4
Z9 5
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2021
VL 27
IS 4
BP 2341
EP 2354
DI 10.1109/TVCG.2019.2952131
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO8XL
UT WOS:000623420400007
PM 31722480
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Zhang, SY
   Han, ZZ
   Lai, YK
   Zwicker, M
   Zhang, H
AF Zhang, Suiyun
   Han, Zhizhong
   Lai, Yu-Kun
   Zwicker, Matthias
   Zhang, Hui
TI Active Arrangement of Small Objects in 3D Indoor Scenes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Shape; Solid modeling; Learning systems;
   Computer graphics; Data mining; Neural networks; 3D object layout;
   active learning; scene enrichment; computer-aided aesthetic design;
   human computer interaction
AB Small object arrangement is very important for creating detailed and realistic 3D indoor scenes. In this article, we present an interactive framework based on active learning to help users create customized arrangements for small objects according to their preferences. To achieve this with minimal user effort, we first learn the prior knowledge about small object arrangement from a 3D indoor scene dataset through a probability mining method, which forms the initial guidance for arranging small objects. Then, users are able to express their preferences on a few small object categories, which are automatically propagated to all the other categories via a novel active learning approach. In the propagation process, we introduce a novel metric to obtain the propagation weights, which measures the degree of interchangeability between two small object categories, and is calculated based on a spatial embedding model learned from the small object neighborhood information extracted from the 3D indoor scene dataset. Experiments show that our framework is able to help users effectively create customized small object arrangements with little effort.
C1 [Zhang, Suiyun; Zhang, Hui] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
   [Zhang, Suiyun; Zhang, Hui] Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing, Peoples R China.
   [Han, Zhizhong; Zwicker, Matthias] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.
   [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF10 3AT, Wales.
C3 Tsinghua University; University System of Maryland; University of
   Maryland College Park; Cardiff University
RP Zhang, H (corresponding author), Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
EM zhangsuiyun13@mails.tsinghua.edu.cn; h312h@umd.edu;
   Yukun.Lai@cs.cardiff.ac.uk; zwicker@cs.umd.edu; huizhang@tsinghua.edu.cn
RI Zhang, Xinyu/HNP-9555-2023; Lai, Yu-Kun/D-2343-2010; Han,
   Zhizhong/AAW-4044-2021
OI Zwicker, Matthias/0000-0001-8630-5515; Lai, Yukun/0000-0002-2094-5680
FU National Natural Science Foundation of China [61373070]; NSF [1813583];
   Tsinghua-Kuaishou Institute of Future Media Data; Direct For Computer &
   Info Scie & Enginr; Div Of Information & Intelligent Systems [1813583]
   Funding Source: National Science Foundation
FX This work was supported by the National Natural Science Foundation of
   China (61373070), NSF (award 1813583), and Tsinghua-Kuaishou Institute
   of Future Media Data.
NR 39
TC 3
Z9 4
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2021
VL 27
IS 4
BP 2250
EP 2264
DI 10.1109/TVCG.2019.2949295
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QO8XL
UT WOS:000623420400001
PM 31670674
OA Green Accepted, Bronze
DA 2025-03-07
ER

PT J
AU Ancona, M
   Beyeler, M
   Gross, M
   Günther, T
AF Ancona, Marco
   Beyeler, Marilou
   Gross, Markus
   Guenther, Tobias
TI MineTime Insight: Visualizing Meeting Habits to Promote Informed
   Scheduling Decisions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Scheduling; Tools; Visual analytics; Productivity;
   Cognition; Data analysis; Scheduling; calendar; personal visual
   analytics; casual information visualization; virtual assistant
AB Corporate meetings are a crucial part of business activities. While numerous academic papers investigated how to make the scheduling process of meetings faster or even automatic, little work has been done yet to facilitate the retrospective reasoning about how time is spent on meetings. Traditional calendar applications do not allow users to extract actionable statistics although it has been shown that reflection-oriented design can increase the users' understanding of their habits and can thereby encourage a shift towards better practices. In this paper, we present MineTime Insight, a tool made of multiple coordinated views for the exploration of personal calendar data, with the overarching goal of improving short and long-term scheduling decisions. Despite being focused on the working environment, our work builds upon recent results in the field of Personal Visual Analytics, as it targets users not necessarily expert in visualization and data analysis. We demonstrate the potential of MineTime Insight, when applied to the agenda of an executive manager. Finally, we discuss the results of an informal user study and a field study. Our results suggest that our visual representations are perceived as easy to understand and helpful towards a change in the scheduling habits.
C1 [Ancona, Marco; Guenther, Tobias] Swiss Fed Inst Technol, Comp Graph Lab CGL, CH-8092 Zurich, Switzerland.
   [Beyeler, Marilou; Gross, Markus] Swiss Fed Inst Technol, Comp Sci, CH-8092 Zurich, Switzerland.
C3 Swiss Federal Institutes of Technology Domain; ETH Zurich; Swiss Federal
   Institutes of Technology Domain; ETH Zurich
RP Ancona, M (corresponding author), Swiss Fed Inst Technol, Comp Graph Lab CGL, CH-8092 Zurich, Switzerland.
EM marco.ancona@ethz.ch; mabeyele@ethz.ch; grossm@ethz.ch;
   grossm.tobias.guenther@ethz.ch
RI Ancona, Marco/D-5534-2018
OI Gross, Markus/0009-0003-9324-779X; Gunther, Tobias/0000-0002-3020-0930;
   Beyeler, Marilou/0000-0003-4920-199X
FU Swiss CTI grant [19005.1 PFES-ES]
FX This work has been partially funded by the Swiss CTI grant 19005.1
   PFES-ES.
NR 53
TC 0
Z9 0
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2021
VL 27
IS 3
BP 1986
EP 1999
DI 10.1109/TVCG.2019.2941208
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QA9EF
UT WOS:000613744500009
PM 31536005
DA 2025-03-07
ER

PT J
AU Chen, H
   Soni, U
   Lu, YF
   Huroyan, V
   Maciejewski, R
   Kobourov, S
AF Chen, Hang
   Soni, Utkarsh
   Lu, Yafeng
   Huroyan, Vahan
   Maciejewski, Ross
   Kobourov, Stephen
TI Same Stats, Different Graphs: Exploring the Space of Graphs in Terms of
   Graph Properties
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Generators; Correlation; Data mining; Space exploration; Visual
   analytics; Tools; Graph mining; graph properties; graph generators
AB Data analysts commonly utilize statistics to summarize large datasets. While it is often sufficient to explore only the summary statistics of a dataset (e.g., min/mean/max), Anscombe's Quartet demonstrates how such statistics can be misleading. We consider a similar problem in the context of graph mining. To study the relationships between different graph properties, we examine low-order non-isomorphic graphs and provide a simple visual analytics system to explore correlations across multiple graph properties. However, for larger graphs, studying the entire space quickly becomes intractable. We use different random graph generation methods to further look into the distribution of graph properties for higher order graphs and investigate the impact of various sampling methodologies. We also describe a method for generating many graphs that are identical over a number of graph properties and statistics yet are clearly different and identifiably distinct.
C1 [Chen, Hang; Huroyan, Vahan; Kobourov, Stephen] Univ Arizona, Dept Comp Sci, Tucson, AZ 85721 USA.
   [Soni, Utkarsh; Lu, Yafeng; Maciejewski, Ross] Arizona State Univ, Sch Comp Informat & Decis Syst Engn, Phoenix, AZ 85004 USA.
C3 University of Arizona; Arizona State University; Arizona State
   University-Downtown Phoenix
RP Chen, H (corresponding author), Univ Arizona, Dept Comp Sci, Tucson, AZ 85721 USA.
EM hangchen@email.arizona.edu; usoni1@asu.edu; lyafeng@asu.edu;
   vahanhuroyan@math.arizona.edu; rmacieje@asu.edu; kobourov@cs.arizona.edu
RI ; Kobourov, Stephen/A-3016-2008
OI Huroyan, Vahan/0000-0002-1596-6962; Chen, Hang/0000-0001-9059-4903;
   Kobourov, Stephen/0000-0002-0477-2724
FU NSF [CCF-1740858, DMS-1839274]
FX This is a journal version of a paper that appeared in the proceedings of
   the 26th Symposium on Graph Drawing and Network Visualization (GD'18).
   This work was supported in part by NSF grants CCF-1740858 and
   DMS-1839274.
NR 61
TC 7
Z9 8
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2021
VL 27
IS 3
BP 2056
EP 2072
DI 10.1109/TVCG.2019.2946558
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA QA9EF
UT WOS:000613744500014
PM 31603821
OA Green Submitted, Bronze
DA 2025-03-07
ER

PT J
AU Fonnet, A
   Prié, Y
AF Fonnet, Adrien
   Prie, Yannick
TI Survey of Immersive Analytics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Three-dimensional displays; Data analysis; Rendering
   (computer graphics); Visualization; Data mining; Task analysis;
   Immersive analytics; survey; virtual environments; immersive
   environments; data visualization; information visualization; scientific
   visualization; visual data mining
ID VIRTUAL-REALITY; VISUAL ANALYSIS; VISUALIZATION; 3D; CAVE; INTERFACES;
   SELECTION; SOFTWARE; STEREO; MOTION
AB Immersive analytics (IA) is a new term referring to the use of immersive technologies for data analysis. Yet such applications are not new, and numerous contributions have been made in the last three decades. However, no survey reviewing all these contributions is available. Here we propose a survey of IA from the early nineties until the present day, describing how rendering technologies, data, sensory mapping, and interaction means have been used to build IA systems, as well as how these systems have been evaluated. The conclusions that emerge from our analysis are that: multi-sensory aspects of IA are under-exploited, the 3DUI and VR community knowledge regarding immersive interaction is not sufficiently utilised, the IA community should focus on converging towards best practices, as well as aim for real life IA systems.
C1 [Fonnet, Adrien; Prie, Yannick] Univ Nantes, LS2N UMR6004 CNRS, F-44322 Nantes, Loire Atlantiqu, France.
C3 Nantes Universite
RP Fonnet, A (corresponding author), Univ Nantes, LS2N UMR6004 CNRS, F-44322 Nantes, Loire Atlantiqu, France.
EM adrien.fonnet@univ-nantes.fr; yannick.prie@univ-nantes.fr
RI Prié, Yannick/AAM-5273-2020
OI Prie, Yannick/0000-0002-7068-0836
FU Pays de la Loire French region through the West Creative Industries
   program
FX This work is part of the IDEA project (Immersive Data Exploration and
   Analysis) funded by the Pays de la Loire French region through the West
   Creative Industries program.
NR 199
TC 94
Z9 100
U1 2
U2 35
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2021
VL 27
IS 3
BP 2101
EP 2122
DI 10.1109/TVCG.2019.2929033
PG 22
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA QA9EF
UT WOS:000613744500017
PM 31352344
DA 2025-03-07
ER

PT J
AU Heine, C
AF Heine, Christian
TI Towards Modeling Visualization Processes as Dynamic Bayesian Networks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Computational modeling; Mathematical model; Task
   analysis; Predictive models; Guidelines; Cognition; Visualization; model
   building; perception; cognition; dynamic Bayesian networks
ID INFORMATION; FRAMEWORK; SCIENCE; DESIGN; TASK; USER
AB Visualization designs typically need to be evaluated with user studies, because their suitability for a particular task is hard to predict. What the field of visualization is currently lacking are theories and models that can be used to explain why certain designs work and others do not. This paper outlines a general framework for modeling visualization processes that can serve as the first step towards such a theory. It surveys related research in mathematical and computational psychology and argues for the use of dynamic Bayesian networks to describe these time-dependent, probabilistic processes. It is discussed how these models could be used to aid in design evaluation. The development of concrete models will be a long process. Thus, the paper outlines a research program sketching how to develop prototypes and their extensions from existing models, controlled experiments, and observational studies.
C1 [Heine, Christian] Univ Leipzig, Leipzig, Germany.
C3 Leipzig University
RP Heine, C (corresponding author), Univ Leipzig, Leipzig, Germany.
EM heine@informatik.uni-leipzig.de
OI Heine, Christian/0000-0001-7067-5650
NR 129
TC 1
Z9 1
U1 3
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1000
EP 1010
DI 10.1109/TVCG.2020.3030395
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100084
PM 33074817
DA 2025-03-07
ER

PT J
AU Jacobsen, B
   Wallinger, M
   Kobourov, S
   Nöllenburg, M
AF Jacobsen, Ben
   Wallinger, Markus
   Kobourov, Stephen
   Noellenburg, Martin
TI MetroSets: Visualizing Sets as Metro Maps
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Layout; Pipelines; Fats; Task analysis; Visual
   analytics; Set visualization; metro map metaphor; hypergraphs
ID NODE-LINK; ALGORITHM; DIAGRAMS; DESIGN; VISUALIZATIONS; LAYOUT
AB We propose MetroSets, a new, flexible online tool for visualizing set systems using the metro map metaphor. We model a given set system as a hypergraph 'H=(V,S), consisting of a set V of vertices and a set S. which contains subsets of V called hyperedges. Our system then computes a metro map representation of 'H., where each hyperedge E in S corresponds to a metro line and each vertex corresponds to a metro station. Vertices that appear in two or more hyperedges are drawn as interchanges in the metro map, connecting the different sets. MetroSets is based on a modular 4-step pipeline which constructs and optimizes a path -based hypergraph support, which is then drawn and schematized using metro map layout algorithms. We propose and implement multiple algorithms for each step of the MetroSet pipeline and provide a functional prototype with easy-to-use preset configurations. Furthermore, using several real -world datasets, we perform an extensive quantitative evaluation of the impact of different pipeline stages on desirable properties of the generated maps, such as octolinearity, monotonicity. and edge uniformity.
C1 [Jacobsen, Ben; Kobourov, Stephen] Univ Arizona, Tucson, AZ 85721 USA.
   [Wallinger, Markus; Noellenburg, Martin] TU Wien, Vienna, Austria.
C3 University of Arizona; Technische Universitat Wien
RP Jacobsen, B (corresponding author), Univ Arizona, Tucson, AZ 85721 USA.
EM bjacobsen@email.arizona.edu; mwallinger@ac.tuwien.ac.at;
   kobourov@cs.arizona.edu; noellenburg@ac.tuwien.ac.at
RI ; Kobourov, Stephen/A-3016-2008
OI Wallinger, Markus/0000-0002-2191-4413; Jacobsen,
   Ben/0000-0002-1934-7712; Kobourov, Stephen/0000-0002-0477-2724
FU NSF [CCF-1740858, CCF-1712119, DMS-1839274]; Vienna Science and
   Technology Fund (WWTF) [ICT19-035]
FX We thank the organizers of the Dagstuhl Seminar 17332 "Scalable Set
   Visualizations", specifically Robert Baker, Nan Cao, Yifan Hu, Michael
   Kaufmann, Tamara Mchedlidze, Sergey Pupyrev, Torsten Ueckerdt, and
   Alexander Wolff. We also thank Miranda Rintoul for her help with the
   experimental analysis. This work is supported by NSF grants CCF-1740858,
   CCF-1712119, and DMS-1839274 and by the Vienna Science and Technology
   Fund (WWTF) through project ICT19-035.
NR 90
TC 22
Z9 23
U1 1
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1257
EP 1267
DI 10.1109/TVCG.2020.3030475
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100108
PM 33052864
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ye, SN
   Chen, ZT
   Chu, XT
   Wang, YF
   Fu, SW
   Shen, LJ
   Zhou, K
   Wu, YC
AF Ye, Shuainan
   Chen, Zhutian
   Chu, Xiangtong
   Wang, Yifan
   Fu, Siwei
   Shen, Lejun
   Zhou, Kun
   Wu, Yingcai
TI ShuttleSpace: Exploring and Analyzing Movement Trajectory in Immersive
   Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Movement trajectory; badminton analytics; virtual reality
AB We present Shuttle Space, an immersive analytics system to assist experts in analyzing trajectory data in badminton. Trajectories in sports, such as the movement of players and balls, contain rich information on player behavior and thus have been widely analyzed by coaches and analysts to improve the players' performance. However, existing visual analytics systems often present the trajectories in court diagrams that are abstractions of reality, thereby causing difficulty for the experts to imagine the situation on the court and understand why the player acted in a certain way. With recent developments in immersive technologies, such as virtual reality (VR), experts gradually have the opportunity to see. feel, explore. and understand these 3D trajectories from the player's perspective. Yet, few research has studied how to support immersive analysis of sports data from such a perspective. Specific challenges are rooted in data presentation (e.g., how to seamlessly combine 2D and 3D visualizations) and interaction (e.g., how to naturally interact with data without keyboard and mouse) in VR. To address these challenges, we have worked closely with domain experts who have worked for a top national badminton team to design ShuttleSpace. Our system leverages 1) the peripheral vision to combine the 2D and 3D visualizations and 2) the VR controller to support natural interactions via a stroke metaphor. We demonstrate the effectiveness of ShuttleSpace through three case studies conducted by the experts with useful insights. We further conduct interviews with the experts whose feedback confirms that our first-person immersive analytics system is suitable and useful for analyzing badminton data.
C1 [Ye, Shuainan; Chu, Xiangtong; Wang, Yifan; Zhou, Kun; Wu, Yingcai] Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China.
   [Chen, Zhutian] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Fu, Siwei] Zhejiang Lab, Hangzhou, Peoples R China.
   [Shen, Lejun] Chengdu Sports Univ, Chengdu, Peoples R China.
C3 Zhejiang University; Hong Kong University of Science & Technology;
   Zhejiang Laboratory; Chengdu Sport University
RP Wu, YC (corresponding author), Zhejiang Univ, State Key Lab CAD CG, Hangzhou, Peoples R China.
EM sn_ye@outlook.com; zhutian.chen@connect.ust.uk;
   xiangtongchuu@outlook.com; yifan_wang@zju.edu.cn; fusiwei339@gmail.com;
   sljcool@sina.com; kunzhou@zju.edu.cn; ycwu@zju.edu.cn
RI chu, xiangtong/GWZ-8395-2022; Wang, Yifan/IRZ-4368-2023; zhou,
   kun/KRP-1631-2024
FU National Key RD Program of China [2018YFB1004300]; NSFC [61761136020,
   61890954]; NSFC-Zhejiang Joint Fund for the Integration of
   Industrialization and Informatization [U1609217]; Zhejiang Provincial
   Natural Science Foundation [LR18F020001]; 100 Talents Program of
   Zhejiang University
FX The work was supported by National Key RD Program of China
   (2018YFB1004300), NSFC (61761136020, 61890954), NSFC-Zhejiang Joint Fund
   for the Integration of Industrialization and Informatization (U1609217),
   Zhejiang Provincial Natural Science Foundation (LR18F020001) and the 100
   Talents Program of Zhejiang University.
NR 48
TC 52
Z9 56
U1 5
U2 56
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 860
EP 869
DI 10.1109/TVCG.2020.3030392
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100071
PM 33048712
DA 2025-03-07
ER

PT J
AU Luo, TR
   Zhang, MM
   Pan, ZG
   Li, Z
   Cai, N
   Miao, JD
   Chen, YB
   Xu, MX
AF Luo, Tianren
   Zhang, Mingmin
   Pan, Zhigeng
   Li, Zheng
   Cai, Ning
   Miao, Jinda
   Chen, Youbin
   Xu, Mingxi
TI Dream-Experiment: A MR User Interface with Natural Multi-channel
   Interaction for Virtual Experiments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 19th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY NOV 09-13, 2020
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, IEEE VGTC, ACM SIGGRAPH, VirBELA, Immers Learning Res Network, Liferay, React Real, Qualcomm
DE Multi-channel interaction; virtual-real occlusion; multi-camera
   collaboration; MR experiments
ID AUGMENTED REALITY; OCCLUSION; MANIPULATION; HAND
AB This paper studies a set of MR technologies for middle school experimental teaching environments and develops a multi-channel MR user interface called Dream-Experiment. The goal of Dream-Experiment is to improve the traditional MR user interface, so that users can get a real, natural 3D interactive experience like real experiments, but without danger and pollution. In terms of visual presentation, we design multi-camera collaborative registration to realize robust 6-DoF MR interactive space, and also define a complete rendering pipeline to provide improved processing of virtual-real objects' occlusion including translucent devices. In the virtual-real interaction, we provide six interaction modes that support visual interaction, tangible interaction, virtual-real gestures with touching, voice, thermal feeling, and olfactory feeling. After users' testing, we find that Dream-Experiment has better interactive efficiency and user experience than traditional MR environments.
C1 [Luo, Tianren; Pan, Zhigeng; Li, Zheng; Cai, Ning] Hangzhou Normal Univ, Virtual Real & Intelligent Syst Res Inst, Hangzhou, Zhejiang, Peoples R China.
   [Zhang, Mingmin; Miao, Jinda] Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
   [Chen, Youbin; Xu, Mingxi] Guangdong Univ Technol Univ, Guangzhou, Guangdong, Peoples R China.
C3 Hangzhou Normal University; Zhejiang University
RP Pan, ZG (corresponding author), Hangzhou Normal Univ, Virtual Real & Intelligent Syst Res Inst, Hangzhou, Zhejiang, Peoples R China.; Zhang, MM (corresponding author), Zhejiang Univ, Coll Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.
EM luo_tianren@qq.com; zhangmm95@zju.edu.cn; zgpan@hznu.edu.cn;
   lizheng1@stu.hznu.edu.cn; caining96@qq.com; 384751024@qq.com;
   297886578@qq.com; xmxsmile@qq.com
RI Luo, Tianren/CAI-9775-2022; Chen, Youbin/AAM-2563-2020
OI Luo, Tianren/0000-0002-6752-9182; Li, Zheng/0000-0003-3309-1087
FU National Key Research and Development Project [2018YEB1004900]; Key
   Hangzhou S&T Innovation Project [20182014B02]
FX This work is supported by National Key Research and Development Project
   (Grant NO. 2018YEB1004900 in China) and Key Hangzhou S&T Innovation
   Project (Grant NO. 20182014B02).
NR 48
TC 16
Z9 19
U1 2
U2 59
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2020
VL 26
IS 12
BP 3524
EP 3534
DI 10.1109/TVCG.2020.3023602
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA OR1DW
UT WOS:000589217900015
PM 32941147
DA 2025-03-07
ER

PT J
AU Lee, C
   Kim, Y
   Jin, S
   Kim, D
   Maciejewski, R
   Ebert, D
   Ko, S
AF Lee, Chunggi
   Kim, Yeonjun
   Jin, Seungmin
   Kim, Dongmin
   Maciejewski, Ross
   Ebert, David
   Ko, Sungahn
TI A Visual Analytics System for Exploring, Monitoring, and Forecasting
   Road Traffic Congestion
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Roads; Forecasting; Data visualization; Surveillance; Task analysis;
   Urban areas; Traffic; road; congestion; visualization; deep learning;
   LSTM; surveillance; forecasting; predictive analysis
ID EXPLORATION; MOBILITY; NETWORKS
AB We present an interactive visual analytics system that enables traffic congestion exploration, surveillance, and forecasting based on vehicle detector data. Through domain expert collaboration, we have extracted task requirements, incorporated the Long Short-Term Memory (LSTM) model for congestion forecasting, and designed a weighting method for detecting the causes of congestion and congestion propagation directions. Our visual analytics system is designed to enable users to explore congestion causes, directions, and severity. Congestion conditions of a city are visualized using a Volume-Speed Rivers (VSRivers) visualization that simultaneously presents traffic volumes and speeds. To evaluate our system, we report performance comparison results, wherein our model is more accurate than other forecasting algorithms. We demonstrate the usefulness of our system in the traffic management and congestion broadcasting domains through three case studies and domain expert feedback.
C1 [Lee, Chunggi; Kim, Yeonjun; Jin, Seungmin; Kim, Dongmin; Ko, Sungahn] UNIST, Ulsan, South Korea.
   [Maciejewski, Ross] Arizona State Univ, Sch Comp Informat & Decis Syst Engn, Tempe, AZ 85287 USA.
   [Ebert, David] Purdue Univ, Elect & Comp Wngineering, W Lafayette, IN 47907 USA.
C3 Ulsan National Institute of Science & Technology (UNIST); Arizona State
   University; Arizona State University-Tempe; Purdue University System;
   Purdue University
RP Ko, S (corresponding author), UNIST, Ulsan, South Korea.
EM cglee@unist.ac.kr; yeonjunkim@unist.ac.kr; dryjins@unist.ac.kr;
   rocky112358@unist.ac.kr; rmacieje@asu.edu; ebertd@purdue.edu;
   sako@unist.ac.kr
RI Jin, Seungmin/AAB-1073-2021
OI Kim, Dongmin/0000-0002-0561-3936; Jin, Seungmin/0000-0002-8242-6157;
   Ebert, David/0000-0001-6177-1296; Ko, Sungahn/0000-0002-7410-5652
FU National Research Foundation of Korea(NRF) - Korea government (MSIT)
   [2017R1C1B1002586]; Institute of Information & communications Technology
   Planning & Evaluation (IITP) - Korea government(MSIT) [2017-0-00692]
FX This work was supported by the National Research Foundation of
   Korea(NRF) grant funded by the Korea government (MSIT) (No.
   2017R1C1B1002586). This work was also supported by Institute of
   Information & communications Technology Planning & Evaluation (IITP)
   grant funded by the Korea government(MSIT) (No.2017-0-00692,
   Transport-aware streaming Technique Enabling Ultra Low-Latency AR/VR
   Services).
NR 62
TC 61
Z9 70
U1 5
U2 75
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV 1
PY 2020
VL 26
IS 11
BP 3133
EP 3146
DI 10.1109/TVCG.2019.2922597
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CK
UT WOS:000574745100001
PM 31199260
DA 2025-03-07
ER

PT J
AU Wang, KC
   Wei, TH
   Shareef, N
   Shen, HW
AF Wang, Ko-Chih
   Wei, Tzu-Hsuan
   Shareef, Naeem
   Shen, Han-Wei
TI Ray-Based Exploration of Large Time-Varying Volume Data Using Per-Ray
   Proxy Distributions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Histograms; Interpolation; Rendering (computer graphics); Transfer
   functions; Data visualization; Data models; Supercomputers; Data
   visualization; time-varying data; large data modeling; scientific
   simulation
ID IN-SITU VISUALIZATION; COMPUTATION; REDUCTION
AB The analysis and visualization of data created from simulations on modern supercomputers is a daunting challenge because the incredible compute power of modern supercomputers allow scientists to generate datasets with very high spatial and temporal resolutions. The limited bandwidth and capacity of networking and storage devices connecting supercomputers to analysis machines become the major bottleneck for data analysis such that simply moving the whole dataset from the supercomputer to a data analysis machine is infeasible. A common approach to visualize high temporal resolution simulation datasets under constrained I/O is to reduce the sampling rate in the temporal domain while preserving the original spatial resolution at the time steps. Data interpolation between the sampled time steps alone may not be a viable option since it may suffer from large errors, especially when using a lower sampling rate. We present a novel ray-based representation storing ray based histograms and depth information that recovers the evolution of volume data between sampled time steps. Our view-dependent proxy allows for a good trade off between compactly representing the time-varying data and leveraging temporal coherence within the data by utilizing interpolation between time steps, ray histograms, depth information, and codebooks. Our approach is able to provide fast rendering in the context of transfer function exploration to support visualization of feature evolution in time-varying data.
C1 [Wang, Ko-Chih; Wei, Tzu-Hsuan; Shareef, Naeem; Shen, Han-Wei] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
C3 University System of Ohio; Ohio State University
RP Wang, KC (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
EM wang.3182@osu.edu; wei.225@osu.edu; shareef.1@osu.edu; shen.94@osu.edu
RI Shen, Han-wei/A-4710-2012
OI Wang, Ko-Chih/0000-0002-7241-1939
FU UT-Battelle LLC [4000159557]; Los Alamos National Laboratory [471415];
   US National Science Foundation [SBE-1738502]
FX This work was supported in part by UT-Battelle LLC 4000159557, Los
   Alamos National Laboratory Contract 471415, and US National Science
   Foundation grant SBE-1738502.
NR 40
TC 3
Z9 5
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV 1
PY 2020
VL 26
IS 11
BP 3299
EP 3313
DI 10.1109/TVCG.2019.2920130
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA NW1CK
UT WOS:000574745100013
PM 31170075
OA Bronze
DA 2025-03-07
ER

PT J
AU Drouin, S
   Di Giovanni, DA
   Kersten-Oertel, M
   Collins, DL
AF Drouin, Simon
   Di Giovanni, Daniel A.
   Kersten-Oertel, Marta
   Collins, D. Louis
TI Interaction Driven Enhancement of Depth Perception in Angiographic
   Volumes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Surgery; Rendering (computer graphics); Three-dimensional displays;
   Tracking; Biomedical imaging; Tools; Navigation; Image-guided surgery;
   volume visualization; interaction techniques; depth cues; evaluation;
   angiography
ID VISUALIZATION; LOCALIZATION; ADAPTATION
AB User interaction has the potential to greatly facilitate the exploration and understanding of 3D medical images for diagnosis and treatment. However, in certain specialized environments such as in an operating room (OR), technical and physical constraints such as the need to enforce strict sterility rules, make interaction challenging. In this paper, we propose to facilitate the intraoperative exploration of angiographic volumes by leveraging the motion of a tracked surgical pointer, a tool that is already manipulated by the surgeon when using a navigation system in the OR. We designed and implemented three interactive rendering techniques based on this principle. The benefit of each of these techniques is compared to its non-interactive counterpart in a psychophysics experiment where 20 medical imaging experts were asked to perform a reaching/targeting task while visualizing a 3D volume of angiographic data. The study showed a significant improvement of the appreciation of local vascular structure when using dynamic techniques, while not having a negative impact on the appreciation of the global structure and only a marginal impact on the execution speed. A qualitative evaluation of the different techniques showed a preference for dynamic chroma-depth in accordance with the objective metrics but a discrepancy between objective and subjective measures for dynamic aerial perspective and shading.
C1 [Drouin, Simon; Di Giovanni, Daniel A.; Collins, D. Louis] McGill Univ, McConnell Brain Imaging Ctr, Montreal Neurol Inst, Dept Biomed Engn, Montreal, PQ, Canada.
   [Kersten-Oertel, Marta] Concordia Univ, Dept Comp Sci & Software Engn, Montreal, PQ, Canada.
C3 Concordia University - Canada
RP Drouin, S (corresponding author), McGill Univ, McConnell Brain Imaging Ctr, Montreal Neurol Inst, Dept Biomed Engn, Montreal, PQ, Canada.
EM simon.drouin@mail.mcgill.ca; daniel.digiovanni@mail.mcgill.ca;
   marta@ap-lab.ca; louis.collins@mcgill.ca
RI Collins, D. Louis/ABD-7708-2021; Kersten-Oertel, Marta/AAC-5882-2019;
   Drouin, Simon/O-1683-2019
OI Collins, D. Louis/0000-0002-8432-7021; Kersten-Oertel,
   Marta/0000-0002-9492-8402
FU Natural Sciences and Engineering Research Council of Canada - Discovery
   Grants program [RGPIN-201503633-15]
FX This work was supported by the Natural Sciences and Engineering Research
   Council of Canada - Discovery Grants program (URL:
   http://www.nserc-crsng.gc.ca/ProfessorsProfesseurs/Grants-Subs/DGIGP-PSI
   GP_eng.asp).Principal investigator: D. Louis Collins. Grant number:
   RGPIN-201503633-15.
NR 40
TC 6
Z9 7
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2020
VL 26
IS 6
BP 2247
EP 2257
DI 10.1109/TVCG.2018.2884940
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA LM5NO
UT WOS:000532295600011
PM 30530366
DA 2025-03-07
ER

PT J
AU Tang, ZY
   Bryan, NJ
   Li, DZY
   Langlois, TR
   Manocha, D
AF Tang, Zhenyu
   Bryan, Nicholas J.
   Li, Dingzeyu
   Langlois, Timothy R.
   Manocha, Dinesh
TI Scene-Aware Audio Rendering via Deep Acoustic Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 27th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 22-26, 2020
CL Atlanta, GA
SP IEEE, IEEE Comp Soc
DE Acoustics; Optimization; Rendering (computer graphics); Visualization;
   Acoustic materials; Frequency estimation; Estimation; Audio rendering;
   audio learning; material optimization
ID SOUND-PROPAGATION; SPEECH ENHANCEMENT; NEURAL-NETWORKS; SIMULATION;
   SCALE; OPTIMIZATION; SOFTWARE; ROOMS
AB We present a new method to capture the acoustic characteristics of real-world rooms using commodity devices, and use the captured characteristics to generate similar sounding sources with virtual models. Given the captured audio and an approximate geometric model of a real-world room, we present a novel learning-based method to estimate its acoustic material properties. Our approach is based on deep neural networks that estimate the reverberation time and equalization of the room from recorded audio. These estimates are used to compute material properties related to room reverberation using a novel material optimization objective. We use the estimated acoustic material characteristics for audio rendering using interactive geometric sound propagation and highlight the performance on many real-world scenarios. We also perform a user study to evaluate the perceptual similarity between the recorded sounds and our rendered audio.
C1 [Tang, Zhenyu; Manocha, Dinesh] Univ Maryland, College Pk, MD 20742 USA.
   [Bryan, Nicholas J.; Li, Dingzeyu; Langlois, Timothy R.] Adobe Res, San Jose, CA USA.
C3 University System of Maryland; University of Maryland College Park;
   Adobe Systems Inc.
RP Tang, ZY (corresponding author), Univ Maryland, College Pk, MD 20742 USA.
EM zhy@cs.umd.edu; nibryan@adobe.com; dinli@adobe.com; tlangloi@adobe.com;
   dm@cs.umd.edu
RI Tang, Zhenyu/ACY-2335-2022; Li, Dingzeyu/AAM-9159-2020
OI Tang, Zhenyu/0000-0001-5544-9727
FU ARO [W911NF-18-1-0313]; NSF [1910940]; Adobe Research; Facebook; Intel;
   Div Of Information & Intelligent Systems; Direct For Computer & Info
   Scie & Enginr [1910940] Funding Source: National Science Foundation
FX The authors would like to thank Chunxiao Cao for sharing the
   bidirectional sound simulation code, James Traer for sharing the MIT IR
   dataset, and anonymous reviewers for their constructive feedback. This
   work was supported in part by ARO grant W911NF-18-1-0313, NSF grant
   #1910940, Adobe Research, Facebook, and Intel.
NR 74
TC 23
Z9 24
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2020
VL 26
IS 5
BP 1991
EP 2001
DI 10.1109/TVCG.2020.2973058
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA LA1WU
UT WOS:000523746000018
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Tone, D
   Iwai, D
   Hiura, S
   Sato, K
AF Tone, Daiki
   Iwai, Daisuke
   Hiura, Shinsaku
   Sato, Kosuke
TI FibAR: Embedding Optical Fibers in 3D Printed Objects for Active Markers
   in Dynamic Projection Mapping
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 27th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 22-26, 2020
CL Atlanta, GA
SP IEEE, IEEE Comp Soc
DE Optical imaging; Cameras; Optical device fabrication; Robustness;
   Three-dimensional displays; Observers; Printers; Projection mapping;
   spatial augmented reality; multi-material 3D printer; optical fiber;
   active marker
ID NONRIGID SURFACE; VISUALIZATION
AB This paper presents a novel active marker for dynamic projection mapping (PM) that emits a temporal blinking pattern of infrared (IR) light representing its ID. We used a multi-material three dimensional (3D) printer to fabricate a projection object with optical fibers that can guide IR light from LEDs attached on the bottom of the object. The aperture of an optical fiber is typically very small; thus, it is unnoticeable to human observers under projection and can be placed on a strongly curved part of a projection surface. In addition, the working range of our system can be larger than previous marker-based methods as the blinking patterns can theoretically be recognized by a camera placed at a wide range of distances from markers. We propose an automatic marker placement algorithm to spread multiple active markers over the surface of a projection object such that its pose can be robustly estimated using captured images from arbitrary directions. We also propose an optimization framework for determining the routes of the optical fibers in such a way that collisions of the fibers can be avoided while minimizing the loss of light intensity in the fibers. Through experiments conducted using three fabricated objects containing strongly curved surfaces, we confirmed that the proposed method can achieve accurate dynamic PMs in a significantly wide working range.
C1 [Tone, Daiki; Iwai, Daisuke; Sato, Kosuke] Osaka Univ, Suita, Osaka, Japan.
   [Iwai, Daisuke] JST, PRESTO, Tokyo, Japan.
   [Hiura, Shinsaku] Univ Hyogo, Kobe, Hyogo, Japan.
C3 Osaka University; Japan Science & Technology Agency (JST); University of
   Hyogo
RP Iwai, D (corresponding author), Osaka Univ, Suita, Osaka, Japan.; Iwai, D (corresponding author), JST, PRESTO, Tokyo, Japan.
EM daisuke.iwai@sys.es.osaka-u.ac.jp
RI Iwai, Daisuke/R-8174-2019
OI Iwai, Daisuke/0000-0002-3493-5635
FU JSPS KAKENHI [151105925]
FX This work has been supported by the JSPS KAKENHI under grant number
   151105925.
NR 39
TC 17
Z9 17
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2020
VL 26
IS 5
BP 2030
EP 2040
DI 10.1109/TVCG.2020.2973444
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA LA1WU
UT WOS:000523746000022
PM 32070979
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Coelho, D
   Chase, I
   Mueller, K
AF Coelho, Darius
   Chase, Ivan
   Mueller, Klaus
TI PeckVis: A Visual Analytics Tool to Analyze Dominance Hierarchies in
   Small Groups
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 16th IEEE Symposium on Visualization for Cyber Security (VizSec) / IEEE
   Visualization Conference (IEEE VIS)
CY OCT 20-23, 2019
CL Vancouver, CANADA
SP IEEE
DE Visual analytics; interaction sequence; dynamic graphs; time series;
   dominance hierarchy
ID NETWORKS; RANKING; GRAPHS
AB The formation of social groups is defined by the interactions among the group members. Studying this group formation process can be useful in understanding the status of members, decision-making behaviors, spread of knowledge and diseases, and much more. A defining characteristic of these groups is the pecking order or hierarchy the members form which help groups work towards their goals. One area of social science deals with understanding the formation and maintenance of these hierarchies, and in our work we provide social scientists with a visual analytics tool - PeckVis - to aid this process. While online social groups or social networks have been studied deeply and lead to a variety of analyses and visualization tools, the study of smaller groups in the field of social science lacks the support of suitable tools. Domain experts believe that visualizing their data can save them time as well as reveal findings they may have failed to observe. We worked alongside domain experts to build an interactive visual analytics system to investigate social hierarchies. Our system can discover patterns and relationships between the members of a group as well as compare different groups. The results are presented to the user in the form of an interactive visual analytics dashboard. We demonstrate that domain experts were able to effectively use our tool to analyze animal behavior data.
C1 [Coelho, Darius; Mueller, Klaus] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Chase, Ivan] SUNY Stony Brook, Dept Sociol, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; Stony Brook University;
   State University of New York (SUNY) System; Stony Brook University
RP Coelho, D (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM dcoelho@cs.stonybrook.edu; ivan.chase@stonybrook.edu;
   mueller@cs.stonybrook.edu
RI Coelho, Darius/ABF-7124-2020
FU NSF [IIS 1527200]
FX This work was supported in part by NSF grant IIS 1527200.
NR 37
TC 6
Z9 7
U1 2
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2020
VL 26
IS 4
BP 1650
EP 1660
DI 10.1109/TVCG.2020.2969056
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA KU2OG
UT WOS:000519547200003
PM 32031940
OA Bronze
DA 2025-03-07
ER

PT J
AU Dimara, E
   Franconeri, S
   Plaisant, C
   Bezerianos, A
   Dragicevic, P
AF Dimara, Evanthia
   Franconeri, Steven
   Plaisant, Catherine
   Bezerianos, Anastasia
   Dragicevic, Pierre
TI A Task-Based Taxonomy of Cognitive Biases for Information Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Taxonomy; Data visualization; Task analysis; Decision making;
   Visualization; Cognition; Systematics; Cognitive bias; visualization;
   taxonomy; classification; decision making
ID DECISION-SUPPORT-SYSTEMS; MERE EXPOSURE; MEMORY; PROBABILITY;
   PSYCHOLOGY; ATTRIBUTION; JUDGMENT; RECOGNITION; UNCERTAINTY; ILLUSION
AB Information visualization designers strive to design data displays that allow for efficient exploration, analysis, and communication of patterns in data, leading to informed decisions. Unfortunately, human judgment and decision making are imperfect and often plagued by cognitive biases. There is limited empirical research documenting how these biases affect visual data analysis activities. Existing taxonomies are organized by cognitive theories that are hard to associate with visualization tasks. Based on a survey of the literature we propose a task-based taxonomy of 154 cognitive biases organized in 7 main categories. We hope the taxonomy will help visualization researchers relate their design to the corresponding possible biases, and lead to new research that detects and addresses biased judgment and decision making in data visualization.
C1 [Dimara, Evanthia; Dragicevic, Pierre] INRIA, F-91405 Orsay, France.
   [Dimara, Evanthia] Sorbonne Univ, F-75006 Paris, France.
   [Franconeri, Steven] Northwestern Univ, Evanston, IL 60208 USA.
   [Plaisant, Catherine] Univ Maryland, College Pk, MD 20742 USA.
   [Plaisant, Catherine] INRIA Fdn, F-91405 Orsay, France.
   [Bezerianos, Anastasia] Univ Paris Sud, CNRS, LRI, INRIA, F-91405 Orsay, France.
   [Bezerianos, Anastasia] Univ Paris Saclay, F-91405 Orsay, France.
C3 Inria; Sorbonne Universite; Northwestern University; University System
   of Maryland; University of Maryland College Park; Microsoft; Centre
   National de la Recherche Scientifique (CNRS); Inria; Universite Paris
   Saclay; Universite Paris Saclay
EM evanthia.dimara@gmail.com; franconeri@northwestern.edu;
   plaisant@cs.umd.edu; anastasia.bezerianos@lri.fr;
   pierre.dragicevic@inria.fr
RI Dragicevic, Pierre/HKV-4981-2023
OI Bezerianos, Anastasia/0000-0002-7142-2548; Franconeri,
   Steven/0000-0001-5244-9764; Plaisant, Catherine/0000-0003-4049-5848;
   Dimara, Evanthia/0000-0001-5212-7888
NR 246
TC 61
Z9 74
U1 2
U2 79
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2020
VL 26
IS 2
BP 1413
EP 1432
DI 10.1109/TVCG.2018.2872577
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA KB6YF
UT WOS:000506637400011
PM 30281459
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Eilemann, S
   Steiner, D
   Pajarola, R
AF Eilemann, Stefan
   Steiner, David
   Pajarola, Renato
TI Equalizer 2.0-Convergence of a Parallel Rendering Framework
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Parallel rendering; scalable visualization; cluster graphics; immersive
   environments; display walls
AB Developing complex, real world graphics applications which leverage multiple GPUs and computers for interactive 3D rendering tasks is a complex task. It requires expertise in distributed systems and parallel rendering in addition to the application domain itself. We present a mature parallel rendering framework which provides a large set of features, algorithms and system integration for a wide range of real-world research and industry applications. Using the $\mathsf{Equalizer}$Equalizer parallel rendering framework, we show how a wide set of generic algorithms can be integrated in the framework to help application scalability and development in many different domains, highlighting how concrete applications benefit from the diverse aspects and use cases of $\mathsf{Equalizer}$Equalizer. We present novel parallel rendering algorithms, powerful abstractions for large visualization setups and virtual reality, as well as new experimental results for parallel rendering and data distribution.
C1 [Eilemann, Stefan; Steiner, David; Pajarola, Renato] Univ Zurich, Visualizat & MultiMedia Lab, Dept Informat, CH-8006 Zurich, Switzerland.
C3 University of Zurich
RP Eilemann, S (corresponding author), Univ Zurich, Visualizat & MultiMedia Lab, Dept Informat, CH-8006 Zurich, Switzerland.
EM eilemann@gmail.com; steiner@ifi.uzh.ch; pajarola@ifi.uzh.ch
OI Pajarola, Renato/0000-0002-6724-526X; Eilemann,
   Stefan/0000-0002-1510-1109
FU Swiss National Science Foundation [200021-116329, 200020-129525]; Swiss
   Commission for Technology and Innovation CTI/KTI Project [9394.2
   PFES-ES]; EU FP7 People Programme (Marie Curie Actions) under REA Grant
   [290227]; Hasler Stiftung grant [12097]; Swiss National Science
   Foundation (SNF) [200020_129525] Funding Source: Swiss National Science
   Foundation (SNF)
FX We would like to thank and acknowledge the following institutions and
   projects for providing the 3D geometry and volume test data sets: the
   Digital Michelangelo Project, Stanford 3D Scanning Repository, Cyberware
   Inc., volvis.org and the Visual Human Project. This work was partially
   supported by the Swiss National Science Foundation Grants 200021-116329
   and 200020-129525, the Swiss Commission for Technology and Innovation
   CTI/KTI Project 9394.2 PFES-ES, the EU FP7 People Programme (Marie Curie
   Actions) under REA Grant Agreement no 290227 and a Hasler Stiftung grant
   (project number 12097). We would also like to thank all supporters and
   contributors of Equalizer, most notably RTT, the Blue Brain Project, the
   University of Siegen, the Electronic Visualization Lab at the University
   of Illinois Chicago and Dardo Kleiner.
NR 30
TC 9
Z9 10
U1 0
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2020
VL 26
IS 2
BP 1292
EP 1307
DI 10.1109/TVCG.2018.2870822
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB6YF
UT WOS:000506637400002
PM 30235135
OA Green Accepted, Green Submitted
DA 2025-03-07
ER

PT J
AU Halladjian, S
   Miao, HC
   Kouril, D
   Gröller, ME
   Viola, I
   Isenberg, T
AF Halladjian, Sarkis
   Miao, Haichao
   Kouril, David
   Groller, M. Eduard
   Viola, Ivan
   Isenberg, Tobias
TI Scale Trotter: Illustrative Visual Travels Across Negative Scales
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Multi-scale visualization; scale transition; abstraction; human genome;
   DNA; Hi-C data
ID VISUALIZATION; ABSTRACTION
AB We present ScaleTrotter, a conceptual framework for an interactive, multi-scale visualization of biological mesoscale data and, specifically, genome data. ScaleTrotter allows viewers to smoothly transition from the nucleus of a cell to the atomistic composition of the DNA, while bridging several orders of magnitude in scale. The challenges in creating an interactive visualization of genome data are fundamentally different in several ways from those in other domains like astronomy that require a multi-scale representation as well. First, genome data has intertwined scale levelsthe DNA is an extremely long, connected molecule that manifests itself at all scale levels. Second, elements of the DNA do not disappear as one zooms outinstead the scale levels at which they are observed group these elements differently. Third, we have detailed information and thus geometry for the entire dataset and for all scale levels, posing a challenge for interactive visual exploration. Finally, the conceptual scale levels for genome data are close in scale space, requiring us to find ways to visually embed a smaller scale into a coarser one. We address these challenges by creating a new multi-scale visualization concept. We use a scale-dependent camera model that controls the visual embedding of the scales into their respective parents, the rendering of a subset of the scale hierarchy, and the location, size, and scope of the view. In traversing the scales, ScaleTrotter is roaming between 2D and 3D visual representations that are depicted in integrated visuals. We discuss, specifically, how this form of multi-scale visualization follows from the specific characteristics of the genome data and describe its implementation. Finally, we discuss the implications of our work to the general illustrative depiction of multi-scale data.
C1 [Halladjian, Sarkis; Isenberg, Tobias] INRIA, Paris, France.
   [Halladjian, Sarkis] Univ Paris Saclay, St Aubin, France.
   [Miao, Haichao; Kouril, David; Groller, M. Eduard] TU Wien, Vienna, Austria.
   [Groller, M. Eduard] VRVis Res Ctr, Vienna, Austria.
   [Viola, Ivan] King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.
C3 Inria; Universite Paris Saclay; Technische Universitat Wien; King
   Abdullah University of Science & Technology
RP Halladjian, S (corresponding author), INRIA, Paris, France.; Halladjian, S (corresponding author), Univ Paris Saclay, St Aubin, France.
EM sarkis.halladjian@inria.fr; miao@cg.tuwien.ac.at;
   dvdkouril@cg.tuwien.ac.at; groeller@cg.tuwien.ac.at;
   ivan.viola@kaust.edu.sa; tobias.isenberg@inria.fr
RI Miao, Haichao/HNJ-6239-2023; Gröller, Eduard/AAH-2111-2020; Isenberg,
   Tobias/A-7575-2008; Viola, Ivan/O-8944-2014
OI Isenberg, Tobias/0000-0001-7953-8644; Miao, Haichao/0000-0001-6580-2918;
   Viola, Ivan/0000-0003-4248-6574
FU ILLUSTRARE grant by Austrian Science Fund (FWF) [I 2953-N31]; ILLUSTRARE
   grant by French National Research Agency (ANR) [ANR-16-CE91-0011-01];
   King Abdullah University of Science and Technology (KAUST)
   [BAS/1/1680-01-01]; ILLVISATION grant by WWTF [VRG11-010]; BMVIT; BMWFW;
   SFG; Vienna Business Agency [854174]; Styria; Agence Nationale de la
   Recherche (ANR) [ANR-16-CE91-0011] Funding Source: Agence Nationale de
   la Recherche (ANR)
FX We thank the genome scientists who provided the data that our tool
   relies on and who answered our questions about it. We also thank
   everyone who provided feedback about our approach. Part of this work was
   funded under the ILLUSTRARE grant by both the Austrian Science Fund
   (FWF): I 2953-N31 and the French National Research Agency (ANR):
   ANR-16-CE91-0011-01. The research was further supported by funding from
   King Abdullah University of Science and Technology (KAUST), under award
   number BAS/1/1680-01-01 and by funding from ILLVISATION grant by WWTF
   (VRG11-010). Authors would like to thank Nanographics GmbH
   (nanographics.at) for providing the Marion Software Framework. This
   paper was partly written in collaboration with the VRVis Competence
   Center. VRVis is funded by BMVIT, BMWFW, Styria, SFG and Vienna Business
   Agency in the scope of COMET - Competence Centers for Excellent
   Technologies (854174), which is managed by FFG.
NR 61
TC 13
Z9 16
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 654
EP 664
DI 10.1109/TVCG.2019.2934334
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100061
PM 31425102
OA Green Submitted, hybrid
DA 2025-03-07
ER

PT J
AU Chen, SL
   Zheng, LT
   Zhang, Y
   Sun, ZX
   Xu, K
AF Chen, Songle
   Zheng, Lintao
   Zhang, Yan
   Sun, Zhixin
   Xu, Kai
TI VERAM: View-Enhanced Recurrent Attention Model for 3D Shape
   Classification
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Shape; Solid modeling; Estimation;
   Visualization; Task analysis; Computational modeling; 3D shape
   classification; multi-view 3D shape recognition; visual attention model;
   recurrent neural network; reinforcement learning; convolutional neural
   network
ID ACTIVE RECOGNITION; POSE ESTIMATION
AB Multi-view deep neural network is perhaps the most successful approach in 3D shape classification. However, the fusion of multi-view features based on max or average pooling lacks a view selection mechanism, limiting its application in, e.g., multi-view active object recognition by a robot. This paper presents VERAM, a view-enhanced recurrent attention model capable of actively selecting a sequence of views for highly accurate 3D shape classification. VERAM addresses an important issue commonly found in existing attention-based models, i.e., the unbalanced training of the subnetworks corresponding to next view estimation and shape classification. The classification subnetwork is easily overfitted while the view estimation one is usually poorly trained, leading to a suboptimal classification performance. This is surmounted by three essential view-enhancement strategies: 1) enhancing the information flow of gradient backpropagation for the view estimation subnetwork, 2) devising a highly informative reward function for the reinforcement training of view estimation and 3) formulating a novel loss function that explicitly circumvents view duplication. Taking grayscale image as input and AlexNet as CNN architecture, VERAM with 9 views achieves instance-level and class-level accuracy of 95.5 and 95.3 percent on ModelNet10, 93.7 and 92.1 percent on ModelNet40, both are the state-of-the-art performance under the same number of views.
C1 [Chen, Songle; Sun, Zhixin] Nanjing Univ Posts & Telecommun, Jiangsu High Technol Res Key Lab Wireless Sensor, Nanjing 210023, Jiangsu, Peoples R China.
   [Zheng, Lintao; Xu, Kai] Natl Univ Def Technol, Sch Comp, Changsha 410073, Hunan, Peoples R China.
   [Zhang, Yan] Nanjing Univ, Dept Comp Sci & Technol, Nanjing 210008, Jiangsu, Peoples R China.
C3 Nanjing University of Posts & Telecommunications; National University of
   Defense Technology - China; Nanjing University
RP Xu, K (corresponding author), Natl Univ Def Technol, Sch Comp, Changsha 410073, Hunan, Peoples R China.
EM chensongle@njupt.edu.cn; lintaozheng1991@gmail.com;
   zhangyannju@nju.edu.cn; sunzx@njupt.edu.cn; kevin.kai.xu@gmail.com
RI Zhang, Yan/AAO-6966-2021
FU National Natural Science Foundation of China [61373135, 61672299,
   61702281, 61532003, 61572507, 61622212]; Postdoctoral Science Foundation
   of Jiangsu Province of China [1701046A]
FX This work was supported in part by the National Natural Science
   Foundation of China (61373135, 61672299, 61702281, 61532003, 61572507
   and 61622212), and the Postdoctoral Science Foundation of Jiangsu
   Province of China (No.1701046A).
NR 50
TC 56
Z9 60
U1 4
U2 40
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2019
VL 25
IS 12
BP 3244
EP 3257
DI 10.1109/TVCG.2018.2866793
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JJ7NV
UT WOS:000494341300004
PM 30137010
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kim, Y
   Eun, H
   Jung, C
   Kim, C
AF Kim, YoonHyung
   Eun, Hyunjun
   Jung, Chanho
   Kim, Changick
TI A Quad Edge-Based Grid Encoding Model for Content-Aware Image
   Retargeting
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computational modeling; Visualization; Strain; Optimization; Distortion;
   Encoding; Adaptation models; Image retargeting; 2D grid deformation;
   saliency detection; line segment detection; image quality assessment
ID PRESERVING APPROACH; QUALITY ASSESSMENT; DEFORMATION
AB In this paper, we present a novel grid encoding model for content-aware image retargeting. In contrast to previous approaches such as vertex-based and axis-aligned grid encoding models, our approach takes each horizontal/vertical distance between two adjacent vertices as an optimization variable. Upon this difference-based encoding scheme, every vertex position of a target grid is subsequently determined after optimizing the one-dimensional values. Our quad edge-based grid model has two major advantages for image retargeting. First, the model enables a grid optimization problem to be developed in a simple quadratic program while ensuring the global convexity of objective functions. Second, due to the independency of variables, spatial regularizations can be applied in a locally adaptive manner to preserve structural components. Based on this model, we propose three quadratic objective functions. Note that, in our work, their linear combination guides a grid deformation process to obtain a visually comfortable retargeting result by preserving salient regions and structural components of an input image. Comparative evaluations have been conducted with ten existing state-of-the-art image retargeting methods, and the results show that our method built upon the quad edge-based model consistently outperforms other previous methods both on qualitative and quantitative perspectives.
C1 [Kim, YoonHyung; Eun, Hyunjun; Kim, Changick] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.
   [Jung, Chanho] Hanbat Natl Univ, Dept Elect Engn, Daejeon 34158, South Korea.
C3 Korea Advanced Institute of Science & Technology (KAIST); Hanbat
   National University
RP Kim, C (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon 34141, South Korea.
EM yhkim1127@kaist.ac.kr; hj.eun@kaist.ac.kr; peterjung@hanbat.ac.kr;
   changick@kaist.ac.kr
OI Eun, Hyunjun/0000-0001-7794-5377
FU MCST (Ministry of Culture, Sports&
   Tourism)/KOCCA(KoreaCreativeContentAgency) [R2016030044]
FX This work was supported by MCST (Ministry of Culture, Sports&
   Tourism)/KOCCA(KoreaCreativeContentAgency) (R2016030044 -Development of
   Centext-Based Sports Video Analysis, Summarization, and Retrieval
   Technologies).
NR 44
TC 8
Z9 8
U1 1
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2019
VL 25
IS 12
BP 3202
EP 3215
DI 10.1109/TVCG.2018.2866106
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JJ7NV
UT WOS:000494341300001
PM 30130231
DA 2025-03-07
ER

PT J
AU Yang, H
   Wang, BY
   Vesdapunt, N
   Guo, MY
   Kang, SB
AF Yang, Huan
   Wang, Baoyuan
   Vesdapunt, Noranart
   Guo, Minyi
   Kang, Sing Bing
TI Personalized Exposure Control Using Adaptive Metering and Reinforcement
   Learning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Auto exposure; reinforcement learning; personalization
ID AUTO-EXPOSURE
AB We propose a reinforcement learning approach for real-time exposure control of a mobile camera that is personalizable. Our approach is based on Markov Decision Process (MDP). In the camera viewfinder or live preview mode, given the current frame, our system predicts the change in exposure so as to optimize the trade-off among image quality, fast convergence, and minimal temporal oscillation. We model the exposure prediction function as a fully convolutional neural network that can be trained through Gaussian policy gradient in an end-to-end fashion. As a result, our system can associate scene semantics with exposure values; it can also be extended to personalize the exposure adjustments for a user and device. We improve the learning performance by incorporating an adaptive metering module that links semantics with exposure. This adaptive metering module generalizes the conventional spot or matrix metering techniques. We validate our system using the MIT FiveK [1] and our own datasets captured using iPhone 7 and Google Pixel. Experimental results show that our system exhibits stable real-time behavior while improving visual quality compared to what is achieved through native camera control.
C1 [Yang, Huan; Guo, Minyi] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200000, Peoples R China.
   [Wang, Baoyuan; Vesdapunt, Noranart; Kang, Sing Bing] Microsoft Res, Redmond, WA 98052 USA.
C3 Shanghai Jiao Tong University; Microsoft
RP Wang, BY (corresponding author), Microsoft Res, Redmond, WA 98052 USA.
EM yanghuanflc@sjtu.edu.cn; baoyuanw@microsoft.com; noves@microsoft.com;
   guo-my@cs.sjtu.edu.cn; sbkang@ieee.org
OI Vesdapunt, Noranart/0000-0002-7473-3149; Wang,
   Baoyuan/0000-0002-8268-7517
NR 37
TC 11
Z9 12
U1 1
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2019
VL 25
IS 10
BP 2953
EP 2968
DI 10.1109/TVCG.2018.2865555
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IV5CC
UT WOS:000484287800007
PM 30113896
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Rojas, J
   Liu, TT
   Kavan, L
AF Rojas, Junior
   Liu, Tiantian
   Kavan, Ladislav
TI Average Vector Field Integration for St. Venant-Kirchhoff Deformable
   Models
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Animation; three-dimensional graphics and realism
ID MOMENTUM CONSERVING ALGORITHMS; TIME INTEGRATION; EXACT ENERGY
AB We propose Average Vector Field (AVF) integration for simulation of deformable solids in physics-based animation. Our method achieves exact energy conservation for the St. Venant-Kirchhoff material without any correction steps or extra parameters. Exact energy conservation implies that our resulting animations 1) cannot explode and 2) do not suffer from numerical damping, which are two common problems with previous numerical integration techniques. Our method produces lively motion even with large time steps as typically used in physics-based animation. Our implicit update rules can be formulated as a minimization problem and solved in a similar way as optimization-based backward Euler, with only a mild computing overhead. Our approach also supports damping and collision response models, making it easy to deploy in practical computer animation pipelines.
C1 [Rojas, Junior; Kavan, Ladislav] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
   [Liu, Tiantian] Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA.
C3 Utah System of Higher Education; University of Utah; University of
   Pennsylvania
RP Rojas, J (corresponding author), Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
EM jrojasdavalos@gmail.com; ltt1598@gmail.com; ladislav@gmail.com
FU US National Science Foundation [IIS-1617172, IIS-1622360]
FX We would like to thank Bernhard Thomaszewski and Eftychios Sifakis for
   the valuable discussions, Petr Kadlecek for his help with rendering and
   Jing Li for her help generating our simulation examples. This material
   is based upon work supported by the US National Science Foundation under
   Grant Numbers IIS-1617172 and IIS-1622360. Any opinions, findings, and
   conclusions or recommendations expressed in this material are those of
   the author(s) and do not necessarily reflect the views of the US
   National Science Foundation. We also gratefully acknowledge the support
   of Activision.
NR 52
TC 2
Z9 2
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2019
VL 25
IS 8
BP 2529
EP 2539
DI 10.1109/TVCG.2018.2851233
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IG2AT
UT WOS:000473597800002
PM 29994399
OA Bronze
DA 2025-03-07
ER

PT J
AU Heinrich, F
   Joeres, F
   Lawonn, K
   Hansen, C
AF Heinrich, Florian
   Joeres, Fabian
   Lawonn, Kai
   Hansen, Christian
TI Comparison of Projective Augmented Reality Concepts to Support Medical
   Needle Insertion
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE Pacific Visualization Symposium (IEEE PacificVis)
CY APR 23-26, 2019
CL Chulalongkorn Univ, Bangkok, THAILAND
SP IEEE, IEEE Visualizat & Graph Tech Comm
HO Chulalongkorn Univ
DE Visualization; augmented reality; evaluation; medical navigation
   systems; instrument guidance; needle placement
ID NAVIGATION SYSTEM; VISUALIZATION; GUIDANCE; ACCURACY; BIOPSY;
   INTERVENTIONS; PLACEMENT
AB Augmented reality (AR) is a promising tool to improve instrument navigation in needle-based interventions. Limited research has been conducted regarding suitable navigation visualizations. In this work, three navigation concepts based on existing approaches were compared in a user study using a projective AR setup. Each concept was implemented with three different scales for accuracy-to-color mapping and two methods of navigation indicator scaling. Participants were asked to perform simulated needle insertion tasks with each of the resulting 18 prototypes. Insertion angle and insertion depth accuracies were measured and analyzed, as well as task completion time and participants' subjectively perceived task difficulty. Results show a clear ranking of visualization concepts across variables. Less consistent results were obtained for the color and indicator scaling factors. Results suggest that logarithmic indicator scaling achieved better accuracy, but participants perceived it to be more difficult than linear scaling. With specific results for angle and depth accuracy, our study contributes to the future composition of improved navigation support and systems for precise needle insertion or similar applications.
C1 [Heinrich, Florian; Joeres, Fabian; Hansen, Christian] Univ Magdeburg, Res Campus STIMULATE, D-39106 Magdeburg, Germany.
   [Lawonn, Kai] Univ Koblenz Landau, Med Visualizat, D-56070 Koblenz, Germany.
C3 Otto von Guericke University; University of Koblenz & Landau
RP Hansen, C (corresponding author), Univ Magdeburg, Res Campus STIMULATE, D-39106 Magdeburg, Germany.
EM lawonn@uni-koblenz.de; hansen@isg.cs.uni-magdeburg.de
OI Joeres, Fabian/0000-0001-8105-9263; Hansen,
   Christian/0000-0002-5734-7529; Heinrich, Florian/0000-0002-8169-3157
FU DFG [HA 7819/1-1, LA 3855/1-1]; European Regional Development Fund as
   part of the initiative "Sachsen-Anhalt WISSENSCHAFT Schwerpunkte"
   [ZS/2016/04/78123]
FX This project was funded by the DFG (HA 7819/1-1 and LA 3855/1-1) and the
   European Regional Development Fund under the operation number
   ZS/2016/04/78123 as part of the initiative "Sachsen-Anhalt WISSENSCHAFT
   Schwerpunkte".
NR 29
TC 21
Z9 22
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2019
VL 25
IS 6
BP 2157
EP 2167
DI 10.1109/TVCG.2019.2903942
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA HW8AC
UT WOS:000466910200002
PM 30892210
DA 2025-03-07
ER

PT J
AU Jones, JA
   Hopper, JE
   Bolas, MT
   Krum, DM
AF Jones, J. Adam
   Hopper, Jonathan E.
   Bolas, Mark T.
   Krum, David M.
TI Orientation Perception in Real and Virtual Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual Environments; Perception; Spatial Orientation; Visual
   Orientation
ID APPARENT LENGTH; DISCRIMINATION; ANGLE
AB Spatial perception in virtual environments has been a topic of intense research. Arguably, the majority of this work has focused on distance perception. However, orientation perception is also an important factor. In this paper, we systematically investigate allocentric orientation judgments in both real and virtual contexts over the course of four experiments. A pattern of sinusoidal judgment errors known to exist in 2D perspective displays is found to persist in immersive virtual environments. This pattern also manifests itself in a real world setting using two differing judgment methods. The findings suggest the presence of a radial anisotropy that persists across viewing contexts. Additionally, there is some evidence to suggest that observers have multiple strategies for processing orientations but further investigation is needed to fully describe this phenomenon. We also offer design suggestions for 3D user interfaces where users may perform orientation judgments.
C1 [Jones, J. Adam; Hopper, Jonathan E.] Univ Mississippi, High Fidel Virtual Environm Lab Hi5 Lab, University, MS 38677 USA.
   [Jones, J. Adam; Hopper, Jonathan E.] Univ Mississippi, Dept Comp & Informat Sci, University, MS 38677 USA.
   [Bolas, Mark T.] Microsoft, Redmond, WA USA.
   [Krum, David M.] Univ Southern Calif, Inst Creat Technol, Mixed Real Lab, Los Angeles, CA USA.
C3 University of Mississippi; University of Mississippi; Microsoft;
   University of Southern California
RP Jones, JA (corresponding author), Univ Mississippi, High Fidel Virtual Environm Lab Hi5 Lab, University, MS 38677 USA.; Jones, JA (corresponding author), Univ Mississippi, Dept Comp & Informat Sci, University, MS 38677 USA.
EM jadamj@acm.org
RI Krum, David/AAS-2694-2020; Jones, Jeffrey/GPP-4287-2022
FU Office of Naval Research [N00014-13-1-0237]
FX This material is based upon work supported by the Office of Naval
   Research under Grant No. N00014-13-1-0237. Any opinions, findings, and
   conclusions expressed in this material are those of the authors and do
   not necessarily reflect the views of ONR. The authors are also grateful
   to the USC-ICT Mixed Reality Lab (MxR) for the use of facilities and
   equipment needed for these studies.
NR 50
TC 3
Z9 5
U1 2
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2019
VL 25
IS 5
BP 2050
EP 2060
DI 10.1109/TVCG.2019.2898798
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA HR3EI
UT WOS:000463019100024
PM 30762557
DA 2025-03-07
ER

PT J
AU Miranda, F
   Doraiswamy, H
   Lage, M
   Wilson, L
   Hsieh, M
   Silva, CT
AF Miranda, Fabio
   Doraiswamy, Harish
   Lage, Marcos
   Wilson, Luc
   Hsieh, Mondrian
   Silva, Claudio T.
TI Shadow Accrual Maps: Efficient Accumulation of City-Scale Shadows Over
   Time
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Shadow accumulation; shadow accrual maps; visual analysis; urban
   development
ID VISUAL ANALYSIS; SOLAR; VISUALIZATION; DAYLIGHT; ENERGY; VOLUME
AB Large scale shadows from buildings in a city play an important role in determining the environmental quality of public spaces. They can be both beneficial, such as for pedestrians during summer, and detrimental, by impacting vegetation and by blocking direct sunlight. Determining the effects of shadows requires the accumulation of shadows over time across different periods in a year. In this paper, we propose a simple yet efficient class of approach that uses the properties of sun movement to track the changing position of shadows within a fixed time interval. We use this approach to extend two commonly used shadow techniques, shadow maps and ray tracing, and demonstrate the efficiency of our approach. Our technique is used to develop an interactive visual analysis system, Shadow Profiler, targeted at city planners and architects that allows them to test the impact of shadows for different development scenarios. We validate the usefulness of this system through case studies set in Manhattan, a dense borough of New York City.
C1 [Miranda, Fabio] NYU, New York, NY 10012 USA.
   [Doraiswamy, Harish] NYU, Ctr Data Sci, New York, NY 10012 USA.
   [Silva, Claudio T.] NYU, Comp Sci & Engn & Data Sci, New York, NY 10012 USA.
   [Lage, Marcos] Univ Fed Fluminense, BR-24220900 Niteroi, RJ, Brazil.
   [Wilson, Luc; Hsieh, Mondrian] Kohn Pedersen Fox Associates PC, New York, NY 10036 USA.
C3 New York University; New York University; New York University;
   Universidade Federal Fluminense
RP Miranda, F (corresponding author), NYU, New York, NY 10012 USA.
EM fmiranda@nyu.edu; harishd@nyu.edu; mlage@ic.uff.br; lwilson@kpf.com;
   mhsieh@kpf.com; csilva@nyu.edu
RI ; Lage, Marcos/K-4098-2012
OI Miranda, Fabio/0000-0001-8612-5805; Lage, Marcos/0000-0003-3868-8886;
   Doraiswamy, Harish/0000-0003-2995-250X; Silva,
   Claudio/0000-0003-2452-2295
FU Moore-Sloan Data Science Environment at NYU; NASA; DOE; Kohn Pedersen
   Fox Associates; US National Science Foundation [CNS-1229185,
   CCF-1533564, CNS-1544753, CNS-1730396]; CNPq; FAPERJ; DARPA D3M program;
   Division Of Computer and Network Systems; Direct For Computer & Info
   Scie & Enginr [1229185] Funding Source: National Science Foundation
FX This work was supported in part by: the Moore-Sloan Data Science
   Environment at NYU; NASA; DOE; Kohn Pedersen Fox Associates; US National
   Science Foundation awards CNS-1229185, CCF-1533564, CNS-1544753,
   CNS-1730396; CNPq; and FAPERJ. C. T. Silva is partially supported by the
   DARPA D3M program. Any opinions, findings, and conclusions or
   recommendations expressed in this material are those of the authors and
   do not necessarily reflect the views of DARPA. Fabio Miranda and Harish
   Doraiswamy have contributed equally to this work.
NR 65
TC 31
Z9 36
U1 4
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2019
VL 25
IS 3
BP 1559
EP 1574
DI 10.1109/TVCG.2018.2802945
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HK3NZ
UT WOS:000457824500010
PM 29994514
OA Bronze, Green Submitted
DA 2025-03-07
ER

PT J
AU Zhou, B
   Chiang, YJ
   Wang, C
AF Zhou, Bo
   Chiang, Yi-Jen
   Wang, Cong
TI Efficient Local Statistical Analysis via Point-Wise Histograms in
   Tetrahedral Meshes and Curvilinear Grids
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Tetrahedral meshes and curvilinear grids; scalar field data; vector
   field data; geometry-based techniques; mathematical foundations for
   visualization
ID HIERARCHICAL EXPLORATION; ISOSURFACE; COMPLEX
AB Local histograms (i.e., point-wise histograms computed from local regions of mesh vertices) have been used in many data analysis and visualization applications. Previous methods for computing local histograms mainly work for regular or rectilinear grids only. In this paper, we develop theory and novel algorithms for computing local histograms in tetrahedral meshes and curvilinear grids. Our algorithms are theoretically sound and efficient, and work effectively and fast in practice. Our main focus is on scalar fields, but the algorithms also work for vector fields as a by-product with small, easy modifications. Our methods can benefit information theoretic and other distribution-driven analysis. The experiments demonstrate the efficacy of our new techniques, including a utility case study on tetrahedral vector field visualization.
C1 [Zhou, Bo; Chiang, Yi-Jen; Wang, Cong] NYU, Tandon Sch Engn, CSE Dept, Brooklyn, NY 11201 USA.
C3 New York University; New York University Tandon School of Engineering
RP Chiang, YJ (corresponding author), NYU, Tandon Sch Engn, CSE Dept, Brooklyn, NY 11201 USA.
EM bz387@nyu.edu; chiang@nyu.edu; cw1068@nyu.edu
FU DOE [DE-SC0004874]; U.S. Department of Energy (DOE) [DE-SC0004874]
   Funding Source: U.S. Department of Energy (DOE)
FX This work was supported in part by DOE Grant DE-SC0004874, program
   manager Lucy Nowell.
NR 51
TC 1
Z9 1
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2019
VL 25
IS 2
BP 1392
EP 1406
DI 10.1109/TVCG.2018.2796555
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG5ZY
UT WOS:000455062000012
PM 29994603
OA Bronze
DA 2025-03-07
ER

PT J
AU Buchmüller, J
   Jäckie, D
   Cakmak, E
   Brandes, U
   Keim, DA
AF Buchmueller, Jun
   Jaeckie, Dominik
   Cakmak, Eren
   Brandes, Ulrik
   Keim, Daniel A.
TI MotionRugs: Visualizing Collective Trends in Space and Time
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Spatio-Temporal Visualization; Spatial Abstraction; Spatial Index
   Structures; Collective Movement
ID ANALYTICS; MOVEMENT
AB Understanding the movement patterns of collectives, such as flocks of birds or fish swarms, is an interesting open research question. The collectives are driven by mutual objectives or react to individual direction changes and external influence factors and stimuli. The challenge in visualizing collective movement data is to show space and time of hundreds of movements at the same time to enable the detection of spatio temporal patterns. In this paper, we propose MotionRugs, a novel space efficient technique for visualizing moving groups of entities. Building upon established space-partitioning strategies, our approach reduces the spatial dimensions in each time step to a one-dimensional ordered representation of the individual entities. By design, MotionRugs provides an overlap-free, compact overview of the development of group movements over time and thus, enables analysts to visually identify and explore group-specific temporal patterns. We demonstrate the usefulness of our approach in the field of fish swarm analysis and report on initial feedback of domain experts from the field of collective behavior.
C1 [Buchmueller, Jun; Jaeckie, Dominik; Cakmak, Eren; Keim, Daniel A.] Univ Konstanz, Constance, Germany.
   [Brandes, Ulrik] Swiss Fed Inst Technol, Zurich, Switzerland.
C3 University of Konstanz; Swiss Federal Institutes of Technology Domain;
   ETH Zurich
RP Buchmüller, J (corresponding author), Univ Konstanz, Constance, Germany.
EM juri.buchmueller@uni-koustauz.de; dominik.jaeckle@uni-koustauz.de;
   eren.cakmak@uni-koustauz.de; ulrik.brandes@gess.ethz.ch;
   kebn@uni-koustauz.de
RI Keim, Daniel/X-7749-2019
OI Cakmak, Eren/0000-0002-1812-2271
NR 54
TC 21
Z9 25
U1 1
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 76
EP 86
DI 10.1109/TVCG.2018.2865049
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA HD6IJ
UT WOS:000452640000008
PM 30136979
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Camisetty, A
   Chandurkar, C
   Sun, MY
   Koop, D
AF Camisetty, Akhilesh
   Chandurkar, Chaitanya
   Sun, Maoyuan
   Koop, David
TI Enhancing Web-based Analytics Applications through Provenance
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Collaboration; provenance; streaming data; history; web
ID VISUAL EXPLORATION; SENSEMAKING
AB Visual analytics systems continue to integrate new technologies and leverage modern environments for exploration and collaboration, making tools and techniques available to a wide audience through web browsers. Many of these systems have been developed with rich interactions, offering users the opportunity to examine details and explore hypotheses that have not been directly encoded by a designer. Understanding is enhanced when users can replay and revisit the steps in the sensemaking process, and in collaborative settings, it is especially important to be able to review not only the current state but also what decisions were made along the way. Unfortunately, many web-based systems lack the ability to capture such reasoning, and the path to a result is transient, forgotten when a user moves to a new view. This paper explores the requirements to augment existing client-side web applications with support for capturing, reviewing, sharing, and reusing steps in the reasoning process. Furthermore, it considers situations where decisions are made with streaming data, and the insights gained from revisiting those choices when more data is available. It presents a proof of concept, the Shareable Interactive Manipulation Provenance framework (SIMProv.js), that addresses these requirements in a modern, client-side JavaScript library, and describes how it can be integrated with existing frameworks.
C1 [Camisetty, Akhilesh; Chandurkar, Chaitanya; Sun, Maoyuan; Koop, David] UMass Dartmouth, Dartmouth, NS 02747, Canada.
RP Camisetty, A (corresponding author), UMass Dartmouth, Dartmouth, NS 02747, Canada.
EM acamisetty@umassd.edu; cchandurkar@umassd.edu; smaoyuan@umassd.edu;
   dkoop@umassd.edu
RI Sun, Maoyuan/AAJ-4301-2020
OI Sun, Maoyuan/0000-0002-0990-2620
NR 49
TC 13
Z9 15
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 131
EP 141
DI 10.1109/TVCG.2018.2865039
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000013
PM 30346289
DA 2025-03-07
ER

PT J
AU Cavallo, M
   Demiralp, Ç
AF Cavallo, Marco
   Demiralp, Cagatay
TI Clustrophile 2: Guided Visual Clustering Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Clustering tour; Guided data analysis; Exploratory data analysis;
   Interactive clustering analysis; Interpretability; Explainability;
   Visual data exploration recommendation; Dimensionality reduction;
   What-if analysis; Clustrophile; Unsupervised learning
ID VISUALIZATION; ALGORITHM; TOOL
AB Data clustering is a common unsupervised learning method frequently used in exploratory data analysis. However, identifying relevant structures in unlabeled, high-dimensional data is nontrivial. requiring iterative experimentation with clustering parameters as well as data features and instances. The number of possible clusterings for a typical dataset is vast, and navigating in this vast space is also challenging. The absence of ground-truth labels makes it impossible to define an optimal solution, thus requiring user judgment to establish what can be considered a satisfiable clustering result. Data scientists need adequate interactive tools to effectively explore and navigate the large clustering space so as to improve the effectiveness of exploratory clustering analysis. We introduce Clustrophile 2, a new interactive tool for guided clustering analysis. Clustrophile 2 guides users in clustering-based exploratory analysis, adapts user feedback to improve user guidance, facilitates the interpretation of clusters, and helps quickly reason about differences between clusterings. To this end, Clustrophile 2 contributes a novel feature, the Clustering Tour, to help users choose clustering parameters and assess the quality of different clustering results in relation to current analysis goals and user expectations. We evaluate Clustrophile 2 through a user study with 12 data scientists, who used our tool to explore and interpret sub-cohorts in a dataset of Parkinson's disease patients. Results suggest that Clustrophile 2 improves the speed and effectiveness of exploratory clustering analysis for both experts and non-experts.
C1 [Cavallo, Marco] IBM Res, Yorktown Hts, NY 10598 USA.
   [Demiralp, Cagatay] MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.
   [Demiralp, Cagatay] Fitnescity Labs, Cambridge, MA USA.
C3 International Business Machines (IBM); IBM USA; Massachusetts Institute
   of Technology (MIT)
RP Cavallo, M (corresponding author), IBM Res, Yorktown Hts, NY 10598 USA.
EM mcavall@us.ibm.com; cagatay@csail.mit.edu
NR 45
TC 61
Z9 71
U1 2
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 267
EP 276
DI 10.1109/TVCG.2018.2864477
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000026
PM 30130194
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kerzner, E
   Goodwin, S
   Dykes, J
   Jones, S
   Meyer, M
AF Kerzner, Ethan
   Goodwin, Sarah
   Dykes, Jason
   Jones, Sara
   Meyer, Miriah
TI A Framework for Creative Visualization-Opportunities Workshops
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE User-centered visualization design; design studies; creativity
   workshops; critically reflective practice
ID DESIGN; REQUIREMENTS
AB Applied visualization researchers often work closely with domain collaborators to explore new and useful applications of visualization. The early stages of collaborations are typically time consuming for all stakeholders as researchers piece together an understanding of domain challenges from disparate discussions and meetings. A number of recent projects, however, report on the use of creative visualization-opportunities (CVO) workshops to accelerate the early stages of applied work, eliciting a wealth of requirements in a few days of focused work. Yet, there is no established guidance for how to use such workshops effectively. In this paper, we present the results of a 2-year collaboration in which we analyzed the use of 17 workshops in 10 visualization contexts. Its primary contribution is a framework for CVO workshops that: 1) identifies a process model for using workshops; 2) describes a structure of what happens within effective workshops; 3) recommends 25 actionable guidelines for future workshops; and 4) presents an example workshop and workshop methods. The creation of this framework exemplifies the use of critical reflection to learn about visualization in practice from diverse studies and experience.
C1 [Kerzner, Ethan; Meyer, Miriah] Univ Utah, Salt Lake City, UT 84112 USA.
   [Goodwin, Sarah] RMIT Univ, Melbourne, Vic, Australia.
   [Goodwin, Sarah] Monash Univ, Clayton, Vic, Australia.
   [Dykes, Jason; Jones, Sara] City Univ London, London, England.
C3 Utah System of Higher Education; University of Utah; Royal Melbourne
   Institute of Technology (RMIT); Monash University; City St Georges,
   University of London; City, University of London
RP Kerzner, E (corresponding author), Univ Utah, Salt Lake City, UT 84112 USA.
EM kerzner@sci.utah.edu; sarah.goodwin@monash.edu; j.dykes@city.ac.uk;
   s.v.jones@city.ac.uk; miriah@cs.utah.edu
RI Dykes, Jason/AAD-6067-2021
OI Jones, Sara/0000-0003-4789-4948; Goodwin, Sarah/0000-0001-8894-8282
FU NSF [IIS-1350896]
FX We are grateful to the participants, facilitators, and fellow
   researchers in all of our workshops. We thank the following people for
   their feedback and contributions to this work: the anonymous reviewers,
   Graham Dove, Tim Dwyer, Peter Hoghton, Christine Pickett, David Rogers,
   Francesca Samsel, members of the Vis Design Lab at the University of
   Utah, and members of the giCentre at City, University of London. This
   work was supported in part by NSF Grant IIS-1350896.
NR 90
TC 43
Z9 46
U1 0
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 748
EP 758
DI 10.1109/TVCG.2018.2865241
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000071
PM 30137005
OA Green Accepted, Green Submitted
DA 2025-03-07
ER

PT J
AU Lu, ZC
   Fan, MM
   Wang, Y
   Zhao, J
   Annett, M
   Wigdor, D
AF Lu, Zhicong
   Fan, Mingming
   Wang, Yun
   Zhao, Jian
   Annett, Michelle
   Wigdor, Daniel
TI InkPlanner: Supporting Prewriting via Intelligent Visual Diagramming
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Writing; prewriting; diagraming; content and structure recommendation;
   pen and touch interfaces
AB Prewriting is the process of generating and organizing ideas before drafting a document. Although often overlooked by novice writers and writing tool developers, prewriting is a critical process that improves the quality of a final document. To better understand current prewriting practices, we first conducted interviews with writing learners and experts. Based on the learners' needs and experts' recommendations, we then designed and developed InkPlanner, a novel pen and touch visualization tool that allows writers to utilize visual diagramming for ideation during prewriting. InkPlanner further allows writers to sort their ideas into a logical and sequential narrative by using a novel widget - NarrativeLine. Using a NarrativeLine, InkPlanner can automatically generate a document outline to guide later drafting exercises. Inkplanner is powered by machine-generated semantic and structural suggestions that are curated from various texts. To qualitatively review the tool and understand how writers use InkPlanner for prewriting, two writing experts were interviewed and a user study was conducted with university students. The results demonstrated that InkPlanner encouraged writers to generate more diverse ideas and also enabled them to think more strategically about how to organize their ideas for later drafting.
C1 [Lu, Zhicong; Fan, Mingming; Wigdor, Daniel] Univ Toronto, Toronto, ON, Canada.
   [Wang, Yun] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Zhao, Jian] FX Palo Alto Lab, Palo Alto, CA USA.
C3 University of Toronto; Hong Kong University of Science & Technology
RP Lu, ZC (corresponding author), Univ Toronto, Toronto, ON, Canada.
EM luzhc@dgp.toronto.edu; mfan@dgp.toronto.edu; ywangch@connect.ust.hk;
   zhao@fxpal.com; mishmashmakers@gmail.com; daniel@dgp.toronto.edu
RI Fan, Mingming/LMM-9437-2024; Lu, Zhicong/JRX-3247-2023
OI Fan, Mingming/0000-0002-0356-4712; Lu, Zhicong/0000-0002-7761-6351
FU NSERC
FX The authors wish to thank Dr. Jane Freeman, Dr. Fanny Chevalier, Dr.
   Daniel Avrahami, and the reviewers for their valuable comments, and the
   experts and the participants for their contributions to this work. This
   work was supported in part by a grant from NSERC.
NR 59
TC 7
Z9 9
U1 0
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 277
EP 287
DI 10.1109/TVCG.2018.2864887
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA HD6IJ
UT WOS:000452640000027
PM 30136955
DA 2025-03-07
ER

PT J
AU Sagristà, A
   Jordan, S
   Müller, T
   Sadlo, F
AF Sagrista, Antoni
   Jordan, Stefan
   Mueller, Thomas
   Sadlo, Filip
TI Gaia Sky: Navigating the Gaia Catalog
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Astronomy visualization; 3D Universe software; star catalog rendering;
   Gaia mission
AB In this paper, we present Gaia Sky, a free and open-source multiplatform 3D Universe system, developed since 2014 in the Data Processing and Analysis Consortium framework of ESA's Gaia mission. Gaia's data release 2 represents the largest catalog of the stars of our Galaxy, comprising 1.3 billion star positions, with parallaxes, proper motions, magnitudes, and colors. In this mission, Gaia Sky is the central tool for off-the-shelf visualization of these data, and for aiding production of outreach material. With its capabilities to effectively handle these data, to enable seamless navigation along the high dynamic range of distances, and at the same time to provide advanced visualization techniques including relativistic aberration and gravitational wave effects, currently no actively maintained cross-platform, modern, and open alternative exists.
C1 [Sagrista, Antoni; Jordan, Stefan; Sadlo, Filip] Heidelberg Univ, Heidelberg, Germany.
   [Mueller, Thomas] Max Planck Inst Astron, Heidelberg, Germany.
C3 Ruprecht Karls University Heidelberg; Max Planck Society
RP Sagristà, A (corresponding author), Heidelberg Univ, Heidelberg, Germany.
EM toni.sagrista@iwr.uni-heidelberg.de; jordan@ari.uni-heidelberg.de;
   tmueller@mpia.de; sadlo@uni-heidelberg.de
RI Anton, Sonia/K-6041-2015
OI Muller, Thomas/0000-0002-2003-4465; Jordan, Stefan/0000-0001-6316-6831;
   Sagrista Selles, Antoni/0000-0001-6191-2028
NR 26
TC 13
Z9 15
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 1070
EP 1079
DI 10.1109/TVCG.2018.2864508
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000102
PM 30130201
DA 2025-03-07
ER

PT J
AU Weng, D
   Chen, R
   Deng, ZK
   Wu, FR
   Chen, JM
   Wu, YC
AF Weng, Di
   Chen, Ran
   Deng, Zikun
   Wu, Feiran
   Chen, Jingmin
   Wu, Yingcai
TI SRVis: Towards Better Spatial Integration in Ranking Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Spatial ranking; visualization
ID EXPLORATION
AB Interactive ranking techniques have substantially promoted analysts' ability in making judicious and informed decisions effectively based on multiple criteria. However, the existing techniques cannot satisfactorily support the analysis tasks involved in ranking large-scale spatial alternatives, such as selecting optimal locations for chain stores, where the complex spatial contexts involved are essential to the decision-making process. Limitations observed in the prior attempts of integrating rankings with spatial contexts motivate us to develop a context-integrated visual ranking technique. Based on a set of generic design requirements we summarized by collaborating with domain experts, we propose SRVis, a novel spatial ranking visualization technique that supports efficient spatial multi-criteria decision-making processes by addressing three major challenges in the aforementioned context integration, namely, a) the presentation of spatial rankings and contexts, b) the scalability of rankings' visual representations, and c) the analysis of context-integrated spatial rankings. Specifically, we encode massive rankings and their cause with scalable matrix-based visualizations and stacked bar charts based on a novel two-phase optimization framework that minimizes the information loss, and the flexible spatial filtering and intuitive comparative analysis are adopted to enable the in-depth evaluation of the rankings and assist users in selecting the best spatial alternative. The effectiveness of the proposed technique has been evaluated and demonstrated with an empirical study of optimization methods, two case studies, and expert interviews.
C1 [Weng, Di; Chen, Ran; Deng, Zikun; Wu, Yingcai] Zhejiang Univ & Alibaba Zhejiang Univ Joint Inst, State Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
   [Wu, Feiran; Chen, Jingmin] Alibaba Grp, Hangzhou, Zhejiang, Peoples R China.
C3 Alibaba Group
RP Wu, YC (corresponding author), Zhejiang Univ & Alibaba Zhejiang Univ Joint Inst, State Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
EM dweng@zju.edu; crcrcry@zju.edu; zikun_rain@zju.edu;
   feiran.wufr@alibaba-inc.com; jingmin.cjm@alibaba-inc.com; ycwu@zju.edu
RI Weng, Di/ABG-7408-2020; Deng, Zikun/IQT-3106-2023
OI Weng, Di/0000-0003-2712-7274
FU National Key R&D Program of China [2018YFB1004300]; NSFC-Zhejiang Joint
   Fund for the Integration of Industrialization and Informatization
   [U1609217]; NSFC [61761136020, 61502416]; Zhejiang Provincial Natural
   Science Foundation [LR18F020001]; 100 Talents Program of Zhejiang
   University; Alibaba-Zhejiang University Joint Institute of Frontier
   Technologies
FX We thank all reviewers for their constructive comments. The work was
   supported by National Key R&D Program of China (2018YFB1004300),
   NSFC-Zhejiang Joint Fund for the Integration of Industrialization and
   Informatization (U1609217), NSFC (61761136020, 61502416), Zhejiang
   Provincial Natural Science Foundation (LR18F020001), the 100 Talents
   Program of Zhejiang University, and Alibaba-Zhejiang University Joint
   Institute of Frontier Technologies.
NR 60
TC 34
Z9 38
U1 0
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 459
EP 469
DI 10.1109/TVCG.2018.2865126
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000044
PM 30188825
DA 2025-03-07
ER

PT J
AU Zhao, Y
   Luo, F
   Chen, M
   Wang, Y
   Xia, J
   Zhou, FF
   Wang, YH
   Chen, Y
   Chen, W
AF Zhao, Ying
   Luo, Feng
   Chen, Minghui
   Wang, Yingchao
   Xia, Jiazhi
   Zhou, Fangfang
   Wang, Yunhai
   Chen, Yi
   Chen, Wei
TI Evaluating Multi-Dimensional Visualizations for Understanding Fuzzy
   Clusters
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Evaluation; multi-dimensional visualization; fuzzy clustering; parallel
   coordinate plot; scatterplot matrix; principal; component analysis;
   radviz
ID DIMENSIONAL STRUCTURES; IDENTIFICATION; SEGMENTATION; SCATTERPLOTS;
   PROJECTION; REDUCTION; RADVIZ
AB Fuzzy clustering assigns a probability of membership for a datum to a cluster, which veritably reflects real-world clustering scenarios but significantly increases the complexity of understanding fuzzy clusters. Many studies have demonstrated that visualization techniques for multi-dimensional data are beneficial to understand fuzzy clusters. However, no empirical evidence exists on the effectiveness and efficiency of these visualization techniques in solving analytical tasks featured by fuzzy clusters. In this paper, we conduct a controlled experiment to evaluate the ability of fuzzy clusters analysis to use four multi-dimensional visualization techniques, namely, parallel coordinate plot, scatterplot matrix, principal component analysis, and Radviz. First, we define the analytical tasks and their representative questions specific to fuzzy clusters analysis. Then, we design objective questionnaires to compare the accuracy, time, and satisfaction in using the four techniques to solve the questions. We also design subjective questionnaires to collect the experience of the volunteers with the four techniques in terms of ease of use, informativeness, and helpfulness. With a complete experiment process and a detailed result analysis, we test against four hypotheses that are formulated on the basis of our experience, and provide instructive guidance for analysts in selecting appropriate and efficient visualization techniques to analyze fuzzy clusters.
C1 [Zhao, Ying; Luo, Feng; Chen, Minghui; Wang, Yingchao; Xia, Jiazhi; Zhou, Fangfang] Cent S Univ, Changsha, Hunan, Peoples R China.
   [Wang, Yunhai] Shandong Univ, Jinan, Shandong, Peoples R China.
   [Chen, Yi] Beijing Technol & Business Univ, Beijing, Peoples R China.
   [Chen, Wei] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
C3 Central South University; Shandong University; Beijing Technology &
   Business University; Zhejiang University
RP Xia, J (corresponding author), Cent S Univ, Changsha, Hunan, Peoples R China.; Chen, W (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
EM zhaoying@csu.edu.cn; luofeng365@csu.edu.cn; minghui@csu.edu.cn;
   0903150322@csu.edu.cn; xiajiazhi@csu.edu.cn; zff@csu.edu.cn;
   cloudseawang@gmail.com; chenyi@th.btbu.edu.cn; chenwei@cad.zju.edu.cn
RI Zhao, Liangyu/IAO-7294-2023; Chen, Wei/AAR-9817-2020; CHEN,
   YAN/N-4135-2018
FU National Science Foundation of China [61672538, 61772315, 61772456];
   Open Project Program of the State Key Lab of CADAMP;CG, Zhejiang
   University [A1812]; Open Research Fund of Beijing Key Laboratory of Big
   Data Technology for Food Safety, Beijing Technology and Business
   University [BKBD-2018KF08]
FX We wish to thank the reviewers for their thoughtful comments, the
   volunteers for their active involvement, and Wei Huang and Yang Shi for
   the fruitful discussions. This work is supported by the National Science
   Foundation of China (No. 61672538, 61772315 and 61772456), the Open
   Project Program of the State Key Lab of CAD&CG, Zhejiang University (No.
   A1812), the Open Research Fund of Beijing Key Laboratory of Big Data
   Technology for Food Safety, Beijing Technology and Business University
   (No. BKBD-2018KF08).
NR 63
TC 59
Z9 73
U1 2
U2 36
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 12
EP 21
DI 10.1109/TVCG.2018.2865020
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000002
PM 30136966
DA 2025-03-07
ER

PT J
AU Guo, KW
   Xu, F
   Wang, YG
   Liu, YB
   Dai, QH
AF Guo, Kaiwen
   Xu, Feng
   Wang, Yangang
   Liu, Yebin
   Dai, Qionghai
TI Robust Non-Rigid Motion Tracking and Surface Reconstruction Using
   <i>L</i><sub>0</sub> Regularization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Performance capture; non-rigid; single-view; L0
ID PERFORMANCE CAPTURE; REGISTRATION; OBJECTS
AB We present a new motion tracking technique to robustly reconstruct non-rigid geometries and motions from a single view depth input recorded by a consumer depth sensor. The idea is based on the observation that most non-rigid motions (especially human-related motions) are intrinsically involved in articulate motion subspace. To take this advantage, we propose a novel L-0 based motion regularizer with an iterative solver that implicitly constrains local deformations with articulate structures, leading to reduced solution space and physical plausible deformations. The L-0 strategy is integrated into the available non-rigid motion tracking pipeline, and gradually extracts articulate joints information online with the tracking, which corrects the tracking errors in the results. The information of the articulate joints is used in the following tracking procedure to further improve the tracking accuracy and prevent tracking failures. Extensive experiments over complex human body motions with occlusions, facial and hand motions demonstrate that our approach substantially improves the robustness and accuracy in motion tracking.
C1 [Guo, Kaiwen; Liu, Yebin; Dai, Qionghai] Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.
   [Guo, Kaiwen; Xu, Feng; Liu, Yebin; Dai, Qionghai] Tsinghua Univ, TNList, Beijing 100084, Peoples R China.
   [Xu, Feng] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.
   [Wang, Yangang] Microsoft Res, Beijing 100080, Peoples R China.
C3 Tsinghua University; Tsinghua University; Tsinghua University;
   Microsoft; Microsoft China
RP Guo, KW (corresponding author), Tsinghua Univ, Dept Automat, Beijing 100084, Peoples R China.; Guo, KW (corresponding author), Tsinghua Univ, TNList, Beijing 100084, Peoples R China.
EM guokaiwen_neu@126.com; feng-xu@tsinghua.edu.cn; ygwang.thu@gmail.com;
   liuyebin@tsinghua.edu.cn; qhdai@tsinghua.edu.cn
RI Liu, Yebin/L-7393-2019; Dai, Qionghai/ABD-5298-2021
FU National key foundation for exploring scientific instrument
   [2013YQ140517]; open funding project of state key laboratory of virtual
   reality technology and systems of Beihang University [BUAA-VR-14KF-08];
   NSFC [61671268, 61522111, 61531014]
FX This work was supported by the National key foundation for exploring
   scientific instrument No. 2013YQ140517, the open funding project of
   state key laboratory of virtual reality technology and systems of
   Beihang University (Grant No. BUAA-VR-14KF-08), and NSFC (No. 61671268,
   61522111, and 61531014). Feng Xu is the corresponding author.
NR 54
TC 13
Z9 14
U1 4
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2018
VL 24
IS 5
BP 1770
EP 1783
DI 10.1109/TVCG.2017.2688331
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE7EW
UT WOS:000431397100007
PM 28368820
DA 2025-03-07
ER

PT J
AU Morgand, A
   Tamaazousti, M
   Bartoli, A
AF Morgand, Alexandre
   Tamaazousti, Mohamed
   Bartoli, Adrien
TI A Geometric Model for Specularity Prediction on Planar Surfaces with
   Multiple Light Sources
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE JOLIMAS; specular reflection; multiple light sources; phong;
   blinn-phong; specularity; prediction; retexturing; quadric; dual space;
   conic; real time
ID ILLUMINATION; REALITY
AB Specularities are often problematic in computer vision since they impact the dynamic range of the image intensity. A natural approach would be to predict and discard them using computer graphics models. However, these models depend on parameters which are difficult to estimate (light sources, objects' material properties and camera). We present a geometric model called JOLIMAS: JOint Light-MAterial Specularity, which predicts the shape of specularities. JOLIMAS is reconstructed from images of specularities observed on a planar surface. It implicitly includes light and material properties, which are intrinsic to specularities. This model was motivated by the observation that specularities have a conic shape on planar surfaces. The conic shape is obtained by projecting a fixed quadric on the planar surface. JOLIMAS thus predicts the specularity using a simple geometric approach with static parameters (object material and light source shape). It is adapted to indoor light sources such as light bulbs and fluorescent lamps. The prediction has been tested on synthetic and real sequences. It works in a multi-light context by reconstructing a quadric for each light source with special cases such as lights being switched on or off. We also used specularity prediction for dynamic retexturing and obtained convincing rendering results.
C1 [Morgand, Alexandre; Tamaazousti, Mohamed] CEA, LIST, F-91191 Gif Sur Yvette, France.
   [Morgand, Alexandre] IP UMR 6602 CNRS UCA CHU, F-72035 Clermont Ferrand, France.
   [Bartoli, Adrien] IP, F-63000 Clermont Ferrand, France.
C3 Universite Paris Saclay; CEA; Centre National de la Recherche
   Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences
   (INSIS)
RP Morgand, A (corresponding author), CEA, LIST, F-91191 Gif Sur Yvette, France.; Morgand, A (corresponding author), IP UMR 6602 CNRS UCA CHU, F-72035 Clermont Ferrand, France.
EM alexandre.morgand@cea.fr; mohamed.tamaazousti@cea.fr;
   adrien.bartoli@gmail.com
OI Morgand, Alexandre/0000-0003-1986-6894
FU EU through the ERC [307483 FLEXABLE]
FX This research has received funding from the EU's FP7 through the ERC
   research grant 307483 FLEXABLE.
NR 44
TC 7
Z9 7
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2018
VL 24
IS 5
BP 1691
EP 1704
DI 10.1109/TVCG.2017.2677445
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE7EW
UT WOS:000431397100001
PM 28278471
DA 2025-03-07
ER

PT J
AU Guo, SN
   Xu, K
   Zhao, RW
   Gotz, D
   Zha, HY
   Cao, N
AF Guo, Shunan
   Xu, Ke
   Zhao, Rongwen
   Gotz, David
   Zha, Hongyuan
   Cao, Nan
TI EventThread: Visual Summarization and Stage Analysis of Event Sequence
   Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Visual Knowledge Representation; Visual Knowledge Discovery; Data
   Clustering; Time Series Data; Illustrative Visualization
ID ALZHEIMERS-DISEASE; MARKOV-MODELS; PROGRESSION
AB Event sequence data such as electronic health records, a person's academic records, or car service records, are ordered series of events which have occurred over a period of time. Analyzing collections of event sequences can reveal common or semantically important sequential patterns. For example, event sequence analysis might reveal frequently used care plans for treating a disease, typical publishing patterns of professors, and the patterns of service that result in a well-maintained car. It is challenging, however, to visually explore large numbers of event sequences, or sequences with large numbers of event types. Existing methods focus on extracting explicitly matching patterns of events using statistical analysis to create stages of event progression over time. However, these methods fail to capture latent clusters of similar but not identical evolutions of event sequences. In this paper, we introduce a novel visualization system named EventThread which clusters event sequences into threads based on tensor analysis and visualizes the latent stage categories and evolution patterns by interactively grouping the threads by similarity into time-specific clusters. We demonstrate the effectiveness of EventThread through usage scenarios in three different application domains and via interviews with an expert user.
C1 [Guo, Shunan; Zha, Hongyuan] East China Normal Univ, Shanghai, Peoples R China.
   [Xu, Ke] Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
   [Zhao, Rongwen; Cao, Nan] Tongji Univ, iDVx Lab, Shanghai, Peoples R China.
   [Gotz, David] Univ North Carolina Chapel Hill, Chapel Hill, NC USA.
C3 East China Normal University; Hong Kong University of Science &
   Technology; Tongji University; University of North Carolina; University
   of North Carolina Chapel Hill; University of North Carolina School of
   Medicine
RP Cao, N (corresponding author), Tongji Univ, iDVx Lab, Shanghai, Peoples R China.
EM g.shunan@gmail.com; kxuak@connect.ust.hk; rongwen.zhao@tongji.edu.cn;
   gotz@unc.edu; zha@sei.ecnu.edu.cn; nan.cao@tongji.edu.cn
RI Guo, Shunan/AAE-2616-2019; Xu, Ke/HSH-1984-2023; Cao, Nan/O-5397-2014
FU NSFC [61602306]; STCSM [15JC1401700]; National Grants for the Thousand
   Young Talents in China; NSFC-Zhejiang Joint Fund for the Integration of
   Industrialization and Information [U1609220]
FX We would like to thank all the reviewers for their constructive
   comments. We also would like to thank all the users and domain experts
   who participated our user study. This work is a part of the research
   supported from NSFC Grants 61602306, STCSM 15JC1401700, the National
   Grants for the Thousand Young Talents in China, and the NSFC-Zhejiang
   Joint Fund for the Integration of Industrialization and Information
   under Grant No. U1609220.
NR 39
TC 75
Z9 89
U1 0
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 56
EP 65
DI 10.1109/TVCG.2017.2745320
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400008
PM 28866586
DA 2025-03-07
ER

PT J
AU Lekschas, F
   Bach, B
   Kerpedjiev, P
   Gehlenborg, N
   Pfister, H
AF Lekschas, Fritz
   Bach, Benjamin
   Kerpedjiev, Peter
   Gehlenborg, Nils
   Pfister, Hanspeter
TI HiPiler: Visual Exploration of Large Genome Interaction Matrices with
   Interactive Small Multiples
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Interactive Small Multiples; Matrix Comparison; Biomedical
   Visualization; Genomics
ID ORGANIZATION; VISUALIZATION; PRINCIPLES; PATTERNS; TIME; MAP
AB This paper presents an interactive visualization interface HiPiler for the exploration and visualization of regions-of-interest in large genome interaction matrices. Genome interaction matrices approximate the physical distance of pairs of regions on the genome to each other and can contain up to 3 million rows and columns with many sparse regions. Regions of interest (ROls) can be defined, e.g., by sets of adjacent rows and columns, or by specific visual patterns in the matrix. However, traditional matrix aggregation or pan-and-zoom interfaces fail in supporting search, inspection, and comparison of ROls in such large matrices. In HiPiler, ROls are first-class objects, represented as thumbnail-like "snippets". Snippets can be interactively explored and grouped or laid out automatically in scatterplots, or through dimension reduction methods. Snippets are linked to the entire navigable genome interaction matrix through brushing and linking. The design of HiPiler is based on a series of semi-structured interviews with 10 domain experts involved in the analysis and interpretation of genome interaction matrices. We describe six exploration tasks that are crucial for analysis of interaction matrices and demonstrate how HiPiler supports these tasks. We report on a user study with a series of data exploration sessions with domain experts to assess the usability of HiPiler as well as to demonstrate respective findings in the data.
C1 [Lekschas, Fritz; Bach, Benjamin; Pfister, Hanspeter] Harvard Univ, Cambridge, MA 02138 USA.
   [Kerpedjiev, Peter; Gehlenborg, Nils] Harvard Med Sch, Boston, MA USA.
C3 Harvard University; Harvard University; Harvard Medical School
RP Lekschas, F (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.
EM lekschas@seasharvard.edu; bbach@seasharvard.edu; pkerp@hms.harvard.edu;
   nils@hms.harvard.edu; pfister@seasharvard.edu
OI Pfister, Hanspeter/0000-0002-3620-2582; Lekschas,
   Fritz/0000-0001-8432-4835
FU National Institutes of Health [U01 CA200059, R00 HG007583]
FX We wish to thank N. Abdennur, B. Alver, H. Belaghzal, A. van den Berg,
   J. Dekker, G. Fudenberg, J. Gibcus, A. Goloborodko, D. Gorkin, M.
   Imakaev, Y. Liu, L. Mirny, J. Nubler, P. Park, H. Strobelt, and S. Wang
   for their invaluable feedback during the development of HiPiler. We also
   like to express our gratitude to the reviewer's useful feedback. This
   work was supported in part by the National Institutes of Health (U01
   CA200059 and R00 HG007583).
NR 45
TC 27
Z9 30
U1 0
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 522
EP 531
DI 10.1109/TVCG.2017.2745978
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400052
PM 28866592
OA Green Submitted, Green Accepted
DA 2025-03-07
ER

PT J
AU Pienta, R
   Hohman, F
   Endert, A
   Tamersoy, A
   Roundy, K
   Gates, C
   Navathe, S
   Chau, DH
AF Pienta, Robert
   Hohman, Fred
   Endert, Alex
   Tamersoy, Acar
   Roundy, Kevin
   Gates, Chris
   Navathe, Shamkant
   Chau, Duen Horng
TI VIGOR: Interactive Visual Exploration of Graph Query Results
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE graph querying; subgraph results; query result visualization
ID VISUALIZATION; PATTERNS
AB Finding patterns in graphs has become a vital challenge in many domains from biological systems, network security, to finance (e.g., finding money laundering rings of bankers and business owners). While there is significant interest in graph databases and querying techniques, less research has focused on helping analysts make sense of underlying patterns within a group of subgraph results. Visualizing graph query results is challenging, requiring effective summarization of a large number of subgraphs, each having potentially shared node-values, rich node features, and flexible structure across queries. We present VIGOR, a novel interactive visual analytics system, for exploring and making sense of query results. VIGOR uses multiple coordinated views, leveraging different data representations and organizations to streamline analysts sensemaking process. VIGOR contributes: (1) an exemplar-based interaction technique, where an analyst starts with a specific result and relaxes constraints to find other similar results or starts with only the structure (i.e., without node value constraints), and adds constraints to narrow in on specific results; and (2) a novel feature-aware subgraph result summarization. Through a collaboration with Symantec, we demonstrate how VIGOR helps tackle real-world problems through the discovery of security blindspots in a cybersecurity dataset with over 11,000 incidents. We also evaluate VIGOR with a within-subjects study, demonstrating VIGOR's ease of use over a leading graph database management system, and its ability to help analysts understand their results at higher speed and make fewer errors.
C1 [Pienta, Robert; Hohman, Fred; Endert, Alex; Navathe, Shamkant; Chau, Duen Horng] Georgia Tech, Atlanta, GA 30332 USA.
   [Tamersoy, Acar; Roundy, Kevin; Gates, Chris] Symantec Res Labs, Mountain View, CA USA.
C3 University System of Georgia; Georgia Institute of Technology; Symantec
RP Pienta, R (corresponding author), Georgia Tech, Atlanta, GA 30332 USA.
EM pientars@gatech.edu; fredhohman@gatech.edu; endert@gatech.edu;
   acar_tamersoy@symantec.com; kevin_roundy@symantec.com;
   chris_gates@symantec.com; sham@gatech.edu; polo@gatech.edu
OI Chau, Polo/0000-0001-9824-3323; Roundy, Kevin
   Alejandro/0000-0002-8285-1647
FU NSF IGERT [1258425]; NSF [IIS-1563816, TWC-1526254, IIS-1217559]; Direct
   For Computer & Info Scie & Enginr; Div Of Information & Intelligent
   Systems [1551614] Funding Source: National Science Foundation
FX This research has been supported in part by NSF IGERT grant 1258425, NSF
   grants IIS-1563816, TWC-1526254, and IIS-1217559.
NR 49
TC 33
Z9 45
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 215
EP 225
DI 10.1109/TVCG.2017.2744898
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400023
PM 28866563
DA 2025-03-07
ER

PT J
AU Zhao, X
   Wu, YH
   Cui, WW
   Du, XN
   Chen, Y
   Wang, Y
   Lee, DL
   Qu, HM
AF Zhao, Xun
   Wu, Yanhong
   Cui, Weiwei
   Du, Xinnan
   Chen, Yuan
   Wang, Yong
   Lee, Dik Lun
   Qu, Huamin
TI SkyLens: Visual Analysis of Skyline on Multi-dimensional Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Skyline query; skyline visualization multi-dimensional data; visual
   analytics; multi-criteria decision making
ID VISUALIZATION
AB Skyline queries have wide-ranging applications in fields that involve multi-criteria decision making, including tourism, retail industry, and human resources. By automatically removing incompetent candidates, skyline queries allow users to focus on a subset of superior data items (i.e.. the skyline), thus reducing the decision-making overhead. However, users are still required to interpret and compare these superior items manually before making a successful choice. This task is challenging because of two issues. First, people usually have fuzzy, unstable, and inconsistent preferences when presented with multiple candidates. Second, skyline queries do not reveal the reasons for the superiority of certain skyline points in a multi-dimensional space. To address these issues, we propose SkyLens, a visual analytic system aiming at revealing the superiority of skyline points from different perspectives and at different scales to aid users in their decision making. Two scenarios demonstrate the usefulness of SkyLens on two datasets with a dozen of attributes. A qualitative study is also conducted to show that users can efficiently accomplish skyline understanding and comparison tasks with SkyLens.
C1 [Zhao, Xun; Wu, Yanhong; Du, Xinnan; Chen, Yuan; Wang, Yong; Lee, Dik Lun; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
   [Cui, Weiwei] Microsoft Res Asia, Beijing, Peoples R China.
C3 Hong Kong University of Science & Technology; Microsoft; Microsoft
   China; Microsoft Research Asia
RP Wu, YH (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
EM xzhaoag@ust.hk; ywubk@ust.hk; weiweicu@microsoft.com; xduac@ust.hk;
   ychencd@ust.hk; ywangct@ust.hk; dlee@ust.hk; huamin@ust.hk
RI Wang, Yong/HKF-3903-2023
OI Wang, Yong/0000-0002-0092-0793
FU HK RGC GRF [16208514, 16241916]
FX The authors wish to thank the anonymous reviewers for their valuable
   comments. This research was supported in part by HK RGC GRF 16208514 and
   16241916.
NR 45
TC 30
Z9 33
U1 2
U2 20
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 246
EP 255
DI 10.1109/TVCG.2017.2744738
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400026
PM 28866558
OA Green Submitted, Green Accepted
DA 2025-03-07
ER

PT J
AU Peng, C
   Sahani, S
   Rushing, J
AF Peng, Chao
   Sahani, Sandip
   Rushing, John
TI A GPU-Accelerated Approach for Feature Tracking in Time-Varying Imagery
   Datasets
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Connected component labeling; time-varying imagery feature detection;
   GPGPU
AB We propose a novel parallel connected component labeling (CCL) algorithm along with efficient out-of-core data management to detect and track feature regions of large time-varying imagery datasets. Our approach contributes to the big data field with parallel algorithms tailored for GPU architectures. We remove the data dependency between frames and achieve pixel-level parallelism. Due to the large size, the entire dataset cannot fit into cached memory. Frames have to be streamed through the memory hierarchy (disk to CPU main memory and then to GPU memory), partitioned, and processed as batches, where each batch is small enough to fit into the GPU. To reconnect the feature regions that are separated due to data partitioning, we present a novel batch merging algorithm to extract the region connection information across multiple batches in a parallel fashion. The information is organized in a memory-efficient structure and supports fast indexing on the GPU. Our experiment uses a commodity workstation equipped with a single GPU. The results show that our approach can efficiently process a weather dataset composed of terabytes of time-varying radar images. The advantages of our approach are demonstrated by comparing to the performance of an efficient CPU cluster implementation which is being used by the weather scientists.
C1 [Peng, Chao] Univ Alabama, Dept Comp Sci, Huntsville, AL 35899 USA.
   [Sahani, Sandip] Apple Inc, Cupertino, CA USA.
   [Rushing, John] Univ Alabama, Informat Technol & Syst Ctr, Huntsville, AL 35899 USA.
C3 University of Alabama System; University of Alabama Huntsville; Apple
   Inc; University of Alabama System; University of Alabama Huntsville
RP Peng, C (corresponding author), Univ Alabama, Dept Comp Sci, Huntsville, AL 35899 USA.
EM chao.peng@uah.edu; sandip.sahani@uah.edu; john.rushing@uah.edu
OI Peng, Chao/0000-0001-8838-2469
FU US National Science Foundation grant [CNS 1464323]; NVIDIA Corporation;
   Direct For Computer & Info Scie & Enginr; Division Of Computer and
   Network Systems [1464323] Funding Source: National Science Foundation
FX This work was partially supported by the US National Science Foundation
   grant, CNS 1464323. We gratefully acknowledge the support of NVIDIA
   Corporation with the donation of GPU devices used for this work. We
   thank National Service Center and Information Technology and System
   Center at UAH for providing the weather dataset and the hardware used
   for this work. We thank Sabin Timalsena and Lizhou Cao for helping with
   visualization. We thank anonymous reviewers for their suggestions and
   comments. We thank Haeyong Chung for his comments.
NR 33
TC 6
Z9 6
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2017
VL 23
IS 10
BP 2262
EP 2274
DI 10.1109/TVCG.2016.2637904
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FG0VB
UT WOS:000409496700006
PM 27959814
OA hybrid
DA 2025-03-07
ER

PT J
AU Hou, F
   He, Y
   Qin, H
   Hao, AM
AF Hou, Fei
   He, Ying
   Qin, Hong
   Hao, Aimin
TI Knot Optimization for Biharmonic B-splines on Manifold Triangle Meshes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Biharmonic B-splines; green's functions; manifold triangle meshes;
   implicit representation; knot optimization
ID APPROXIMATION
AB Biharmonic B-splines, proposed by Feng and Warren, are an elegant generalization of univariate B-splines to planar and curved domains with fully irregular knot configuration. Despite the theoretic breakthrough, certain technical difficulties are imperative, including the necessity of Voronoi tessellation, the lack of analytical formulation of bases on general manifolds, expensive basis re-computation during knot refinement/removal, being applicable for simple domains only (e.g., such as euclidean planes, spherical and cylindrical domains, and tori). To ameliorate, this paper articulates a new biharmonic B-spline computing paradigm with a simple formulation. We prove that biharmonic B-splines have an equivalent representation, which is solely based on a linear combination of Green's functions of the bi-Laplacian operator. Consequently, without explicitly computing their bases, biharmonic B-splines can bypass the Voronoi partitioning and the discretization of bi-Laplacian, enable the computational utilities on any compact 2-manifold. The new representation also facilitates optimization-driven knot selection for constructing biharmonic B-splines on manifold triangle meshes. We develop algorithms for spline evaluation, data interpolation and hierarchical data decomposition. Our results demonstrate that biharmonic B-splines, as a new type of spline functions with theoretic and application appeal, afford progressive update of fully irregular knots, free of singularity, without the need of explicit parameterization, making it ideal for a host of graphics tasks on manifolds.
C1 [Hou, Fei; He, Ying] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Hao, Aimin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
C3 Nanyang Technological University; State University of New York (SUNY)
   System; Stony Brook University; Beihang University
RP Hou, F (corresponding author), Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
EM houfei@ntu.edu.sg; yhe@ntu.edu.sg; qin@cs.sunysb.edu; ham@hua.edu.cn
RI Zhao, Mingyu/HHS-0141-2022; He, Ying/A-3708-2011
OI He, Ying/0000-0002-6749-4485
FU NSFC [61300068, 61190120, 61190121, 61190125, 61532002, 61672149,
   61672077]; US National Science Foundation [IIS-0949467, IIS-1047715,
   IIS-1049448];  [MOE2013-T2-2-011];  [RG23/15]
FX We thank the reviewers for their detailed and constructive comments,
   which help us to improve the quality of the paper significantly. This
   project was partially supported by MOE2013-T2-2-011, RG23/15, NSFC
   (Grant No. 61300068, 61190120, 61190121, 61190125, 61532002, 61672149,
   61672077) and US National Science Foundation (Grant No. IIS-0949467,
   IIS-1047715 and IIS-1049448). Ying He is the corresponding author.
NR 39
TC 7
Z9 7
U1 2
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2017
VL 23
IS 9
BP 2082
EP 2095
DI 10.1109/TVCG.2016.2605092
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FC7XA
UT WOS:000407054600004
PM 27608469
OA Green Accepted, Bronze
DA 2025-03-07
ER

PT J
AU Grosset, AVP
   Prasad, M
   Christensen, C
   Knoll, A
   Hansen, C
AF Grosset, A. V. Pascal
   Prasad, Manasa
   Christensen, Cameron
   Knoll, Aaron
   Hansen, Charles
TI TOD-Tree: Task-Overlapped Direct Send Tree Image Compositing for Hybrid
   MPI Parallelism and GPUs
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Distributed volume rendering; image compositing; parallel processing
ID COMMUNICATION; VISUALIZATION
AB Modern supercomputers have thousands of nodes, each with CPUs and/or GPUs capable of several teraflops. However, the network connecting these nodes is relatively slow, on the order of gigabits per second. For time-critical workloads such as interactive visualization, the bottleneck is no longer computation but communication. In this paper, we present an image compositing algorithm that works on both CPU-only and GPU-accelerated supercomputers and focuses on communication avoidance and overlapping communication with computation at the expense of evenly balancing the workload. The algorithm has three stages: a parallel direct send stage, followed by a tree compositing stage and a gather stage. We compare our algorithm with radix-k and binary-swap from the IceT library in a hybrid OpenMP/MPI setting on the Stampede and Edison supercomputers, show strong scaling results and explain how we generally achieve better performance than these two algorithms. We developed a GPU-based image compositing algorithm where we use CUDA kernels for computation and GPU Direct RDMA for inter-node GPU communication. We tested the algorithm on the Piz Daint GPU-accelerated supercomputer and show that we achieve performance on par with CPUs. Last, we introduce a workflow in which both rendering and compositing are done on the GPU.
C1 [Grosset, A. V. Pascal; Christensen, Cameron; Knoll, Aaron; Hansen, Charles] Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
   [Prasad, Manasa] Google, Mountain View, CA 94043 USA.
C3 Utah System of Higher Education; University of Utah; Google Incorporated
RP Grosset, AVP (corresponding author), Univ Utah, Sci Comp & Imaging Inst, Salt Lake City, UT 84112 USA.
EM pgrosset@sci.utah.edu; pbmanasa@gmail.com; cam@sci.utah.edu;
   knolla@sci.utah.edu; hansen@cs.utah.edu
RI Grosset, Pascal/AAS-7792-2020
OI Grosset, Pascal/0000-0003-2192-3843
FU DOE; NNSA [DE-NA0002375]; DOE SciDAC Institute of Scalable Data
   Management Analysis and Visualization DOE [DE-SC0007446]; NSF
   [ACI-1339881, IIS-1162013]; Office of Advanced Cyberinfrastructure
   (OAC); Direct For Computer & Info Scie & Enginr [1339863] Funding
   Source: National Science Foundation; U.S. Department of Energy (DOE)
   [DE-SC0007446] Funding Source: U.S. Department of Energy (DOE)
FX This research was supported by the DOE, NNSA, Award DE-NA0002375:
   (PSAAP) Carbon-Capture Multidisciplinary Simulation Center, the DOE
   SciDAC Institute of Scalable Data Management Analysis and Visualization
   DOE DE-SC0007446, NSF ACI-1339881, and NSF IIS-1162013.
NR 46
TC 9
Z9 10
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2017
VL 23
IS 6
BP 1677
EP 1690
DI 10.1109/TVCG.2016.2542069
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ET8DC
UT WOS:000400527500010
PM 26992102
OA Green Published, Bronze
DA 2025-03-07
ER

PT J
AU Sun, GD
   Liang, RH
   Qu, HM
   Wu, YC
AF Sun, Guodao
   Liang, Ronghua
   Qu, Huamin
   Wu, Yingcai
TI Embedding Spatio-Temporal Information into Maps by Route-Zooming
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Spatio-temporal visualization; occlusion-free visualization;
   least-square optimization
ID TRAJECTORY DATA; TIME-SERIES; VISUALIZATION; EXPLORATION; ANALYTICS
AB Analysis and exploration of spatio-temporal data such as traffic flow and vehicle trajectories have become important in urban planning and management. In this paper, we present a novel visualization technique called route-zooming that can embed spatio-temporal information into a map seamlessly for occlusion-free visualization of both spatial and temporal data. The proposed technique can broaden a selected route in a map by deforming the overall road network. We formulate the problem of route-zooming as a nonlinear least squares optimization problem by defining an energy function that ensures the route is broadened successfully on demand while the distortion caused to the road network is minimized. The spatio-temporal information can then be embedded into the route to reveal both spatial and temporal patterns without occluding the spatial context information. The route-zooming technique is applied in two instantiations including an interactive metro map for city tourism and illustrative maps to highlight information on the broadened roads to prove its applicability. We demonstrate the usability of our spatio-temporal visualization approach with case studies on real traffic flow data. We also study various design choices in our method, including the encoding of the time direction and choices of temporal display, and conduct a comprehensive user study to validate our embedded visualization design.
C1 [Sun, Guodao; Liang, Ronghua] Zhejiang Univ Technol, Coll Informat Engn, Hangzhou, Zhejiang, Peoples R China.
   [Liang, Ronghua] Zhejiang Univ Technol, Comp Sci, Hangzhou, Zhejiang, Peoples R China.
   [Qu, Huamin] Hong Kong Univ Sci & Technol, Dept Comp Sci & Engn CSE, Hong Kong, Hong Kong, Peoples R China.
   [Wu, Yingcai] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
C3 Zhejiang University of Technology; Zhejiang University of Technology;
   Hong Kong University of Science & Technology; Zhejiang University
RP Liang, RH (corresponding author), Zhejiang Univ Technol, Coll Informat Engn, Hangzhou, Zhejiang, Peoples R China.
EM godoor.sun@gmail.com; rhliang@zjut.edu.cn; huamin@cse.ust.hk;
   ycwu@zju.edu.cn
RI liang, ronghua/H-4463-2012; Sun, Guodao/AAN-4428-2021
FU National 973 Program of China [2015CB352503]; NSFC [61502416]; Zhejiang
   Provincial NSFC [LR14F020002]; joint project Data-Driven Intelligent
   Transportation between China and Europe; Ministry of Science and
   Technology of China [HK RGC GRF 618313]; Microsoft Research Asia;
   National 973 Program of China [2015CB352503]; NSFC [61502416]; Zhejiang
   Provincial NSFC [LR14F020002]; joint project Data-Driven Intelligent
   Transportation between China and Europe; Ministry of Science and
   Technology of China [HK RGC GRF 618313]; Microsoft Research Asia
FX The work is supported by National 973 Program of China (2015CB352503),
   NSFC (No. 61502416), Zhejiang Provincial NSFC (No. LR14F020002), joint
   project Data-Driven Intelligent Transportation between China and Europe
   announced by the Ministry of Science and Technology of China, grant HK
   RGC GRF 618313, and a grant from Microsoft Research Asia. Ronghua Liang
   is the corresponding author.
NR 30
TC 47
Z9 52
U1 2
U2 33
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2017
VL 23
IS 5
BP 1506
EP 1519
DI 10.1109/TVCG.2016.2535234
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ET8CT
UT WOS:000400526600008
PM 26930685
DA 2025-03-07
ER

PT J
AU Rahimian, P
   Kearney, JK
AF Rahimian, Pooya
   Kearney, Joseph K.
TI Optimal Camera Placement for Motion Capture Systems
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Optimization methods; simulated annealing; augmented reality; virtual
   reality
ID ALGORITHM
AB Optical motion capture is based on estimating the three-dimensional positions of markers by triangulation from multiple cameras. Successful performance depends on points being visible from at least two cameras and on the accuracy of the triangulation. Triangulation accuracy is strongly related to the positions and orientations of the cameras. Thus, the configuration of the camera network has a critical impact on performance. A poor camera configuration may result in a low quality three-dimensional (3D) estimation and consequently low quality of tracking. This paper introduces and compares two methods for camera placement. The first method is based on a metric that computes target point visibility in the presence of dynamic occlusion from cameras with "good" views. The second method is based on the distribution of views of target points. Efficient algorithms, based on simulated annealing, are introduced for estimating the optimal configuration of cameras for the two metrics and a given distribution of target points. The accuracy and robustness of the algorithms are evaluated through both simulation and empirical measurement. Implementations of the two methods are available for download as tools for the community.
C1 [Rahimian, Pooya; Kearney, Joseph K.] Univ Iowa, Dept Comp Sci, 14 MacLean Hall, Iowa City, IA 52242 USA.
C3 University of Iowa
RP Rahimian, P (corresponding author), Univ Iowa, Dept Comp Sci, 14 MacLean Hall, Iowa City, IA 52242 USA.
EM Pooya-Rahimian@uiowa.edu; Pooya-Rahimian@uiowa.edu
FU National Science Foundation [BCS-1251694, CNS-1305131]; US Department of
   Transportation, Research and Innovative Technology Administration, Prime
   DFDA [20.701, DTRT13-G-UTC53]
FX This research was supported by National Science Foundation awards
   BCS-1251694 and CNS-1305131, and by the US Department of Transportation,
   Research and Innovative Technology Administration, Prime DFDA No.
   20.701, Award No. DTRT13-G-UTC53.
NR 30
TC 40
Z9 44
U1 0
U2 27
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2017
VL 23
IS 3
BP 1209
EP 1221
DI 10.1109/TVCG.2016.2637334
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM8CR
UT WOS:000395539300007
PM 27959813
OA Bronze
DA 2025-03-07
ER

PT J
AU Gomez, SR
   Jianu, R
   Cabeen, R
   Guo, H
   Laidlaw, DH
AF Gomez, Steven R.
   Jianu, Radu
   Cabeen, Ryan
   Guo, Hua
   Laidlaw, David H.
TI Fauxvea: Crowdsourcing Gaze Location Estimates for Visualization
   Analysis Tasks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Eye tracking; crowdsourcing; focus window; information visualization;
   visual analysis; user studies
ID EYE; ATTENTION
AB We present the design and evaluation of a method for estimating gaze locations during the analysis of static visualizations using crowdsourcing. Understanding gaze patterns is helpful for evaluating visualizations and user behaviors, but traditional eye-tracking studies require specialized hardware and local users. To avoid these constraints, we developed a method called Fauxvea, which crowdsources visualization tasks on the Web and estimates gaze fixations through cursor interactions without eye-tracking hardware. We ran experiments to evaluate how gaze estimates from our method compare with eye-tracking data. First, we evaluated crowdsourced estimates for three common types of information visualizations and basic visualization tasks using Amazon Mechanical Turk (MTurk). In another, we reproduced findings from a previous eye-tracking study on tree layouts using our method on MTurk. Results from these experiments show that fixation estimates using Fauxvea are qualitatively and quantitatively similar to eye tracking on the same stimulus-task pairs. These findings suggest that crowdsourcing visual analysis tasks with static information visualizations could be a viable alternative to traditional eye-tracking studies for visualization research and design.
C1 [Gomez, Steven R.; Cabeen, Ryan; Guo, Hua; Laidlaw, David H.] Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
   [Jianu, Radu] Florida Int Univ, Sch Comp & Informat Sci, Miami, FL 33199 USA.
C3 Brown University; State University System of Florida; Florida
   International University
RP Gomez, SR (corresponding author), Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.
EM steveg@cs.brown.edu; rdjianu@cis.fiu.edu; cabeen@cs.brown.edu;
   huag@cs.brown.edu; dhl@cs.brown.edu
RI Laidlaw, David/M-4686-2019
FU US National Science Foundation (NSF) [IIS-10-16623]
FX This work was supported in part by US National Science Foundation (NSF)
   award IIS-10-16623. All opinions, findings, conclusions, or
   recommendations expressed in this document are those of the authors and
   do not necessarily reflect the views of the sponsoring agencies.
NR 31
TC 9
Z9 9
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2017
VL 23
IS 2
BP 1042
EP 1055
DI 10.1109/TVCG.2016.2532331
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EM8CG
UT WOS:000395538200005
PM 26915125
OA Green Accepted, Bronze
DA 2025-03-07
ER

PT J
AU Cordeil, M
   Dwyer, T
   Klein, K
   Laha, B
   Marriott, K
   Thomas, BH
AF Cordeil, Maxime
   Dwyer, Tim
   Klein, Karsten
   Laha, Bireswar
   Marriott, Kim
   Thomas, Bruce H.
TI Immersive Collaborative Analysis of Network Connectivity: CAVE-style or
   Head-Mounted Display?
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Oculus Rift; CAVE; Immersive Analytics; Collaboration; 3D Network
ID VISUALIZATION; ENVIRONMENT; CONFLICT
AB High-quality immersive display technologies are becoming mainstream with the release of head-mounted displays (HMDs) such as the Oculus Rift. These devices potentially represent an affordable alternative to the more traditional, centralised CAVE-style immersive environments. One driver for the development of CAVE-style immersive environments has been collaborative sense-making. Despite this, there has been little research on the effectiveness of collaborative visualisation in CAVE-style facilities, especially with respect to abstract data visualisation tasks. Indeed, very few studies have focused on the use of these displays to explore and analyse abstract data such as networks and there have been no formal user studies investigating collaborative visualisation of abstract data in immersive environments. In this paper we present the results of the first such study. It explores the relative merits of HMD and CAVE-style immersive environments for collaborative analysis of network connectivity, a common and important task involving abstract data. We find significant differences between the two conditions in task completion time and the physical movements of the participants within the space: participants using the HMD were faster while the CAVE2 condition introduced an asymmetry in movement between collaborators. Otherwise, affordances for collaborative data analysis offered by the low-cost HMD condition were not found to be different for accuracy and communication with the CAVE2. These results are notable, given that the latest HMDs will soon be accessible (in terms of cost and potentially ubiquity) to a massive audience.
C1 [Cordeil, Maxime; Dwyer, Tim; Klein, Karsten; Marriott, Kim] Monash Univ, Clayton, Vic 3800, Australia.
   [Thomas, Bruce H.] Univ South Australia, Adelaide, SA 5001, Australia.
   [Laha, Bireswar] Stanford Univ, Stanford, CA 94305 USA.
C3 Monash University; University of South Australia; Stanford University
RP Cordeil, M (corresponding author), Monash Univ, Clayton, Vic 3800, Australia.
EM maxime.cordeil@monash.edu; laha@stanford.edu; Bruce.Thomas@unisa.edu.au
RI Thomas, Bruce/A-1470-2008
OI Thomas, Bruce/0000-0002-9148-085X; Cordeil, Maxime/0000-0002-9732-4874;
   Dwyer, Tim/0000-0002-9076-9571
NR 41
TC 115
Z9 130
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 441
EP 450
DI 10.1109/TVCG.2016.2599107
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600047
PM 27875160
DA 2025-03-07
ER

PT J
AU Gerrits, T
   Roessl, C
   Theisel, H
AF Gerrits, Tim
   Roessl, Christian
   Theisel, Holger
TI Glyphs for General Second-Order 2D and 3D Tensors
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Glyph-based Techniques; Tensor Field Data; Flow Visualization
ID VISUALIZATION
AB Glyphs are a powerful tool for visualizing second-order tensors in a variety of scientic data as they allow to encode physical behavior in geometric properties. Most existing techniques focus on symmetric tensors and exclude non-symmetric tensors where the eigenvectors can be non-orthogonal or complex. We present a new construction of 2d and 3d tensor glyphs based on piecewise rational curves and surfaces with the following properties: invariance to (a) isometries and (b) scaling, (c) direct encoding of all real eigenvalues and eigenvectors, (d) one-to-one relation between the tensors and glyphs, (e) glyph continuity under changing the tensor. We apply the glyphs to visualize the Jacobian matrix fields of a number of 2d and 3d vector fields.
C1 [Gerrits, Tim; Roessl, Christian; Theisel, Holger] Univ Magdeburg, Visual Comp Grp, Magdeburg, Germany.
C3 Otto von Guericke University
RP Gerrits, T (corresponding author), Univ Magdeburg, Visual Comp Grp, Magdeburg, Germany.
EM gerrits@isg.cs.uni-magdeburg.de; roessl@isg.cs.uni-magdeburg.de;
   theisel@isg.cs.uni-magdeburg.de
OI Gerrits, Tim/0000-0001-9296-7224
FU DFG [TH 692/13-1]
FX This work was supported by DFG grant TH 692/13-1. The authors would like
   to thank Gordon Kindlmann for stimulating discussion. The cylinder data
   set was provided by Tino Weinkauf, the Rayleigh-Benard convection was
   simulated with NaSt3DGP. We thank Tobias Gunther and Thomas Seidl for
   help with rendering some pictures and the video. Some pictures were
   rendered with Amira.
NR 20
TC 14
Z9 17
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 980
EP 989
DI 10.1109/TVCG.2016.2598998
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600101
PM 27875211
DA 2025-03-07
ER

PT J
AU Liu, DY
   Weng, D
   Li, YH
   Bao, J
   Zheng, Y
   Qu, HM
   Wu, YC
AF Liu, Dongyu
   Weng, Di
   Li, Yuhong
   Bao, Jie
   Zheng, Yu
   Qu, Huamin
   Wu, Yingcai
TI SmartAdP: Visual Analytics of Large-scale Taxi Trajectories for
   Selecting Billboard Locations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE optimal billboard locations; taxi trajectory; visual analytics;
   comparative analysis
ID VISUALIZATION; MODEL; EXPLORATION; MOVEMENT
AB The problem of formulating solutions immediately and comparing them rapidly for billboard placements has plagued advertising planners for a long time, owing to the lack of efficient tools for in-depth analyses to make informed decisions. In this study, we attempt to employ visual analytics that combines the state-of-the-art mining and visualization techniques to tackle this problem using large-scale GPS trajectory data. In particular, we present SmartAdP, an interactive visual analytics system that deals with the two major challenges including finding good solutions in a huge solution space and comparing the solutions in a visual and intuitive manner. An interactive framework that integrates a novel visualization-driven data mining model enables advertising planners to effectively and efficiently formulate good candidate solutions. In addition, we propose a set of coupled visualizations: a solution view with metaphor-based glyphs to visualize the correlation between different solutions; a location view to display billboard locations in a compact manner; and a ranking view to present multi-typed rankings of the solutions. This system has been demonstrated using case studies with a real-world dataset and domain-expert interviews. Our approach can be adapted for other location selection problems such as selecting locations of retail stores or restaurants using trajectory data.
C1 [Liu, Dongyu] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
   [Liu, Dongyu; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Hong Kong, Peoples R China.
   [Weng, Di; Wu, Yingcai] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
   [Li, Yuhong] Univ Macau, Macau, Peoples R China.
   [Bao, Jie; Zheng, Yu] Microsoft Res, Beijing, Peoples R China.
C3 Zhejiang University; Hong Kong University of Science & Technology;
   Zhejiang University; University of Macau; Microsoft; Microsoft China
RP Wu, YC (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
EM dliuae@cse.ust.hk; mystery.wd@gmail.com; yb27407@umac.mo;
   jiebao@microsoft.com; yuzheng@microsoft.com; huamin@cse.ust.hk;
   ycwu@cad.zju.edu.cn
RI Zheng, Yu/GRJ-5808-2022; LI, YUHONG/HNI-6925-2023; Weng,
   Di/ABG-7408-2020; Liu, Dongyu/AGT-1288-2022
OI Weng, Di/0000-0003-2712-7274; Liu, Dongyu/0000-0002-8915-2785
FU National 973 Program of China [2015CB352503]; Fundamental Research Funds
   for Central Universities [2016QNA5014]; National Natural Science
   Foundation of China [61502416]; Ministry of Education of China
   [188170-170160502]; Zhejiang University; RGC [GRF16241916]; Microsoft
   Research Asia;  [ITS/170/15FP]
FX The work is supported by National 973 Program of China (2015CB352503),
   the Fundamental Research Funds for Central Universities (2016QNA5014),
   National Natural Science Foundation of China (No. 61502416), the
   research fund of the Ministry of Education of China (188170-170160502),
   100 Talents Program of Zhejiang University, RGC GRF16241916,
   ITS/170/15FP, and a grant from Microsoft Research Asia.
NR 48
TC 138
Z9 158
U1 0
U2 43
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 1
EP 10
DI 10.1109/TVCG.2016.2598432
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600003
PM 27514046
DA 2025-03-07
ER

PT J
AU Sacha, D
   Zhang, LS
   Sedlmair, M
   Lee, JA
   Peltonen, J
   Weiskopf, D
   North, SC
   Keim, DA
AF Sacha, Dominik
   Zhang, Leishi
   Sedlmair, Michael
   Lee, John A.
   Peltonen, Jaakko
   Weiskopf, Daniel
   North, Stephen C.
   Keim, Daniel A.
TI Visual Interaction with Dimensionality Reduction: A Structured
   Literature Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Interactive visualization; machine learning; visual analytics;
   dimensionality reduction
ID INFORMATION VISUALIZATION; SEMANTIC INTERACTION; EXPLORATION;
   COMPUTATION; PROJECTIONS
AB Dimensionality Reduction (DR) is a core building block in visualizing multidimensional data. For DR techniques to be useful in exploratory data analysis, they need to be adapted to human needs and domain-specific problems, ideally, interactively, and on-the-fly. Many visual analytics systems have already demonstrated the benefits of tightly integrating DR with interactive visualizations. Nevertheless, a general, structured understanding of this integration is missing. To address this, we systematically studied the visual analytics and visualization literature to investigate how analysts interact with automatic DR techniques. The results reveal seven common interaction scenarios that are amenable to interactive control such as specifying algorithmic constraints, selecting relevant features, or choosing among several DR algorithms. We investigate specific implementations of visual analysis systems integrating DR, and analyze ways that other machine learning methods have been combined with DR. Summarizing the results in a "human in the loop" process model provides a general lens for the evaluation of visual interactive DR systems. We apply the proposed model to study and classify several systems previously described in the literature, and to derive future research opportunities.
C1 [Sacha, Dominik; Keim, Daniel A.] Univ Konstanz, Constance, Germany.
   [Zhang, Leishi] Middlesex Univ, London N17 8HR, England.
   [Sedlmair, Michael] Univ Vienna, A-1010 Vienna, Austria.
   [Lee, John A.] Catholic Univ Louvain, ISSS, IREC, MIRO, Louvain, Belgium.
   [Lee, John A.] Belgian FRS FNRS, Brussels, Belgium.
   [Peltonen, Jaakko] Aalto Univ, Helsinki Inst Informat Technol, Espoo, Finland.
   [Peltonen, Jaakko] Univ Tampere, Tampere, Finland.
   [Weiskopf, Daniel] Univ Stuttgart, VISUS, Stuttgart, Germany.
   [North, Stephen C.] Infovisible LLC, Oldwick, NJ USA.
C3 University of Konstanz; Middlesex University; University of Vienna;
   Universite Catholique Louvain; Aalto University; University of Helsinki;
   University of Stuttgart
RP Sacha, D (corresponding author), Univ Konstanz, Constance, Germany.
EM dominik.sacha@uni-konstanz.de; l.x.zhang@mdx.ac.uk;
   michael.sedlmair@univie.ac.at; john.lee@uclouvain.be;
   jaakko.peltonen@aalto.fi; weiskopf@visus.uni-stuttgart.de;
   s.c.n@ieee.org; daniel.keim@uni-konstanz.de
RI Keim, Daniel/X-7749-2019; Peltonen, Jaakko/O-5172-2016; Weiskopf,
   Daniel/KWT-7459-2024; Lee, John/N-7434-2013
OI Lee, John/0000-0001-5218-759X; Weiskopf, Daniel/0000-0003-1174-1026;
   Peltonen, Jaakko/0000-0003-3485-8585; Sedlmair,
   Michael/0000-0001-7048-9292; Zhang, Leishi/0000-0002-3158-2328
FU EU project VALCRI [FP7-SEC-2013-608142]
FX This paper is a result of Dagstuhl Seminar 15101, "Bridging Information
   Visualization with Machine Learning". The authors thank all participants
   for inspiring and fruitful discussions. This work was partially
   supported by the EU project VALCRI (FP7-SEC-2013-608142).
NR 59
TC 83
Z9 95
U1 0
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 241
EP 250
DI 10.1109/TVCG.2016.2598495
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600027
PM 27875141
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Tam, GKL
   Kothari, V
   Chen, M
AF Tam, Gary K. L.
   Kothari, Vivek
   Chen, Min
TI An Analysis of Machine- and Human-Analytics in Classification
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Visual analytics; classification; decision tree; model; facial
   expression; visualization image; information theory
ID VISUAL ANALYTICS; VISUALIZATION
AB In this work, we present a study that traces the technical and cognitive processes in two visual analytics applications to a common theoretic model of soft knowledge that may be added into a visual analytics process for constructing a decision-tree model. Both case studies involved the development of classification models based on the "bag of features" approach. Both compared a visual analytics approach using parallel coordinates with a machine-learning approach using information theory. Both found that the visual analytics approach had some advantages over the machine learning approach, especially when sparse datasets were used as the ground truth. We examine various possible factors that may have contributed to such advantages, and collect empirical evidence for supporting the observation and reasoning of these factors. We propose an information-theoretic model as a common theoretic basis to explain the phenomena exhibited in these two case studies. Together we provide interconnected empirical and theoretical evidence to support the usefulness of visual analytics.
C1 [Tam, Gary K. L.] Swansea Univ, Swansea, W Glam, Wales.
   [Kothari, Vivek; Chen, Min] Univ Oxford, Oxford, England.
C3 Swansea University; University of Oxford
RP Tam, GKL (corresponding author), Swansea Univ, Swansea, W Glam, Wales.
EM k.l.tam@swansea.ac.uk; vivekoxford5@gmail.com; min.chen@oerc.ox.ac.uk
RI Tam, Gary KL/E-5098-2011
OI Tam, Gary KL/0000-0001-7387-5180
NR 51
TC 47
Z9 56
U1 1
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 71
EP 80
DI 10.1109/TVCG.2016.2598829
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600010
PM 27875135
OA Green Accepted, Green Submitted
DA 2025-03-07
ER

PT J
AU Durupinar, F
   Güdükbay, U
   Aman, A
   Badler, NI
AF Durupinar, Funda
   Gudukbay, Ugur
   Aman, Aytek
   Badler, Norman I.
TI Psychological Parameters for Crowd Simulation: From Audiences to Mobs
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Crowd simulation; autonomous agents; simulation of affect; crowd
   taxonomy; mob behavior; OCEAN personality model; OCC model; PAD model
ID MODEL; PERSONALITY
AB In the social psychology literature, crowds are classified as audiences and mobs. Audiences are passive crowds, whereas mobs are active crowds with emotional, irrational and seemingly homogeneous behavior. In this study, we aim to create a system that enables the specification of different crowd types ranging from audiences to mobs. In order to achieve this goal we parametrize the common properties of mobs to create collective misbehavior. Because mobs are characterized by emotionality, we describe a framework that associates psychological components with individual agents comprising a crowd and yields emergent behaviors in the crowd as a whole. To explore the effectiveness of our framework we demonstrate two scenarios simulating the behavior of distinct mob types.
C1 [Durupinar, Funda; Gudukbay, Ugur; Aman, Aytek] Bilkent Univ, Dept Comp Engn, TR-06800 Ankara, Turkey.
   [Badler, Norman I.] Univ Penn, Dept Comp & Informat Sci, 3330 Walnut St, Philadelphia, PA 19104 USA.
C3 Ihsan Dogramaci Bilkent University; University of Pennsylvania
RP Durupinar, F (corresponding author), Bilkent Univ, Dept Comp Engn, TR-06800 Ankara, Turkey.
EM fundad@cs.bilkent.edu.tr; gudukbay@cs.bilkent.edu.tr;
   aytek.aman@cs.bilkent.edu.tr; badler@seas.upenn.edu
RI Durupınar, Funda/AAC-5300-2020; Gudukbay, Ugur/F-1012-2011
OI Gudukbay, Ugur/0000-0003-2462-6959
FU Scientific and Technological Research Council of Turkey (TUBITAK)
   [112E110]
FX This research is supported by The Scientific and Technological Research
   Council of Turkey (TUBITAK) under Grant No. 112E110.
NR 52
TC 94
Z9 108
U1 3
U2 47
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2016
VL 22
IS 9
BP 2145
EP 2159
DI 10.1109/TVCG.2015.2501801
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA DU4FD
UT WOS:000382166900007
PM 26595922
OA Green Published
DA 2025-03-07
ER

PT J
AU Li, XS
   He, XW
   Liu, XH
   Zhang, JJ
   Liu, BQ
   Wu, EH
AF Li, Xiaosheng
   He, Xiaowei
   Liu, Xuehui
   Zhang, Jian J.
   Liu, Baoquan
   Wu, Enhua
TI Multiphase Interface Tracking with Fast Semi-Lagrangian Contouring
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Multiphase interface tracking; semi-Lagrangian contouring; fluid
   simulation
ID LEVEL SET METHOD; ANIMATION; VOLUME
AB We propose a semi-Lagrangian method for multiphase interface tracking. In contrast to previous methods, our method maintains an explicit polygonal mesh, which is reconstructed from an unsigned distance function and an indicator function, to track the interface of arbitrary number of phases. The surface mesh is reconstructed at each step using an efficient multiphase polygonization procedure with precomputed stencils while the distance and indicator function are updated with an accurate semi-Lagrangian path tracing from the meshes of the last step. Furthermore, we provide an adaptive data structure, multiphase distance tree, to accelerate the updating of both the distance function and the indicator function. In addition, the adaptive structure also enables us to contour the distance tree accurately with simple bisection techniques. The major advantage of our method is that it can easily handle topological changes without ambiguities and preserve both the sharp features and the volume well. We will evaluate its efficiency, accuracy and robustness in the results part with several examples.
C1 [Li, Xiaosheng; He, Xiaowei; Liu, Xuehui; Wu, Enhua] Chinese Acad Sci, State Key Lab Comp Sci, Inst Software, Beijing 100190, Peoples R China.
   [Li, Xiaosheng; He, Xiaowei] Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
   [Zhang, Jian J.] Bournemouth Univ, Natl Ctr Comp Animat, Poole BH12 5BB, Dorset, England.
   [Liu, Baoquan] Univ Bedfordshire, Dept Comp Sci & Technol, Luton, Beds, England.
   [Wu, Enhua] Univ Macau, Taipa, Macau, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS; Bournemouth
   University; University of Bedfordshire; University of Macau
RP Li, XS (corresponding author), Chinese Acad Sci, State Key Lab Comp Sci, Inst Software, Beijing 100190, Peoples R China.; Li, XS (corresponding author), Univ Chinese Acad Sci, Beijing 100190, Peoples R China.
EM lixs@ios.ac.cn; lxh@ios.ac.cn; jzhang@bournemouth.ac.uk;
   Baoquan.Liu@beds.ac.uk; ehwu@umac.mo
RI Liu, Xuehui/AAA-3050-2020
OI he, xiao wei/0000-0002-8870-2482
FU National Natural Science Foundation (NSF) of China [61272326, 51475394,
   6140051239]; University of Macau [MYRG202(Y1-L4)-FST11-WEH,
   MYRG2014-00139-FST]; Project of Macao Science and Technology Development
   Fund (FDCT) "Adaptive Approach in Fluid Dynamic Simulation"; EU
   [612627]; BIS of the British Government; Ministry of Education of China
FX The authors would like to thank the anonymous reviewers for their
   valuable comments and suggestions to improve the quality of the paper.
   This research is supported by the National Natural Science Foundation
   (NSF) of China under Grant No. 61272326, No. 51475394 and No.
   6140051239, and by Grants of University of Macau
   ((MYRG202(Y1-L4)-FST11-WEH), (MYRG2014-00139-FST)), the Project of Macao
   Science and Technology Development Fund (FDCT) "Adaptive Approach in
   Fluid Dynamic Simulation", and EU FP7 under grant agreement
   [612627-"AniNex"], BIS of the British Government and Ministry of
   Education of China (Sino-UK Higher Education Research Partnership for
   PhD Studies Project).
NR 44
TC 8
Z9 9
U1 1
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2016
VL 22
IS 8
BP 1973
EP 1986
DI 10.1109/TVCG.2015.2476788
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DR6UC
UT WOS:000380035800004
PM 26353373
DA 2025-03-07
ER

PT J
AU Shivashankar, N
   Pranav, P
   Natarajan, V
   van de Weygaert, R
   Bos, EGP
   Rieder, S
AF Shivashankar, Nithin
   Pranav, Pratyush
   Natarajan, Vijay
   van de Weygaert, Rien
   Bos, E. G. Patrick
   Rieder, Steven
TI Felix: A Topology Based Framework for Visual Exploration of Cosmic
   Filaments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Morse-Smale complexes; tessellations; cosmology theory; cosmic web;
   large-scale structure of the universe
ID DARK-MATTER; SPIN ALIGNMENT; GRAVITATIONAL-INSTABILITY; INFLATIONARY
   UNIVERSE; ANGULAR-MOMENTUM; GALAXY; VOIDS; WEB; COMPUTATION; CLUSTERS
AB The large-scale structure of the universe is comprised of virialized blob-like clusters, linear filaments, sheet-like walls and huge near empty three-dimensional voids. Characterizing the large scale universe is essential to our understanding of the formation and evolution of galaxies. The density range of clusters, walls and voids are relatively well separated, when compared to filaments, which span a relatively larger range. The large scale filamentary network thus forms an intricate part of the cosmic web. In this paper, we describe Felix, a topology based framework for visual exploration of filaments in the cosmic web. The filamentary structure is represented by the ascending manifold geometry of the 2-saddles in the Morse-Smale complex of the density field. We generate a hierarchy of Morse-Smale complexes and query for filaments based on the density ranges at the end points of the filaments. The query is processed efficiently over the entire hierarchical Morse-Smale complex, allowing for interactive visualization. We apply Felix to computer simulations based on the heuristic Voronoi kinematic model and the standard LCDM cosmology, and demonstrate its usefulness through two case studies. First, we extract cosmic filaments within and across cluster like regions in Voronoi kinematic simulation datasets. We demonstrate that we produce similar results to existing structure finders. Second, we extract different classes of filaments based on their density characteristics from the LCDM simulation datasets. Filaments that form the spine of the cosmic web, which exist in high density regions in the current epoch, are isolated using Felix. Also, filaments present in void-like regions are isolated and visualized. These filamentary structures are often over shadowed by higher density range filaments and are not easily characterizable and extractable using other filament extraction methodologies.
C1 [Shivashankar, Nithin] Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560038, Karnataka, India.
   [Pranav, Pratyush; van de Weygaert, Rien; Bos, E. G. Patrick] Univ Groningen, Kapteyn Astron Inst, NL-9700 AB Groningen, Netherlands.
   [Natarajan, Vijay] Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560038, Karnataka, India.
   [Natarajan, Vijay] Indian Inst Sci, Supercomp Educ & Res Ctr, Bangalore 560038, Karnataka, India.
   [Rieder, Steven] RIKEN, Adv Inst Computat Sci, Wako, Saitama, Japan.
C3 Indian Institute of Science (IISC) - Bangalore; University of Groningen;
   Kapteyn Astronomical Institute; Indian Institute of Science (IISC) -
   Bangalore; Indian Institute of Science (IISC) - Bangalore; RIKEN
RP Shivashankar, N (corresponding author), Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560038, Karnataka, India.
EM nithin@csa.iisc.ernet.in; pratyuze@gmail.com; vijayn@csa.iisc.ernet.in;
   weygaert@astro.rug.nl; pbos@astro.rug.nl; steven.rieder@riken.jp
RI Pranav, Pratyush/AAI-2588-2020; Rieder, Steven/C-8742-2016; Pranav,
   Pratyush/GQQ-8796-2022
OI Rieder, Steven/0000-0003-3688-5798; Pranav, Pratyush/0000-0003-1494-0856
FU Department of Science and Technology, India [SR/S3/EECE/0086/2012]; John
   Templeton Foundation [FP5136-O]
FX Vijay Natarajan and Nithin Shivashankar acknowledge support by the
   Department of Science and Technology, India (grant no.
   SR/S3/EECE/0086/2012). Rien van de Weygaert, Pratyush Pranav, Steven
   Rieder, and E.G. Patrick Bos acknowledge support by the John Templeton
   Foundation (grant no. FP5136-O). The authors also thank the anonymous
   reviewers for their valuable review comments.
NR 83
TC 43
Z9 43
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2016
VL 22
IS 6
BP 1745
EP 1759
DI 10.1109/TVCG.2015.2452919
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DO0MX
UT WOS:000377474100011
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhang, Q
   Nie, YW
   Zhang, L
   Xiao, CX
AF Zhang, Qing
   Nie, Yongwei
   Zhang, Ling
   Xiao, Chunxia
TI Underexposed Video Enhancement via Perception-Driven Progressive Fusion
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Underexposed video enhancement; visual perception; fusion; filtering
ID CONTEXT ENHANCEMENT; IMAGE; TONE; FRAMEWORK; DISPLAY
AB Underexposed video enhancement aims at revealing hidden details that are barely noticeable in LDR video frames with noise. Previous work typically relies on a single heuristic tone mapping curve to expand the dynamic range, which inevitably leads to uneven exposure and visual artifacts. In this paper, we present a novel approach for underexposed video enhancement using an efficient perception-driven progressive fusion. For an input underexposed video, we first remap each video frame using a series of tentative tone mapping curves to generate an multi-exposure image sequence that contains different exposed versions of the original video frame. Guided by some visual perception quality measures encoding the desirable exposed appearance, we locate all the best exposed regions from multi-exposure image sequences and then integrate them into a well-exposed video in a temporally consistent manner. Finally, we further perform an effective texture-preserving spatio-temporal filtering on this well-exposed video to obtain a high-quality noise-free result. Experimental results have shown that the enhanced video exhibits uniform exposure, brings out noticeable details, preserves temporal coherence, and avoids visual artifacts. Besides, we demonstrate applications of our approach to a set of problems including video dehazing, video denoising and HDR video reconstruction.
C1 [Zhang, Qing; Nie, Yongwei; Zhang, Ling; Xiao, Chunxia] Wuhan Univ, Comp Sch, Wuhan 430072, Peoples R China.
C3 Wuhan University
RP Zhang, Q (corresponding author), Wuhan Univ, Comp Sch, Wuhan 430072, Peoples R China.
EM zhangqing.whu.cs@gmail.com; nieyongwei@gmail.com; lingzhang@whu.edu.cn;
   cxxiao@whu.edu.cn
RI Zhang, Qing/ABB-1569-2021
FU National Basic Research Program of China [2012CB725303]; NSFC
   [61472288]; NCET [NCET-13-0441]; Key Grant Project of Hubei province
   [2013AAA02]
FX The authors would like to thank the anonymous reviewers for their
   valuable comments and insightful suggestions. This work was partly
   supported by the National Basic Research Program of China (No.
   2012CB725303), the NSFC (No. 61472288), NCET (NCET-13-0441) and the Key
   Grant Project of Hubei province (2013AAA02). Chunxia Xiao is the
   corresponding author.
NR 42
TC 30
Z9 32
U1 0
U2 29
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2016
VL 22
IS 6
BP 1773
EP 1785
DI 10.1109/TVCG.2015.2461157
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DO0MX
UT WOS:000377474100013
DA 2025-03-07
ER

PT J
AU Samaraweera, G
   Guo, RK
   Quarles, J
AF Samaraweera, Gayani
   Guo, Rongkai
   Quarles, John
TI Head Tracking Latency in Virtual Environments Revisited: Do Users with
   Multiple Sclerosis Notice Latency Less?
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Latency; multiple sclerosis; rehabilitation; user studies; virtual
   reality
ID REHABILITATION; WALKING; DELAY
AB Latency (i.e., time delay) in a virtual environment is known to disrupt user performance, presence and induce simulator sickness. Thus, with emerging use of virtual rehabilitation, the target populations' latency perception thresholds need to be considered to fully understand and possibly control the implications of latency in a Virtual Rehabilitation environment. We present a study that quantifies the latency discrimination thresholds of a yet untested population-a specific subset of mobility impaired participants where participants suffer from Multiple Sclerosis-and compare the results to a control group of healthy participants. The study was modeled after previous latency discrimination research and shows significant differences in latency perception between the two populations with MS participants showing lower sensitivity to latency than healthy participants.
C1 [Samaraweera, Gayani; Quarles, John] Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX USA.
   [Guo, Rongkai] Kennesaw State Univ, Dept Software Engn & Game Dev, San Antonio, TX USA.
C3 University of Texas System; University of Texas at San Antonio (UTSA)
RP Samaraweera, G; Quarles, J (corresponding author), Univ Texas San Antonio, Dept Comp Sci, San Antonio, TX USA.; Guo, RK (corresponding author), Kennesaw State Univ, Dept Software Engn & Game Dev, San Antonio, TX USA.
EM inl548@my.utsa.edu; rguo@kennesaw.edu; john.quarles@utsa.edu
FU National Science Foundation [IIS-1153229, IIS-1218283]; Direct For
   Computer & Info Scie & Enginr; Div Of Information & Intelligent Systems
   [1218283] Funding Source: National Science Foundation
FX The authors wish to thank the study participants and Dr. Suzanne Gazda
   from the Neurology Institute of San Antonio. This work was supported by
   grants from the National Science Foundation (IIS-1153229 and
   IIS-1218283).
NR 35
TC 8
Z9 9
U1 0
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2016
VL 22
IS 5
BP 1630
EP 1636
DI 10.1109/TVCG.2015.2443783
PG 7
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DH7ZH
UT WOS:000373012300012
PM 27045917
DA 2025-03-07
ER

PT J
AU Sui, W
   Wang, LF
   Fan, B
   Xiao, HF
   Wu, HY
   Pan, CH
AF Sui, Wei
   Wang, Lingfeng
   Fan, Bin
   Xiao, Hongfei
   Wu, Huaiyu
   Pan, Chunhong
TI Layer-Wise Floorplan Extraction for Automatic Urban Building
   Reconstruction
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE 3D reconstruction; 3D modelling; image based modelling; point clouds
ID ENERGY MINIMIZATION
AB Urban building reconstruction is an important step for urban digitization and realisticvisualization. In this paper, we propose a novel automatic method to recover urban building geometry from 3D point clouds. The proposed method is suitable for buildings composed of planar polygons and aligned with the gravity direction, which are quite common in the city. Our key observation is that the building shapes are usually piecewise constant along the gravity direction and determined by several dominant shapes. Based on this observation, we formulate building reconstruction as an energy minimization problem under the Markov Random Field (MRF) framework. Specifically, point clouds are first cutinto a sequence of slices along the gravity direction. Then, floorplans are reconstructed by extracting boundaries of these slices, among which dominant floorplans are extracted and propagated to other floors via MRF. To guarantee correct propagation, a new distance measurement for floorplans is designed, which first encodes floorplans into strings and then calculates distances between their corresponding strings. Additionally, an image based editing method is also proposed to recover detailed window structures. Experimental results on both synthetic and real data sets have validated the effectiveness of our method.
C1 [Sui, Wei; Wang, Lingfeng; Fan, Bin; Xiao, Hongfei; Wu, Huaiyu; Pan, Chunhong] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Automation, CAS
RP Sui, W; Wang, LF; Fan, B; Xiao, HF; Wu, HY; Pan, CH (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing, Peoples R China.
EM wsui@nlpr.ia.ac.cn; lfwang@nlpr.ia.ac.cn; bfan@nlpr.ia.ac.cn;
   hfxiao@nlpr.ia.ac.cn; hywu@nlpr.ia.ac.cn; chpan@nlpr.ia.ac.cn
RI Fan, Bin/HKN-3438-2023
FU National Natural Science Foundation of China [91338202, 61331018,
   61403376, 61272049]; joint research fund for UCAS and CAS institute
   [Y55201TY00]
FX This research work is supported by National Natural Science Foundation
   of China under Grants 91338202, 61331018, 61403376 and 61272049, and the
   joint research fund for UCAS and CAS institute (Y55201TY00). The authors
   thank all the related reviewers for their efforts and valuable comments
   as well as the associated editors for their kind help.
NR 33
TC 19
Z9 22
U1 2
U2 16
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2016
VL 22
IS 3
BP 1261
EP 1277
DI 10.1109/TVCG.2015.2505296
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE2DC
UT WOS:000370435700008
PM 26661472
DA 2025-03-07
ER

PT J
AU Dong, WM
   Wu, FZ
   Kong, Y
   Mei, X
   Lee, TY
   Zhang, XP
AF Dong, Weiming
   Wu, Fuzhang
   Kong, Yan
   Mei, Xing
   Lee, Tong-Yee
   Zhang, Xiaopeng
TI Image Retargeting by Texture-Aware Synthesis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Natural image; texture detection; texture-based significance map;
   texture-aware synthesis
AB Real-world images usually contain vivid contents and rich textural details, which will complicate the manipulation on them. In this paper, we design a new framework based on exampled-based texture synthesis to enhance content-aware image retargeting. By detecting the textural regions in an image, the textural image content can be synthesized rather than simply distorted or cropped. This method enables the manipulation of textural & non-textural regions with different strategies since they have different natures. We propose to retarget the textural regions by example-based synthesis and non-textural regions by fast multi-operator. To achieve practical retargeting applications for general images, we develop an automatic and fast texture detection method that can detect multiple disjoint textural regions. We adjust the saliency of the image according to the features of the textural regions. To validate the proposed method, comparisons with state-of-the-art image retargeting techniques and a user study were conducted. Convincing visual results are shown to demonstrate the effectiveness of the proposed method.
C1 [Dong, Weiming; Wu, Fuzhang; Kong, Yan; Mei, Xing; Zhang, Xiaopeng] Chinese Acad Sci, Inst Automat, NLPR LIAMA, Beijing, Peoples R China.
   [Lee, Tong-Yee] Natl Cheng Kung Univ, Dept Comp Sci & Informat Engn, Tainan 701, Taiwan.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; National
   Cheng Kung University
RP Dong, WM (corresponding author), Chinese Acad Sci, Inst Automat, NLPR LIAMA, Beijing, Peoples R China.
EM wmdong@nlpr.ia.ac.cn; fuzhangwu@nlpr.ia.ac.cn; yan.kong@nlpr.ia.ac.cn;
   xmei@nlpr.ia.ac.cn; tonylee@mail.ncku.edu.tw; xpzhang@nlpr.ia.ac.cn
RI DONG, Weiming/AAG-7678-2020
FU National Natural Science Foundation of China [61172104, 61271430,
   61201402, 61372184, 61372168, 61331018]; CASIA-Tencent BestImage joint
   research project; Ministry of Science and Technology, Taiwan
   [MOST-103-2221-E-006-106-MY3]
FX The authors would like to thank anonymous reviewers for their valuable
   comments. This work was supported by National Natural Science Foundation
   of China under nos. 61172104, 61271430, 61201402, 61372184, 61372168,
   and 61331018), by CASIA-Tencent BestImage joint research project, and by
   the Ministry of Science and Technology with contract No.
   MOST-103-2221-E-006-106-MY3, Taiwan.
NR 51
TC 26
Z9 27
U1 0
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2016
VL 22
IS 2
BP 1088
EP 1101
DI 10.1109/TVCG.2015.2440255
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DA3TF
UT WOS:000367721100006
PM 26731453
DA 2025-03-07
ER

PT J
AU Guo, HQ
   Phillips, CL
   Peterka, T
   Karpeyev, D
   Glatz, A
AF Guo, Hanqi
   Phillips, Carolyn L.
   Peterka, Tom
   Karpeyev, Dmitry
   Glatz, Andreas
TI Extracting, Tracking, and Visualizing Magnetic Flux Vortices in 3D
   Complex-Valued Superconductor Simulation Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Superconductor; Vortex extraction; Feature tracking; Unstructured grid
AB We propose a method for the vortex extraction and tracking of superconducting magnetic flux vortices for both structured and unstructured mesh data. In the Ginzburg-Landau theory, magnetic flux vortices are well-defined features in a complex-valued order parameter field, and their dynamics determine electromagnetic properties in type-II superconductors. Our method represents each vortex line (a 10 curve embedded in 3D space) as a connected graph extracted from the discretized field in both space and time. For a time-varying discrete dataset, our vortex extraction and tracking method is as accurate as the data discretization. We then apply 3D visualization and 2D event diagrams to the extraction and tracking results to help scientists understand vortex dynamics and macroscale superconductor behavior in greater detail than previously possible.
C1 [Guo, Hanqi; Phillips, Carolyn L.; Peterka, Tom; Karpeyev, Dmitry] Argonne Natl Lab, Div Math & Comp Sci, Argonne, IL 60439 USA.
C3 United States Department of Energy (DOE); Argonne National Laboratory
RP Guo, HQ (corresponding author), Argonne Natl Lab, Div Math & Comp Sci, 9700 S Cass Ave, Argonne, IL 60439 USA.
EM hguo@anl.gov; cphillips@anl.gov; tpeterka@mcs.anl.gov
RI Guo, Hanqi/AAL-1929-2021; Glatz, Andreas/S-2121-2017; Guo,
   Hanqi/ADW-4234-2022
OI Glatz, Andreas/0000-0002-2007-3851; Guo, Hanqi/0000-0001-7776-1834
FU U.S. Department of Energy, Office of Science [DE-AC02-06CH11357]; U.S.
   Department of Energy, Office of Advanced Scientific Computing Research,
   Scientific Discovery through Advanced Computing (SciDAC) program
FX We thank Chunhui Liu for useful discussions. This material is based upon
   work supported by the U.S. Department of Energy, Office of Science,
   under contract number DE-AC02-06CH11357. This work is also supported by
   the U.S. Department of Energy, Office of Advanced Scientific Computing
   Research, Scientific Discovery through Advanced Computing (SciDAC)
   program.
NR 39
TC 13
Z9 15
U1 2
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 827
EP 836
DI 10.1109/TVCG.2015.2466838
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400088
PM 26529730
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Klemm, P
   Lawonn, K
   Glasser, S
   Niemann, U
   Hegenscheid, K
   Völzke, H
   Preim, B
AF Klemm, Paul
   Lawonn, Kai
   Glasser, Sylvia
   Niemann, Uli
   Hegenscheid, Katrin
   Voelzke, Henry
   Preim, Bernhard
TI 3D Regression Heat Map Analysis of Population Study Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Interactive Visual Analysis; Regression Analysis; Heat Map;
   Epidemiology; Breast Cancer; Hepatic Steatosis
ID INTERACTIVE VISUAL ANALYSIS; COHORT; VISUALIZATION; FRAMEWORK
AB Epidemiological studies comprise heterogeneous data about a subject group to define disease-specific risk factors. These data contain information (features) about a subject's lifestyle, medical status as well as medical image data. Statistical regression analysis is used to evaluate these features and to identify feature combinations indicating a disease (the target feature). We propose an analysis approach of epidemiological data sets by incorporating all features in an exhaustive regression-based analysis. This approach combines all independent features w.r.t. a target feature. It provides a visualization that reveals insights into the data by highlighting relationships. The 3D Regression Heat Map, a novel 3D visual encoding, acts as an overview of the whole data set. It shows all combinations of two to three independent features with a specific target disease. Slicing through the 3D Regression Heat Map allows for the detailed analysis of the underlying relationships. Expert knowledge about disease-specific hypotheses can be included into the analysis by adjusting the regression model formulas. Furthermore, the influences of features can be assessed using a difference view comparing different calculation results. We applied our 3D Regression Heat Map method to a hepatic steatosis data set to reproduce results from a data mining-driven analysis. A qualitative analysis was conducted on a breast density data set. We were able to derive new hypotheses about relations between breast density and breast lesions with breast cancer. With the 3D Regression Heat Map, we present a visual overview of epidemiological data that allows for the first time an interactive regression-based analysis of large feature sets with respect to a disease.
C1 [Klemm, Paul; Lawonn, Kai; Glasser, Sylvia; Niemann, Uli; Preim, Bernhard] Univ Magdeburg, D-39106 Magdeburg, Germany.
   [Hegenscheid, Katrin; Voelzke, Henry] Ernst Moritz Arndt Univ Greifswald, Greifswald, Germany.
   [Lawonn, Kai] Delft Univ Technol, Delft, Netherlands.
C3 Otto von Guericke University; Universitat Greifswald; Delft University
   of Technology
RP Klemm, P (corresponding author), Univ Magdeburg, D-39106 Magdeburg, Germany.
EM klemm@ovgu.de; lawonn@ovgu.de; glasser@ovgu.de; uli.niemann@ovgu.de;
   katrin.hegenscheid@uni-greifswald.de; voelzke@uni-greifswald.de;
   preim@ovgu.de
RI Preim, Bernhard/AAF-6565-2021
OI Klemm, Paul/0000-0002-5985-1737; Niemann, Uli/0000-0001-9634-2248
FU Federal Ministry of Education and Research [03ZIK012]; Ministry of
   Cultural Affairs; Social Ministry of the Federal State of
   Mecklenburg-West Pomerania; Siemens Healthcare, Erlangen, Germany;
   Federal State of Mecklenburg-Vorpommern; DFG [1335]; Federal Ministry of
   Education and Research within Forschungscampus STIMULATE [13GW0095A]
FX SHIP is part of the Community Medicine Research net of the University of
   Greifswald, Germany, which is funded by the Federal Ministry of
   Education and Research (grant no. 03ZIK012), the Ministry of Cultural
   Affairs as well as the Social Ministry of the Federal State of
   Mecklenburg-West Pomerania. Whole-body MR imaging was supported by a
   joint grant from Siemens Healthcare, Erlangen, Germany and the Federal
   State of Mecklenburg-Vorpommern. This work was supported by the DFG
   Priority Program 1335: Scalable Visual Analytics. Sylvia Glasser and Kai
   Lawonn are funded by the Federal Ministry of Education and Research
   within the Forschungscampus STIMULATE under grant number '13GW0095A'.
NR 43
TC 30
Z9 32
U1 2
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 81
EP 90
DI 10.1109/TVCG.2015.2468291
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400013
PM 26529689
DA 2025-03-07
ER

PT J
AU Nguyen, PH
   Xu, K
   Wheat, A
   Wong, BLW
   Attfield, S
   Fields, B
AF Nguyen, Phong H.
   Xu, Kai
   Wheat, Ashley
   Wong, B. L. William
   Attfield, Simon
   Fields, Bob
TI Sense Path: Understanding the Sensemaking Process through Analytic
   Provenance
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Sensemaking; analytic provenance; transcription; coding; qualitative
   research; timeline visualization
ID INFORMATION; FRAMEWORK; USERS
AB Sensemaking is described as the process of comprehension, finding meaning and gaining insight from information, producing new knowledge and informing further action. Understanding the sensemaking process allows building effective visual analytics tools to make sense of large and complex datasets. Currently, it is often a manual and time-consuming undertaking to comprehend this: researchers collect observation data, transcribe screen capture videos and think-aloud recordings, identify recurring patterns, and eventually abstract the sensemaking process into a general model. In this paper, we propose a general approach to facilitate such a qualitative analysis process, and introduce a prototype, SensePath, to demonstrate the application of this approach with a focus on browser-based online sensemaking. The approach is based on a study of a number of qualitative research sessions including observations of users performing sensemaking tasks and post hoc analyses to uncover their sensemaking processes. Based on the study results and a follow-up participatory design session with HCI researchers, we decided to focus on the transcription and coding stages of thematic analysis. SensePath automatically captures user's sensemaking actions, i.e., analytic provenance, and provides multi-linked views to support their further analysis. A number of other requirements elicited from the design session are also implemented in SensePath, such as easy integration with existing qualitative analysis workflow and non-intrusive for participants. The tool was used by an experienced HCI researcher to analyze two sensemaking sessions. The researcher found the tool intuitive and considerably reduced analysis time, allowing better understanding of the sensemaking process.
C1 [Nguyen, Phong H.; Xu, Kai; Wheat, Ashley; Wong, B. L. William; Attfield, Simon; Fields, Bob] Middlesex Univ, London N17 8HR, England.
C3 Middlesex University
RP Nguyen, PH (corresponding author), Middlesex Univ, London N17 8HR, England.
EM p.nguyen@mdx.ac.uk; k.xu@mdx.ac.uk; a.wheat@mdx.ac.uk; w.wong@mdx.ac.uk;
   s.attfield@mdx.ac.uk; b.fields@mdx.ac.uk
RI Nguyen, Phong H/HHN-2723-2022
OI Nguyen, Phong/0000-0001-5643-0585; Fields, Bob/0000-0003-1117-1844; Xu,
   Kai/0000-0003-2242-5440
NR 42
TC 30
Z9 34
U1 1
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 41
EP 50
DI 10.1109/TVCG.2015.2467611
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400009
PM 26357398
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Scheepens, R
   Hurter, C
   van de Wetering, H
   van Wijk, JJ
AF Scheepens, Roeland
   Hurter, Christophe
   van de Wetering, Huub
   van Wijk, Jarke J.
TI Visualization, Selection, and Analysis of Traffic Flows
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Moving Object Visualization; traffic flows; interaction
ID EXPLORATION; AGGREGATION; GRAPH
AB Visualization of the trajectories of moving objects leads to dense and cluttered images, which hinders exploration and understanding. It also hinders adding additional visual information, such as direction, and makes it difficult to interactively extract traffic flows, i.e., subsets of trajectories. In this paper we present our approach to visualize traffic flows and provide interaction tools to support their exploration. We show an overview of the traffic using a density map. The directions of traffic flows are visualized using a particle system on top of the density map. The user can extract traffic flows using a novel selection widget that allows for the intuitive selection of an area, and filtering on a range of directions and any additional attributes. Using simple, visual set expressions, the user can construct more complicated selections. The dynamic behaviors of selected flows may then be shown in annotation windows in which they can be interactively explored and compared. We validate our approach through use cases where we explore and analyze the temporal behavior of aircraft and vessel trajectories, e.g., landing and takeoff sequences, or the evolution of flight route density. The aircraft use cases have been developed and validated in collaboration with domain experts.
C1 [Scheepens, Roeland; van de Wetering, Huub; van Wijk, Jarke J.] Eindhoven Univ Technol, Dept Math & Comp Sci, NL-5600 MB Eindhoven, Netherlands.
   [Hurter, Christophe] French Civil Aviat Univ ENAC Toulouse, Interact Comp Lab LII, Toulouse, France.
C3 Eindhoven University of Technology
RP Scheepens, R (corresponding author), Eindhoven Univ Technol, Dept Math & Comp Sci, POB 513, NL-5600 MB Eindhoven, Netherlands.
EM R.J.Scheepens@tue.nl; christophe.hurte@enac.fr; H.v.d.Wetering@tue.nl;
   J.J.v.Wijk@tue.nl
RI Hurter, Christophe/AHB-0811-2022
OI Scheepens, Roeland/0000-0003-4974-7036; van de Wetering,
   Huub/0000-0002-0517-1322; Hurter, Christophe/0000-0003-4318-6717
FU Dutch national program COMMIT
FX We would like to thank the air traffic controllers and domain experts
   for their expert feedback during the design process. This publication
   was supported by the Dutch national program COMMIT. The research work
   was carried out as part of the Metis project under the responsibility of
   Embedded Systems Innovation by TNO with Thales Nederland B.V. as the
   carrying industrial partner.
NR 36
TC 63
Z9 68
U1 2
U2 42
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 379
EP 388
DI 10.1109/TVCG.2015.2467112
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400043
PM 26390467
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhang, CG
   Schultz, T
   Lawonn, K
   Eisemann, E
   Vilanova, A
AF Zhang, Changgong
   Schultz, Thomas
   Lawonn, Kai
   Eisemann, Elmer
   Vilanova, Anna
TI Glyph-based Comparative Visualization for Diffusion Tensor Fields
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Glyph Design; Comparative Visualization; Diffusion Tensor Field
ID MRI; UNCERTAINTY; SCHEMES
AB Diffusion Tensor Imaging (DTI) is a magnetic resonance imaging modality that enables the in-vivo reconstruction and visualization of fibrous structures. To inspect the local and individual diffusion tensors, glyph-based visualizations are commonly used since they are able to effectively convey full aspects of the diffusion tensor. For several applications it is necessary to compare tensor fields, e.g., to study the effects of acquisition parameters, or to investigate the influence of pathologies on white matter structures. This comparison is commonly done by extracting scalar information out of the tensor fields and then comparing these scalar fields, which leads to a loss of information. If the glyph representation is kept, simple juxtaposition or superposition can be used. However, neither facilitates the identification and interpretation of the differences between the tensor fields. Inspired by the checkerboard style visualization and the superquadric tensor glyph, we design a new glyph to locally visualize differences between two diffusion tensors by combining juxtaposition and explicit encoding. Because tensor scale, anisotropy type, and orientation are related to anatomical information relevant for DTI applications, we focus on visualizing tensor differences in these three aspects. As demonstrated in a user study, our new glyph design allows users to efficiently and effectively identify the tensor differences. We also apply our new glyphs to investigate the differences between DTI datasets of the human brain in two different contexts using different b-values, and to compare datasets from a healthy and HIV-infected subject.
C1 [Zhang, Changgong; Lawonn, Kai; Eisemann, Elmer; Vilanova, Anna] Delft Univ Technol, Comp Graph & Visualizat Grp, NL-2600 AA Delft, Netherlands.
   [Schultz, Thomas] Univ Bonn, Visualizat & Med Image Anal Grp, Bonn, Germany.
   [Lawonn, Kai] Univ Magdeburg, Visualizat Grp, D-39106 Magdeburg, Germany.
C3 Delft University of Technology; University of Bonn; Otto von Guericke
   University
RP Zhang, CG (corresponding author), Delft Univ Technol, Comp Graph & Visualizat Grp, NL-2600 AA Delft, Netherlands.
EM c.zhang-3@tudelft.nl; schultz@cs.uni-bonn.de;
   lawonn@isg.cs.uni-magdeburg.de; e.eisemann@tudelft.nl;
   a.vilanova@tudelft.nl
RI Schultz, Thomas/IWV-2288-2023
FU BMBF [STIMULATE-OVGU: 13GW0095A]
FX The authors would like to thank Dr. Itamar Rouen (C.J.Gorter Center for
   High-field MRI, Leiden University Medical Center) for providing the DTI
   datasets with two different b-values. The HIV dataset was provided by
   the AGEhIV cohort study group and Co-morbidity in Relation to AIDS
   (COBRA) project. The control subject was provided by the Municipal
   Health Service Amsterdam. The authors would also like to thank Jianfei
   Yang and Joor Arkesteijn (Quantitative Imaging Group, Delft University
   of Technology) for their valuable feedback during the glyph design
   process. Kai Lawonn was partially funded by the BMBF (STIMULATE-OVGU:
   13GW0095A).
NR 43
TC 28
Z9 33
U1 0
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 797
EP 806
DI 10.1109/TVCG.2015.2467435
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400085
PM 26529729
DA 2025-03-07
ER

PT J
AU Manz, T
   Lekschas, F
   Greene, E
   Finak, G
   Gehlenborg, N
AF Manz, Trevor
   Lekschas, Fritz
   Greene, Evan
   Finak, Greg
   Gehlenborg, Nils
TI A General Framework for Comparing Embedding Visualizations Across
   Class-Label Hierarchies
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Measurement; Distortion; Image color
   analysis; Distortion measurement; Pipelines; visualization; comparison;
   high-dimensional data; dimensionality reduction; embeddings
ID QUALITY
AB Projecting high-dimensional vectors into two dimensions for visualization, known as embedding visualization, facilitates perceptual reasoning and interpretation. Comparing multiple embedding visualizations drives decision-making in many domains, but traditional comparison methods are limited by a reliance on direct point correspondences. This requirement precludes comparisons without point correspondences, such as two different datasets of annotated images, and fails to capture meaningful higher-level relationships among point groups. To address these shortcomings, we propose a general framework for comparing embedding visualizations based on shared class labels rather than individual points. Our approach partitions points into regions corresponding to three key class concepts-confusion, neighborhood, and relative size-to characterize intra- and inter-class relationships. Informed by a preliminary user study, we implemented our framework using perceptual neighborhood graphs to define these regions and introduced metrics to quantify each concept. We demonstrate the generality of our framework with usage scenarios from machine learning and single-cell biology, highlighting our metrics' ability to draw insightful comparisons across label hierarchies. To assess the effectiveness of our approach, we conducted an evaluation study with five machine learning researchers and six single-cell biologists using an interactive and scalable prototype built with Python, JavaScript, and Rust. Our metrics enable more structured comparisons through visual guidance and increased participants' confidence in their findings.
C1 [Manz, Trevor; Gehlenborg, Nils] Harvard Med Sch, Boston, MA 02115 USA.
   [Lekschas, Fritz; Greene, Evan; Finak, Greg] Ozette Technol, Seattle, WA USA.
C3 Harvard University; Harvard Medical School
RP Manz, T (corresponding author), Harvard Med Sch, Boston, MA 02115 USA.
EM trevor_manz@g.harvard.edu; fritz@ozette.com; evan@ozette.com;
   greg@ozette.com; nils@hms.harvard.edu
OI Manz, Trevor/0000-0001-7694-5164; Gehlenborg, Nils/0000-0003-0327-8297;
   Lekschas, Fritz/0000-0001-8432-4835; Finak, Greg/0000-0003-4341-9090
FU National Institutes of Health [OT2OD033758, R33CA263666, UM1HG011536]
FX We wish to thank all participants from our user studies, who informed
   the implementation of and helped evaluate our framework. T.M. conducted
   portions of this work during an internship at Ozette Technologies. This
   work was also supported by the National Institutes of Health
   (OT2OD033758, R33CA263666, UM1HG011536). F.L., E.G., and G.F. are
   employees of and hold stock and/or stock options in Ozette Technologies.
   N.G. is a co-founder and equity owner of Datavisyn.
NR 72
TC 0
Z9 0
U1 3
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2025
VL 31
IS 1
BP 283
EP 293
DI 10.1109/TVCG.2024.3456370
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA O3I5E
UT WOS:001370107500003
PM 39255153
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Yang, FC
   Duque, K
   Mousas, C
AF Yang, Fu-Chia
   Duque, Kevin
   Mousas, Christos
TI The Effects of Depth of Knowledge of a Virtual Agent
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual reality; virtual agent; conversational AI; depth of knowledge;
   knowledge bank; prompt engineering; Virtual reality; virtual agent;
   conversational AI; depth of knowledge; knowledge bank; prompt
   engineering
ID ANTHROPOMORPHISM; INTELLIGENCE; INFORMATION
AB We explored the impact of depth of knowledge on conversational agents and human perceptions in a virtual reality (VR) environment. We designed experimental conditions with low, medium, and high depths of knowledge in the domain of game development and tested them among 27 game development students. We aimed to understand how the agent's predefined knowledge levels affected the participants' perceptions of the agent and its knowledge. Our findings showed that participants could distinguish between different knowledge levels of the virtual agent. Moreover, the agent's depth of knowledge significantly impacted participants' perceptions of intelligence, rapport, factuality, the uncanny valley effect, anthropomorphism, and willingness for future interaction. We also found strong correlations between perceived knowledge, perceived intelligence, factuality, and willingness for future interactions. We developed design guidelines for creating conversational agents from our data and observations. This study contributes to the human-agent interaction field in VR settings by providing empirical evidence on the importance of tailoring virtual agents' depth of knowledge to improve user experience, offering insights into designing more engaging and effective conversational agents.
C1 [Yang, Fu-Chia; Mousas, Christos] Purdue Univ, Dept Comp & Informat Technol, W Lafayette, IN 47907 USA.
   [Duque, Kevin] Tecnol Monterrey, Engn & Sci, Monterrey 64849, NL, Mexico.
C3 Purdue University System; Purdue University; Tecnologico de Monterrey
RP Yang, FC (corresponding author), Purdue Univ, Dept Comp & Informat Technol, W Lafayette, IN 47907 USA.
EM yang1684@purdue.edu; A01174501@exatec.tec.mx; cmousas@purdue.edu
RI Mousas, Christos/AGV-3533-2022
OI Mousas, Christos/0000-0003-0955-7959
NR 82
TC 0
Z9 0
U1 8
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7140
EP 7151
DI 10.1109/TVCG.2024.3456198
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300037
PM 39255105
DA 2025-03-07
ER

PT J
AU Xu, TC
   Ren, XH
   Yang, JL
   Sheng, B
   Wu, EH
AF Xu, Tianchen
   Ren, Xiaohua
   Yang, Jiale
   Sheng, Bin
   Wu, Enhua
TI Efficient Binocular Rendering of Volumetric Density Fields With Coupled
   Adaptive Cube-Map Ray Marching for Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Binocular views; density field; global illumination; ray marching;
   real-time rendering; virtual reality; volume rendering; Binocular views;
   density field; global illumination; ray marching; real-time rendering;
   virtual reality; volume rendering
ID GPU
AB Creating visualizations of multiple volumetric density fields is demanding in virtual reality (VR) applications, which often include divergent volumetric density distributions mixed with geometric models and physics-based simulations. Real-time rendering of such complex environments poses significant challenges for rendering quality and performance. This article presents a novel scheme for efficient real-time rendering of varying translucent volumetric density fields with global illumination (GI) effects on high-resolution binocular VR displays. Our scheme proposes creative solutions to address three challenges involved in the target problem. First, to tackle the doubled heavy workloads of binocular ray marching, we explore the anti-aliasing principles and more advanced potentials of ray marching on interior cube-map faces, and propose a coupled ray-marching technique that converges to multi-resolution cube maps with interleaved adaptive sampling. Second, we devise a fully dynamic ambient GI approximation method that leverages spherical-harmonics (SH) transform information of the phase function to reduce the huge amount of ray sampling required for GI while ensuring fidelity. The method catalyzes spatial ray-marching reuse and adaptive temporal accumulation. Third, we deploy a two-phase ray-tracing algorithm with a tiled k-buffer to achieve fast processing of order-independent transparency (OIT) for multiple volume instances. Consequently, high-quality and high-performance real-time dynamic volume rendering can be achieved under constrained budgets controlled by developers. As our solution supports mixed mesh-volume rendering, the test results prove the practical usefulness of our approach for high-resolution binocular VR rendering on hybrid multi-volumetric and geometric environments.
C1 [Xu, Tianchen; Wu, Enhua] Chinese Acad Sci, Inst Software, State Key Lab CS, Beijing 100190, Peoples R China.
   [Xu, Tianchen; Wu, Enhua] Univ Chinese Acad Sci, Beijing 101408, Peoples R China.
   [Xu, Tianchen; Yang, Jiale] Adv Micro Devices AMD Inc, Shanghai 201210, Peoples R China.
   [Ren, Xiaohua] Tencent, Multimedia Res Ctr, Shenzhen 518054, Peoples R China.
   [Yang, Jiale; Sheng, Bin] Shanghai Jiao Tong Univ, Dept Comp Sci & Engn, Shanghai 200240, Peoples R China.
   [Wu, Enhua] Univ Macau, FST, Taipa, Macao, Peoples R China.
C3 Chinese Academy of Sciences; Institute of Software, CAS; Chinese Academy
   of Sciences; University of Chinese Academy of Sciences, CAS; Tencent;
   Shanghai Jiao Tong University; University of Macau
RP Wu, EH (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab CS, Beijing 100190, Peoples R China.
EM tianchenx@outlook.com; xiaohua_ren@126.com; yangjiale@sjtu.edu.cn;
   shengbin@cs.sjtu.edu.cn; ehwu@um.edu.mo
RI Sheng, Bin/LMO-9532-2024; Xu, Tianchen/T-9694-2019
OI Ren, Xiaohua/0000-0002-3196-9880; wu, en hua/0000-0002-2174-1428; Xu,
   Tianchen/0000-0002-5031-0715; Sheng, Bin/0000-0001-8678-2784
FU National Natural Science Foundation of China [62332015, 62072449,
   62272298]; AMD Shanghai-Khronos3D
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62332015, 62072449, and 62272298, and
   in part by AMD Shanghai-Khronos3D.
NR 41
TC 0
Z9 0
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6625
EP 6638
DI 10.1109/TVCG.2023.3322416
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600005
PM 37801374
DA 2025-03-07
ER

PT J
AU Coscia, A
   Endert, A
AF Coscia, Adam
   Endert, Alex
TI KnowledgeVIS: Interpreting Language Models by Comparing
   Fill-in-the-Blank Prompts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Predictive models; Task analysis; Analytical models; Transformers;
   Semantics; Visual analytics; Adaptation models; language models;
   prompting; interpretability; machine learning
AB Recent growth in the popularity of large language models has led to their increased usage for summarizing, predicting, and generating text, making it vital to help researchers and engineers understand how and why they work. We present KnowledgeVIS, a human-in-the-loop visual analytics system for interpreting language models using fill-in-the-blank sentences as prompts. By comparing predictions between sentences, KnowledgeVIS reveals learned associations that intuitively connect what language models learn during training to natural language tasks downstream, helping users create and test multiple prompt variations, analyze predicted words using a novel semantic clustering technique, and discover insights using interactive visualizations. Collectively, these visualizations help users identify the likelihood and uniqueness of individual predictions, compare sets of predictions between prompts, and summarize patterns and relationships between predictions across all prompts. We demonstrate the capabilities of KnowledgeVIS with feedback from six NLP experts as well as three different use cases: (1) probing biomedical knowledge in two domain-adapted models; and (2) evaluating harmful identity stereotypes and (3) discovering facts and relationships between three general-purpose models.
C1 [Coscia, Adam; Endert, Alex] Georgia Inst Technol, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology
RP Coscia, A (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM acoscia6@gatech.edu; endert@gatech.edu
RI Coscia, Adam/GUO-9221-2022
OI Coscia, Adam/0000-0002-0429-9295
FU NSF [IIS-1750474, DRL-2247790]
FX This work was supported by NSF under Grants IIS-1750474 and DRL-2247790.
NR 53
TC 0
Z9 0
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2024
VL 30
IS 9
BP 6520
EP 6532
DI 10.1109/TVCG.2023.3346713
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA A6O5Q
UT WOS:001283711000002
PM 38145514
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Angelini, M
   Blasilli, G
   Lenti, S
   Santucci, G
AF Angelini, Marco
   Blasilli, Graziano
   Lenti, Simone
   Santucci, Giuseppe
TI A Visual Analytics Conceptual Framework for Explorable and Steerable
   Partial Dependence Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Predictive models; Computational modeling; Machine learning; Analytical
   models; Visual analytics; Market research; Behavioral sciences; partial
   dependence plot; visual analytics
AB Machine learning techniques are a driving force for research in various fields, from credit card fraud detection to stock analysis. Recently, a growing interest in increasing human involvement has emerged, with the primary goal of improving the interpretability of machine learning models. Among different techniques, Partial Dependence Plots (PDP) represent one of the main model-agnostic approaches for interpreting how the features influence the prediction of a machine learning model. However, its limitations (i.e., visual interpretation, aggregation of heterogeneous effects, inaccuracy, and computability) could complicate or misdirect the analysis. Moreover, the resulting combinatorial space can be challenging to explore both computationally and cognitively when analyzing the effects of more features at the same time. This article proposes a conceptual framework that enables effective analysis workflows, mitigating state-of-the-art limitations. The proposed framework allows for exploring and refining computed partial dependences, observing incrementally accurate results, and steering the computation of new partial dependences on user-selected subspaces of the combinatorial and intractable space. With this approach, the user can save both computational and cognitive costs, in contrast with the standard monolithic approach that computes all the possible combinations of features on all their domains in batch. The framework is the result of a careful design process involving experts' knowledge during its validation and informed the development of a prototype, W4SP(1), that demonstrates its applicability traversing its different paths. A case study shows the advantages of the proposed approach.
C1 [Angelini, Marco; Lenti, Simone] Roma Sapienza Univ Rome, I-00185 Rome, Italy.
RP Lenti, S (corresponding author), Roma Sapienza Univ Rome, I-00185 Rome, Italy.
EM angelini@dis.uniroma1.it; blasilli@diag.uniroma1.it;
   lenti@diag.uniroma1.it; santucci@diag.uniroma1.it
RI Blasilli, Graziano/HLQ-6056-2023; Lenti, Simone/ABA-3229-2020; Santucci,
   Giuseppe/F-3907-2011
OI Lenti, Simone/0000-0001-8281-3723; Angelini, Marco/0000-0001-9051-6972;
   Blasilli, Graziano/0000-0003-3339-6403; Santucci,
   Giuseppe/0000-0003-4350-1123
NR 56
TC 8
Z9 8
U1 4
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4497
EP 4513
DI 10.1109/TVCG.2023.3263739
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400033
PM 37027262
OA Green Published
DA 2025-03-07
ER

PT J
AU Chittaro, L
AF Chittaro, Luca
TI Improving Knowledge Retention and Perceived Control Through Serious
   Games: A Study About Assisted Emergency Evacuation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Educational games; user interaction; user study; disability; training;
   safety
ID SELF-EFFICACY; EXTERNAL CONTROL; SAFETY LOCUS; INVOLVEMENT; PERFORMANCE;
   EXPERIENCE; PROMOTE; DESIGN
AB Digital games for education and training, also called serious games (SGs), have shown beneficial effects on learning in several studies. In addition, some studies are suggesting that SGs could improve user's perceived control, which affects the likelihood that the learned content will be applied in the real world. However, most SG studies tend to focus on immediate effects, providing no indication on knowledge and perceived control over time, especially in contrast with nongame approaches. Moreover, SG research on perceived control has focused mainly on self-efficacy, disregarding the complementary construct of locus of control (LOC). This article advances both lines of research, assessing user's knowledge and LOC over time, with a SG as well as traditional printed materials that teach the same content. Results show that the SG was more effective than printed materials for knowledge retention over time, and a better retention outcome was found also for LOC. An additional contribution of the paper is the proposal of a novel SG that targets the inclusivity goal of safe evacuation for all, extending SG research to a domain not dealt with before, i.e., assisting persons with disabilities in emergencies.
C1 [Chittaro, Luca] Univ Udine, Human Comp Interact Lab, Dept Math Comp Sci & Phys, I-33100 Udine, Italy.
C3 University of Udine
RP Chittaro, L (corresponding author), Univ Udine, Human Comp Interact Lab, Dept Math Comp Sci & Phys, I-33100 Udine, Italy.
EM luca.chittaro@uniud.it
OI CHITTARO, Luca/0000-0001-5975-4294
FU Friuli Venezia Giulia region
FX This work was supported by the Friuli Venezia Giulia region through the
   Project Servizi avanzati per il soccorso sanitario al disabile basati su
   tecnologie ICTinnovative (Advanced emergency medical services for the
   disabled based oninnovative ICT technologies). Recommended for
   acceptance by J. Stefanucci.
NR 60
TC 4
Z9 4
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5339
EP 5349
DI 10.1109/TVCG.2023.3292473
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400092
PM 37405887
OA Green Submitted, hybrid
DA 2025-03-07
ER

PT J
AU Edirimuni, DD
   Lu, XQ
   Li, G
   Robles-Kelly, A
AF Edirimuni, Dasith de Silva
   Lu, Xuequan
   Li, Gang
   Robles-Kelly, Antonio
TI Contrastive Learning for Joint Normal Estimation and Point Cloud
   Filtering
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Filtering; Point cloud compression; Estimation; Three-dimensional
   displays; Task analysis; Noise measurement; Training; Contrastive
   learning; machine learning; normal estimation; point cloud filtering
ID SURFACE
AB Point cloud filtering and normal estimation are two fundamental research problems in the 3D field. Existing methods usually perform normal estimation and filtering separately and often show sensitivity to noise and/or inability to preserve sharp geometric features such as corners and edges. In this article, we propose a novel deep learning method to jointly estimate normals and filter point clouds. We first introduce a 3D patch based contrastive learning framework, with noise corruption as an augmentation, to train a feature encoder capable of generating faithful representations of point cloud patches while remaining robust to noise. These representations are consumed by a simple regression network and supervised by a novel joint loss, simultaneously estimating point normals and displacements that are used to filter the patch centers. Experimental results show that our method well supports the two tasks simultaneously and preserves sharp features and fine details. It generally outperforms state-of-the-art techniques on both tasks.
C1 [Edirimuni, Dasith de Silva; Lu, Xuequan; Li, Gang; Robles-Kelly, Antonio] Deakin Univ, Sch Informat Technol, Waurn Ponds, VIC 3216, Australia.
   [Robles-Kelly, Antonio] Def Sci & Technol Grp, Edinburg, SA 5111, Australia.
C3 Deakin University; Defence Science & Technology
RP Lu, XQ (corresponding author), Deakin Univ, Sch Informat Technol, Waurn Ponds, VIC 3216, Australia.
EM dtdesilva@deakin.edu.au; xuequan.lu@deakin.edu.au;
   gang.li@deakin.edu.au; antonio.robles-kelly@deakin.edu.au
OI Lu, Xuequan/0000-0003-0959-408X; de Silva Edirimuni,
   Dasith/0000-0003-4997-5434
NR 62
TC 4
Z9 4
U1 5
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 4527
EP 4541
DI 10.1109/TVCG.2023.3263866
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400004
PM 37030701
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Hou, SY
   Tao, HY
   Bao, HJ
   Xu, WW
AF Hou, Shuaiying
   Tao, Hongyu
   Bao, Hujun
   Xu, Weiwei
TI A Two-Part Transformer Network for Controllable Motion Synthesis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Computational modeling; Solid modeling; Training; Transformers;
   Manifolds; Correlation; Biological system modeling; Human motion
   synthesis; transformer; deep learning; heterogeneous motion; body parts
AB Although part-based motion synthesis networks have been investigated to reduce the complexity of modeling heterogeneous human motions, their computational cost remains prohibitive in interactive applications. To this end, we propose a novel two-part transformer network that aims to achieve high-quality, controllable motion synthesis results in real-time. Our network separates the skeleton into the upper and lower body parts, reducing the expensive cross-part fusion operations, and models the motions of each part separately through two streams of auto-regressive modules formed by multi-head attention layers. However, such a design might not sufficiently capture the correlations between the parts. We thus intentionally let the two parts share the features of the root joint and design a consistency loss to penalize the difference in the estimated root features and motions by these two auto-regressive modules, significantly improving the quality of synthesized motions. After training on our motion dataset, our network can synthesize a wide range of heterogeneous motions, like cartwheels and twists. Experimental and user study results demonstrate that our network is superior to state-of-the-art human motion synthesis networks in the quality of generated motions.
C1 [Hou, Shuaiying; Tao, Hongyu; Bao, Hujun; Xu, Weiwei] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University
RP Xu, WW (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM 11721044@zju.edu.cn; 3170102625@zju.edu.cn; bao@cad.zju.edu.cn;
   18058700512@163.com
RI Xu, Weiwei/B-5045-2017
OI Xu, Weiwei/0000-0003-3756-3539; Hou, Shuaiying/0000-0003-0689-9240; Bao,
   Hujun/0000-0002-2662-0334
FU National Natural Science Foundation of China [61732016]; Information
   Technology Center and State Key Lab of CAD&CG at Zhejiang University
FX Weiwei Xu is partially supported by the National Natural Science
   Foundation of China under Grant 61732016. And we also thank the support
   provided by the Information Technology Center and State Key Lab of
   CAD&CG at Zhejiang University.
NR 66
TC 2
Z9 2
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG
PY 2024
VL 30
IS 8
BP 5047
EP 5062
DI 10.1109/TVCG.2023.3284402
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XP6C3
UT WOS:001262914400037
PM 37294654
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Benda, B
   Sargunam, SP
   Nourani, M
   Ragan, ED
AF Benda, Brett
   Sargunam, Shyam Prathish
   Nourani, Mahsan
   Ragan, Eric D.
TI An Evaluation of View Rotation Techniques for Seated Navigation in
   Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Legged locomotion; Turning; Tracking; Teleportation; Virtual
   environments; Navigation; Three-dimensional displays; Human-centered
   computing; human-computer interaction; virtual reality
ID HEAD MOVEMENTS; ENVIRONMENTS; LOCOMOTION; TRAVEL
AB Head tracking is commonly used in VR applications to allow users to naturally view 3D content using physical head movement, but many applications also support turning with hand-held controllers. Controller and joystick controls are convenient for practical settings where full 360-degree physical rotation is not possible, such as when the user is sitting at a desk. Though controller-based rotation provides the benefit of convenience, previous research has demonstrated that virtual or joystick-controlled view rotation to have drawbacks of sickness and disorientation compared to physical turning. To combat such issues, researchers have considered various techniques such as speed adjustments or reduced field of view, but data is limited on how different variations for joystick rotation influences sickness and orientation perception. Our studies include different variations of techniques such as joystick rotation, resetting, and field-of-view reduction. We investigate trade-offs among different techniques in terms of sickness and the ability to maintain spatial orientation. In two controlled experiments, participants traveled through a sequence of rooms and were tested on spatial orientation, and we also collected subjective measures of sickness and preference. Our findings indicate a preference by users towards directly-manipulated joystick-based rotations compared to user-initiated resetting and minimal effects of technique on spatial awareness.
C1 [Benda, Brett; Nourani, Mahsan; Ragan, Eric D.] Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32611 USA.
   [Sargunam, Shyam Prathish] Autodesk, San Rafael, CA 94903 USA.
C3 State University System of Florida; University of Florida; Autodesk,
   Inc.
RP Benda, B (corresponding author), Univ Florida, Dept Comp & Informat Sci & Engn, Gainesville, FL 32611 USA.
EM brett.benda@ufl.edu; shyam.prathish@gmail.com; mahsannourani@ufl.edu;
   eragan@ufl.edu
RI Sargunam, Shyam Prathish/JZD-0838-2024
OI Benda, William/0000-0002-1825-6392
NR 55
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4257
EP 4270
DI 10.1109/TVCG.2023.3258693
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700048
PM 37030847
DA 2025-03-07
ER

PT J
AU Kawabe, T
   Ujitoko, Y
AF Kawabe, Takahiro
   Ujitoko, Yusuke
TI Softness Perception of Visual Objects Controlled by Touchless Inputs:
   The Role of Effective Distance of Hand Movements
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Deformation; Visualization; Cameras; Material properties; Springs;
   Monitoring; Fabrics; Material perception; pseudo-haptics; touchless
   inputs; softness
ID HAPTIC FEEDBACK; SEEING LIQUIDS; RATIO
AB Feedback on the material properties of a visual object is essential in enhancing the users' perceptual experience of the object when users control the object with touchless inputs. Focusing on the softness perception of the object, we examined how the effective distance of hand movements influenced the degree of the object's softness perceived by users. In the experiments, participants moved their right hand in front of a camera which tracked their hand position. A textured 2D or 3D object on display deformed depending on the participant's hand position. In addition to establishing a ratio of deformation magnitude to the distance of hand movements, we altered the effective distance of hand movement, within which the hand movement could deform the object. Participants rated the strength of perceived softness (Experiments 1 and 2) and other perceptual impressions (Experiment 3). A longer effective distance produced a softer impression of the 2D and 3D objects. The saturation speed of object deformation due to the effective distance was not a critical determinant. The effective distance also modulated other perceptual impressions than softness. The role of the effective distance of hand movements on perceptual impressions of objects under touchless control is discussed.
C1 [Kawabe, Takahiro; Ujitoko, Yusuke] NTT Corp, NTT Commun Sci Labs, Atsugi, Kanagawa 2430198, Japan.
C3 Nippon Telegraph & Telephone Corporation
RP Kawabe, T (corresponding author), NTT Corp, NTT Commun Sci Labs, Atsugi, Kanagawa 2430198, Japan.
EM takkawabe@gmail.com; yusuke.ujitoko@gmail.com
RI Ujitoko, Yusuke/AAV-2457-2021; Kawabe, Takahiro/V-3676-2017
OI Kawabe, Takahiro/0000-0002-9888-8866; Ujitoko,
   Yusuke/0000-0001-6059-9324
NR 60
TC 0
Z9 0
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4154
EP 4169
DI 10.1109/TVCG.2023.3254522
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700065
PM 37028284
DA 2025-03-07
ER

PT J
AU Xiao, WP
   Xu, C
   Mai, JJ
   Xu, XM
   Li, Y
   Li, CZ
   Liu, XT
   He, SF
AF Xiao, Wenpeng
   Xu, Cheng
   Mai, Jiajie
   Xu, Xuemiao
   Li, Yue
   Li, Chengze
   Liu, Xueting
   He, Shengfeng
TI Appearance-Preserved Portrait-to-Anime Translation via Proxy-Guided
   Domain Adaptation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Adaptation models; Training; Strain; Shape; Semantics; Faces; Deformable
   models; Portrait-to-anime translation; coser portrait proxy; domain
   adaptation
ID GENERATIVE ADVERSARIAL NETWORKS
AB Converting a human portrait to anime style is a desirable but challenging problem. Existing methods fail to resolve this problem due to the large inherent gap between two domains that cannot be overcome by a simple direct mapping. For this reason, these methods struggle to preserve the appearance features in the original photo. In this article, we discover an intermediate domain, the coser portrait (portraits of humans costuming as anime characters), that helps bridge this gap. It alleviates the learning ambiguity and loosens the mapping difficulty in a progressive manner. Specifically, we start from learning the mapping between coser and anime portraits, and present a proxy-guided domain adaptation learning scheme with three progressive adaptation stages to shift the initial model to the human portrait domain. In this way, our model can generate visually pleasant anime portraits with well-preserved appearances given the human portrait. Our model adopts a disentangled design by breaking down the translation problem into two specific subtasks of face deformation and portrait stylization. This further elevates the generation quality. Extensive experimental results show that our model can achieve visually compelling translation with better appearance preservation and perform favorably against the existing methods both qualitatively and quantitatively. Our code and datasets are available at https://github.com/NeverGiveU/PDA-Translation.
C1 [Xiao, Wenpeng; Xu, Cheng; Xu, Xuemiao; Li, Yue] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.
   [Mai, Jiajie] Kings Coll London, London WC2R 2LS, England.
   [Xu, Xuemiao] Minist Educ, Key Lab Big Data & Intelligent Robot & Guangdong, State Key Lab Subtrop Bldg Sci, Prov Key Lab Computat Intelligence & Cyberspace In, Guangdong Province510641, Guangzhou, Peoples R China.
   [Li, Chengze; Liu, Xueting] Caritas Inst Higher Educ, Hong Kong, Peoples R China.
   [He, Shengfeng] Singapore Management Univ, Sch Comp & Informat Syst, Singapore 188065, Singapore.
C3 South China University of Technology; University of London; King's
   College London; Saint Francis University Hong Kong; Singapore Management
   University
RP Xu, XM (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.; He, SF (corresponding author), Singapore Management Univ, Sch Comp & Informat Syst, Singapore 188065, Singapore.
EM wpxiao.littleblack@gmail.com; cschengxu@gmail.com; k20035517@kcl.ac.uk;
   xuemx@scut.edu.cn; liyue@scut.edu.cn; czli@cihe.edu.hk;
   tliu@cihe.edu.hk; shengfenghe7@gmail.com
RI Xiao, Wenpeng/IQR-6827-2023; Xu, Cheng/HZL-1279-2023; Li,
   Chengze/AAU-7168-2021; He, Shengfeng/E-5682-2016
OI Li, Chengze/0000-0002-1519-750X; He, Shengfeng/0000-0002-3802-4644; Liu,
   Xueting/0000-0002-0868-5353; Xu, Cheng/0000-0002-4281-6214; xu,
   xuemiao/0000-0002-8006-3663
FU Guangdong International Technology Cooperation Project
   [2022A0505050009]; Key-Area Research and Development Program of
   Guangdong Province, China [2020B010165004, 2020B010166003]; National
   Natural Science Foundation of China [61972162]; Guangdong International
   Science and Technology Cooperation Project [2021A0505030009]; Guangdong
   Natural Science Foundation [2021A1515012625]; Guangzhou Basic and
   Applied Research Project [202102021074]; CCF-Tencent Open Research fund
   under Grant CCF-Tencent [RAGR20210114]; Research Grants Council of the
   Hong Kong Special Administrative Region, China, Project
   [UGC/FDS11/E02/21]
FX This work was supported in part by Guangdong International Technology
   Cooperation Project under Grant 2022A0505050009, in part by the Key-Area
   Research and Development Program of Guangdong Province, China under
   Grants 2020B010165004 and 2020B010166003, in part by the National
   Natural Science Foundation of China under Grant 61972162, in part by
   Guangdong International Science and Technology Cooperation Project under
   Grant 2021A0505030009, in part by Guangdong Natural Science Foundation
   under Grant 2021A1515012625, in part by Guangzhou Basic and Applied
   Research Project under Grant 202102021074, in part by CCF-Tencent Open
   Research fund under Grant CCF-Tencent RAGR20210114, and in part by Grant
   from the Research Grants Council of the Hong Kong Special Administrative
   Region, China, Project under Grant UGC/FDS11/E02/21.
NR 53
TC 2
Z9 2
U1 9
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3104
EP 3120
DI 10.1109/TVCG.2022.3228707
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700032
PM 37015410
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Rodrigues, N
   Dennig, FL
   Brandt, V
   Keim, DA
   Weiskopf, D
AF Rodrigues, Nils
   Dennig, Frederik L.
   Brandt, Vincent
   Keim, Daniel A.
   Weiskopf, Daniel
TI Comparative Evaluation of Animated Scatter Plot Transitions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Animation; Data visualization; Task analysis; Three-dimensional
   displays; Visual analytics; Splines (mathematics); Shape; Visualization;
   scatter plot; animation; quantitative user study; multidimensional data;
   coordinated and multiple views
AB Scatter plots are popular for displaying 2D data, but in practice, many data sets have more than two dimensions. For the analysis of such multivariate data, it is often necessary to switch between scatter plots of different dimension pairs, e.g., in a scatter plot matrix (SPLOM). Alternative approaches include a "grand tour" for an overview of the entire data set or creating artificial axes from dimensionality reduction (DR). A cross-cutting concern in all techniques is the ability of viewers to find correspondence between data points in different views. Previous work proposed animations to preserve the mental map between view changes and to trace points as well as clusters between scatter plots of the same underlying data set. In this article, we evaluate a variety of spline- and rotation-based view transitions in a crowdsourced user study focusing on ecological validity. Using the study results, we assess each animation's suitability for tracing points and clusters across view changes. We evaluate whether the order of horizontal and vertical rotation is relevant for task accuracy. The results show that rotations with an orthographic camera or staged expansion of a depth axis significantly outperform all other animation techniques for the traceability of individual points. Further, we provide a ranking of the animated transition techniques for traceability of individual points. However, we could not find any significant differences for the traceability of clusters. Furthermore, we identified differences by animation direction that could guide further studies to determine potential confounds for these differences. We publish the study data for reuse and provide the animation framework as a D3.js plug-in.
C1 [Rodrigues, Nils; Keim, Daniel A.] Univ Stuttgart, Visualizat Res Ctr VISUS, D-70174 Stuttgart, Germany.
   [Dennig, Frederik L.; Keim, Daniel A.] Univ Konstanz, D-78464 Constance, Germany.
   [Brandt, Vincent] Univ Stuttgart, Stuttgart, Germany.
C3 University of Stuttgart; University of Konstanz; University of Stuttgart
RP Rodrigues, N (corresponding author), Univ Stuttgart, Visualizat Res Ctr VISUS, D-70174 Stuttgart, Germany.
EM nils.rodrigues@visus.uni-stuttgart.de;
   frederik.l.dennig@uni-konstanz.de; st161848@stud.uni-stuttgart.de;
   daniel.a.keim@uni-konstanz.de; daniel.weiskopf@visus.uni-stuttgart.de
RI Dennig, Frederik/HTL-3123-2023; Weiskopf, Daniel/KWT-7459-2024
OI Weiskopf, Daniel/0000-0003-1174-1026; Rodrigues,
   Nils/0000-0002-1485-8249; Dennig, Frederik L./0000-0003-1116-8450
FU Deutsche Forschungsgemeinschaft
FX No Statement Available
NR 53
TC 0
Z9 0
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2024
VL 30
IS 6
BP 2929
EP 2941
DI 10.1109/TVCG.2024.3388558
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WC8Z6
UT WOS:001252775500011
PM 38625781
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ai, H
   Cao, ZD
   Lu, HN
   Chen, C
   Ma, J
   Zhou, PY
   Kim, TK
   Hui, P
   Wang, L
AF Ai, Hao
   Cao, Zidong
   Lu, Haonan
   Chen, Chen
   Ma, Jian
   Zhou, Pengyuan
   Kim, Tae-Kyun
   Hui, Pan
   Wang, Lin
TI Dream360: Diverse and Immersive Outdoor Virtual Scene Creation via
   Transformer-Based 360° Image Outpainting
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE 360 image outpainting; virtual scene creation; vision transformer
AB 360 degrees images, with a field-of-view (FoV) of 180 degrees x 360 degrees, provide immersive and realistic environments for emerging virtual reality (VR) applications, such as virtual tourism, where users desire to create diverse panoramic scenes from a narrow FoV photo they take from a viewpoint via portable devices. It thus brings us to a technical challenge: 'How to allow the users to freely create diverse and immersive virtual scenes from a narrow FoV image with a specified viewport?' To this end, we propose a transformer-based 360 degrees image outpainting framework called Dream360, which can generate diverse, high-fidelity, and high-resolution panoramas from user-selected viewports, considering the spherical properties of 360 degrees images. Compared with existing methods, e.g., [3], which primarily focus on inputs with rectangular masks and central locations while overlooking the spherical property of 360 degrees images, our Dream360 offers higher outpainting flexibility and fidelity based on the spherical representation. Dream360 comprises two key learning stages: (I) codebook-based panorama outpainting via Spherical-VQGAN (S-VQGAN), and (II) frequency-aware refinement with a novel frequency-aware consistency loss. Specifically, S-VQGAN learns a sphere-specific codebook from spherical harmonic (SH) values, providing a better representation of spherical data distribution for scene modeling. The frequency-aware refinement matches the resolution and further improves the semantic consistency and visual fidelity of the generated results. Our Dream360 achieves significantly lower Frechet Inception Distance (FID) scores and better visual fidelity than existing methods. We also conducted a user study involving 15 participants to interactively evaluate the quality of the generated results in VR, demonstrating the flexibility and superiority of our Dream360 framework.
C1 [Ai, Hao; Cao, Zidong; Hui, Pan; Wang, Lin] HKUST GZ, Guangzhou, Peoples R China.
   [Lu, Haonan; Chen, Chen; Ma, Jian] OPPO, Shenzhen, Peoples R China.
   [Zhou, Pengyuan] USTC, Hefei, Peoples R China.
   [Kim, Tae-Kyun] Korea Adv Inst Sci & Technol, Daejeon, South Korea.
   [Kim, Tae-Kyun] ICI PLC, London, England.
   [Hui, Pan; Wang, Lin] HKUST, Hong Kong, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Korea Advanced Institute of Science & Technology (KAIST);
   Imperial Chemical Industries; Hong Kong University of Science &
   Technology
RP Ai, H (corresponding author), HKUST GZ, Guangzhou, Peoples R China.
EM aihao199712@gmail.com; zcao740@connect.hkust-gz.edu.cn;
   luhaonan@oppo.com; chenchen4@oppo.com; majian2@oppo.com;
   pyzhou@ustc.edu.cn; kimtaekyun@kaist.ac.kr; panhui@ust.hk;
   linwang@ust.hk
RI JIAN, MA/KQU-7977-2024; Zhou, Peng Yuan/AAJ-2139-2021; Ai,
   Hao/KHV-9503-2024; Kim, Tae-Kyun (T-K)/HTL-2208-2023
OI Kim, Tae-Kyun (T-K)/0000-0002-7587-6053; Zhou,
   Pengyuan/0000-0002-7909-4059; Ai, Hao/0000-0003-2104-3352
FU OPPO Research Fund
FX No Statement Available
NR 68
TC 1
Z9 1
U1 5
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2734
EP 2744
DI 10.1109/TVCG.2024.3372085
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400065
PM 38437117
DA 2025-03-07
ER

PT J
AU Bernal-Berdun, E
   Vallejo, M
   Sun, Q
   Serrano, A
   Gutierrez, D
AF Bernal-Berdun, Edurne
   Vallejo, Mateo
   Sun, Qi
   Serrano, Ana
   Gutierrez, Diego
TI Modeling the Impact of Head-Body Rotations on Audio-Visual Spatial
   Perception for Virtual Reality Applications
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual Reality; Audio-Visual Spatial Perception
ID SPACE; LOCALIZATION; ENVIRONMENTS; COMPRESSION; THRESHOLDS
AB Humans perceive the world by integrating multimodal sensory feedback, including visual and auditory stimuli, which holds true in virtual reality (VR) environments. Proper synchronization of these stimuli is crucial for perceiving a coherent and immersive VR experience. In this work, we focus on the interplay between audio and vision during localization tasks involving natural head-body rotations. We explore the impact of audio-visual offsets and rotation velocities on users' directional localization acuity for various viewing modes. Using psychometric functions, we model perceptual disparities between visual and auditory cues and determine offset detection thresholds. Our findings reveal that target localization accuracy is affected by perceptual audio-visual disparities during head-body rotations, but remains consistent in the absence of stimuli-head relative motion. We then showcase the effectiveness of our approach in predicting and enhancing users' localization accuracy within realistic VR gaming applications. To provide additional support for our findings, we implement a natural VR game wherein we apply a compensatory audio-visual offset derived from our measured psychometric functions. As a result, we demonstrate a substantial improvement of up to 40% in participants' target localization accuracy. We additionally provide guidelines for content creation to ensure coherent and seamless VR experiences.
C1 [Bernal-Berdun, Edurne; Vallejo, Mateo; Serrano, Ana; Gutierrez, Diego] Univ Zaragoza, I3A, Zaragoza, Spain.
   [Sun, Qi] NYU, New York, NY USA.
C3 University of Zaragoza; New York University
RP Bernal-Berdun, E (corresponding author), Univ Zaragoza, I3A, Zaragoza, Spain.
EM edurnebernal@unizar.es; mvallejo@unizar.es; qisun@nyu.edu;
   anase@unizar.es; diegog@unizar.es
RI Sun, Qi/AGY-5791-2022; Serrano Pacheu, Ana Belen/ABC-3358-2021
OI Sun, Qi/0000-0002-3094-5844; Bernal-Berdun, Edurne/0000-0002-5275-8652;
   Serrano Pacheu, Ana Belen/0000-0002-7796-3177; Vallejo Dominguez,
   Mateo/0009-0008-8365-2405
FU European Union's Horizon 2020 research and innovation program
FX No Statement Available
NR 50
TC 3
Z9 3
U1 8
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2624
EP 2632
DI 10.1109/TVCG.2024.3372112
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OT7U2
UT WOS:001209605200006
PM 38446650
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Salagean, A
   Wu, M
   Fletcher, G
   Cosker, D
   Fraser, DS
AF Salagean, Anca
   Wu, Michelle
   Fletcher, George
   Cosker, Darren
   Fraser, Danae Stanton
TI The Utilitarian Virtual Self - Using Embodied Personalized Avatars to
   Investigate Moral Decision-Making in Semi-Autonomous Vehicle Dilemmas
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Ethics; Avatars; Motor drives; Decision making; Automobiles; Electronic
   mail; Vehicles; Human-centered computing-Empirical studies in HCI;
   Virtual reality; Usability testing
ID BODY OWNERSHIP; ENVIRONMENTS; JUDGMENT; SENSE; EXPERIENCE; BEHAVIOR
AB Embodied personalized avatars are a promising new tool to investigate moral decision-making by transposing the user into the "middle of the action" in moral dilemmas. Here, we tested whether avatar personalization and motor control could impact moral decision-making, physiological reactions and reaction times, as well as embodiment, presence and avatar perception. Seventeen participants, who had their personalized avatars created in a previous study, took part in a range of incongruent (i.e., harmful action led to better overall outcomes) and congruent (i.e., harmful action led to trivial outcomes) moral dilemmas as the drivers of a semi-autonomous car. They embodied four different avatars (counterbalanced - personalized motor control, personalized no motor control, generic motor control, generic no motor control). Overall, participants took a utilitarian approach by performing harmful actions only to maximize outcomes. We found increased physiological arousal (SCRs and heart rate) for personalized avatars compared to generic avatars, and increased SCRs in motor control conditions compared to no motor control. Participants had slower reaction times when they had motor control over their avatars, possibly hinting at more elaborate decision-making processes. Presence was also higher in motor control compared to no motor control conditions. Embodiment ratings were higher for personalized avatars, and generally, personalization and motor control were perceptually positive features. These findings highlight the utility of personalized avatars and open up a range of future research possibilities that could benefit from the affordances of this technology and simulate, more closely than ever, real-life action.
C1 [Salagean, Anca] Univ Bath, Comp Sci Dept, Bath, England.
   [Salagean, Anca; Fraser, Danae Stanton] Univ Bath, Psychol Dept, Bath, England.
   [Wu, Michelle; Fletcher, George; Cosker, Darren] Univ Bath, CAMERA, Bath, England.
   [Cosker, Darren] Microsoft UK, London, England.
C3 University of Bath; University of Bath; University of Bath
RP Salagean, A (corresponding author), Univ Bath, Comp Sci Dept, Bath, England.; Salagean, A (corresponding author), Univ Bath, Psychol Dept, Bath, England.
EM as3101@bath.ac.uk; mw2634@bath.ac.uk; gf321@bath.ac.uk;
   dpc22@bath.ac.uk; pssds@bath.ac.uk
RI Salagean, Anca/LYP-1945-2024
OI Stanton Fraser, Danae/0000-0002-3062-731X; Cosker,
   Darren/0000-0001-5177-4741; Fletcher, George/0000-0002-8824-723X
FU Bristol and Bath Creative R&D funded by the AHRC Creative Industries
   Cluster Programme
FX No Statement Available
NR 79
TC 0
Z9 0
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2162
EP 2172
DI 10.1109/TVCG.2024.3372121
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OF4H3
UT WOS:001205832400018
PM 38437115
DA 2025-03-07
ER

PT J
AU Li, H
   Yang, XR
   Zhai, HJ
   Liu, YQ
   Bao, HJ
   Zhang, GF
AF Li, Hai
   Yang, Xingrui
   Zhai, Hongjia
   Liu, Yuqian
   Bao, Hujun
   Zhang, Guofeng
TI Vox-Surf: Voxel-Based Implicit Surface Representation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Surface reconstruction; Image reconstruction; Geometry; Rendering
   (computer graphics); Three-dimensional displays; Feature extraction;
   Surface treatment; Implicit representation; surface reconstruction;
   scene editing
AB Virtual content creation and interaction play an important role in modern 3D applications. Recovering detailed 3D models from real scenes can significantly expand the scope of its applications and has been studied for decades in the computer vision and computer graphics community. In this work, we propose Vox-Surf, a voxel-based implicit surface representation. Our Vox-Surf divides the space into finite sparse voxels, where each voxel is a basic geometry unit that stores geometry and appearance information on its corner vertices. Due to the sparsity inherited from the voxel representation, Vox-Surf is suitable for almost any scene and can be easily trained end-to-end from multiple view images. We utilize a progressive training process to gradually cull out empty voxels and keep only valid voxels for further optimization, which greatly reduces the number of sample points and improves inference speed. Experiments show that our Vox-Surf representation can learn fine surface details and accurate colors with less memory and faster rendering than previous methods. The resulting fine voxels can also be considered as the bounding volumes for collision detection, which is useful in 3D interactions. We also show the potential application of Vox-Surf in scene editing and augmented reality. The source code is publicly available at https://github.com/zju3dv/Vox-Surf.
C1 [Li, Hai; Zhai, Hongjia; Bao, Hujun; Zhang, Guofeng] Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
   [Yang, Xingrui] Univ Bristol, Visual Informat Lab, Bristol BS8 1TL, England.
   [Liu, Yuqian] SenseTime, Autonomous Driving Grp, Shanghai 200233, Peoples R China.
C3 Zhejiang University; University of Bristol
RP Zhang, GF (corresponding author), Zhejiang Univ, State Key Lab CAD & CG, Hangzhou 310027, Zhejiang, Peoples R China.
EM garyli@zju.edu.cn; x.yang@bristol.ac.uk; zhj1999@zju.edu.cn;
   liuyuqian@senseauto.com; baohujun@zju.edu.cn; zhangguofeng@zju.edu.cn
RI liu, yuqian/ITV-9412-2023; Zhang, Ge/K-9118-2019
OI Yang, Xingrui/0000-0001-6812-3072; Li, Hai/0000-0002-5114-6566; Zhai,
   Hongjia/0000-0002-7729-8787; Bao, Hujun/0000-0002-2662-0334; Zhang,
   Guofeng/0000-0001-5661-8430
FU National Natural Science Foundation of China
FX No Statement Available
NR 43
TC 7
Z9 7
U1 3
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2024
VL 30
IS 3
BP 1743
EP 1755
DI 10.1109/TVCG.2022.3225844
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IN0A9
UT WOS:001166876500002
PM 36459607
DA 2025-03-07
ER

PT J
AU Assor, A
   Prouzeau, A
   Hachet, M
   Dragicevic, P
AF Assor, Ambre
   Prouzeau, Arnaud
   Hachet, Martin
   Dragicevic, Pierre
TI Handling Non-Visible Referents in Situated Visualizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Taxonomy; Models; Frameworks; Theory; Mobile; AR/VR/Immersive;
   Specialized Input/Display Hardware
ID MIXED REALITY; TAXONOMY
AB Situated visualizations are a type of visualization where data is presented next to its physical referent (i.e., the physical object, space, or person it refers to), often using augmented-reality displays. While situated visualizations can be beneficial in various contexts and have received research attention, they are typically designed with the assumption that the physical referent is visible. However, in practice, a physical referent may be obscured by another object, such as a wall, or may be outside the user's visual field. In this paper, we propose a conceptual framework and a design space to help researchers and user interface designers handle non-visible referents in situated visualizations. We first provide an overview of techniques proposed in the past for dealing with non-visible objects in the areas of 3D user interfaces, 3D visualization, and mixed reality. From this overview, we derive a design space that applies to situated visualizations and employ it to examine various trade-offs, challenges, and opportunities for future research in this area.
C1 [Assor, Ambre; Prouzeau, Arnaud; Hachet, Martin; Dragicevic, Pierre] Univ Bordeaux, Inria, CNRS, Bordeaux, France.
C3 Universite de Bordeaux; Inria; Centre National de la Recherche
   Scientifique (CNRS)
RP Assor, A (corresponding author), Univ Bordeaux, Inria, CNRS, Bordeaux, France.
EM ambre.assor@inria.fr; arnaud.prouzeau@inria.fr; martin.hachet@inria.fr;
   pierre.dragicevic@inria.fr
RI Dragicevic, Pierre/HKV-4981-2023; Prouzeau, Arnaud/AAU-3378-2021
OI Prouzeau, Arnaud/0000-0003-3800-5870; Assor, Ambre/0000-0003-3304-4459;
   Hachet, Martin/0000-0003-3889-2529
FU Agence Nationale de la Recherche (ANR)
FX No Statement Available
NR 97
TC 1
Z9 2
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 1336
EP 1346
DI 10.1109/TVCG.2023.3327361
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500014
PM 37878456
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Bae, SS
   Fujiwara, T
   Ynnerman, A
   Do, EYL
   Rivera, ML
   Szafir, DA
AF Bae, S. Sandra
   Fujiwara, Takanori
   Ynnerman, Anders
   Do, Ellen Yi-Luen
   Rivera, Michael L.
   Szafir, Danielle Albers
TI A Computational Design Pipeline to Fabricate Sensing Network
   Physicalizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Physicalization; tangible interfaces; 3D printing; computational
   fabrication; design automation; network data
ID INTERACTIVE VISUALIZATION; EXPLORATION; PROVENANCE; MODELS
AB Interaction is critical for data analysis and sensemaking. However, designing interactive physicalizations is challenging as it requires cross-disciplinary knowledge in visualization, fabrication, and electronics. Interactive physicalizations are typically produced in an unstructured manner, resulting in unique solutions for a specific dataset, problem, or interaction that cannot be easily extended or adapted to new scenarios or future physicalizations. To mitigate these challenges, we introduce a computational design pipeline to 3D print network physicalizations with integrated sensing capabilities. Networks are ubiquitous, yet their complex geometry also requires significant engineering considerations to provide intuitive, effective interactions for exploration. Using our pipeline, designers can readily produce network physicalizations supporting selection-the most critical atomic operation for interaction-by touch through capacitive sensing and computational inference. Our computational design pipeline introduces a new design paradigm by concurrently considering the form and interactivity of a physicalization into one cohesive fabrication workflow. We evaluate our approach using (i) computational evaluations, (ii) three usage scenarios focusing on general visualization tasks, and (iii) expert interviews. The design paradigm introduced by our pipeline can lower barriers to physicalization research, creation, and adoption.
C1 [Bae, S. Sandra; Do, Ellen Yi-Luen; Rivera, Michael L.] Univ Colorado, Boulder, CO 80309 USA.
   [Fujiwara, Takanori; Ynnerman, Anders] Linkoping Univ, Linkoping, Sweden.
   [Szafir, Danielle Albers] Univ North Carolina Chapel Hill, Chapel Hill, NC USA.
C3 University of Colorado System; University of Colorado Boulder; Linkoping
   University; University of North Carolina School of Medicine; University
   of North Carolina; University of North Carolina Chapel Hill
RP Bae, SS (corresponding author), Univ Colorado, Boulder, CO 80309 USA.
EM sandra.bae@colorado.edu; takanori.fujiwara@liu.se;
   anders.ynnerman@liu.se; ellen.do@colorado.edu; mrivera@colorado.edu;
   danielle.szafir@cs.unc.edu
RI Do, Ellen Yi-Luen/B-3621-2009
OI Rivera, Michael/0000-0002-5998-8039; Ynnerman,
   Anders/0000-0002-9466-9826; Bae, Sandra/0000-0002-2023-6219; Do, Ellen
   Yi-Luen/0000-0002-9948-6375
FU U.S. National Science Foundation
FX No Statement Available
NR 94
TC 2
Z9 2
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 913
EP 923
DI 10.1109/TVCG.2023.3327198
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500062
PM 37906495
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Hull, M
   Pednekar, V
   Murray, H
   Roy, N
   Tung, E
   Routray, S
   Guerin, C
   Chen, J
   Wang, ZJ
   Lee, S
   Roozbahani, M
   Chau, DH
AF Hull, Matthew
   Pednekar, Vivian
   Murray, Hannah
   Roy, Nimisha
   Tung, Emmanuel
   Routray, Susanta
   Guerin, Connor
   Chen, Justin
   Wang, Zijie J.
   Lee, Seongmin
   Roozbahani, Mahdi
   Chau, Duen Horng
TI VISGRADER: Automatic Grading of D3 Visualizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Automatic grading; D3 visualization; large class; Selenium; Gradescope
   grading platform
AB Manually grading D3 data visualizations is a challenging endeavor, and is especially difficult for large classes with hundreds of students. Grading an interactive visualization requires a combination of interactive, quantitative, and qualitative evaluation that are conventionally done manually and are difficult to scale up as the visualization complexity, data size, and number of students increase. We present VisGrader, a first-of-its kind automatic grading method for D3 visualizations that scalably and precisely evaluates the data bindings, visual encodings, interactions, and design specifications used in a visualization. Our method enhances students' learning experience, enabling them to submit their code frequently and receive rapid feedback to better inform iteration and improvement to their code and visualization design. We have successfully deployed our method and auto-graded D3 submissions from more than 4000 students in a visualization course at Georgia Tech, and received positive feedback for expanding its adoption.
C1 [Hull, Matthew; Pednekar, Vivian; Murray, Hannah; Roy, Nimisha; Tung, Emmanuel; Routray, Susanta; Guerin, Connor; Chen, Justin; Wang, Zijie J.; Lee, Seongmin; Roozbahani, Mahdi; Chau, Duen Horng] Georgia Inst Technol, Atlanta, GA 30332 USA.
C3 University System of Georgia; Georgia Institute of Technology
RP Hull, M (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM matthewhull@gatech.edu; vpednekar3@gatech.edu; hmurray9@gatech.edu;
   nroy9@gatech.edu; tunge@gatech.edu; sroutray@gatech.edu;
   cguerin6@gatech.edu; chen3001@gatech.edu; jayw@gatech.edu;
   seongmin@gatech.edu; mahdir@gatech.edu; polo@gatech.edu
RI Roy, Nimisha/GMW-7646-2022; Murray, Hannah/JWO-5292-2024
OI Tung, Emmanuel/0009-0005-8511-9215; Roozbahani, M.
   Mahdi/0000-0001-5462-1409; Wang, Zijie Jay/0000-0003-4360-1423
NR 48
TC 1
Z9 1
U1 4
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 617
EP 627
DI 10.1109/TVCG.2023.3327181
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500013
PM 37883258
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kim, DH
   Choi, S
   Kim, J
   Setlur, V
   Agrawala, M
AF Kim, Dae Hyun
   Choi, Seulgi
   Kim, Juho
   Setlur, Vidya
   Agrawala, Maneesh
TI EC: A Tool for Guiding Chart and Caption Emphasis&lt;sc/&gt;&lt;sc/&gt;
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Chart and text takeaways; visual prominence; authoring; captions
ID VISUALIZATION
AB Recent work has shown that when both the chart and caption emphasize the same aspects of the data, readers tend to remember the doubly-emphasized features as takeaways; when there is a mismatch, readers rely on the chart to form takeaways and can miss information in the caption text. Through a survey of 280 chart-caption pairs in real-world sources (e.g., news media, poll reports, government reports, academic articles, and Tableau Public), we find that captions often do not emphasize the same information in practice, which could limit how effectively readers take away the authors' intended messages. Motivated by the survey findings, we present Emphasischecker, an interactive tool that highlights visually prominent chart features as well as the features emphasized by the caption text along with any mismatches in the emphasis. The tool implements a time-series prominent feature detector based on the Ramer-Douglas-Peucker algorithm and a text reference extractor that identifies time references and data descriptions in the caption and matches them with chart data. This information enables authors to compare features emphasized by these two modalities, quickly see mismatches, and make necessary revisions. A user study confirms that our tool is both useful and easy to use when authoring charts and captions.
C1 [Kim, Dae Hyun; Choi, Seulgi; Kim, Juho] Korea Adv Inst Sci & Technol, Daejeon, South Korea.
   [Kim, Dae Hyun; Agrawala, Maneesh] Stanford Univ, Stanford, CA USA.
   [Setlur, Vidya] Tableau Res, Amherst, MA USA.
   [Agrawala, Maneesh] Roblox, San Mateo, CA USA.
C3 Korea Advanced Institute of Science & Technology (KAIST); Stanford
   University
RP Kim, DH (corresponding author), Korea Adv Inst Sci & Technol, Daejeon, South Korea.
EM dhkim16@cs.stanford.edu; igules8925@kaist.ac.kr; juhokim@kaist.ac.kr;
   vsetlur@tableau.com; maneesh@cs.stanford.edu
RI Kim, Juho/A-7091-2016; Kim, Dae Hyun/JJD-4264-2023
OI Kim, Juho/0000-0001-6348-4127; Kim, Dae Hyun/0000-0002-8657-9986; Choi,
   Seulgi/0000-0002-9334-0471; Agrawala, Maneesh/0000-0002-8996-7327;
   Setlur, Vidya/0000-0003-3722-406X
FU Brown Institute for Media Innovation
FX No Statement Available
NR 81
TC 1
Z9 1
U1 2
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 120
EP 130
DI 10.1109/TVCG.2023.3327150
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500032
PM 37922182
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kruchten, N
   Mcnutt, AM
   Mcguffin, MJ
AF Kruchten, Nicolas
   Mcnutt, Andrew M.
   Mcguffin, Michael J.
TI Metrics-Based Evaluation and Comparison of Visualization Notations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Measurement; Usability; Data visualization; Visualization; Libraries;
   Task analysis; Grammar; Notation; Evaluation; Language design; API
   design; Domain-specific languages
ID API USABILITY; GRAMMAR
AB A visualization notation is a recurring pattern of symbols used to author specifications of visualizations, from data transformation to visual mapping. Programmatic notations use symbols defined by grammars or domain-specific languages (e.g. ggplot2, dplyr, Vega-Lite) or libraries (e.g. Matplotlib, Pandas). Designers and prospective users of grammars and libraries often evaluate visualization notations by inspecting galleries of examples. While such collections demonstrate usage and expressiveness, their construction and evaluation are usually ad hoc, making comparisons of different notations difficult. More rarely, experts analyze notations via usability heuristics, such as the Cognitive Dimensions of Notations framework. These analyses, akin to structured close readings of text, can reveal design deficiencies, but place a burden on the expert to simultaneously consider many facets of often complex systems. To alleviate these issues, we introduce a metrics-based approach to usability evaluation and comparison of notations in which metrics are computed for a gallery of examples across a suite of notations. While applicable to any visualization domain, we explore the utility of our approach via a case study considering statistical graphics that explores 40 visualizations across 9 widely used notations. We facilitate the computation of appropriate metrics and analysis via a new tool called NotaScope. We gathered feedback via interviews with authors or maintainers of prominent charting libraries (n = 6). We find that this approach is a promising way to formalize, externalize, and extend evaluations and comparisons of visualization notations.
C1 [Kruchten, Nicolas; Mcguffin, Michael J.] Ecole Technol Super, Montreal, PQ, Canada.
   [Mcnutt, Andrew M.] Univ Chicago, Chicago, IL USA.
C3 University of Quebec; Ecole de Technologie Superieure - Canada;
   University of Chicago
RP Kruchten, N (corresponding author), Ecole Technol Super, Montreal, PQ, Canada.
EM nicolas@kruchten.com; mcnutt@uchicago.edu; michael.mcguffin@etsmtl.ca
OI McNutt, Andrew/0000-0001-8255-4258
FU NSERC
FX No Statement Available
NR 94
TC 1
Z9 1
U1 3
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 425
EP 435
DI 10.1109/TVCG.2023.3326907
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500095
PM 37874719
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Schmidt, J
   Pointner, B
   Miksch, S
AF Schmidt, Johanna
   Pointner, Bernhard
   Miksch, Silvia
TI Visual Analytics for Understanding Draco's Knowledge Base
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Aggregates; Task analysis; Recommender systems;
   Costs; Bars; Visual analytics; Visual Analytics; Hypergraph
   visualization; Rule-based recommendation systems
ID OF-THE-ART; VISUALIZATION; DESIGN; COMMUNITIES
AB Draco has been developed as an automated visualization recommendation system formalizing design knowledge as logical constraints in ASP (Answer-Set Programming). With an increasing set of constraints and incorporated design knowledge, even visualization experts lose overview in Draco and struggle to retrace the automated recommendation decisions made by the system. Our paper proposes an Visual Analytics (VA) approach to visualize and analyze Draco's constraints. Our VA approach is supposed to enable visualization experts to accomplish identified tasks regarding the knowledge base and support them in better understanding Draco. We extend the existing data extraction strategy of Draco with a data processing architecture capable of extracting features of interest from the knowledge base. A revised version of the ASP grammar provides the basis for this data processing strategy. The resulting incorporated and shared features of the constraints are then visualized using a hypergraph structure inside the radial-arranged constraints of the elaborated visualization. The hierarchical categories of the constraints are indicated by arcs surrounding the constraints. Our approach is supposed to enable visualization experts to interactively explore the design rules' violations based on highlighting respective constraints or recommendations. A qualitative and quantitative evaluation of the prototype confirms the prototype's effectiveness and value in acquiring insights into Draco's recommendation process and design constraints.
C1 [Schmidt, Johanna; Pointner, Bernhard] VRVis Zentrum Virtual Real & Visualisierung Forsc, Vienna, Austria.
   [Miksch, Silvia] TU Wien, Ctr Visual Analyt Sci & Technol CVAST, Vienna, Austria.
C3 Technische Universitat Wien
RP Schmidt, J (corresponding author), VRVis Zentrum Virtual Real & Visualisierung Forsc, Vienna, Austria.
EM johanna.schmidt@vrvis.at; pointner@vrvis.at; silvia.miksch@tuwien.ac.at
OI Schmidt, Johanna/0000-0002-9638-6344; Miksch, Silvia/0000-0003-4427-5703
FU BMK
FX No Statement Available
NR 53
TC 0
Z9 0
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 392
EP 402
DI 10.1109/TVCG.2023.3326912
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500050
PM 37874727
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Shi, Y
   Chen, BC
   Chen, Y
   Jin, ZC
   Xu, K
   Jiao, XH
   Gao, T
   Cao, N
AF Shi, Yang
   Chen, Bingchang
   Chen, Ying
   Jin, Zhuochen
   Xu, Ke
   Jiao, Xiaohan
   Gao, Tian
   Cao, Nan
TI Supporting Guided Exploratory Visual Analysis on Time Series Data with
   Reinforcement Learning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Time Series Data; Exploratory Visual Analysis; Reinforcement Learning
ID VISUALIZATION; GENERATION; GUIDANCE
AB The exploratory visual analysis (EVA) of time series data uses visualization as the main output medium and input interface for exploring new data. However, for users who lack visual analysis expertise, interpreting and manipulating EVA can be challenging. Thus, providing guidance on EVA is necessary and two relevant questions need to be answered. First, how to recommend interesting insights to provide a first glance at data and help develop an exploration goal. Second, how to provide step-by-step EVA suggestions to help identify which parts of the data to explore. In this work, we present a reinforcement learning (RL)-based system, Visail, which generates EVA sequences to guide the exploration of time series data. As a user uploads a time series dataset, Visail can generate step-by-step EVA suggestions, while each step is visualized as an annotated chart combined with textual descriptions. The RL-based algorithm uses exploratory data analysis knowledge to construct the state and action spaces for the agent to imitate human analysis behaviors in data exploration tasks. In this way, the agent learns the strategy of generating coherent EVA sequences through a well-designed network. To evaluate the effectiveness of our system, we conducted an ablation study, a user study, and two case studies. The results of our evaluation suggested that Visail can provide effective guidance on supporting EVA on time series data.
C1 [Shi, Yang; Chen, Bingchang; Chen, Ying; Jiao, Xiaohan; Gao, Tian; Cao, Nan] Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai, Peoples R China.
   [Shi, Yang; Chen, Bingchang; Chen, Ying; Jiao, Xiaohan; Gao, Tian; Cao, Nan] Huawei Cloud Comp Technol Co Ltd, Shenzhen, Peoples R China.
C3 Tongji University
RP Cao, N (corresponding author), Tongji Univ, Intelligent Big Data Visualizat Lab, Shanghai, Peoples R China.; Cao, N (corresponding author), Huawei Cloud Comp Technol Co Ltd, Shenzhen, Peoples R China.
EM yangshi.idvx@tongji.edu.cn; 2131933@tongji.edu.cn;
   2131926@tongji.edu.cn; chjzcjames@gmail.com; lukexuke@gmail.com;
   xh_xiaohan@tongji.edu.cn; gaotian@tongji.edu.cn; nan.cao@tongji.edu.cn
RI Chen, Bingchang/LXU-3160-2024; Cao, Nan/O-5397-2014
OI Cao, Nan/0000-0003-1316-7515
FU NSFC
FX No Statement Available
NR 77
TC 1
Z9 1
U1 3
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 1172
EP 1182
DI 10.1109/TVCG.2023.3327200
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500132
PM 37883260
DA 2025-03-07
ER

PT J
AU Xiao, SS
   Huang, SZ
   Lin, Y
   Ye, YL
   Zeng, W
AF Xiao, Shishi
   Huang, Suizi
   Lin, Yue
   Ye, Yilin
   Zeng, Wei
TI Let the Chart Spark: Embedding Semantic Context into Chart with
   Text-to-Image Generative Model
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE pictorial visualization; generative model; authoring tool
ID VISUALIZATION; DESIGN
AB Pictorial visualization seamlessly integrates data and semantic context into visual representation, conveying complex information in an engaging and informative manner. Extensive studies have been devoted to developing authoring tools to simplify the creation of pictorial visualizations. However, mainstream works follow a retrieving-and-editing pipeline that heavily relies on retrieved visual elements from a dedicated corpus, which often compromise data integrity. Text-guided generation methods are emerging, but may have limited applicability due to their predefined entities. In this work, we propose ChartSpark, a novel system that embeds semantic context into chart based on text-to-image generative models. ChartSpark generates pictorial visualizations conditioned on both semantic context conveyed in textual inputs and data information embedded in plain charts. The method is generic for both foreground and background pictorial generation, satisfying the design practices identified from empirical research into existing pictorial visualizations. We further develop an interactive visual interface that integrates a text analyzer, editing module, and evaluation module to enable users to generate, modify, and assess pictorial visualizations. We experimentally demonstrate the usability of our tool, and conclude with a discussion of the potential of using text-to-image generative models combined with an interactive interface for visualization design.
C1 [Xiao, Shishi; Huang, Suizi; Lin, Yue; Ye, Yilin; Zeng, Wei] Hong Kong Univ Sci & Technol Guangzhou, Hong Kong Univ Sci & Technol Guangzhou, Guangzhou, Peoples R China.
   [Ye, Yilin; Zeng, Wei] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
C3 Hong Kong University of Science & Technology (Guangzhou); Hong Kong
   University of Science & Technology
RP Zeng, W (corresponding author), Hong Kong Univ Sci & Technol Guangzhou, Hong Kong Univ Sci & Technol Guangzhou, Guangzhou, Peoples R China.
EM sxiao713@connect.hkust-gz.edu.cn; shuang310@connect.hkust-gz.edu.cn;
   ylin491@connect.hkust-gz.edu.cn; yyebd@connect.hkust-gz.edu.cn;
   weizeng@hkust-gz.edu.cn
RI Ye, Yilin/GRR-8394-2022
FU National Natural Science Foundation of China
FX No Statement Available
NR 61
TC 5
Z9 5
U1 9
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 284
EP 294
DI 10.1109/TVCG.2023.3326913
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500011
PM 37878451
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Mao, AH
   Dong, WB
   Xie, CQ
   Wang, HM
   Liu, YJ
   Li, GQ
   He, Y
AF Mao, Aihua
   Dong, Wenbo
   Xie, Chaoqiang
   Wang, Huamin
   Liu, Yong-Jin
   Li, Guiqing
   He, Ying
TI Yarn-Level Simulation of Hygroscopicity of Woven Textiles
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Textiles; Liquids; Computational modeling; Yarn; Fluids; Fabrics;
   Visualization; Anisotropic textile; fluid dynamics; physical models;
   microscopic simulation; liquid-textile interaction
ID MOISTURE DIFFUSION; COUPLED DIFFUSION; HEAT; DYNAMICS; SORPTION;
   FABRICS; MODEL
AB Simulating liquid-textile interaction has received great attention in computer graphics recently. Most existing methods take textiles as particles or parameterized meshes. Although these methods can generate visually pleasing results, they cannot simulate water content at a microscopic level due to the lack of geometrically modeling of textile's anisotropic structure. In this paper, we develop a method for yarn-level simulation of hygroscopicity of textiles and evaluate it using various quantitative metrics. We model textiles in a fiber-yarn-fabric multi-scale manner and consider the dynamic coupled physical mechanisms of liquid spreading, including wetting, wicking, moisture sorption/desorption, and transient moisture-heat transfer in textiles. Our method can accurately simulate liquid spreading on textiles with different fiber materials and geometrical structures with consideration of air temperatures and humidity conditions. It visualizes the hygroscopicity of textiles to demonstrate their moisture management ability. We conduct qualitative and quantitative experiments to validate our method and explore various factors to analyze their influence on liquid spreading and hygroscopicity of textiles.
C1 [Mao, Aihua; Dong, Wenbo; Li, Guiqing] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.
   [Xie, Chaoqiang] South China Univ Technol, Sch Software Engn, Guangzhou 510641, Guangdong, Peoples R China.
   [Wang, Huamin] Style3D Res, Shanghai 200001, Peoples R China.
   [Liu, Yong-Jin] Tsinghua Univ, Dept Comp Sci & Technol, MOE Key Lab Pervas Comp, BNRist, Beijing 100084, Peoples R China.
   [He, Ying] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore 639798, Singapore.
C3 South China University of Technology; South China University of
   Technology; Tsinghua University; Nanyang Technological University
RP Wang, HM (corresponding author), Style3D Res, Shanghai 200001, Peoples R China.
EM ahmao@scut.edu.cn; 875158248@scut.edu.cn; 201921043082@mail.scut.edu.cn;
   wanghmin@style3d.com; liuyongjin@tsinghua.edu.cn; ligq@scut.edu.cn;
   yhe@ntu.edu.sg
RI Dong, Wenbo/ABC-7930-2021; Wang, Huamin/D-2600-2012; He,
   Ying/A-3708-2011
OI Wang, Huamin/0000-0002-8153-2337; Aihua, Mao/0000-0001-6861-9414
FU NSF of Guangdong Province [2019A1515010833, 2022A1515011573]; Natural
   Science Foundation of China [61725204, 61972160]; Tsinghua University
   Initiative Scientific Research Program [20211080093]; Ministry of
   Education, Singapore, under its Academic Research Fund Tier 1 [RG20/20]
FX This work was partially supported in part by the NSF of Guangdong
   Province under Grants 2019A1515010833 and 2022A1515011573, in part by
   the Natural Science Foundation of China under Grants 61725204 and
   61972160, in part by Tsinghua University Initiative Scientific Research
   Program under Grant 20211080093, and in part by the Ministry of
   Education, Singapore, under its Academic Research Fund Tier 1 (RG20/20).
NR 53
TC 0
Z9 0
U1 8
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5250
EP 5264
DI 10.1109/TVCG.2022.3206579
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300030
PM 36103450
DA 2025-03-07
ER

PT J
AU Yuan, GZQ
   Fu, QC
   Mi, ZX
   Luo, YM
   Tao, WB
AF Yuan, Ganzhangqin
   Fu, Qiancheng
   Mi, Zhenxing
   Luo, Yiming
   Tao, Wenbing
TI SSRNet: Scalable 3D Surface Reconstruction Network
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Surface reconstruction; Point cloud compression; Octrees; Surface
   treatment; Learning systems; Three-dimensional displays; Reconstruction
   algorithms; Surface reconstruction; implicit function; oriented point
   clouds; large-scale point clouds
ID POINTS
AB Learning-based surface reconstruction methods have received considerable attention in recent years due to their excellent expressiveness. However, existing learning-based methods lack scalability in processing large-scale point clouds. This paper proposes a novel scalable learning-based 3D surface reconstruction method based on octree, called SSRNet. SSRNet works in a scalable reconstruction pipeline, which divides oriented point clouds into different local parts and then processes them in parallel. Accommodating this scalable design pattern, SSRNet constructs local geometric features for octree vertices. Such features comprise the relation between the vertices and the implicit surface, ensuring geometric perception. Focusing on local geometric information also enables the network to avoid the overfitting problem and generalize well on different datasets. Finally, as a learning-based method, SSRNet can process large-scale point clouds in a short time. And to further solve the efficiency problem, we provide a lightweight and efficient version that is about five times faster while maintaining reconstruction performance. Experiments show that our methods achieve state-of-the-art performance with outstanding efficiency.
C1 [Yuan, Ganzhangqin; Fu, Qiancheng; Tao, Wenbing] Huazhong Univ Sci & Technol, Natl Key Lab Sci & Technol Multispectral Informat, Sch Artificial Intelligence & Automat, Wuhan 430074, Hubei, Peoples R China.
   [Mi, Zhenxing] Hong Kong Univ Sci & Technol, Kowloon Tong, Hong Kong, Peoples R China.
   [Luo, Yiming] Imperial Coll London, London SW7 2AZ, England.
C3 Huazhong University of Science & Technology; Hong Kong University of
   Science & Technology; Imperial College London
RP Tao, WB (corresponding author), Huazhong Univ Sci & Technol, Natl Key Lab Sci & Technol Multispectral Informat, Sch Artificial Intelligence & Automat, Wuhan 430074, Hubei, Peoples R China.
EM gzq_yuan@hust.edu.cn; fqc98@hust.edu.cn; zmiaa@connect.ust.hk;
   yiming_luo@163.com; wenbingtao@hust.edu.cn
OI Mi, Zhenxing/0000-0001-6526-1621; Tao, Wenbing/0000-0003-3284-864X
FU National Natural Science Foundation of China [62176096]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62176096.
NR 61
TC 4
Z9 4
U1 6
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 4906
EP 4919
DI 10.1109/TVCG.2022.3193406
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300007
PM 35877800
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Huang, XC
   Riddell, J
   Xiao, RB
AF Huang, Xincheng
   Riddell, James
   Xiao, Robert
TI Virtual Reality Telepresence: 360-Degree Video Streaming with
   Edge-Compute Assisted Static Foveated Compression
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 17th International Conference on High Speed Machining (HSM)
CY OCT 25-28, 2023
CL Nanjing, PEOPLES R CHINA
DE Streaming media; 5G mobile communication; Servers; Telepresence;
   Headphones; Bandwidth; Visualization; 360-Degree Video; Virtual Reality
AB Real-time communication with immersive 360 degrees video can enable users to be telepresent within a remotely streamed environment. Increasingly, users are shifting to mobile devices and connecting to the Internet via mobile-cellular networks. As the ideal media for 360 degrees videos, some VR headsets now also come with cellular capacity, giving them potential for mobile applications. However, streaming high-quality 360 degrees live video poses challenges for network bandwidth, particularly on cellular connections. To reduce bandwidth requirements, videos can be compressed using viewport-adaptive streaming or foveated rendering techniques. Such approaches require very low latency in order to be effective, which has previously limited their applications on traditional cellular networks. In this work, we demonstrate an end-to-end virtual reality telepresence system that streams similar to 6K 360 degrees video over 5G millimeter-wave (mmW) radio. Our use of 5G technologies, in conjunction with mobile edge compute nodes, substantially reduces latency when compared with existing 4G networks, enabling high-efficiency foveated compression over modern cellular networks on par with WiFi. We performed a technical evaluation of our system's visual quality post-compression with peak signal-to-noise ratio (PSNR) and FOVVideoVDP. We also conducted a user study to evaluate users' sensitivity to compressed video. Our findings demonstrate that our system achieves visually indistinguishable video streams while using up to 80% less data when compared with un-foveated video. We demonstrate our video compression system in the context of an immersive, telepresent video calling application.
C1 [Huang, Xincheng; Riddell, James; Xiao, Robert] Univ British Columbia, Vancouver, BC, Canada.
C3 University of British Columbia
RP Huang, XC (corresponding author), Univ British Columbia, Vancouver, BC, Canada.
EM xchuang@cs.ubc.ca; riddell6@student.ubc.ca; brx@cs.ubc.ca
RI Xiao, Robert/MHR-2554-2025
OI Xiao, Robert/0000-0003-4306-8825; Huang, Xincheng/0000-0001-6923-6490;
   Riddell, James/0009-0007-3261-5412
FU Natural Science and Engineering Research Council of Canada (NSERC)
   [RGPIN-2019-05624]; Rogers Communications Inc. under the Rogers-UBC
   Collaborative Research Grant: Augmented and Virtual Reality. We thank
   Ailin Saggau-Lyons for invaluable help in the initial development of the
   system
FX This work was supported in part by the Natural Science and Engineering
   Research Council of Canada (NSERC) under Discovery Grant
   RGPIN-2019-05624 and by Rogers Communications Inc. under the Rogers-UBC
   Collaborative Research Grant: Augmented and Virtual Reality. We thank
   Ailin Saggau-Lyons for invaluable help in the initial development of the
   system.
NR 51
TC 3
Z9 3
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2023
VL 29
IS 11
BP 4525
EP 4534
DI 10.1109/TVCG.2023.3320255
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA X6ZW5
UT WOS:001099919100018
PM 37788199
DA 2025-03-07
ER

PT J
AU Calvo, L
   Cucchietti, F
   Pérez-Montoro, M
AF Calvo, Luz
   Cucchietti, Fernando
   Perez-Montoro, Mario
TI Measuring the Effectiveness of Static Maps to Communicate Changes Over
   Time
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Information visualization; cognition; static maps; user interfaces;
   perception
ID DESIGN; FAMILIARITY; PERCEPTION; INFORMATION; LIKING; ISSUES
AB Both in digital and print media, it is common to use static maps to show the evolution of values in various regions over time. The ability to communicate local or global trends, while reducing the cognitive load on readers, is of vital importance for an audience that is not always well versed in map interpretation. This study aims to measure the efficiency of four static maps (choropleth, tile grid map and their banded versions) to test their usefulness in presenting changes over time from a user experience perspective. We first evaluate the effectiveness of these map types by quantitative performance analysis (time and success rates). In a second phase, we gather qualitative data to detect which type of map favors decision-making. On a quantitative level, our results show that certain types of maps work better to show global trends, while other types are more useful when analyzing regional trends or detecting the regions that fit a specific pattern. On a qualitative level, those representations which are already familiar to the user are often better valued despite having lower measured success rates.
C1 [Calvo, Luz; Cucchietti, Fernando] Barcelona Supercomp Ctr BSC, CASE Dept, Barcelona 08034, Spain.
   [Perez-Montoro, Mario] Univ Barcelona, Dept Lib & Informat Sci & Audiovisual Commun, Barcelona 08007, Spain.
C3 Universitat Politecnica de Catalunya; Barcelona Supercomputer Center
   (BSC-CNS); University of Barcelona
RP Calvo, L (corresponding author), Barcelona Supercomp Ctr BSC, CASE Dept, Barcelona 08034, Spain.
EM luz.calvo@bsc.es; fernando.cucchietti@bsc.es; perez-montoro@ub.edu
RI Pérez-Montoro, Mario/E-1472-2012; Cucchietti, Fernando/C-7765-2016
OI Cucchietti, Fernando/0000-0002-9027-1263
FU Bioethics Committee of the Universitat de Barcelona Institutional Review
   Board [IRB00003099]
FX This work involved human subjects or animals in its research. Approval
   of all ethical and experimental procedures and protocols was granted by
   Bioethics Committee of the Universitat de Barcelona Institutional Review
   Board under Application No. IRB00003099 and performed in line with the
   bioethics directives of Universitat de Barcelona.
NR 108
TC 4
Z9 4
U1 1
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2023
VL 29
IS 10
BP 4243
EP 4255
DI 10.1109/TVCG.2022.3188940
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8ZW3
UT WOS:001060356200015
PM 35820017
OA Green Published
DA 2025-03-07
ER

PT J
AU Mori, S
   Schmalstieg, D
   Kalkofen, D
AF Mori, Shohei
   Schmalstieg, Dieter
   Kalkofen, Denis
TI Good Keyframes to Inpaint
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Diminished reality; inpainting; keyframe; good keyframes to inpaint;
   SLAM
ID DIMINISHED REALITY
AB Diminished Reality (DR) propagates pixels from a keyframe to subsequent frames for real-time inpainting. Keyframe selection has a significant impact on the inpainting quality, but untrained users struggle to identify good keyframes. Automatic selection is not straightforward either, since no previous work has formalized or verified what determines a good keyframe. We propose a novel metric to select good keyframes to inpaint. We examine the heuristics adopted in existing DR inpainting approaches and derive multiple simple criteria measurable from SLAM. To combine these criteria, we empirically analyze their effect on the quality using a novel representative test dataset. Our results demonstrate that the combined metric selects RGBD keyframes leading to high-quality inpainting results more often than a baseline approach in both color and depth domains. Also, we confirmed that our approach has a better ranking ability of distinguishing good and bad keyframes. Compared to random selections, our metric selects keyframes that would lead to higher-quality and more stably converging inpainting results. We present three DR examples, automatic keyframe selection, user navigation, and marker hiding.
C1 [Mori, Shohei; Schmalstieg, Dieter; Kalkofen, Denis] Graz Univ Technol, Inst Comp Graph & Vis, A-8010 Graz, Austria.
C3 Graz University of Technology
RP Mori, S (corresponding author), Graz Univ Technol, Inst Comp Graph & Vis, A-8010 Graz, Austria.
EM s.mori.jp@ieee.org; schmalstieg@tugraz.at; kalkofen@icg.tugraz.at
RI Mori, Shohei/AAL-6642-2020
OI Mori, Shohei/0000-0003-0540-7312; Kalkofen, Denis/0000-0002-0359-206X;
   Schmalstieg, Dieter/0000-0003-2813-2235
FU Austrian Science Fund FWF [P33634]; Austrian Science Fund (FWF) [P33634]
   Funding Source: Austrian Science Fund (FWF)
FX This work was supported in part by Austrian Science Fund FWF under Grant
   P33634.
NR 38
TC 3
Z9 3
U1 1
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2023
VL 29
IS 9
BP 3989
EP 4000
DI 10.1109/TVCG.2022.3176958
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA O2BA8
UT WOS:001041912300019
PM 35605001
DA 2025-03-07
ER

PT J
AU Aristidou, A
   Yiannakidis, A
   Aberman, K
   Cohen-Or, D
   Shamir, A
   Chrysanthou, Y
AF Aristidou, Andreas
   Yiannakidis, Anastasios
   Aberman, Kfir
   Cohen-Or, Daniel
   Shamir, Ariel
   Chrysanthou, Yiorgos
TI Rhythm is a Dancer: Music-Driven Motion Synthesis With Global Structure
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Animation; global structure consistency; motion motifs; music-driven;
   motion signatures
ID BEAT TRACKING; STYLE; PHYSICS
AB Synthesizing human motion with a global structure, such as a choreography, is a challenging task. Existing methods tend to concentrate on local smooth pose transitions and neglect the global context or the theme of the motion. In this work, we present a music-driven motion synthesis framework that generates long-term sequences of human motions which are synchronized with the input beats, and jointly form a global structure that respects a specific dance genre. In addition, our framework enables generation of diverse motions that are controlled by the content of the music, and not only by the beat. Our music-driven dance synthesis framework is a hierarchical system that consists of three levels: pose, motif, and choreography. The pose level consists of an LSTM component that generates temporally coherent sequences of poses. The motif level guides sets of consecutive poses to form a movement that belongs to a specific distribution using a novel motion perceptual-loss. And the choreography level selects the order of the performed movements and drives the system to follow the global structure of a dance genre. Our results demonstrate the effectiveness of our music-driven framework to generate natural and consistent movements on various dance types, having control over the content of the synthesized motions, and respecting the overall structure of the dance.
C1 [Aristidou, Andreas; Yiannakidis, Anastasios; Chrysanthou, Yiorgos] Univ Cyprus, Dept Comp Sci, CY-1678 Nicosia, Cyprus.
   [Aristidou, Andreas; Yiannakidis, Anastasios; Chrysanthou, Yiorgos] CYENS Ctr Excellence, CY-1016 Nicosia, Cyprus.
   [Aberman, Kfir; Cohen-Or, Daniel] Tel Aviv Univ, Dept Comp Sci, IL-6997801 Tel Aviv, Israel.
   [Shamir, Ariel] Reichman Univ, Interdisciplinary Ctr Herzliya, Dept Comp Sci, IL-4610101 Herzliyya, Israel.
C3 University of Cyprus; Tel Aviv University; Reichman University
RP Aristidou, A (corresponding author), Univ Cyprus, Dept Comp Sci, CY-1678 Nicosia, Cyprus.
EM a.aristidou@ieee.org; tasyiann@gmail.com; kfiraberman@gmail.com;
   cohenor@gmail.com; arik@idc.ac.il; yiorgos@cs.ucy.ac.cy
RI Aristidou, Andreas/AAI-8096-2020
OI Aristidou, Andreas/0000-0001-7754-0791; Yiannakidis,
   Anastasios/0000-0002-3721-6548; Shamir, Ariel/0000-0001-7082-7845
FU University of Cyprus; European Union [739578]; Government of the
   Republic of Cyprus through the Ministry of Research, Innovation and
   Digital Policy
FX This work was Supported in part by the University of Cyprus, in part by
   the European Union's Horizon 2020 Research and Innovation Programme
   under Grant 739578, and in part by the Government of the Republic of
   Cyprus through the Deputy Ministry of Research, Innovation and Digital
   Policy.
NR 92
TC 18
Z9 18
U1 2
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2023
VL 29
IS 8
BP 3519
EP 3534
DI 10.1109/TVCG.2022.3163676
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L3CU5
UT WOS:001022080200006
PM 35353702
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Hu, ZM
   Bulling, A
   Li, S
   Wang, GP
AF Hu, Zhiming
   Bulling, Andreas
   Li, Sheng
   Wang, Guoping
TI EHTask: Recognizing User Tasks From Eye and Head Movements in Immersive
   Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Videos; Head; Visualization; Virtual reality; Magnetic
   heads; Solid modeling; Visual attention; task recognition; eye
   movements; head movements; deep learning; virtual reality
ID GAZE PREDICTION
AB Understanding human visual attention in immersive virtual reality (VR) is crucial for many important applications, including gaze prediction, gaze guidance, and gaze-contingent rendering. However, previous works on visual attention analysis typically only explored one specific VR task and paid less attention to the differences between different tasks. Moreover, existing task recognition methods typically focused on 2D viewing conditions and only explored the effectiveness of human eye movements. We first collect eye and head movements of 30 participants performing four tasks, i.e., Free viewing, Visual search, Saliency, and Track, in 15 360-degree VR videos. Using this dataset, we analyze the patterns of human eye and head movements and reveal significant differences across different tasks in terms of fixation duration, saccade amplitude, head rotation velocity, and eye-head coordination. We then propose EHTask- a novel learning-based method that employs eye and head movements to recognize user tasks in VR. We show that our method significantly outperforms the state-of-the-art methods derived from 2D viewing conditions both on our dataset (accuracy of 84.4% versus 62.8%) and on a real-world dataset (61.9% versus 44.1%). As such, our work provides meaningful insights into human visual attention under different VR tasks and guides future work on recognizing user tasks in VR.
C1 [Hu, Zhiming; Li, Sheng; Wang, Guoping] Peking Univ, Sch Comp Sci, Beijing 100871, Peoples R China.
   [Bulling, Andreas] Univ Stuttgart, D-70174 Stuttgart, Germany.
   [Li, Sheng; Wang, Guoping] Peking Univ, Natl Biomed Imaging Ctr, Beijing 100871, Peoples R China.
C3 Peking University; University of Stuttgart; Peking University
RP Li, S (corresponding author), Peking Univ, Sch Comp Sci, Beijing 100871, Peoples R China.; Li, S (corresponding author), Peking Univ, Natl Biomed Imaging Ctr, Beijing 100871, Peoples R China.
EM jimmyhu@pku.edu.cn; andreas.bulling@vis.uni-stuttgart.de;
   lisheng@pku.edu.cn; wgp@pku.edu.cn
RI wang, guoping/KQU-3394-2024
OI Li, Sheng/0000-0002-8901-2184; Hu, Zhiming/0000-0002-5105-9753
FU National Key R&D Program of China [2017YFB1002700]; National Natural
   Science Foundation of China [61632003, 61631001, 62172013]; European
   Research Council (ERC) [801708]
FX This work was supported in part by the National Key R & D Program of
   China under Grant 2017YFB1002700 and in part by the National Natural
   Science Foundation of China under Grants 61632003, 61631001, and
   62172013. Andreas Bulling's work was funded by the European Research
   Council (ERC; grant agreement 801708).
NR 56
TC 17
Z9 17
U1 6
U2 36
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2023
VL 29
IS 4
BP 1992
EP 2004
DI 10.1109/TVCG.2021.3138902
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D9DT5
UT WOS:000971666900007
PM 34962869
DA 2025-03-07
ER

PT J
AU Xu, CQ
   Neuroth, T
   Fujiwara, T
   Liang, RH
   Ma, KL
AF Xu, Chaoqing
   Neuroth, Tyson
   Fujiwara, Takanori
   Liang, Ronghua
   Ma, Kwan-Liu
TI A Predictive Visual Analytics System for Studying Neurodegenerative
   Disease Based on DTI Fiber Tracts
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Diseases; Data visualization; Tensors; Diffusion tensor imaging; Visual
   analytics; Rendering (computer graphics); Feature extraction; Brain
   fiber tracts; neurodegenerative disease; machine learning; predictive
   visual analytics; visualization
ID DIFFUSION-TENSOR; PARKINSONS-DISEASE; WHITE-MATTER; ALZHEIMERS-DISEASE;
   SUBSTANTIA-NIGRA; BRAIN; MRI; CLASSIFICATION; ATROPHY; IMAGES
AB Diffusion tensor imaging (DTI) has been used to study the effects of neurodegenerative diseases on neural pathways, which may lead to more reliable and early diagnosis of these diseases as well as a better understanding of how they affect the brain. We introduce a predictive visual analytics system for studying patient groups based on their labeled DTI fiber tract data and corresponding statistics. The system's machine-learning-augmented interface guides the user through an organized and holistic analysis space, including the statistical feature space, the physical space, and the space of patients over different groups. We use a custom machine learning pipeline to help narrow down this large analysis space and then explore it pragmatically through a range of linked visualizations. We conduct several case studies using DTI and T1-weighted images from the research database of Parkinson's Progression Markers Initiative.
C1 [Xu, Chaoqing; Liang, Ronghua] Zhejiang Univ Technol, Coll Comp Sci, Hangzhou 310023, Zhejiang, Peoples R China.
   [Neuroth, Tyson; Fujiwara, Takanori; Ma, Kwan-Liu] Univ Calif Davis, Davis, CA 95616 USA.
C3 Zhejiang University of Technology; University of California System;
   University of California Davis
RP Liang, RH (corresponding author), Zhejiang Univ Technol, Coll Comp Sci, Hangzhou 310023, Zhejiang, Peoples R China.
EM superclearxu@gmail.com; taneuroth@ucdavis.edu; tfujiwara@ucdavis.edu;
   rhliang@zjut.edu.cn; klma@ucdavis.edu
RI Fujiwara, Takanori/AAY-5045-2020
OI Fujiwara, Takanori/0000-0002-6382-2752; Xu,
   Chaoqing/0000-0003-0955-5611; Ma, Kwan-Liu/0000-0001-8086-0366
FU Michael J. Fox Foundation for Parkinson's Research; AbbVie; Allergan;
   Avid Radiopharmaceuticals; Biogen; BioLegend; Bristol-Myers Squibb;
   Celgene; Covance; GE Healthcare; Genentech; GlaxoSmithKline; Golub
   Capital; Handl Therapeutics; Insitro; Lilly; Lundbeck; Merck; Meso Scale
   Discovery; Pfizer; Piramal; Prevail; Roche; Sanofi Genzyme; Servier;
   Takeda; TEVA; UCB; Verily; Voyager
FX We thank Dr. Pauline Maillard from the Department of Neurology, the
   University of California at Davis, Dr. Xiu-fang Xu from Hangzhou Medical
   College, and Dr. Chao Lin from the Children's Hospital of Zhejiang
   University School of Medicine who provided insight and expertise that
   greatly assisted the research. We would also like to show our grati-tude
   to Dr. Shunyuan Guo and Dr. Gaoping Lin from Zhe-jiang Provincial
   People's Hospital for sharing their pearls of wisdom with us during this
   research. Data used in the prep-aration of this article were obtained
   from the Parkinson's Progression Markers Initiative (PPMI) database
   (www. ppmiinfo.org/data). For up-to-date information on the study, visit
   www.ppmiinfo.org. PPMI-a public-private partnership-is funded by the
   Michael J. Fox Foundation for Parkinson's Research and funding partners,
   including AbbVie, Allergan, Avid Radiopharmaceuticals, Biogen,
   BioLegend, Bristol-Myers Squibb, Celgene, Covance, GE Healthcare,
   Genentech, GlaxoSmithKline, Golub Capital, Handl Therapeutics, Insitro,
   Lilly, Lundbeck, Merck, Meso Scale Discovery, Pfizer, Piramal, Prevail,
   Roche, Roche, Sanofi Genzyme, Servier, Takeda, TEVA, UCB, Verily, and
   Voyager.
NR 94
TC 3
Z9 4
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2023
VL 29
IS 4
BP 2020
EP 2035
DI 10.1109/TVCG.2021.3137174
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA D9DT5
UT WOS:000971666900009
PM 34965212
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Lin, J
   Cai, Y
   Wu, X
   Lu, JW
AF Lin, Jie
   Cai, Yi
   Wu, Xin
   Lu, Jianwei
TI Graph-Based Information Block Detection in Infographic With Gestalt
   Organization Principles
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Feature extraction; Visualization; Data visualization; Semantics;
   Organizations; Layout; Task analysis; Infographic; deep learning;
   graph-based approach; information block detection
ID RECOGNITION; PSYCHOLOGY
AB An infographic is a type of visualization chart that displays pieces of information through information blocks. Existing information block detection work utilizes spatial proximity to group elements into several information blocks. However, prior studies ignore the chromatic and structural features of the infographic, resulting in incorrect omissions when detecting information blocks. To alleviate this kind of error, we use a scene graph to represent an infographic and propose a graph-based information block detection model to group elements based on Gestalt Organization Principles (spatial proximity, chromatic similarity, and structural similarity principle). We also construct a new dataset for information block detection. Quantitative and qualitative experiments show that our model can detect the information blocks in the infographic more effectively compared with the spatial proximity-based method.
C1 [Lin, Jie; Cai, Yi; Wu, Xin; Lu, Jianwei] South China Univ Technol, Sch Software Engn, Guangzhou 510641, Guangdong, Peoples R China.
   [Lin, Jie; Cai, Yi; Wu, Xin; Lu, Jianwei] SouthChina Univ Technol, Key Lab Big Data & Intelligent Robot, Minist Educ, Guangzhou 510641, Guangdong, Peoples R China.
C3 South China University of Technology; South China University of
   Technology
RP Cai, Y (corresponding author), South China Univ Technol, Sch Software Engn, Guangzhou 510641, Guangdong, Peoples R China.; Cai, Y (corresponding author), SouthChina Univ Technol, Key Lab Big Data & Intelligent Robot, Minist Educ, Guangzhou 510641, Guangdong, Peoples R China.
EM se_jielin@mail.scut.edu.cn; ycai@scut.edu.cn; sexinwu@mail.scut.edu.cn;
   jianweilu@mail.scut.edu.cn
FU National Natural Science Foundation of China [62076100, 61802130];
   National Key Research and Development Program of China; Guang-dong
   Natural Science Foundation [2019A1515012152]; Fundamental Research Funds
   for the Central Universities, SCUT [D2201300, D2210010]; Science and
   Technology Programs of Guangzhou [201902010046]; Science and Technology
   Planning Project of Guangdong Province [2020B0101100002]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62076100 and 61802130, National Key Research and
   Development Program of China (Standard knowledge graph for epidemic
   prevention and production recovering intelligent service platform and
   its applications), Guang-dong Natural Science Foundation under Grant
   2019A1515012152, the Fundamental Research Funds for the Central
   Universities, SCUT under Grants D2201300 and D2210010, the Science and
   Technology Programs of Guangzhou under Grant 201902010046, and the
   Science and Technology Planning Project of Guangdong Province under
   Grant 2020B0101100002.
NR 55
TC 4
Z9 4
U1 6
U2 30
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2023
VL 29
IS 3
BP 1705
EP 1718
DI 10.1109/TVCG.2021.3130071
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8N3OW
UT WOS:000925059900008
PM 34813475
DA 2025-03-07
ER

PT J
AU Shin, D
   Jo, J
   Kim, B
   Song, H
   Cho, SH
   Seo, J
AF Shin, DongHwa
   Jo, Jaemin
   Kim, Bohyoung
   Song, Hyunjoo
   Cho, Shin-Hyung
   Seo, Jinwook
TI RCMVis: A Visual Analytics System for Route Choice Modeling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Analytical models; Visual analytics; Roads; Data models; Trajectory;
   Computational modeling; Data visualization; Route choice modeling; urban
   planning; trajectory data; origin-destination; visual analytics
ID VISUALIZATION; PERSPECTIVE; MOVEMENT
AB We present RCMVis, a visual analytics system to support interactive Route Choice Modeling analysis. It aims to model which characteristics of routes, such as distance and the number of traffic lights, affect travelers' route choice behaviors and how much they affect the choice during their trips. Through close collaboration with domain experts, we designed a visual analytics framework for Route Choice Modeling. The framework supports three interactive analysis stages: exploration, modeling, and reasoning. In the exploration stage, we help analysts interactively explore trip data from multiple origin-destination (OD) pairs and choose a subset of data they want to focus on. To this end, we provide coordinated multiple OD views with different foci that allow analysts to inspect, rank, and compare OD pairs in terms of their multidimensional attributes. In the modeling stage, we integrate a $k$k-medoids clustering method and a path-size logit model into our system to enable analysts to model route choice behaviors from trips with support for feature selection, hyperparameter tuning, and model comparison. Finally, in the reasoning stage, we help analysts rationalize and refine the model by selectively inspecting the trips that strongly support the modeling result. For evaluation, we conducted a case study and interviews with domain experts. The domain experts discovered unexpected insights from numerous modeling results, allowing them to explore the hyperparameter space more effectively to gain better results. In addition, they gained OD- and road-level insights into which data mainly supported the modeling result, enabling further discussion of the model.
C1 [Shin, DongHwa; Seo, Jinwook] Seoul Natl Univ, Dept Comp Sci & Engn, Seoul 08826, South Korea.
   [Jo, Jaemin] Sungkyunkwan Univ, Coll Comp & Informat, Suwon 16419, Gyeonggi Do, South Korea.
   [Kim, Bohyoung] Hankuk Univ Foreign Studies, Div Biomed Engn, Seoul 02450, South Korea.
   [Song, Hyunjoo] Soongsil Univ, Sch Comp Sci & Engn, Seoul 06978, South Korea.
   [Cho, Shin-Hyung] Georgia Inst Technol, Sch Civil & Environm Engn, Atlanta, GA 30332 USA.
C3 Seoul National University (SNU); Sungkyunkwan University (SKKU); Hankuk
   University Foreign Studies; Soongsil University; University System of
   Georgia; Georgia Institute of Technology
RP Seo, J (corresponding author), Seoul Natl Univ, Dept Comp Sci & Engn, Seoul 08826, South Korea.; Jo, J (corresponding author), Sungkyunkwan Univ, Coll Comp & Informat, Suwon 16419, Gyeonggi Do, South Korea.
EM dhshin@hcil.snu.ac.kr; jmjo@skku.edu; bkim@hufs.ac.kr; hsong@ssu.ac.kr;
   scho370@gatech.edu; jseo@snu.ac.kr
RI Song, hyunjoo/GWC-1292-2022; Kim, Jae/J-5431-2012; Cho,
   Shin-Hyung/ABH-3915-2020
OI Shin, DongHwa/0000-0001-9460-809X; Seo, Jinwook/0000-0002-7734-822X;
   Cho, Shin-Hyung/0000-0001-6499-1497
FU National Research Foundation of Korea (NRF) - Korea government (MSIT)
   [NRF-2019R1A2C2089062, NRF-2019R1A2C1088900]; Hankuk University of
   Foreign Studies Research Fund
FX This work was supported in part by the National Research Foundation of
   Korea(NRF) grants funded by the Korea government (MSIT) under Grants
   NRF-2019R1A2C2089062 and NRF-2019R1A2C1088900, and in part by the Hankuk
   University of Foreign Studies Research Fund.
NR 43
TC 5
Z9 5
U1 1
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2023
VL 29
IS 3
BP 1799
EP 1817
DI 10.1109/TVCG.2021.3131824
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8N3OW
UT WOS:000925059900014
PM 34851827
OA hybrid
DA 2025-03-07
ER

PT J
AU Hu, BY
   Ye, CY
   Su, JP
   Liu, LG
AF Hu, Bo-Yi
   Ye, Chunyang
   Su, Jian-Ping
   Liu, Ligang
TI Manifold-Constrained Geometric Optimization via Local Parameterizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Geometric optimization; manifold constrains; developable-surface; low
   distortion parameterizations
ID DISCRETE UNIFORMIZATION THEOREM; CUT CONSTRUCTION; SURFACE; QUALITY;
   MAPS
AB Many geometric optimization problems contain manifold constraints that restrict the optimized vertices on some specified manifold surface. The constraints are highly nonlinear and non-convex, therefore existing methods usually suffer from a breach of condition or low optimization quality. In this article, we present a novel divide-and-conquer methodology for manifold-constrained geometric optimization problems. Central to our methodology is to use local parameterizations to decouple the optimization with hard constraints, which transforms nonlinear constraints into linear constraints. We decompose the input mesh into a set of developable or nearly-developable overlapping patches with disc topology, then flatten each patch into the planar domain with very low isometric distortion, optimize vertices with linear constraints and recover the patch. Finally, we project it onto the constrained manifold surface. We demonstrate the applicability and robustness of our methodology through a variety of geometric optimization tasks. Experimental results show that our method performs much better than existing methods.
C1 [Hu, Bo-Yi; Ye, Chunyang; Su, Jian-Ping; Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Liu, LG (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Peoples R China.
EM hbyiyiyi@mail.ustc.edu.cn; yechyang@mail.ustc.edu.cn;
   SJPing@mail.ustc.edu.cn; lgliu@ustc.edu.cn
RI Liu, Ligang/IZQ-5817-2023; su, Jian-Ping/ABC-5407-2021
OI Su, Jian-Ping/0000-0003-3692-6510
FU National Natural Science Foundation of China [62025207]; Zhejiang Lab
   [2019NB0AB03]
FX This work was supported by the National Natural Science Foundation of
   China under Grant 62025207 and Zhejiang Lab under Grant
   2019NB0AB03.(Corresponding author: Ligang Liu.
NR 51
TC 1
Z9 1
U1 0
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2023
VL 29
IS 2
BP 1318
EP 1329
DI 10.1109/TVCG.2021.3112896
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7M2HO
UT WOS:000906475100003
PM 34529566
DA 2025-03-07
ER

PT J
AU Zhou, ZL
   Wang, WY
   Guo, MT
   Wang, Y
   Gotz, D
AF Zhou, Zhilan
   Wang, Wenyuan
   Guo, Mengtian
   Wang, Yue
   Gotz, David
TI A Design Space for Surfacing Content Recommendations in Visual Analytic
   Platforms
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Adaptive Visualization; Recommendation; Literature Survey; Design Space
ID VISUALIZATION; SYSTEMS
AB Recommendation algorithms have been leveraged in various ways within visualization systems to assist users as they perform of a range of information tasks. One common focus for these techniques has been the recommendation of content, rather than visual form, as a means to assist users in the identification of information that is relevant to their task context. A wide variety of techniques have been proposed to address this general problem, with a range of design choices in how these solutions surface relevant information to users. This paper reviews the state-of-the-art in how visualization systems surface recommended content to users during users' visual analysis; introduces a four-dimensional design space for visual content recommendation based on a characterization of prior work; and discusses key observations regarding common patterns and future research opportunities.
C1 [Zhou, Zhilan; Gotz, David] Univ North Carolina Chapel Hill, Dept Comp Sci, Chapel Hill, NC 27599 USA.
   [Wang, Wenyuan; Guo, Mengtian; Wang, Yue] Univ North Carolina Chapel Hill, Sch Informat, Lib Sci, Chapel Hill, NC USA.
   [Wang, Wenyuan; Guo, Mengtian; Wang, Yue] Univ North Carolina Chapel Hill, Lib Sci, Chapel Hill, NC USA.
C3 University of North Carolina; University of North Carolina Chapel Hill;
   University of North Carolina School of Medicine; University of North
   Carolina; University of North Carolina Chapel Hill; University of North
   Carolina School of Medicine; University of North Carolina School of
   Medicine; University of North Carolina; University of North Carolina
   Chapel Hill
RP Zhou, ZL (corresponding author), Univ North Carolina Chapel Hill, Dept Comp Sci, Chapel Hill, NC 27599 USA.
EM zzl@email.unc.edu; vaapad@live.unc.edu; mtguo@email.unc.edu;
   wangyue@unc.edu; gotz@unc.edu
RI ; Zhou, Zhilan/HZK-7461-2023
OI Wang, Yue/0000-0002-0278-2347; Zhou, Zhilan/0000-0003-1236-1287; Gotz,
   David/0000-0002-6424-7374
FU National Science Foundation [1704018]; Direct For Computer & Info Scie &
   Enginr; Div Of Information & Intelligent Systems [1704018] Funding
   Source: National Science Foundation
FX The research reported in this article was supported in part by a grant
   from the National Science Foundation (#1704018).
NR 83
TC 3
Z9 3
U1 3
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2023
VL 29
IS 1
BP 84
EP 94
DI 10.1109/TVCG.2022.3209445
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7F6YZ
UT WOS:000901991800001
PM 36194706
OA Green Submitted
DA 2025-03-07
ER

PT J
AU He, Y
   Liu, YT
   Jin, YH
   Zhang, SH
   Lai, YK
   Hu, SM
AF He, Yu
   Liu, Ying-Tian
   Jin, Yi-Han
   Zhang, Song-Hai
   Lai, Yu-Kun
   Hu, Shi-Min
TI Context-Consistent Generation of Indoor Virtual Environments Based on
   Geometry Constraints
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Solid modeling; Sensors; Layout; Computational modeling; Virtual
   environments; Valves; Upper bound; Virtual reality; obstacle awareness;
   user interaction; scene generation; geometric constraints; contextual
   relation; layout patterns
ID REAL
AB In this article, we propose a system that can automatically generate immersive and interactive virtual reality (VR) scenes by taking real-world geometric constraints into account. Our system can not only help users avoid real-world obstacles in virtual reality experiences, but also provide context-consistent contents to preserve their sense of presence. To do so, our system first identifies the positions and bounding boxes of scene objects as well as a set of interactive planes from 3D scans. Then context-consistent virtual objects that have similar geometric properties to the real ones can be automatically selected and placed into the virtual scene, based on learned object association relations and layout patterns from large amounts of indoor scene configurations. We regard virtual object replacement as a combinatorial optimization problem, considering both geometric and contextual consistency constraints. Quantitative and qualitative results show that our system can generate plausible interactive virtual scenes that highly resemble real environments, and have the ability to keep the sense of presence for users in their VR experiences.
C1 [He, Yu; Liu, Ying-Tian; Zhang, Song-Hai; Hu, Shi-Min] Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
   [Jin, Yi-Han] Tianjin Univ Technol, Dept Comp Sci & Engn, Tianjin 300384, Peoples R China.
   [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF10 3AT, Wales.
C3 Tsinghua University; Tianjin University of Technology; Cardiff
   University
RP Zhang, SH (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing 100084, Peoples R China.
EM hooyeeevan2511@gmail.com; liuyingt20@mails.tsinghua.edu.cn;
   1127703753@qq.com; shz@tsinghua.edu.cn; Yukun.Lai@cs.cardiff.ac.uk;
   shimin@tsinghua.edu.cn
RI He, Yu/ABD-4698-2021; Lai, Yu-Kun/D-2343-2010; Hu,
   Shi-Min/AAW-1952-2020; Yu, He/B-5774-2017
OI Yu, He/0000-0002-0357-681X; Lai, Yukun/0000-0002-2094-5680; Hu,
   Shi-Min/0000-0001-7507-6542
FU National Key Technology RD Program [2017YFB1002604]; National Natural
   Science Foundation of China [61521002, 61772298]; Research Grant of
   Beijing Higher Institution Engineering Research Center; Tsinghua-Tencent
   Joint Laboratory for Internet Innovation Technology
FX The authors would like to thank Prof. Sheng-Yong Chen from the Tianjin
   University of Technology for his valuable comments in the conception of
   this work and the process of experiments which have greatly improved the
   manuscript. This work was supported in part by the National Key
   Technology R&D Program under Grant 2017YFB1002604, in part by the
   National Natural Science Foundation of China under Grants 61521002 and
   61772298, and in part by the Research Grant of Beijing Higher
   Institution Engineering Research Center and Tsinghua-Tencent Joint
   Laboratory for Internet Innovation Technology.
NR 47
TC 6
Z9 10
U1 3
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 3986
EP 3999
DI 10.1109/TVCG.2021.3111729
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400003
PM 34506285
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Liu, JY
   Hui, BY
   Li, K
   Liu, YK
   Lai, YK
   Zhang, YX
   Liu, YB
   Yang, JY
AF Liu, Jingying
   Hui, Binyuan
   Li, Kun
   Liu, Yunke
   Lai, Yu-Kun
   Zhang, Yuxiang
   Liu, Yebin
   Yang, Jingyu
TI Geometry-Guided Dense Perspective Network for Speech-Driven Facial
   Animation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Facial animation; Solid modeling; Faces;
   Geometry; Correlation; Decoding; Speech-driven; 3D facial animation;
   geometry-guided; speaker-independent
AB Realistic speech-driven 3D facial animation is a challenging problem due to the complex relationship between speech and face. In this paper, we propose a deep architecture, called Geometry-guided Dense Perspective Network (GDPnet), to achieve speaker-independent realistic 3D facial animation. The encoder is designed with dense connections to strengthen feature propagation and encourage the re-use of audio features, and the decoder is integrated with an attention mechanism to adaptively recalibrate point-wise feature responses by explicitly modeling interdependencies between different neuron units. We also introduce a non-linear face reconstruction representation as a guidance of latent space to obtain more accurate deformation, which helps solve the geometry-related deformation and is good for generalization across subjects. Huber and HSIC (Hilbert-Schmidt Independence Criterion) constraints are adopted to promote the robustness of our model and to better exploit the non-linear and high-order correlations. Experimental results on the public dataset and real scanned dataset validate the superiority of our proposed GDPnet compared with state-of-the-art model. The code is available for research purposes at http://cic.tju.edu.cn/faculty/likun/projects/GDPnet.
C1 [Liu, Jingying; Hui, Binyuan; Li, Kun; Liu, Yunke] Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
   [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF24 3AA, Wales.
   [Zhang, Yuxiang; Liu, Yebin] Tsinghua Univ, Dept Automat, Beijing 10084, Peoples R China.
   [Yang, Jingyu] Tianjin Univ, Sch Elect & Informat Engn, Tianjin 300072, Peoples R China.
C3 Tianjin University; Cardiff University; Tsinghua University; Tianjin
   University
RP Li, K (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin 300350, Peoples R China.
EM 981132775@qq.com; 787782917@qq.com; huybery@gmail.com; lik@tju.edu.cn;
   Yukunlai@cs.cardiff.ac.uk; yx-z19@mails.tsinghua.edu.cn;
   liuyebin@mail.tsinghua.edu.cn; yjy@tju.edu.cn
RI Lai, Yu-Kun/D-2343-2010; Zhang, Yuxiang/AHA-3178-2022; YANG, JINGYU
   (Gracy)/AAD-3341-2021; Li, Kun/EPZ-3203-2022; Li, Yan/JRW-0176-2023
OI Lai, Yukun/0000-0002-2094-5680
FU National Natural Science Foundation of China [62171317, 62122058,
   61771339]
FX This work was supported by the National Natural Science Foundation of
   China under Grants 62171317, 62122058, and 61771339. The authors would
   like to thank the Associate Editor and anonymous reviews for their help
   in improving this paper. Jingying Liu and Binyuan Hui are equal
   contribution to this work.
NR 43
TC 10
Z9 11
U1 1
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4873
EP 4886
DI 10.1109/TVCG.2021.3107669
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400065
PM 34449390
OA Green Accepted, Green Submitted
DA 2025-03-07
ER

PT J
AU Liu, Z
   Li, YL
   Wang, WN
   Liu, LG
   Chen, RJ
AF Liu, Zheng
   Li, Yanlei
   Wang, Weina
   Liu, Ligang
   Chen, Renjie
TI Mesh Total Generalized Variation for Denoising
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Noise reduction; TV; Faces; Smoothing methods; Optimization; Image
   restoration; Three-dimensional displays; Mesh denoising; total
   generalized variation; augmented Lagrangian method; total variation;
   normal filtering
ID REGULARIZATION; DIFFUSION; ROF
AB Recent studies have shown that the Total Generalized Variation (TGV) is highly effective in preserving sharp features as well as smooth transition variations for image processing tasks. However, currently there is no existing work that is suitable for applying TGV to 3D data, in particular, triangular meshes. In this article, we develop a novel framework for discretizing second-order TGV on triangular meshes. Further, we propose a TGV-based variational method for the denoising of face normal fields on triangular meshes. The TGV regularizer in our method is composed of a first-order term and a second-order term, which are automatically balanced. The first-order term allows our TGV regularizer to locate and preserve sharp features, while the second-order term allows our regularizer to recognize and recover smoothly curved regions. To solve the optimization problem, we introduce an efficient iterative algorithm based on variable-splitting and augmented Lagrangian method. Extensive results and comparisons on synthetic and real scanning data validate that the proposed method outperforms the state-of-the-art visually and numerically.
C1 [Liu, Zheng; Li, Yanlei] China Univ Geosci, Natl Engn Res Ctr Geog Informat Syst, Sch Geog & Informat Engn, Wuhan 430074, Hubei, Peoples R China.
   [Wang, Weina] Hangzhou Dianzi Univ, Dept Math, Hangzhou 310018, Zhejiang, Peoples R China.
   [Liu, Ligang; Chen, Renjie] Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Anhui, Peoples R China.
C3 China University of Geosciences; Hangzhou Dianzi University; Chinese
   Academy of Sciences; University of Science & Technology of China, CAS
RP Chen, RJ (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Anhui, Peoples R China.
EM liu.zheng.jojo@gmail.com; leelele@cug.edu.cn; wnwang@hdu.edu.cn;
   lgliu@ustc.edu.cn; renjiec@ustc.edu.cn
RI Chen, Renjie/AFU-3325-2022; Liu, Ligang/IZQ-5817-2023; chen,
   renjie/I-5995-2016
OI chen, renjie/0000-0001-8395-4392; Liu, Zheng/0000-0001-6713-6680
FU NSF of China [62072422, 12001144, 62025207, 62076227, 61702467];
   National Key R&D Program of China [2020YFC1523102]; NSF of Anhui
   Province, China [2008085MF195]; Youth Science and Technology Foundation
   of Gansu [20JR5RA050]; NSF of Zhejiang Province, China [LQ20A010007];
   Zhejiang Lab [2019NB0AB03]
FX This work was supported in part by the NSF of China under Grants
   62072422, 12001144, 62025207, 62076227, and 61702467, in part by the
   National Key R&D Program of China under Grant 2020YFC1523102, in part by
   the NSF of Anhui Province, China, under Grant 2008085MF195, in part by
   the Youth Science and Technology Foundation of Gansu under Grant
   20JR5RA050, in part by the NSF of Zhejiang Province, China, under Grant
   LQ20A010007, and in part by the Zhejiang Lab under Grant 2019NB0AB03.
NR 49
TC 20
Z9 21
U1 2
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4418
EP 4433
DI 10.1109/TVCG.2021.3088118
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400033
PM 34115587
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Otaran, A
   Farkhatdinov, I
AF Otaran, Ata
   Farkhatdinov, Ildar
TI Haptic Ankle Platform for Interactive Walking in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Legged locomotion; Haptic interfaces; Avatars; Tracking; Foot; Robots;
   Rendering (computer graphics); Locomotion in VR; walking-in-place;
   human-robot interaction; proprioceptive feedback
AB This article presents an impedance type ankle haptic interface for providing users with an immersive navigation experience in virtual reality (VR). The ankle platform, actuated by an electric motor with feedback control, enables the use of foot-tapping gestures to create a walking experience like a real one and to haptically render different types of walking terrains. Experimental studies demonstrated that the interface can be easily used to generate virtual walking and is capable of rendering terrains, such as hard and soft surfaces, and multi-layer complex dynamic terrains. The designed system is a seated-type VR locomotion interface, therefore allowing its user to maintain a stable seated posture to comfortably navigate a virtual scene.
C1 [Otaran, Ata; Farkhatdinov, Ildar] Queen Mary Univ London, Sch Elect Engn & Comp Sci, London, England.
   [Farkhatdinov, Ildar] Imperial Coll Sci Technol & Med, Dept Bioengn, London SW7 2BX, England.
C3 University of London; Queen Mary University London; Imperial College
   London
RP Otaran, A (corresponding author), Queen Mary Univ London, Sch Elect Engn & Comp Sci, London, England.
EM a.otaran@qmul.ac.uk; i.farkhatdinov@qmul.ac.uk
RI Farkhatdinov, Ildar/A-9905-2012
FU UK EPSRC [EP/R02572X/1]; Queen MaryUniversity of London; ISCF
   [EP/R02572X/1] Funding Source: UKRI
FX This work was supported in part by the UK EPSRC under Grant EP/R02572X/1
   and in part by the Ph.D. studentship of Queen MaryUniversity of London.
NR 40
TC 13
Z9 14
U1 2
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 3974
EP 3985
DI 10.1109/TVCG.2021.3111675
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400002
PM 34506284
OA Green Submitted, hybrid
DA 2025-03-07
ER

PT J
AU Li, C
   Kon, ALL
   Shing, HH
AF Li, Chen
   Kon, Angel Lo Lo
   Shing, Horace Ho
TI Use Virtual Reality to Enhance Intercultural Sensitivity: A Randomised
   Parallel Longitudinal Study
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 21st IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 17-22, 2022
CL Singapore, SINGAPORE
SP IEEE
DE Virtual reality; presence; intercultural sensitivity; emotional empathy
ID EMPATHY; COMPETENCE; EXPERIENCE; TECHNOLOGY; BEHAVIOR; PROGRAM; IMPACT
AB Prior studies suggest that emotional empathy is one of the components of intercultural sensitivity - the affective dimension under the concept of intercultural communication competence. Based on existing theories and findings, this paper reports a randomised parallel longitudinal study investigating the use of virtual reality (VR) exposure to enhance intercultural sensitivity. A total of 80 participants (36 females and 44 males) joined the study and were included in the data analysis. The participants were randomly assigned to the VR group, the video group, and the control group. Their intercultural sensitivity was measured three times: one week before the exposure (T-1), right after the exposure (T-2), and three weeks after the exposure (T-3). The results suggested that (1) the intercultural sensitivity of the VR group was significantly enhanced in both within-subject comparisons and between-subject comparisons, (2) there were no significant differences in intercultural sensitivity between the VR group and the video group at T-2, but the VR group retained the enhancement better at T-3, and (3) the sense of presence and emotional empathy well predicted the change in intercultural sensitivity of the VR group. The results, together with the participants' feedback and comments, provide new insights into the practice of using VR for intercultural sensitivity training and encourage future research on exploring the contributing factors of the results.
C1 [Li, Chen] Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
   [Kon, Angel Lo Lo; Shing, Horace Ho] City Univ Hong Kong, Ctr Innovat Applicat Internet & Multimedia Techno, Hong Kong, Peoples R China.
   [Shing, Horace Ho] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.
C3 Hong Kong Polytechnic University; City University of Hong Kong; City
   University of Hong Kong
RP Li, C (corresponding author), Hong Kong Polytech Univ, Dept Comp, Hong Kong, Peoples R China.
EM richard-chen.li@polyu.edu.hk; angelluluk@gmail.com;
   horace.ip@cityu.edu.hk
OI Li, Chen/0000-0002-3782-0737
FU Hong Kong Polytechnic University [P0035264]; Centre for Innovative
   Applications of Internet and Multimedia Technologies (AIMtech Centre),
   City University of Hong Kong
FX This project is supported by The Hong Kong Polytechnic University
   (project no.: P0035264) and the Centre for Innovative Applications of
   Internet and Multimedia Technologies (AIMtech Centre), City University
   of Hong Kong.
NR 70
TC 2
Z9 2
U1 5
U2 23
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2022
VL 28
IS 11
BP 3673
EP 3683
DI 10.1109/TVCG.2022.3203091
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 5M3ZS
UT WOS:000871038200012
PM 36048997
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Yu, K
   Zacharis, K
   Eck, U
   Navab, N
AF Yu, Kevin
   Zacharis, Kostantinos
   Eck, Ulrich
   Navab, Nassir
TI Projective Bisector Mirror (PBM): Concept and Rationale
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 21st IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 17-22, 2022
CL Singapore, SINGAPORE
SP IEEE
DE Mirror Geometry; Multi-camera system; Augmented Reality
ID AR
AB Our world is full of cameras, whether they are installed in the environment or integrated into mobile devices such as mobile phones or head-mounted displays. Displaying external camera views in our egocentric view with a picture-in-picture approach allows us to understand their view; however, it would not allow us to correlate their viewpoint with our perceived reality. We introduce Projective Bisector Mirrors for visualizing a camera view comprehensibly in the egocentric view of an observer with the metaphor of a virtual mirror. Our concept projects the image of a capturing camera onto the bisecting plane between the capture and the observer camera. We present extensive mathematical descriptions of this novel paradigm for multi-view visualization, discuss the effects of tracking errors and provide concrete implementation for multiple exemplary use-cases.
C1 [Yu, Kevin; Eck, Ulrich; Navab, Nassir] Tech Univ Munich, Comp Aided Med Procedures, Munich, Germany.
   [Zacharis, Kostantinos] Tech Univ Munich, Res Grp MITI, Hosp Rechts Isar, Munich, Germany.
C3 Technical University of Munich; Technical University of Munich
RP Yu, K (corresponding author), Tech Univ Munich, Comp Aided Med Procedures, Munich, Germany.
EM kevin.yu@tum.de
NR 60
TC 4
Z9 4
U1 1
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2022
VL 28
IS 11
BP 3694
EP 3704
DI 10.1109/TVCG.2022.3203108
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 5M3ZS
UT WOS:000871038200014
PM 36048998
DA 2025-03-07
ER

PT J
AU Wagner, J
   Stuerzlinger, W
   Nedel, L
AF Wagner, Jorge
   Stuerzlinger, Wolfgang
   Nedel, Luciana
TI The Effect of Exploration Mode and Frame of Reference in Immersive
   Analytics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Navigation; Task analysis; Data visualization; Legged locomotion;
   Three-dimensional displays; Inspection; Affordances; Navigation
   techniques; frame of reference; immersive analytics; space-time cube
ID 3D VISUALIZATIONS; NAVIGATION; MOVEMENT
AB The design space for user interfaces for Immersive Analytics applications is vast. Designers can combine navigation and manipulation to enable data exploration with ego- or exocentric views, have the user operate at different scales, or use different forms of navigation with varying levels of physical movement. This freedom results in a multitude of different viable approaches. Yet, there is no clear understanding of the advantages and disadvantages of each choice. Our goal is to investigate the affordances of several major design choices, to enable both application designers and users to make better decisions. In this article, we assess two main factors, exploration mode and frame of reference, consequently also varying visualization scale and physical movement demand. To isolate each factor, we implemented nine different conditions in a Space-Time Cube visualization use case and asked 36 participants to perform multiple tasks. We analyzed the results in terms of performance and qualitative measures and correlated them with participants' spatial abilities. While egocentric room-scale exploration significantly reduced mental workload, exocentric exploration improved performance in some tasks. Combining navigation and manipulation made tasks easier by reducing workload, temporal demand, and physical effort.
C1 [Wagner, Jorge; Nedel, Luciana] Univ Fed Rio Grande do Sul, BR-90040 Porto Alegre, RS, Brazil.
   [Stuerzlinger, Wolfgang] Simon Fraser Univ, Burnaby, BC V5A 1S6, Canada.
C3 Universidade Federal do Rio Grande do Sul; Simon Fraser University
RP Wagner, J (corresponding author), Univ Fed Rio Grande do Sul, BR-90040 Porto Alegre, RS, Brazil.
EM jawfilho@inf.ufrgs.br; w.s@sfu.ca; nedel@inf.ufrgs.br
RI Wagner, Jorge/AAV-7597-2020; Nedel, Luciana/G-3506-2012
OI Wagner Filho, Jorge Alberto/0000-0002-7016-3142; Stuerzlinger,
   Wolfgang/0000-0002-7110-5024; Nedel, Luciana/0000-0002-2390-1392
FU CNPq-Brazil; Global Affairs Canada; NSERC; Coordenacao de
   Aperfeicoamento de Pessoal de Ni'vel Superior -Brasil (CAPES) [001]
FX The authors thank the study participants for their feedback and the
   reviewers for their insightful comments. They acknowledge financial
   support from CNPq-Brazil, Global Affairs Canada, and NSERC. This study
   was financed in part by the Coordenacao de Aperfeicoamento de Pessoal de
   Ni ' vel Superior -Brasil (CAPES) -Finance Code 001.
NR 59
TC 14
Z9 15
U1 1
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2022
VL 28
IS 9
BP 3252
EP 3264
DI 10.1109/TVCG.2021.3060666
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3K0HP
UT WOS:000833767700014
PM 33606632
DA 2025-03-07
ER

PT J
AU Heimerl, F
   Kralj, C
   Möller, T
   Gleicher, M
AF Heimerl, Florian
   Kralj, Christoph
   Moeller, Torsten
   Gleicher, Michael
TI <i>embComp</i>: Visual Interactive Comparison of Vector Embeddings
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Measurement; Visualization; Dimensionality reduction; Task analysis; Two
   dimensional displays; Stress; Object recognition; Visual analytics;
   visual comparison; machine learning; vector embeddings
ID DIMENSIONALITY REDUCTION; EXPLORATION; VISUALIZATION
AB This article introduces embComp, a novel approach for comparing two embeddings that capture the similarity between objects, such as word and document embeddings. We survey scenarios where comparing these embedding spaces is useful. From those scenarios, we derive common tasks, introduce visual analysis methods that support these tasks, and combine them into a comprehensive system. One of embComp's central features are overview visualizations that are based on metrics for measuring differences in the local structure around objects. Summarizing these local metrics over the embeddings provides global overviews of similarities and differences. Detail views allow comparison of the local structure around selected objects and relating this local information to the global views. Integrating and connecting all of these components, embComp supports a range of analysis workflows that help understand similarities and differences between embedding spaces. We assess our approach by applying it in several use cases, including understanding corpora differences via word vector embeddings, and understanding algorithmic differences in generating embeddings.
C1 [Heimerl, Florian; Gleicher, Michael] Univ Wisconsin Madison UW Madison, Dept Comp Sci, Madison, WI 53718 USA.
   [Kralj, Christoph; Moeller, Torsten] Univ Vienna, Fac Comp Sci & Data Sci, Uni Vienna Sensengasse 6, A-1090 Vienna, Austria.
C3 University of Vienna
RP Heimerl, F (corresponding author), Univ Wisconsin Madison UW Madison, Dept Comp Sci, Madison, WI 53718 USA.
EM heimerl@cs.wisc.edu; christoph.kralj@univie.ac.at;
   torsten.moeller@univie.ac.at; gleicher@cs.wisc.edu
RI Heimerl, Florian/AAY-9917-2020
OI Gleicher, Michael/0000-0003-3295-4071; Moller,
   Torsten/0000-0003-1192-0710
FU US National Science Foundation [1841349]; DARPA Award [FA8750-17-20107];
   Direct For Computer & Info Scie & Enginr; Div Of Information &
   Intelligent Systems [1841349] Funding Source: National Science
   Foundation
FX This work was funded in part by US National Science Foundation Award
   1841349 and DARPA Award FA8750-17-20107. The authors would like to thank
   our domain collaborators including Michael Witmore, Jonathan Hope, and
   Gary Lupyan.
NR 63
TC 19
Z9 19
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2022
VL 28
IS 8
BP 2953
EP 2969
DI 10.1109/TVCG.2020.3045918
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 2P6BI
UT WOS:000819823600011
PM 33347410
OA Bronze, Green Submitted
DA 2025-03-07
ER

PT J
AU Dewez, D
   Hoyet, L
   Lécuyer, A
   Argelaguet, F
AF Dewez, Diane
   Hoyet, Ludovic
   Lecuyer, Anatole
   Argelaguet, Ferran
TI Do You Need Another Hand? Investigating Dual Body Representations During
   Anisomorphic 3D Manipulation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 12-16, 2022
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, ChristchurchNZ, Virbela, Univ Canterbury, Immers Learning Res Network, Qualcomm, HIT Lab NZ, Appl Immers Gaming Initiat
DE Avatars; Task analysis; Distortion; Arms; Visualization;
   Three-dimensional displays; Rubber; Avatar; Sense of embodiment;
   Interaction; Virtual reality
ID SELF-AVATAR; EMBODIMENT; SENSE; IMPACT; ARM
AB In virtual reality, several manipulation techniques distort users' motions, for example to reach remote objects or increase precision. These techniques can become problematic when used with avatars, as they create a mismatch between the real performed action and the corresponding displayed action, which can negatively impact the sense of embodiment. In this paper, we propose to use a dual representation during anisomorphic interaction. A co-located representation serves as a spatial reference and reproduces the exact users' motion, while an interactive representation is used for distorted interaction. We conducted two experiments, investigating the use of dual representations with amplified motion (with the Go-Go technique) and decreased motion (with the PRISM technique). Two visual appearances for the interactive representation and the co-located one were explored. This exploratory study investigating dual representations in this context showed that people globally preferred having a single representation, but opinions diverged for the Go-Go technique. Also, we could not find significant differences in terms of performance. While interacting seemed more important than showing exact movements for agency during out-of-reach manipulation, people felt more in control of the realistic arm during close manipulation.
C1 [Dewez, Diane; Hoyet, Ludovic; Lecuyer, Anatole; Argelaguet, Ferran] Univ Rennes, IRISA, CNRS, INRIA, Irisa, France.
C3 Inria; Centre National de la Recherche Scientifique (CNRS)
RP Argelaguet, F (corresponding author), Univ Rennes, IRISA, CNRS, INRIA, Irisa, France.
EM diane.dewez@inria.fr; ludovic.hoyet@inria.fr; anatole.lecuyer@inria.fr;
   ferran.argelaguet@inria.fr
RI Hoyet, Ludovic/IWU-9100-2023
OI Hoyet, Ludovic/0000-0002-7373-6049
FU Inria Research Challenge Avatar
FX We wish to thank all the participants who took part in these two
   experiments. This work was sponsored by the Inria Research Challenge
   Avatar.
NR 47
TC 8
Z9 8
U1 0
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY 1
PY 2022
VL 28
IS 5
BP 2047
EP 2057
DI 10.1109/TVCG.2022.3150501
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 1R1AK
UT WOS:000803110400021
PM 35167468
OA Green Published
DA 2025-03-07
ER

PT J
AU Vasiou, E
   Shkurko, K
   Brunvand, E
   Yuksel, C
AF Vasiou, Elena
   Shkurko, Konstantin
   Brunvand, Erik
   Yuksel, Cem
TI Mach-RT: A Many Chip Architecture for High Performance Ray Tracing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Ray tracing; Acceleration; Instruction sets; System-on-chip; Memory
   management; Hardware; Raytracing hardware; graphics accelerators
ID ENERGY
AB Data movement, particularly access to the main memory, has been the bottleneck of most computing problems. Ray tracing is no exception. We propose an unconventional solution that combines a ray ordering scheme that minimizes access to the scene data with a large on-chip buffer acting as near-compute storage that is spread over multiple chips. We demonstrate the effectiveness of our approach by introducing Mach-RT (Many chip - Ray Tracing), a new hardware architecture for accelerating ray tracing. Extending the concept of dual streaming, we optimize the main memory accesses to a level that allows the same memory system to service multiple processor chips at the same time. While a multiple chip solution might seem to imply increased energy consumption as well, because of the reduced memory traffic we are able to demonstrate, performance increases while maintaining reasonable energy usage compared to academic and commercial architectures. This article extends our previous work E. Vasiou, K. Shkurko, E. Brunvand, and C. Yuksel, "Mach-RT: A many chip architecture for high-performance ray tracing," in Proc. High-Perform. Graph. Conf., 2019 with design space exploration of the L3 cache size, more detailed evaluation of energy and memory performance, a discussion of energy delay product, and a brief exploration of boards with 16 chips. We also introduce new treelet enqueueing logic for the predictive scheduler.
C1 [Vasiou, Elena; Shkurko, Konstantin; Brunvand, Erik; Yuksel, Cem] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
C3 Utah System of Higher Education; University of Utah
RP Vasiou, E (corresponding author), Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.
EM elvasiou@cs.utah.edu; kshkurko@cs.utah.edu; elb@cs.utah.edu;
   cem@cemyuksel.com
FU US National Science Foundation [1409129]; Direct For Computer & Info
   Scie & Enginr; Division Of Computer and Network Systems [1409129]
   Funding Source: National Science Foundation
FX This material is based upon work supported by the US National Science
   Foundation under Grant no. 1409129. Scene data: Fairy Forest: U. Utah,
   Crytek Sponza: F. Meinl at Crytek and M. Dabrovic, Dragon: Stanford CG
   Lab., Vegetation: S. Laine, and San Miguel: G. Leal Laguno.
NR 72
TC 0
Z9 0
U1 1
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2022
VL 28
IS 3
BP 1585
EP 1596
DI 10.1109/TVCG.2020.3021048
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP1EJ
UT WOS:000748371200010
PM 32870795
OA Bronze
DA 2025-03-07
ER

PT J
AU Bressa, N
   Korsgaard, H
   Tabard, A
   Houben, S
   Vermeulen, J
AF Bressa, Nathalie
   Korsgaard, Henrik
   Tabard, Aurelien
   Houben, Steven
   Vermeulen, Jo
TI What's the Situation with Situated Visualization? A Survey and
   Perspectives on Situatedness
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Human computer interaction; Encoding; Visual
   analytics; Terminology; Keyword search; Augmented reality; Situated
   visualization; literature survey; situatedness
ID AMBIENT DISPLAYS; INFORMATION; CONTEXT; SPACE
AB Situated visualization is an emerging concept within visualization, in which data is visualized in situ, where it is relevant to people. The concept has gained interest from multiple research communities, including visualization, human-computer interaction (HCI) and augmented reality. This has led to a range of explorations and applications of the concept, however, this early work has focused on the operational aspect of situatedness leading to inconsistent adoption of the concept and terminology. First, we contribute a literature survey in which we analyze 44 papers that explicitly use the term "situated visualization" to provide an overview of the research area, how it defines situated visualization, common application areas and technology used, as well as type of data and type of visualizations. Our survey shows that research on situated visualization has focused on technology-centric approaches that foreground a spatial understanding of situatedness. Secondly, we contribute five perspectives on situatedness (space, time, place, activity, and community) that together expand on the prevalent notion of situatedness in the corpus. We draw from six case studies and prior theoretical developments in HCI. Each perspective develops a generative way of looking at and working with situatedness in design and research. We outline future directions, including considering technology, material and aesthetics, leveraging the perspectives for design, and methods for stronger engagement with target audiences. We conclude with opportunities to consolidate situated visualization research.
C1 [Bressa, Nathalie; Korsgaard, Henrik; Vermeulen, Jo] Aarhus Univ, Aarhus, Denmark.
   [Tabard, Aurelien] Univ Claude Bernard Lyon 1, LIRIS, CNRS, UMR5205, F-69621 Lyon, France.
   [Houben, Steven] Eindhoven Univ Technol, Eindhoven, Netherlands.
   [Vermeulen, Jo] Autodesk Res, Toronto, ON, Canada.
C3 Aarhus University; Universite Claude Bernard Lyon 1; Institut National
   des Sciences Appliquees de Lyon - INSA Lyon; Centre National de la
   Recherche Scientifique (CNRS); Eindhoven University of Technology;
   Autodesk, Inc.
RP Bressa, N (corresponding author), Aarhus Univ, Aarhus, Denmark.
EM nathalie.bressa@cc.au.dk; korsgaard@cs.au.dk;
   aurelien.tabard@univ-lyon1.fr; s.houben@tue.nl;
   jo.vermeulen@autodesk.com
OI Korsgaard, Henrik/0000-0003-0584-6059
NR 108
TC 48
Z9 51
U1 1
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 107
EP 117
DI 10.1109/TVCG.2021.3114835
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000027
PM 34587065
OA Green Published
DA 2025-03-07
ER

PT J
AU Pu, JS
   Shao, H
   Gao, BY
   Zhu, ZG
   Zhu, YL
   Rao, YB
   Xiang, Y
AF Pu, Jiansu
   Shao, Hui
   Gao, Boyang
   Zhu, Zhengguo
   Zhu, Yanlin
   Rao, Yunbo
   Xiang, Yong
TI matExplorer: Visual Exploration on Predicting Ionic Conductivity for
   Solid-state Electrolytes
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Electrolytes; Solids; Liquids; Conductivity; Batteries; Lithium; Thermal
   stability; Interactive visualization; machine learning; materials
   discovery; ionic conductivity; high-dimensional data; solid-state
   electrolytes
ID OF-THE-ART; ANALYTICS SYSTEM; DISCOVERY; PROGRESS
AB Lithium ion batteries (LIBs) are widely used as important energy sources for mobile phones, electric vehicles, and drones. Experts have attempted to replace liquid electrolytes with solid electrolytes that have wider electrochemical window and higher stability due to the potential safety risks, such as electrolyte leakage, flammable solvents, poor thermal stability, and many side reactions caused by liquid electrolytes. However, finding suitable alternative materials using traditional approaches is very difficult due to the incredibly high cost in searching. Machine learning (ML)-based methods are currently introduced and used for material prediction. However, learning tools designed for domain experts to conduct intuitive performance comparison and analysis of ML models are rare. In this case, we propose an interactive visualization system for experts to select suitable ML models and understand and explore the predication results comprehensively. Our system uses a multifaceted visualization scheme designed to support analysis from various perspectives, such as feature distribution, data similarity, model performance, and result presentation. Case studies with actual lab experiments have been conducted by the experts, and the final results confirmed the effectiveness and helpfulness of our system.
C1 [Pu, Jiansu; Shao, Hui; Gao, Boyang; Zhu, Zhengguo] Univ Elect Sci & Technol, Sch Comp Sci & Engn, VisBig Lab, Chengdu, Peoples R China.
   [Zhu, Yanlin] Shenzhen Clean Energy Res Inst, Shenzhen, Peoples R China.
C3 University of Electronic Science & Technology of China
RP Pu, JS (corresponding author), Univ Elect Sci & Technol, Sch Comp Sci & Engn, VisBig Lab, Chengdu, Peoples R China.
EM jiansu.pu@uestc.edu.cn; zhuyanlin@uceri.com
RI Zhu, Yanlin/KEH-0907-2024
OI Zhu, Yanlin/0009-0008-2037-5631
FU National Natural Science Foundation of China [61872066, U19A2078];
   Science and Technology project of Sichuan [2020YFG0056]; science and
   technology project of Sichuan [2019YFG0504, 2020YFG0459, 2021YFG0314]
FX We would like to thank anonymous reviewers for their valuable comments
   and Prof. Zongkai YAN for participating this project as domain
   experts.This work was funded by the National Natural Science Foundation
   of China (Grant Nos. 61872066, and U19A2078), and the Science and
   Technology project of Sichuan(No. 2020YFG0056). This project is also
   partially by the science and technology project of Sichuan
   (No.2019YFG0504, 2020YFG0459, 2021YFG0314).
NR 47
TC 2
Z9 2
U1 9
U2 59
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 65
EP 75
DI 10.1109/TVCG.2021.3114812
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000023
PM 34587048
DA 2025-03-07
ER

PT J
AU Wang, YQ
   Chen, L
   Jo, JM
   Wang, YH
AF Wang, Yinqiao
   Chen, Lu
   Jo, Jaemin
   Wang, Yunhai
TI Joint <i>t</i>-SNE for Comparable Projections of Multiple
   High-Dimensional Datasets
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Task analysis; Optimization; Time measurement; Position
   measurement; Gain measurement; Distortion; High-dimensional data;
   projection; embedding; t-stochastic neighbor embedding
ID GRAPH; LAYOUT
AB We present Joint t-Stochastic Neighbor Embedding (Joint t-SNE), a technique to generate comparable projections of multiple high-dimensional datasets. Although t-SNE has been widely employed to visualize high-dimensional datasets from various domains, it is limited to projecting a single dataset. When a series of high-dimensional datasets, such as datasets changing over time, is projected independently using t-SNE, misaligned layouts are obtained. Even items with identical features across datasets are projected to different locations, making the technique unsuitable for comparison tasks. To tackle this problem, we introduce edge similarity, which captures the similarities between two adjacent time frames based on the Graphlet Frequency Distribution (GFD). We then integrate a novel loss term into the t-SNE loss function, which we call vector constraints, to preserve the vectors between projected points across the projections, allowing these points to serve as visual landmarks for direct comparisons between projections. Using synthetic datasets whose ground-truth structures are known, we show that Joint t-SNE outperforms existing techniques, including Dynamic t-SNE, in terms of local coherence error, Kullback-Leibler divergence, and neighborhood preservation. We also showcase a real-world use case to visualize and compare the activation of different layers of a neural network.
C1 [Wang, Yinqiao; Chen, Lu; Wang, Yunhai] Shandong Univ, Jinan, Shandong, Peoples R China.
   [Jo, Jaemin] Sungkyunkwan Univ, Seoul, South Korea.
C3 Shandong University; Sungkyunkwan University (SKKU)
RP Wang, YQ (corresponding author), Shandong Univ, Jinan, Shandong, Peoples R China.; Jo, JM (corresponding author), Sungkyunkwan Univ, Seoul, South Korea.
EM Infamywong@gmail.com; chenlu.scien@gmail.com; jmjo@skku.edu;
   cloudseawang@gmail.com
OI Chen, Lu/0000-0002-2628-3876
FU NSFC [61772315, 61861136012]; Open Project Program of State Key
   Laboratory of Virtual Reality Technology and Systems, Beihang University
   [VRLAB2020C08]; CAS grant [GJHZ1862]
FX This work is supported by the grants of the NSFC (61772315,
   61861136012), the Open Project Program of State Key Laboratory of
   Virtual Reality Technology and Systems, Beihang University
   (No.VRLAB2020C08), and the CAS grant (GJHZ1862).
NR 44
TC 14
Z9 15
U1 3
U2 32
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 623
EP 632
DI 10.1109/TVCG.2021.3114765
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000070
PM 34587021
DA 2025-03-07
ER

PT J
AU Hetzel, L
   Dudley, J
   Feit, AM
   Kristensson, PO
AF Hetzel, Lorenz
   Dudley, John
   Feit, Anna Maria
   Kristensson, Per Ola
TI Complex Interaction as Emergent Behaviour: Simulating Mid-Air Virtual
   Keyboard Typing using Reinforcement Learning
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 20th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 04-08, 2021
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, IEEE VGTC, ACM SIGARCH, Qualcomm, Hevolus Innovat, Hakuhodo DY Holdings, Masmec, Intuitive
DE Keyboards; Biological system modeling; Task analysis; Reinforcement
   learning; Computational modeling; Solid modeling; Biomechanics;
   Reinforcement learning; virtual reality; user model
ID TEXT ENTRY; MUSCLE; NOISE
AB Accurately modelling user behaviour has the potential to significantly improve the quality of human-computer interaction. Traditionally, these models are carefully hand-crafted to approximate specific aspects of well-documented user behaviour. This limits their availability in virtual and augmented reality where user behaviour is often not yet well understood. Recent efforts have demonstrated that reinforcement learning can approximate human behaviour during simple goal-oriented reaching tasks. We build on these efforts and demonstrate that reinforcement learning can also approximate user behaviour in a complex mid-air interaction task: typing on a virtual keyboard. We present the first reinforcement learning-based user model for mid-air and surface-aligned typing on a virtual keyboard. Our model is shown to replicate high-level human typing behaviour. We demonstrate that this approach may be used to augment or replace human testing during the validation and development of virtual keyboards.
C1 [Hetzel, Lorenz; Dudley, John; Kristensson, Per Ola] Univ Cambridge, Cambridge, England.
   [Hetzel, Lorenz] Swiss Fed Inst Technol, Zurich, Switzerland.
   [Feit, Anna Maria] Saarland Univ, Saarbrucken, Germany.
C3 University of Cambridge; Swiss Federal Institutes of Technology Domain;
   ETH Zurich; Saarland University
RP Hetzel, L (corresponding author), Univ Cambridge, Cambridge, England.; Hetzel, L (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.
EM hetzell@ethz.ch; jjd50@cam.ac.uk; feit@cs.uni-saarland.de;
   pok21@cam.ac.uk
RI Feit, Anna/AAB-5510-2021
OI Feit, Anna Maria/0000-0003-4168-6099
FU EPSRC [EP/S027432/1]; EPSRC [EP/S027432/1] Funding Source: UKRI
FX This work was carried out when Lorenz Hetzel was a visiting student at
   the University of Cambridge. The authors would like to thank Otmar
   Hilliges for his advice and support. John Dudley and Per Ola Kristensson
   were supported by EPSRC (grant EP/S027432/1).
NR 55
TC 8
Z9 8
U1 3
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2021
VL 27
IS 11
BP 4140
EP 4149
DI 10.1109/TVCG.2021.3106494
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA WN2ZT
UT WOS:000711642700008
PM 34449380
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kimura, S
   Iwai, D
   Punpongsanon, P
   Sato, K
AF Kimura, Sorashi
   Iwai, Daisuke
   Punpongsanon, Parinya
   Sato, Kosuke
TI Multifocal Stereoscopic Projection Mapping
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 20th IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 04-08, 2021
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, IEEE VGTC, ACM SIGARCH, Qualcomm, Hevolus Innovat, Hakuhodo DY Holdings, Masmec, Intuitive
DE Stereo image processing; Three-dimensional displays; Optical imaging;
   Adaptive optics; Glass; Lenses; Observers; Stereoscopic projection
   mapping; multifocal display; vergence-accommodation conflict
ID AUGMENTED REALITY; EYEGLASSES; SHAPE; LENS
AB Stereoscopic projection mapping (PM) allows a user to see a three-dimensional (3D) computer-generated (CG) object floating over physical surfaces of arbitrary shapes around us using projected imagery. However, the current stereoscopic PM technology only satisfies binocular cues and is not capable of providing correct focus cues, which causes a vergence-accommodation conflict (VAC). Therefore, we propose a multifocal approach to mitigate VAC in stereoscopic PM. Our primary technical contribution is to attach electrically focus-tunable lenses (ETLs) to active shutter glasses to control both vergence and accommodation. Specifically, we apply fast and periodical focal sweeps to the ETLs, which causes the "virtual image" (as an optical term) of a scene observed through the ETLs to move back and forth during each sweep period. A 3D CG object is projected from a synchronized high-speed projector only when the virtual image of the projected imagery is located at a desired distance. This provides an observer with the correct focus cues required. In this study, we solve three technical issues that are unique to stereoscopic PM: (1) The 3D CG object is displayed on non-planar and even moving surfaces; (2) the physical surfaces need to be shown without the focus modulation; (3) the shutter glasses additionally need to be synchronized with the ETLs and the projector. We also develop a novel compensation technique to deal with the "lens breathing" artifact that varies the retinal size of the virtual image through focal length modulation. Further, using a proof-of-concept prototype, we demonstrate that our technique can present the virtual image of a target 3D CG object at the correct depth. Finally, we validate the advantage provided by our technique by comparing it with conventional stereoscopic PM using a user study on a depth-matching task.
C1 [Kimura, Sorashi; Iwai, Daisuke; Punpongsanon, Parinya; Sato, Kosuke] Osaka Univ, Grad Sch Engn Sci, Suita, Osaka, Japan.
   [Iwai, Daisuke] Japan Sci & Technol Agcy, PRESTO, Kawaguchi, Saitama, Japan.
C3 Osaka University; Japan Science & Technology Agency (JST)
RP Kimura, S (corresponding author), Osaka Univ, Grad Sch Engn Sci, Suita, Osaka, Japan.
RI Iwai, Daisuke/R-8174-2019; PUNPONGSANON, PARINYA/B-4884-2013
OI Iwai, Daisuke/0000-0002-3493-5635
FU JSPS KAKENHI, Japan [JP18K19817, JP20H05958]; JST, PRESTO, Japan
   [JPMJPR19J2]; Grants-in-Aid for Scientific Research [20H05958] Funding
   Source: KAKEN
FX This work was supported by JSPS KAKENHI Grant Numbers JP18K19817 and
   JP20H05958 and JST, PRESTO Grant Number JPMJPR19J2, Japan.
NR 78
TC 2
Z9 2
U1 2
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2021
VL 27
IS 11
BP 4256
EP 4266
DI 10.1109/TVCG.2021.3106486
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA WN2ZT
UT WOS:000711642700019
PM 34449374
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Liu, DM
   Xiong, C
   Liu, XP
AF Liu, Daoming
   Xiong, Chi
   Liu, Xiaopei
TI Vectorizing Quantum Turbulence Vortex-Core Lines for Real-Time
   Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Mathematical model; Visualization; Real-time systems; Data
   visualization; Quantum mechanics; Quantum computing; Quantum turbulence;
   vortex-core line vectorization; real-time visualization; graph
   representation
ID LIQUID-HELIUM; VORTICES; IDENTIFICATION
AB Vectorizing vortex-core lines is crucial for high-quality visualization and analysis of turbulence. While several techniques exist in the literature, they can only be applied to classical fluids. As quantum fluids with turbulence are gaining attention in physics, extracting and visualizing vortex-core lines for quantum fluids is increasingly desirable. In this article, we develop an efficient vortex-core line vectorization method for quantum fluids enabling real-time visualization of high-resolution quantum turbulence structure. From a dataset obtained through simulation, our technique first identifies vortex nodes based on the circulation field. To vectorize the vortex-core lines interpolating these vortex nodes, we propose a novel graph-based data structure, with iterative graph reduction and density-guided local optimization, to locate sub-grid-scale vortex-core line samples more precisely, which are then vectorized by continuous curves. This vortex-core representation naturally captures complex topology, such as branching during reconnection. Our vectorization approach reduces memory consumption by orders of magnitude, enabling real-time visualization performance. Different types of interactive visualizations are demonstrated to show the effectiveness of our technique, which could help further research on quantum turbulence.
C1 [Liu, Daoming; Liu, Xiaopei] ShanghaiTech Univ, Shanghai Engn Res Ctr Intelligent Vis & Imaging, Sch Informat Sci & Technol, Pudong 201210, Peoples R China.
   [Xiong, Chi] Nanyang Technol Univ, Inst Adv Studies, Nanyang Ave, Singapore 639798, Singapore.
   [Xiong, Chi] Nanyang Technol Univ, Sch Phys & Math Sci, Nanyang Ave, Singapore 639798, Singapore.
C3 ShanghaiTech University; Nanyang Technological University; Nanyang
   Technological University
RP Liu, XP (corresponding author), ShanghaiTech Univ, Shanghai Engn Res Ctr Intelligent Vis & Imaging, Sch Informat Sci & Technol, Pudong 201210, Peoples R China.
EM liudm@shanghaitech.edu.cn; xiongchi@ntu.edu.sg;
   liuxp@shanghaitech.edu.cn
RI Liu, Daoming/KXR-2662-2024; Xiong, Chi/AAF-9134-2021
OI Liu, Daoming/0000-0002-2091-7081; Xiong, Chi/0000-0002-6230-6334
FU ShanghaiTech University; Young Scientists Fund of the National Natural
   Science Foundation of China [61502305]
FX This work was fully supported by the startup funding of ShanghaiTech
   University, and partly supported by the Young Scientists Fund of the
   National Natural Science Foundation of China (Grant No. 61502305). The
   authors would also like to thank Professor Mathieu Desbrun from Caltech
   to help proofread the article.
NR 62
TC 2
Z9 2
U1 5
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2021
VL 27
IS 9
BP 3794
EP 3807
DI 10.1109/TVCG.2020.2981460
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA TS3CP
UT WOS:000679532000015
PM 32191891
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Feng, XL
   Bao, ZY
   Wei, S
AF Feng, Xianglong
   Bao, Zeyang
   Wei, Sheng
TI LiveObj: Object Semantics-based Viewport Prediction for Live Mobile
   Virtual Reality Streaming
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Streaming media; Bandwidth; Real-time systems; Semantics; Trajectory;
   Sports; Resists
ID TRACKING
AB Virtual reality (VR) video streaming (a.k.a., 360-degree video streaming) has been gaining popularity recently as a new form of multimedia providing the users with immersive viewing experience. However, the high volume of data for the 360-degree video frames creates significant bandwidth challenges. Research efforts have been made to reduce the bandwidth consumption by predicting and selectively streaming the user's viewports. However, the existing approaches require historical user or video data and cannot be applied to live streaming, the most attractive VR streaming scenario. We develop a live viewport prediction mechanism, namely LiveObj, by detecting the objects in the video based on their semantics. The detected objects are then tracked to infer the user's viewport in real time by employing a reinforcement learning algorithm. Our evaluations based on 48 users watching 10 VR videos demonstrate high prediction accuracy and significant bandwidth savings obtained by LiveObj. Also, LiveObj achieves real-time performance with low processing delays, meeting the requirement of live VR streaming.
C1 [Feng, Xianglong; Bao, Zeyang; Wei, Sheng] Rutgers State Univ, New Brunswick, NJ 02115 USA.
C3 Rutgers University System; Rutgers University New Brunswick
RP Feng, XL (corresponding author), Rutgers State Univ, New Brunswick, NJ 02115 USA.
EM xianglong.feng@rutgers.edu; zb95@scarletmail.rutgers.edu;
   sheng.wei@rutgers.edu
FU National Science Foundation [CNS-1910085]
FX We appreciate the constructive review comments provided by the anonymous
   reviewers. This work was partially supported by the National Science
   Foundation under award CNS-1910085.
NR 33
TC 18
Z9 20
U1 5
U2 24
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2021
VL 27
IS 5
BP 2736
EP 2745
DI 10.1109/TVCG.2021.3067686
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA SR5YU
UT WOS:000661120200022
PM 33793401
DA 2025-03-07
ER

PT J
AU Zhao, JH
   Liu, X
   Guo, C
   Qian, ZC
   Chen, YV
AF Zhao, Junhan
   Liu, Xiang
   Guo, Chen
   Qian, Zhenyu Cheryl
   Chen, Yingjie Victor
TI Phoenixmap: An Abstract Approach to Visualize 2D Spatial Distributions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Graphical models; Distribution functions; Heating
   systems; Image color analysis; Visualization; Geospatial analysis; Data
   visualization; visualization; algorithms; geospastial analysis
ID MAPS; ANALYTICS
AB The multidimensional nature of spatial data poses a challenge for visualization. In this paper, we introduce Phoenixmap, a simple abstract visualization method to address the issue of visualizing multiple spatial distributions at once. The Phoenixmap approach starts by identifying the enclosed outline of the point collection, then assigns different widths to outline segments according to the segments' corresponding inside regions. Thus, one 2D distribution is represented as an outline with varied thicknesses. Phoenixmap is capable of overlaying multiple outlines and comparing them across categories of objects in a 2D space. We chose heatmap as a benchmark spatial visualization method and conducted user studies to compare performances among Phoenixmap, heatmap, and dot distribution map. Based on the analysis and participant feedback, we demonstrate that Phoenixmap 1) allows users to perceive and compare spatial distribution data efficiently; 2) frees up graphics space with a concise form that can provide visualization design possibilities like overlapping; and 3) provides a good quantitative perceptual estimating capability given the proper legends. Finally, we discuss several possible applications of Phoenixmap and present one visualization of multiple species of birds' active regions in a nature preserve.
C1 [Zhao, Junhan; Chen, Yingjie Victor] Purdue Univ, Dept Comp Graph Technol, W Lafayette, IN 47907 USA.
   [Liu, Xiang] Purdue Univ, Dept Comp & Informat Technol, W Lafayette, IN 47907 USA.
   [Guo, Chen] James Madison Univ, Sch Media Arts & Design, Harrisonburg, VA 22807 USA.
   [Qian, Zhenyu Cheryl] Purdue Univ, Dept Art & Design, W Lafayette, IN 47907 USA.
C3 Purdue University System; Purdue University; Purdue University System;
   Purdue University; James Madison University; Purdue University System;
   Purdue University
RP Chen, YV (corresponding author), Purdue Univ, Dept Comp Graph Technol, W Lafayette, IN 47907 USA.
EM zhao835@purdue.edu; xiang35@purdue.edu; guo4cx@jmu.edu;
   qianz@purdue.edu; victorchen@purdue.edu
RI Qian, Cheryl Zhenyu/AAD-8705-2022
OI Guo, Chen/0000-0002-5130-0455; Zhao, Junhan/0000-0002-0316-8365; Chen,
   Yingjie/0000-0001-6705-3535; QIAN, Cheryl Zhenyu/0000-0002-7310-8608
NR 45
TC 6
Z9 8
U1 1
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2021
VL 27
IS 3
BP 2000
EP 2014
DI 10.1109/TVCG.2019.2945960
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA QA9EF
UT WOS:000613744500010
PM 31603789
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Fu, JY
   Zhu, B
   Cui, WW
   Ge, S
   Wang, Y
   Zhang, HD
   Huang, H
   Tang, YY
   Zhang, DM
   Ma, XJ
AF Fu, Jiayun
   Zhu, Bin
   Cui, Weiwei
   Ge, Song
   Wang, Yun
   Zhang, Haidong
   Huang, He
   Tang, Yuanyuan
   Zhang, Dongmei
   Ma, Xiaojing
TI Chartem: Reviving Chart Images with Data Embedding
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Chart embedding; background embedding; data embedding; chart image;
   chart reuse
ID WATERMARKING
AB In practice, charts are widely stored as bitmap images. Although easily consumed by humans, they are not convenient for other uses. For example, changing the chart style or type or a data value in a chart image practically requires creating a completely new chart, which is often a time-consuming and error-prone process. To assist these tasks, many approaches have been proposed to automatically extract information from chart images with computer vision and machine learning techniques. Although they have achieved promising preliminary results, there are still a lot of challenges to overcome in terms of robustness and accuracy. In this paper, we propose a novel alternative approach called Chartem to address this issue directly from the root. Specifically, we design a data-embedding schema to encode a significant amount of information into the background of a chart image without interfering human perception of the chart. The embedded information, when extracted from the image, can enable a variety of visualization applications to reuse or repurpose chart images. To evaluate the effectiveness of Chartem, we conduct a user study and performance experiments on Chartem embedding and extraction algorithms. We further present several prototype applications to demonstrate the utility of Chartem.
C1 [Fu, Jiayun; Wang, Yun; Ma, Xiaojing] Huazhong Univ Sci & Technol, Sch Cyber Sci & Thchnol, Big Data Sec Engn Res Ctr, Natl Eng Res Ctr Big Data Tech & Sys, Wuhan, Peoples R China.
   [Zhu, Bin; Cui, Weiwei; Ge, Song; Wang, Yun; Zhang, Haidong; Huang, He; Zhang, Dongmei] Microsoft Res Asia, Beijing, Peoples R China.
C3 Huazhong University of Science & Technology; Microsoft; Microsoft China;
   Microsoft Research Asia
RP Fu, JY (corresponding author), Huazhong Univ Sci & Technol, Sch Cyber Sci & Thchnol, Big Data Sec Engn Res Ctr, Natl Eng Res Ctr Big Data Tech & Sys, Wuhan, Peoples R China.
EM fujiayun@hust.edu.cn; binzhu@microsoft.com; weiweicu@microsoft.com;
   songge@microsoft.com; wangyun@microsoft.com; haizhang@microsoft.com;
   rayhuang@microsoft.com; tangyuanyuan@hust.edu.cn;
   dongmeiz@microsoft.com; lindahust@hust.edu.cn
RI Tang, Yuanyuan/AAR-7478-2021; zhang, dongmei/B-8011-2013; Huang,
   He/LIG-6876-2024; Zhu, Bin/HHZ-1819-2022
NR 45
TC 17
Z9 19
U1 1
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 337
EP 346
DI 10.1109/TVCG.2020.3030351
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100022
PM 33315567
DA 2025-03-07
ER

PT J
AU Gedicke, S
   Bonerath, A
   Niedermann, B
   Haunert, JH
AF Gedicke, Sven
   Bonerath, Annika
   Niedermann, Benjamin
   Haunert, Jan-Henrik
TI Zoomless Maps: External Labeling Methods for the Interactive Exploration
   of Dense Point Sets at a Fixed Map Scale
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE external labeling; interactive maps; map exploration; small screens;
   algorithms; optimization
ID ALGORITHMS; ASSIGNMENT
AB Visualizing spatial data on small-screen devices such as smartphones and smartwatches poses new challenges in computational cartography. The current interfaces for map exploration require their users to zoom in and out frequently. Indeed, zooming and panning are tools suitable for choosing the map extent corresponding to an area of interest. They are not as suitable, however, for resolving the graphical clutter caused by a high feature density since zooming in to a large map scale leads to a loss of context. Therefore, in this paper, we present new external labeling methods that allow a user to navigate through dense sets of points of interest while keeping the current map extent fixed. We provide a unified model, in which labels are placed at the boundary of the map and visually associated with the corresponding features via connecting lines, which are called leaders. Since the screen space is limited, labeling all features at the same time is impractical. Therefore, at any time, we label a subset of the features. We offer interaction techniques to change the current selection of features systematically and, thus, give the user access to all features. We distinguish three methods, which allow the user either to slide the labels along the bottom side of the map or to browse the labels based on pages or stacks. We present a generic algorithmic framework that provides us with the possibility of expressing the different variants of interaction techniques as optimization problems in a unified way. We propose both exact algorithms and fast and simple heuristics that solve the optimization problems taking into account different criteria such as the ranking of the labels, the total leader length as well as the distance between leaders. In experiments on real-world data we evaluate these algorithms and discuss the three variants with respect to their strengths and weaknesses proving the flexibility of the presented algorithmic framework.
C1 [Gedicke, Sven; Bonerath, Annika; Niedermann, Benjamin; Haunert, Jan-Henrik] Univ Bonn, Geoinformat Grp, Inst Geodesy & Geoinformat, Bonn, Germany.
C3 University of Bonn
RP Gedicke, S (corresponding author), Univ Bonn, Geoinformat Grp, Inst Geodesy & Geoinformat, Bonn, Germany.
EM Gedicke@igg.uni-bonn.de; Bonerath@igg.uni-bonn.de;
   Niedermann@igg.uni-bonn.de; Haunert@igg.uni-bonn.de
RI Haunert, Jan-Henrik/B-7419-2015
OI Gedicke, Sven/0000-0002-5416-3564; Haunert,
   Jan-Henrik/0000-0001-8005-943X; Bonerath, Annika/0000-0002-8427-3246
FU Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under
   Germany's Excellence Strategy [EXC 2070 - 390732324]; German Research
   Foundation (DFG) [5451/6-1]
FX Partially funded by the Deutsche Forschungsgemeinschaft (DFG, German
   Research Foundation) under Germany's Excellence Strategy EXC 2070
   -390732324. Partially funded by Zoomless Maps: Models and Algorithms for
   the Exploration of Dense Maps with a fixed Scale of the German Research
   Foundation (DFG) [grant number 5451/6-1]
NR 36
TC 7
Z9 9
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1247
EP 1256
DI 10.1109/TVCG.2020.3030399
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100107
PM 33048715
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Kiesel, D
   Riehmann, P
   Wachsmuth, H
   Stein, B
   Froehlich, B
AF Kiesel, Dora
   Riehmann, Patrick
   Wachsmuth, Henning
   Stein, Benno
   Froehlich, Bernd
TI Visual Analysis of Argumentation in Essays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Task analysis; Annotations; Skin; Visual analytics; Histograms; Data
   visualization; Information Visualization; Text Analysis; User
   Interfaces; Visual Analytics; Argumentation Visualization; Glyph-based
   Techniques; Text and Document Data; Tree-based Visualization;
   Coordinated and Multiple Views; Close and Distant Reading
ID EXPLORATION
AB This paper presents a visual analytics system for exploring, analyzing and comparing argument structures in essay corpora. We provide an overview of the corpus by a list of ArguLines which represent the argument units of each essay by a sequence of glyphs. Each glyph encodes the stance, the depth and the relative position of an argument unit. The overview can be ordered in various ways to reveal patterns and outliers. Subsets of essays can be selected and analyzed in detail using the Argument Unit Occurrence Tree which aggregates the argument structures using hierarchical histograms. This hierarchical view facilitates the estimation of statistics and trends concerning the progression of the argumentation in the essays. It also provides insights into the commonalities and differences between selected subsets. The text view is the necessary textual basis to verify conclusions from the other views and the annotation process. Linking the views and interaction techniques for visual filtering, studying the evolution of stance within a subset of essays and scrutinizing the order of argumentative units enable a deep analysis of essay corpora. Our expert reviews confirmed the utility of the system and revealed detailed and previously unknown information about the argumentation in our sample corpus.
C1 [Kiesel, Dora; Riehmann, Patrick; Stein, Benno; Froehlich, Bernd] Bauhaus Univ Weimar, Weimar, Germany.
   [Wachsmuth, Henning] Paderborn Univ, Paderborn, Germany.
C3 Bauhaus-Universitat Weimar; University of Paderborn
RP Kiesel, D (corresponding author), Bauhaus Univ Weimar, Weimar, Germany.
EM dora.kiesel@uni-weimar.de; patrick.riehmann@uni-weimar.de;
   henningw@upb.de; benno.stein@uni-weimar.de;
   bernd.froehlich@uni-weimar.de
RI Wachsmuth, Henning/AAH-7299-2021
OI Wachsmuth, Henning/0000-0003-2792-621X
FU German Federal Ministry of Education and Research (BMBF) [03PSIPT5A];
   Thuringian Ministry of Economy, Science and Digital Society
FX This work was supported in part by the German Federal Ministry of
   Education and Research (BMBF) under grant 03PSIPT5A (project Provenance
   Analytics) as well as by the Thuringian Ministry of Economy, Science and
   Digital Society in form of a scholarship.
NR 48
TC 5
Z9 5
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1139
EP 1148
DI 10.1109/TVCG.2020.3030425
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100097
PM 33048728
DA 2025-03-07
ER

PT J
AU Nguyen, DB
   Monico, RO
   Chen, GN
AF Nguyen, Duong B.
   Monico, Rodolfo Ostilla
   Chen, Guoning
TI A Visualization Framework for Multi-scale Coherent Structures in
   Taylor-Couette Turbulence
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Three-dimensional displays; Two dimensional displays;
   Feature extraction; Periodic structures; Standards; Rendering (computer
   graphics); Flow visualization; Taylor-Couette turbulence; coherent
   structures
ID OF-THE-ART; TOPOLOGY SIMPLIFICATION; FLOW VISUALIZATION; VECTOR-FIELDS;
   STATISTICS; EXTRACTION; WAVELETS
AB Taylor-Couette flow (TCF) is the turbulent fluid motion created between two concentric and independently rotating cylinders. It has been heavily researched in fluid mechanics thanks to the various nonlinear dynamical phenomena that are exhibited in the flow. As many dense coherent structures overlap each other in TCF, it is challenging to isolate and visualize them, especially when the cylinder rotation ratio is changing. Previous approaches rely on 2D cross sections to study TCF due to its simplicity, which cannot provide the complete information of TCF. In the meantime, standard visualization techniques, such as volume rendering / iso-surfacing of certain attributes and the placement of integral curves/surfaces, usually produce cluttered visualization. To address this challenge and to support domain experts in the analysis of TCF, we developed a visualization framework to separate large-scale structures from the dense, small-scale structures and provide an effective visual representation of these structures. Instead of using a single physical attribute as the standard approach which cannot efficiently separate structures in different scales for TCF, we adapt the feature level-set method to combine multiple attributes and use them as a filter to separate large- and small-scale structures. To visualize these structures, we apply the iso-surface extraction on the kernel density estimate of the distance field generated from the feature level-set. The proposed methods successfully reveal 3D large-scale coherent structures of TCF with different control parameter settings, which are difficult to achieve with the conventional methods.
C1 [Nguyen, Duong B.; Monico, Rodolfo Ostilla; Chen, Guoning] Univ Houston, Houston, TX 77004 USA.
C3 University of Houston System; University of Houston
RP Nguyen, DB (corresponding author), Univ Houston, Houston, TX 77004 USA.
EM duongnguyenbinh@gmail.com; rostilla@central.uh.edu; chengu@cs.uh.edu
RI Ostilla Monico, Rodolfo/HDN-9526-2022
OI Chen, Guoning/0000-0003-0581-6415
FU NSF [IIS 1553329]
FX We thank the anonymous reviewers for their valuable suggestions. This
   research was supported by NSF IIS 1553329.
NR 49
TC 6
Z9 6
U1 1
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 902
EP 912
DI 10.1109/TVCG.2020.3028892
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100075
PM 33026991
DA 2025-03-07
ER

PT J
AU Ondov, BD
   Yang, FM
   Kay, M
   Elmqvist, N
   Franconeri, S
AF Ondov, Brian D.
   Yang, Fumeng
   Kay, Matthew
   Elmqvist, Niklas
   Franconeri, Steven
TI Revealing Perceptual Proxies with Adversarial Examples
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Perceptual proxies; vision science; crowdsourced evaluation
ID RANKING VISUALIZATIONS; INFORMATION; OPTIMIZATION; INFERENCE; MODELS
AB Data visualizations convert numbers into visual marks so that our visual system can extract data from an image instead of raw numbers. Clearly, the visual system does not compute these values as a computer would, as an arithmetic mean or a correlation. Instead, it extracts these patterns using perceptual proxies; heuristic shortcuts of the visual marks, such as a center of mass or a shape envelope. Understanding which proxies people use would lead to more effective visualizations. We present the results of a series of crowdsourced experiments that measure how powerfully a set of candidate proxies can explain human performance when comparing the mean and range of pairs of data series presented as bar charts. We generated datasets where the correct answer the series with the larger arithmetic mean or range was pitted against an "adversarial" series that should be seen as larger if the viewer uses a particular candidate proxy. We used both Bayesian logistic regression models and a robust Bayesian mixed-effects linear model to measure how strongly each adversarial proxy could drive viewers to answer incorrectly and whether different individuals may use different proxies. Finally, we attempt to construct adversarial datasets from scratch, using an iterative crowdsourcing procedure to perform black-box optimization.
C1 [Ondov, Brian D.] NIH, Bethesda, MD 20892 USA.
   [Ondov, Brian D.; Elmqvist, Niklas] Univ Maryland, College Pk, MD 20742 USA.
   [Yang, Fumeng] Brown Univ, Providence, RI 02912 USA.
   [Kay, Matthew] Univ Michigan, Ann Arbor, MI 48109 USA.
   [Franconeri, Steven] Northwestern Univ, Evanston, IL USA.
C3 National Institutes of Health (NIH) - USA; University System of
   Maryland; University of Maryland College Park; Brown University;
   University of Michigan System; University of Michigan; Northwestern
   University
RP Ondov, BD (corresponding author), NIH, Bethesda, MD 20892 USA.; Ondov, BD (corresponding author), Univ Maryland, College Pk, MD 20742 USA.
EM ondovb@umd.edu; fy@brown.edu; mjskay@umich.edu; elm@umd.edu;
   franconeri@northwestern.edu
RI Kay, Matthew/AAN-2490-2021; Yang, Fumeng/HME-2828-2023
OI Kay, Matthew/0000-0001-9446-0419; Elmqvist, Niklas/0000-0001-5805-5301
FU U.S. National Science Foundation [IIS-1901485]; Intramural Research
   Program of the National Human Genome Research Institute, U.S. National
   Institutes of Health
FX This work was supported partly by grant IIS-1901485 from the U.S.
   National Science Foundation and partly by the Intramural Research
   Program of the National Human Genome Research Institute, a part of the
   U.S. National Institutes of Health. Any opinions, findings, and
   conclusions or recommendations expressed here are those of the authors
   and do not necessarily reflect the views of the funding agencies.
NR 51
TC 11
Z9 11
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2021
VL 27
IS 2
BP 1073
EP 1083
DI 10.1109/TVCG.2020.3030429
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA WF5FO
UT WOS:000706330100091
PM 33095716
DA 2025-03-07
ER

PT J
AU Li, YJ
   Zhai, X
   Hou, F
   Liu, YW
   Hao, AM
   Qin, H
AF Li, Yingjia
   Zhai, Xiao
   Hou, Fei
   Liu, Yawen
   Hao, Aimin
   Qin, Hong
TI Vectorized Painting with Temporal Diffusion Curves
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Painting; Mathematical model; Graphics; Real-time systems; Pigments;
   Heating systems; Oils; Vector graphics; heat equation; procedural model;
   real-time application
AB This paper presents a vector painting system for digital artworks. We first propose Temporal Diffusion Curve (TDC), a new form of vector graphics, and a novel random-access solver for modeling the evolution of strokes. With the help of a procedural stroke processing function, the TDC strokes can achieve various shapes and effects for multiple art styles. Based on these, we build a painting system of great potential. Thanks to the random-access solver, our method has real-time performance regardless of the rendering resolution, provides straightforward editing possibilities on strokes both at runtime and afterward, and is effective and straightforward for art production. Compared with the previous Diffusion Curve, our method uses strokes as the basic graphics primitives, which are able to intersect each other and much more consistent with the intuition and painting habits of human. We finally demonstrate that professional artists can create multiple genres of artworks with our painting system.
C1 [Li, Yingjia; Zhai, Xiao; Hao, Aimin] Beihang Univ, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Hou, Fei] Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.
   [Hou, Fei] Univ Chinese Acad Sci, Beijing 100049, Peoples R China.
   [Liu, Yawen] Beijing Piesat Informat Technol Co Ltd, Beijing 100195, Peoples R China.
   [Hao, Aimin] Beihang Univ, Res Inst Frontier Sci, Beijing 100191, Peoples R China.
   [Qin, Hong] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 Beihang University; Chinese Academy of Sciences; Institute of Software,
   CAS; Chinese Academy of Sciences; University of Chinese Academy of
   Sciences, CAS; Beihang University; State University of New York (SUNY)
   System; Stony Brook University
RP Hou, F (corresponding author), Chinese Acad Sci, Inst Software, State Key Lab Comp Sci, Beijing 100190, Peoples R China.; Qin, H (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM liyingjiasss@gmail.com; zhaixiao43@gmail.com; houfei@ios.ac.cn;
   945664721@qq.com; ham@buaa.edu.cn; qin@cs.sunysb.edu
RI Zhao, Mingyu/HHS-0141-2022
OI Hou, Fei/0000-0001-8226-6635; Zhai, Xiao/0000-0001-8964-3704
FU NSFC [61872347, 61532002, 61672077]; Special Plan for the Development of
   Distinguished Young Scientists of ISCAS [Y8RC535018]; USA National
   Science Foundation [IIS-1715985, IIS-1812606]; National Key R&D Program
   of China [2017YFF0106407]; CAS Key Research Program of Frontier Sciences
   [QYZDY-SSW-JSC041]; Applied Basic Research Program of Qingdao
   [161013xx]; Capital Health Research and Development of Special
   [2016-1-4011]; Fundamental Research Funds for the Central Universities;
   Beijing Natural Science Foundation-Haidian Primitive Innovation Joint
   Fund [L182016]
FX This projectwas partially supported by NSFC Grants (61872347, 61532002,
   61672077), Special Plan for the Development of Distinguished Young
   Scientists of ISCAS (Y8RC535018), USA National Science Foundation
   IIS-1715985, IIS-1812606, National Key R&D Program of China
   (2017YFF0106407), CAS Key Research Program of Frontier Sciences
   (QYZDY-SSW-JSC041), Applied Basic Research Program of Qingdao
   (161013xx), Capital Health Research and Development of Special
   (2016-1-4011), Fundamental Research Funds for the Central Universities
   and Beijing Natural Science Foundation-Haidian Primitive Innovation
   Joint Fund (L182016). We would like to thank Jiawen Zeng for helping
   record the voice over of the supplementary video, available online. Y.
   Li and X. Zhai contributed equally and should be regarded as co-first
   authors.
NR 29
TC 4
Z9 4
U1 0
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN 1
PY 2021
VL 27
IS 1
BP 228
EP 240
DI 10.1109/TVCG.2019.2929808
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OY4TT
UT WOS:000594242000018
PM 31329122
OA Bronze
DA 2025-03-07
ER

PT J
AU Wang, BB
   Ge, LS
   Holzschuch, N
AF Wang, Beibei
   Ge, Liangsheng
   Holzschuch, Nicolas
TI Precomputed Multiple Scattering for Rapid Light Simulation in
   Participating Media
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Scattering; Media; Photonics; Computational modeling; Lighting;
   Rendering (computer graphics); Light sources; Participating media;
   precomputation; multiple scattering
AB Rendering translucent materials is costly: light transport algorithms need to simulate a large number of scattering events inside the material before reaching convergence. The cost is especially high for materials with a large albedo or a small mean-free-path, where higher-order scattering effects dominate. We present a new method for fast computation of global illumination with participating media. Our method uses precomputed multiple scattering effects, stored in two compact tables. These precomputed multiple scattering tables are easy to integrate with any illumination simulation algorithm. We give examples for virtual ray lights (VRL), photon mapping with beams and paths (UPBP), Metropolis Light Transport with Manifold Exploration (MEMLT). The original algorithms are in charge of low-order scattering, combined with multiple scattering computed using our table. Our results show significant improvements in convergence speed and memory costs, with negligible impact on accuracy.
C1 [Wang, Beibei] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Key Lab Intelligent Percept & Syst High Dimens In, PCA Lab,Minist Educ, Nanjing 210094, Peoples R China.
   [Wang, Beibei] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Socia, Nanjing 210094, Peoples R China.
   [Ge, Liangsheng] Shandong Univ, Sch Software, Jinan Shi 250100, Peoples R China.
   [Holzschuch, Nicolas] Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France.
C3 Nanjing University of Science & Technology; Nanjing University of
   Science & Technology; Shandong University; Communaute Universite
   Grenoble Alpes; Universite Grenoble Alpes (UGA); Institut National
   Polytechnique de Grenoble; Inria; Centre National de la Recherche
   Scientifique (CNRS)
RP Wang, BB (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Key Lab Intelligent Percept & Syst High Dimens In, PCA Lab,Minist Educ, Nanjing 210094, Peoples R China.; Wang, BB (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Socia, Nanjing 210094, Peoples R China.
EM bebei.wang@gmail.com; liangsheng_ge@163.com; nicolas.holzschuch@inria.fr
RI Holzschuch, Nicolas/E-8861-2014
FU National Key R&D Program of China [2017YFB0203000]; National Natural
   Science Foundation of China [61802187, 61872223]; Natural Science
   Foundation of Jiangsu [BK20170857]; fundamental research funds for the
   central universities [30918011320]; Open Project Program of the State
   Key Lab of CAD&CG Zhejiang University [A1804]; ANR project
   [ANR-15-CE38-0005]; Agence Nationale de la Recherche (ANR)
   [ANR-15-CE38-0005] Funding Source: Agence Nationale de la Recherche
   (ANR)
FX We thank the reviewers for the valuable comments. This work has been
   partially supported by the National Key R&D Program of China under grant
   No. 2017YFB0203000, the National Natural Science Foundation of China
   under grant No. 61802187, 61872223, the Natural Science Foundation of
   Jiangsu under grant No. BK20170857, the fundamental research funds for
   the central universities No. 30918011320, Open Project Program of the
   State Key Lab of CAD&CG Zhejiang University under grant No. A1804 and
   ANR project ANR-15-CE38-0005 "Materials".
NR 29
TC 2
Z9 3
U1 2
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2020
VL 26
IS 7
BP 2456
EP 2470
DI 10.1109/TVCG.2018.2890466
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA MB9QV
UT WOS:000542933100010
PM 30605103
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Al Zayer, M
   MacNeilage, P
   Folmer, E
AF Al Zayer, Majed
   MacNeilage, Paul
   Folmer, Eelke
TI Virtual Locomotion: A Survey
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Navigation; Legged locomotion; Task analysis; Monitoring; Visualization;
   Three-dimensional displays; Space exploration; Virtual reality; virtual
   locomotion; virtual navigation; survey; taxonomy
ID WALKING-IN-PLACE; REDIRECTED WALKING; SPATIAL ORIENTATION; REAL-WALKING;
   ENVIRONMENTS; INTERFACE; TRAVEL; NAVIGATION; ROTATION; TRACKING
AB Virtual reality (VR) has enjoyed significant popularity in recent years. Where navigation has been a fundamental appeal of 3D applications for decades, facilitating this in VR has been quite a challenge. Over the past decades, various virtual locomotion techniques (VLTs) have been developed that aim to offer natural, usable and efficient ways of navigating VR without inducing VR sickness. Several studies of these techniques have been conducted in order to evaluate their performance in various study conditions and virtual contexts. Taxonomies have also been proposed to either place similar techniques in meaningful categories or decompose them to their underlying design components. In this survey, we aim to aggregate and understand the current state of the art of VR locomotion research and discuss the design implications of VLTs in terms of strengths, weaknesses and applicability.
C1 [Al Zayer, Majed] Univ Nevada, Reno, NV 89557 USA.
   [MacNeilage, Paul] Univ Nevada, Dept Psychol, Cognit & Brain Sci, Reno, NV 89557 USA.
   [Folmer, Eelke] Univ Nevada, Dept Comp Sci, Reno, NV 89557 USA.
C3 Nevada System of Higher Education (NSHE); University of Nevada Reno;
   Nevada System of Higher Education (NSHE); University of Nevada Reno;
   Nevada System of Higher Education (NSHE); University of Nevada Reno
RP Al Zayer, M (corresponding author), Univ Nevada, Reno, NV 89557 USA.
EM malzayer@cse.unr.edu; pmacneilage@unr.edu; efolmer@unr.edu
OI Al Zayer, Majed/0000-0002-7704-1308
NR 215
TC 104
Z9 109
U1 3
U2 31
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2020
VL 26
IS 6
BP 2315
EP 2334
DI 10.1109/TVCG.2018.2887379
PG 20
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA LM5NO
UT WOS:000532295600016
PM 30575541
DA 2025-03-07
ER

PT J
AU Hasanzadeh, S
   Polys, NF
   de la Garza, JM
AF Hasanzadeh, Sogand
   Polys, Nicholas F.
   de la Garza, Jesus M.
TI Presence, Mixed Reality, and Risk-Taking Behavior: A Study in Safety
   Interventions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 27th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 22-26, 2020
CL Atlanta, GA
SP IEEE, IEEE Comp Soc
DE Safety; Haptic interfaces; Training; Virtual environments; Physiology;
   Human factors; Mixed-reality; passive haptics; presence; human factors;
   risk-taking behavior; X3D; construction safety
ID VIRTUAL-REALITY; HAPTIC FEEDBACK; PERSONALITY
AB Immersive environments have been successfully applied to a broad range of safety training in high-risk domains. However, very little research has used these systems to evaluate the risk-taking behavior of construction workers. In this study, we investigated the feasibility and usefulness of providing passive haptics in a mixed-reality environment to capture the risk-taking behavior of workers, identify at-risk workers, and propose injury-prevention interventions to counteract excessive risk-taking and risk-compensatory behavior. Within a mixed-reality environment in a CAVE-like display system, our subjects installed shingles on a (physical) sloped roof of a (virtual) two-story residential building on a morning in a suburban area. Through this controlled, within-subject experimental design, we exposed each subject to three experimental conditions by manipulating the level of safety intervention. Workers' subjective reports, physiological signals, psychophysical responses, and reactionary behaviors were then considered as promising measures of Presence. The results showed that our mixed-reality environment was a suitable platform for triggering behavioral changes under different experimental conditions and for evaluating the risk perception and risk-taking behavior of workers in a risk-free setting. These results demonstrated the value of immersive technology to investigate natural human factors.
C1 [Hasanzadeh, Sogand] Virginia Tech, Dept Civil & Environm Engn, Blacksburg, VA 24061 USA.
   [Polys, Nicholas F.] Virginia Tech, Dept Comp Sci, Blacksburg, VA USA.
   [de la Garza, Jesus M.] Clemson Univ, Dept Civil Engn, Clemson, SC 29631 USA.
C3 Virginia Polytechnic Institute & State University; Virginia Polytechnic
   Institute & State University; Clemson University
RP Hasanzadeh, S (corresponding author), Virginia Tech, Dept Civil & Environm Engn, Blacksburg, VA 24061 USA.
EM sogandm@vt.edu; npolys@vt.edu; jdelaga@clemson.edu
OI Hasanzadeh, Sogand/0000-0001-6214-3443; Polys,
   Nicholas/0000-0002-8503-970X
FU VT Advanced Research Computing the Visionarium Lab
FX The authors wish to thank VT Advanced Research Computing the Visionarium
   Lab for the resources and support for this study, and Drs. Joseph L.
   Gabbard, Lance Arsenault, Michael Garvin, and E. Scott Geller for their
   considerable and vital support in this study.
NR 57
TC 31
Z9 32
U1 1
U2 39
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2020
VL 26
IS 5
BP 2115
EP 2125
DI 10.1109/TVCG.2020.2973055
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA LA1WU
UT WOS:000523746000030
PM 32070965
DA 2025-03-07
ER

PT J
AU Meng, XX
   Du, RF
   Varshney, A
AF Meng, Xiaoxu
   Du, Ruofei
   Varshney, Amitabh
TI Eye-dominance-guided Foveated Rendering
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 27th IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 22-26, 2020
CL Atlanta, GA
SP IEEE, IEEE Comp Soc
DE Rendering (computer graphics); Visualization; Kernel; Visual systems;
   Pipelines; Sensitivity; Solid modeling; Virtual reality; foveated
   rendering; perception; gaze-contingent rendering; ocular dominance; eye
   tracking
AB Optimizing rendering performance is critical for a wide variety of virtual reality (VR) applications. Foveated rendering is emerging as an indispensable technique for reconciling interactive frame rates with ever-higher head-mounted display resolutions. Here, we present a simple yet effective technique for further reducing the cost of foveated rendering by leveraging ocular dominance - the tendency of the human visual system to prefer scene perception from one eye over the other. Our new approach, eye-dominance-guided foveated rendering (EFR), renders the scene at a lower foveation level (with higher detail) for the dominant eye than the non-dominant eye. Compared with traditional foveated rendering, EFR can be expected to provide superior rendering performance while preserving the same level of perceived visual quality.
C1 [Meng, Xiaoxu; Varshney, Amitabh] Univ Maryland, College Pk, MD 20742 USA.
   [Du, Ruofei] Google LLC, Menlo Pk, CA USA.
C3 University System of Maryland; University of Maryland College Park;
   Google Incorporated
RP Meng, XX (corresponding author), Univ Maryland, College Pk, MD 20742 USA.
EM xmeng525@umiacs.umd.edu; ruofei@google.com; varshney@umiacs.umd.edu
RI Du, Ruofei/AAL-4845-2020
OI Varshney, Amitabh/0000-0002-9873-2212; Du, Ruofei/0000-0003-2471-9776
FU NSF [14-29404, 15-64212, 18-23321]; State of Maryland's MPower
   initiative
FX We would like to thank the anonymous reviewers for the helpful comments
   that have significantly improved this paper. We also greatly appreciate
   the insightful suggestions by Dr. Eric Turner from Google. This work has
   been supported in part by the NSF Grants 14-29404, 15-64212, 18-23321,
   and the State of Maryland's MPower initiative. Any opinions, findings,
   conclusions, or recommendations expressed in this article are those of
   the authors and do not necessarily reflect the views of the research
   sponsors.
NR 44
TC 32
Z9 36
U1 1
U2 27
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2020
VL 26
IS 5
BP 1972
EP 1980
DI 10.1109/TVCG.2020.2973442
PG 9
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA LA1WU
UT WOS:000523746000016
PM 32086213
DA 2025-03-07
ER

PT J
AU Xing, GY
   Liu, YL
   Ling, HB
   Granier, X
   Zhang, YC
AF Xing, Guanyu
   Liu, Yanli
   Ling, Haibin
   Granier, Xavier
   Zhang, Yanci
TI Automatic Spatially Varying Illumination Recovery of Indoor Scenes Based
   on a Single RGB-D Image
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Lighting; Light sources; Geometry; Cameras; Three-dimensional displays;
   Dynamic range; Probes; Illumination recovery; automatic; indoor scenes;
   single RGB-D image
ID RADIANCE; OBJECTS
AB We propose an automatic framework to recover the illumination of indoor scenes based on a single RGB-D image. Unlike previous works, our method can recover spatially varying illumination without using any lighting capturing devices or HDR information. The recovered illumination can produce realistic rendering results. To model the geometry of the visible and invisible parts of scenes corresponding to the input RGB-D image, we assume that all objects shown in the image are located in a box with six faces and build a planar-based geometry model based on the input depth map. We then present a confidence-scoring based strategy to separate the light sources from the highlight areas. The positions of light sources both in and out of the camera's view are calculated based on the classification result and the recovered geometry model. Finally, an iterative procedure is proposed to calculate the colors of light sources and the materials in the scene. In addition, a data-driven method is used to set constraints on the light source intensities. Using the estimated light sources and geometry model, environment maps at different points in the scene are generated that can model the spatial variance of illumination. The experimental results demonstrate the validity and flexibility of our approach.
C1 [Xing, Guanyu] Sichuan Univ, Natl Key Lab Fundamental Sci Synthet Vis, Chengdu 610041, Peoples R China.
   [Xing, Guanyu] Univ Elect Sci & Technol China, Sch Comp Sci & Engn, Chengdu 610054, Peoples R China.
   [Liu, Yanli; Zhang, Yanci] Sichuan Univ, Coll Comp Sci, Chengdu 610041, Peoples R China.
   [Ling, Haibin] Temple Univ, Dept Comp & Informat Sci, Ctr Data Analyt & Biomed Informat, Philadelphia, PA 19122 USA.
   [Granier, Xavier] Inst Opt, LP2N Lab, Grad Sch, Bordeaux, France.
C3 Sichuan University; University of Electronic Science & Technology of
   China; Sichuan University; Pennsylvania Commonwealth System of Higher
   Education (PCSHE); Temple University; Universite de Bordeaux
RP Liu, YL (corresponding author), Sichuan Univ, Coll Comp Sci, Chengdu 610041, Peoples R China.
EM xgyuestc@gmail.com; yanliliu@scu.edu.cn; hbling@temple.edu;
   xavier.granier@institutoptique.fr; yczhang@scu.edu.cn
OI Granier, Xavier/0000-0003-1349-2297
FU National Natural Science Foundation of China [61572333,61402081]; Open
   Project Program of the Science and Technology on Optical Radiation
   Laboratory [61424080109]; China National Key Research and Development
   Plan [2016YFB1001200]; US National Science Foundation [1618398,
   IIS-1814745, IIS-1350521]; Direct For Computer & Info Scie & Enginr;
   Division Of Computer and Network Systems [1618398] Funding Source:
   National Science Foundation
FX We thank all the anonymous reviewers for their insightful comments and
   constructive suggestions. This research is supported by National Natural
   Science Foundation of China (Grant No.61572333,61402081), Open Project
   Program of the Science and Technology on Optical Radiation
   Laboratory(Grant No. 61424080109), China National Key Research and
   Development Plan (Grant No.2016YFB1001200). Ling is supported in part by
   US National Science Foundation (Grant No.1618398, IIS-1814745 and
   IIS-1350521). Yanli Liu is the corresponding author.
NR 33
TC 5
Z9 5
U1 0
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2020
VL 26
IS 4
BP 1672
EP 1685
DI 10.1109/TVCG.2018.2876541
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KU2OG
UT WOS:000519547200005
PM 30371374
OA Green Submitted, Bronze
DA 2025-03-07
ER

PT J
AU Guo, JW
   Xu, SB
   Yan, DM
   Cheng, ZL
   Jaeger, M
   Zhang, XP
AF Guo, Jianwei
   Xu, Shibiao
   Yan, Dong-Ming
   Cheng, Zhanglin
   Jaeger, Marc
   Zhang, Xiaopeng
TI Realistic Procedural Plant Modeling from Multiple View Images
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Solid modeling; Computational modeling;
   Image reconstruction; Shape; Geometry; Vegetation; Multi-view images;
   dense depth map; procedural modeling; plant reconstruction
ID 3D RECONSTRUCTION; TREE MODELS; STEREO; INFORMATION; EFFICIENT
AB In this paper, we describe a novel procedural modeling technique for generating realistic plant models from multi-view photographs. The realism is enhanced via visual and spatial information acquired from images. In contrast to previous approaches that heavily rely on user interaction to segment plants or recover branches in images, our method automatically estimates an accurate depth map of each image and extracts a 3D dense point cloud by exploiting an efficient stereophotogrammetry approach. Taking this point cloud as a soft constraint, we fit a parametric plant representation to simulate the plant growth progress. In this way, we are able to synthesize parametric plant models from real data provided by photos and 3D point clouds. We demonstrate the robustness of the proposed approach by modeling various plants with complex branching structures and significant self-occlusions. We also demonstrate that the proposed framework can be used to reconstruct ground-covering plants, such as bushes and shrubs which have been given little attention in the literature. The effectiveness of our approach is validated by visually and quantitatively comparing with the state-of-the-art approaches.
C1 [Guo, Jianwei; Xu, Shibiao; Yan, Dong-Ming; Zhang, Xiaopeng] Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.
   [Cheng, Zhanglin] Chinese Acad Sci, SIAT, Shenzhen VisuCA Key Lab, Beijing 100864, Peoples R China.
   [Jaeger, Marc] Montpellier Univ, ICAR, LIRMM, F-34090 Montpellier, France.
C3 Chinese Academy of Sciences; Institute of Automation, CAS; Chinese
   Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS;
   Centre National de la Recherche Scientifique (CNRS); Universite
   Paul-Valery; Universite Perpignan Via Domitia; Universite de Montpellier
RP Yan, DM (corresponding author), Chinese Acad Sci, Natl Lab Pattern Recognit, Inst Automat, Beijing 100190, Peoples R China.; Cheng, ZL (corresponding author), Chinese Acad Sci, SIAT, Shenzhen VisuCA Key Lab, Beijing 100864, Peoples R China.
EM jianwei.guo@nlpr.ia.ac.cn; shibiao.xu@nlpr.ia.ac.cn;
   yandongming@gmail.com; zhanglin.cheng@gmail.com; marc.jaeger@cirad.fr;
   xpzhang@nlpr.ia.ac.cn
RI Cheng, Zhanglin/AAP-1760-2021
OI Guo, Jianwei/0000-0002-3376-1725; Yan, Dong-Ming/0000-0003-2209-2404;
   Cheng, Zhanglin/0000-0002-3360-2679
FU National Natural Science Foundation of China [61331018, 61761003,
   61802406, 61772523, 61671451]; Beijing Natural Science Foundation
   [4184102]; CAS [GJHZ1862]
FX We thank anonymous reviewers for their valuable comments. This work is
   partially funded by the National Natural Science Foundation of China
   (61331018, 61761003, 61802406, 61772523, 61671451), the Beijing Natural
   Science Foundation (4184102), and the CAS Grant (GJHZ1862).
NR 69
TC 33
Z9 36
U1 2
U2 39
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2020
VL 26
IS 2
BP 1372
EP 1384
DI 10.1109/TVCG.2018.2869784
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB6YF
UT WOS:000506637400008
PM 30222577
DA 2025-03-07
ER

PT J
AU Nsonga, B
   Scheuermann, G
   Gumhold, S
   Ventosa-Molina, J
   Koschichow, D
   Fröhlich, J
AF Nsonga, Baldwin
   Scheuermann, Gerik
   Gumhold, Stefan
   Ventosa-Molina, Jordi
   Koschichow, Denis
   Froehlich, Jochen
TI Analysis of the Near-Wall Flow in a Turbine Cascade by Splat
   Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Flow Visualization; Visualization in Physical Sciences and Engineering;
   Feature Detection and Tracking; Vector Field Data
ID LARGE-EDDY SIMULATION; TURBULENT BOUNDARY-LAYERS; OPEN-CHANNEL FLOW;
   HEAT-TRANSFER; SHEAR; EQUATION
AB Turbines are essential components of jet planes and power plants. Therefore, their efficiency and service life are of central engineering interest. In the case of jet planes or thermal power plants, the heating of the turbines due to the hot gas flow is critical. Besides effective cooling, it is a major goal of engineers to minimize heat transfer between gas flow and turbine by design. Since it is known that splat events have a substantial impact on the heat transfer between flow and immersed surfaces, we adapt a splat detection and visualization method to a turbine cascade simulation in this case study. Because splat events are small phenomena, we use a direct numerical simulation resolving the turbulence in the flow as the base of our analysis. The outcome shows promising insights into splat formation and its relation to vortex structures. This may lead to better turbine design in the future.
C1 [Nsonga, Baldwin; Scheuermann, Gerik] Univ Leipzig, Leipzig, Germany.
   [Gumhold, Stefan] Tech Univ Dresden, Inst Software & Multimedia, Dresden, Germany.
   [Ventosa-Molina, Jordi; Koschichow, Denis; Froehlich, Jochen] Tech Univ Dresden, Inst Fluid Mech, Dresden, Germany.
C3 Leipzig University; Technische Universitat Dresden; Technische
   Universitat Dresden
RP Nsonga, B (corresponding author), Univ Leipzig, Leipzig, Germany.
EM nsonga@informatik.uni-leipzig.de; scheuermann@informatik.uni-leipzig.de;
   stefan.gumhold@tu-dresden.de; jordi.ventosa_molina@tu-dresden.de;
   jochen.froehlich@tu-dresden.de
RI Frohlich, Jochen/B-4275-2010
OI Frohlich, Jochen/0000-0003-1653-5686; Nsonga,
   Baldwin/0000-0002-0651-952X
FU German Federal Ministry of Education and Research within the project
   Competence Center for Scalable Data Services and Solutions (ScaDS)
   Dresden/Leipzig [BMBF 01IS14014B]; DFG [FR1593/15-1, PAK948]
FX This work was funded by the German Federal Ministry of Education and
   Research within the project Competence Center for Scalable Data Services
   and Solutions (ScaDS) Dresden/Leipzig (BMBF 01IS14014B). JF and JVM
   acknowledge funding by DFG under FR1593/15-1 within PAK948.
NR 39
TC 3
Z9 3
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 719
EP 728
DI 10.1109/TVCG.2019.2934367
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100066
PM 31442978
OA Green Submitted, Green Accepted
DA 2025-03-07
ER

PT J
AU Wagner, JA
   Stuerzlinger, W
   Nedel, L
AF Wagner Filho, Jorge A.
   Stuerzlinger, Wolfgang
   Nedel, Luciana
TI Evaluating an Immersive Space-Time Cube Geovisualization for Intuitive
   Trajectory Data Exploration
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Space-time cube; Trajectory visualization; Immersive analytics
ID VISUALIZATION; 3D; REPRESENTATION; FRAMEWORK
AB A Space-Time Cube enables analysts to clearly observe spatio-temporal features in movement trajectory datasets in geovisualization. However, its general usability is impacted by a lack of depth cues, a reported steep learning curve, and the requirement for efficient 3D navigation. In this work, we investigate a Space-Time Cube in the Immersive Analytics domain. Based on a review of previous work and selecting an appropriate exploration metaphor, we built a prototype environment where the cube is coupled to a virtual representation of the analysts real desk, and zooming and panning in space and time are intuitively controlled using mid-air gestures. We compared our immersive environment to a desktop-based implementation in a user study with 20 participants across 7 tasks of varying difficulty, which targeted different user interface features. To investigate how performance is affected in the presence of clutter, we explored two scenarios with different numbers of trajectories. While the quantitative performance was similar for the majority of tasks, large differences appear when we analyze the patterns of interaction and consider subjective metrics. The immersive version of the Space-Time Cube received higher usability scores, much higher user preference, and was rated to have a lower mental workload, without causing participants discomfort in 25-minute-long VR sessions.
C1 [Wagner Filho, Jorge A.; Nedel, Luciana] Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil.
   [Wagner Filho, Jorge A.; Stuerzlinger, Wolfgang] Simon Fraser Univ, Burnaby, BC, Canada.
C3 Universidade Federal do Rio Grande do Sul; Simon Fraser University
RP Wagner, JA (corresponding author), Univ Fed Rio Grande do Sul, Porto Alegre, RS, Brazil.; Wagner, JA (corresponding author), Simon Fraser Univ, Burnaby, BC, Canada.
EM jawfilho@inf.ufigs.br; ws@sfu.ca; nedel@inf.ufrgs.br
RI Wagner, Jorge/AAV-7597-2020; Nedel, Luciana/G-3506-2012
OI Stuerzlinger, Wolfgang/0000-0002-7110-5024; Nedel,
   Luciana/0000-0002-2390-1392; Wagner Filho, Jorge
   Alberto/0000-0002-7016-3142
FU CNPq-Brazil; Global Affairs Canada; Coordenacao de Aperfeicoamento de
   Pessoal de Nivel Superior - Brasil (CAPES) [001]
FX The authors thank the user study participants for their feedback and the
   reviewers for their insightful contributions. We also acknowledge
   financial support from CNPq-Brazil and Global Affairs Canada. This study
   was financed in part by the Coordenacao de Aperfeicoamento de Pessoal de
   Nivel Superior - Brasil (CAPES) - Finance Code 001.
NR 59
TC 73
Z9 81
U1 9
U2 39
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 514
EP 524
DI 10.1109/TVCG.2019.2934415
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA KB0CF
UT WOS:000506166100048
PM 31581085
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Whitlock, M
   Wu, KK
   Szafir, D
AF Whitlock, Matt
   Wu, Keke
   Szafir, Danielle
TI Designing for Mobile and Immersive Visual Analytics in the Field
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Immersive Analytics; Augmented Reality; Mobile Visualization; Outdoor
   Visualization; Emergency Response
ID SITUATION AWARENESS; AUGMENTED REALITY; INFORMATION; FUTURE; HAND
AB Data collection and analysis in the field is critical for operations in domains such as environmental science and public safety. However, field workers currently face data- and platform-oriented issues in efficient data collection and analysis in the field, such as limited connectivity, screen space, and attentional resources. In this paper, we explore how visual analytics tools might transform field practices by more deeply integrating data into these operations. We use a design probe coupling mobile, cloud, and immersive analytics components to guide interviews with ten experts from five domains to explore how visual analytics could support data collection and analysis needs in the field. The results identify shortcomings of current approaches and target scenarios and design considerations for future field analysis systems. We embody these findings in FieldView, an extensible, open-source prototype designed to support critical use cases for situated field analysis. Our findings suggest the potential for integrating mobile and immersive technologies to enhance datas utility for various field operations and new directions for visual analytics tools to transform fieldwork.
C1 [Whitlock, Matt; Wu, Keke; Szafir, Danielle] Univ Colorado, Boulder, CO 80309 USA.
C3 University of Colorado System; University of Colorado Boulder
RP Whitlock, M (corresponding author), Univ Colorado, Boulder, CO 80309 USA.
EM matthemwhitlock@colorado.edu; keke.wu@colorado.edu;
   danielle.szafir@colorado.edu
OI Wu, Keke/0000-0003-2361-7284
FU NSF [1764092]; IGP grant from the University of Colorado Boulder; Direct
   For Computer & Info Scie & Enginr; Div Of Information & Intelligent
   Systems [1764092] Funding Source: National Science Foundation
FX The authors thank the expert participants whose feedback drove this
   work. This work was supported by NSF Award #1764092 and an IGP grant
   from the University of Colorado Boulder.
NR 61
TC 21
Z9 28
U1 0
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 503
EP 513
DI 10.1109/TVCG.2019.2934282
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100047
PM 31425088
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhao, Y
   Luo, XB
   Lin, XR
   Wang, HR
   Kui, XY
   Zhou, FF
   Wang, JS
   Chen, Y
   Chen, W
AF Zhao, Ying
   Luo, Xiaobo
   Lin, Xiaoru
   Wang, Hairong
   Kui, Xiaoyan
   Zhou, Fangfang
   Wang, Jinsong
   Chen, Yi
   Chen, Wei
TI Visual Analytics for Electromagnetic Situation Awareness in Radio
   Monitoring and Management
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Radio monitoring and management; radio signal data; radio spectrum data;
   situation awareness; visual analytics
ID CLUSTERING-ALGORITHM; TIME; VISUALIZATION; TOPICS
AB Traditional radio monitoring and management largely depend on radio spectrum data analysis, which requires considerable domain experience and heavy cognition effort and frequently results in incorrect signal judgment and incomprehensive situation awareness. Faced with increasingly complicated electromagnetic environments, radio supervisors urgently need additional data sources and advanced analytical technologies to enhance their situation awareness ability. This paper introduces a visual analytics approach for electromagnetic situation awareness. Guided by a detailed scenario and requirement analysis, we first propose a signal clustering method to process radio signal data and a situation assessment model to obtain qualitative and quantitative descriptions of the electromagnetic situations. We then design a two-module interface with a set of visualization views and interactions to help radio supervisors perceive and understand the electromagnetic situations by a joint analysis of radio signal data and radio spectrum data. Evaluations on real-world data sets and an interview with actual users demonstrate the effectiveness of our prototype system. Finally, we discuss the limitations of the proposed approach and provide future work directions.
C1 [Zhao, Ying; Luo, Xiaobo; Lin, Xiaoru; Kui, Xiaoyan; Zhou, Fangfang] Cent South Univ, Sch Comp Sci & Engn, Changsha, Hunan, Peoples R China.
   [Wang, Hairong] Cent South Univ, Sch Automat, Changsha, Hunan, Peoples R China.
   [Wang, Jinsong] Southwest Elect & Telecom Engn Inst, Shanghai, Peoples R China.
   [Chen, Yi] Beijing Technol & Business Univ, Beijing Key Lab Big Data Technol Food Safety, Beijing, Peoples R China.
   [Chen, Wei] Zhejiang Univ, Key Lab CAD & CG, Hangzhou, Zhejiang, Peoples R China.
C3 Central South University; Central South University; Beijing Technology &
   Business University; Zhejiang University
RP Kui, XY; Zhou, FF (corresponding author), Cent South Univ, Sch Comp Sci & Engn, Changsha, Hunan, Peoples R China.
EM zhaoying@csu.edu.cn; luoxiaobo@csu.edu.cn; 0904130203@csu.edu.cn;
   hairongwang@csu.edu.cn; xykui@csu.edu.cn; zff@csu.edu.cn;
   jinsong.wang@126.com; chenyi@th.btbu.edu.cn; chenwei@cad.zju.edu.cn
RI Zhao, Liangyu/IAO-7294-2023; Chen, Wei/AAR-9817-2020; Wang,
   Hairong/HGF-1825-2022
FU National Key Research and Development Program of China [2018YFB1700403];
   National Natural Science & Technology Fundamental Resources
   Investigation Program of China [2018FY10090002]; National Natural
   Science Foundation of China [61872388, 61772456, 61761136020, 61672538,
   61502540]; National Science Foundation of Hunan Province [2019JJ40406];
   Open Project Program of the State Key Lab of CAD & CG, Zhejiang
   University [A1902]; Open Research Fund of Beijing Key Laboratory of Big
   Data Technology for Food Safety, Beijing Technology and Business
   University [BKBD-2018KF08]
FX We wish to thank all the anonymous reviewers for their valuable
   comments. The work is supported in part by the National Key Research and
   Development Program of China (No. 2018YFB1700403), the National Natural
   Science & Technology Fundamental Resources Investigation Program of
   China (No. 2018FY10090002), the National Natural Science Foundation of
   China (No. 61872388, 61772456, 61761136020, 61672538, and 61502540), the
   National Science Foundation of Hunan Province (No. 2019JJ40406), the
   Open Project Program of the State Key Lab of CAD & CG, Zhejiang
   University (No. A1902), and the Open Research Fund of Beijing Key
   Laboratory of Big Data Technology for Food Safety, Beijing Technology
   and Business University (No. BKBD-2018KF08).
NR 73
TC 43
Z9 54
U1 10
U2 62
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2020
VL 26
IS 1
BP 590
EP 600
DI 10.1109/TVCG.2019.2934655
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA KB0CF
UT WOS:000506166100055
PM 31443001
DA 2025-03-07
ER

PT J
AU Okoe, M
   Jianu, R
   Kobourov, S
AF Okoe, Mershack
   Jianu, Radu
   Kobourov, Stephen
TI Node-Link or Adjacency Matrices: Old Question, New Insights
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Evaluation; user study; graphs; networks; node-link; adjacency matrices
ID GRAPH VISUALIZATIONS; INFORMATION; CYTOSCAPE; DIAGRAMS; SYSTEM
AB Visualizing network data is applicable in domains such as biology, engineering, and social sciences. We report the results of a study comparing the effectiveness of the two primary techniques for showing network data: node-link diagrams and adjacency matrices. Specifically, an evaluation with a large number of online participants revealed statistically significant differences between the two visualizations. Our work adds to existing research in several ways. First, we explore a broad spectrum of network tasks, many of which had not been previously evaluated. Second, our study uses two large datasets, typical of many real-life networks not explored by previous studies. Third, we leverage crowdsourcing to evaluate many tasks with many participants. This paper is an expanded journal version of a Graph Drawing (GD'17) conference paper. We evaluated a second dataset, added a qualitative feedback section, and expanded the procedure, results, discussion, and limitations sections.
C1 [Okoe, Mershack] Florida Int Univ, Miami, FL 33199 USA.
   [Jianu, Radu] City Univ London, London EC1V 0HB, England.
   [Kobourov, Stephen] Univ Arizona, Tucson, AZ 85721 USA.
C3 State University System of Florida; Florida International University;
   City St Georges, University of London; City, University of London;
   University of Arizona
RP Okoe, M (corresponding author), Florida Int Univ, Miami, FL 33199 USA.
EM meshhome16@gmail.com; rdjianu@gmail.com; kobourov@cs.arizona.edu
RI Kobourov, Stephen/A-3016-2008
OI Kobourov, Stephen/0000-0002-0477-2724
FU NSF [CCF-1740858, CCF-1712119]
FX Research supported in part by NSF grants CCF-1740858 and CCF-1712119.
NR 65
TC 43
Z9 47
U1 1
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2019
VL 25
IS 10
BP 2940
EP 2952
DI 10.1109/TVCG.2018.2865940
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IV5CC
UT WOS:000484287800006
PM 30130228
OA Green Published, Green Accepted
DA 2025-03-07
ER

PT J
AU Zheng, JX
   Pawar, S
   Goodman, DFM
AF Zheng, Jonathan X.
   Pawar, Samraat
   Goodman, Dan F. M.
TI Graph Drawing by Stochastic Gradient Descent
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Graph drawing; multidimensional scaling; constraints; relaxation;
   stochastic gradient descent
AB A popular method of force-directed graph drawing is multidimensional scaling using graph-theoretic distances as input. We present an algorithm to minimize its energy function, known as stress, by using stochastic gradient descent (SGD) to move a single pair of vertices at a time. Our results show that SGD can reach lower stress levels faster and more consistently than majorization, without needing help from a good initialization. We then show how the unique properties of SGD make it easier to produce constrained layouts than previous approaches. We also show how SGD can be directly applied within the sparse stress approximation of Ortmann et al. [1], making the algorithm scalable up to large graphs.
C1 [Zheng, Jonathan X.; Goodman, Dan F. M.] Imperial Coll London, Dept Elect & Elect Engn, London SW7 2AZ, England.
   [Pawar, Samraat] Imperial Coll London, Dept Life Sci, Ascot SL5 7PY, Berks, England.
C3 Imperial College London; Imperial College London
RP Zheng, JX (corresponding author), Imperial Coll London, Dept Elect & Elect Engn, London SW7 2AZ, England.
EM jxz12@ic.ac.uk; s.pawar@ic.ac.uk; d.goodman@ic.ac.uk
RI Pawar, Samraat/E-7388-2012
OI Pawar, Samraat/0000-0001-8375-5684; Goodman, Dan/0000-0003-1007-6474
FU EPSRC [1859662] Funding Source: UKRI
NR 33
TC 46
Z9 52
U1 1
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2019
VL 25
IS 9
BP 2738
EP 2748
DI 10.1109/TVCG.2018.2859997
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA IN8OV
UT WOS:000478940300004
PM 30047888
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Dolonius, D
   Sintorn, E
   Ampe, VK
   Assarsson, U
AF Dolonius, Dan
   Sintorn, Erik
   Ampe, Viktor K.
   Assarsson, Ulf
TI Compressing Color Data for Voxelized Surface Geometry
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Voxel; sparse voxel octree; directed acyclic graph; space filling curve;
   color compression; ASTC; BC; JPEG; PNG
ID FRAMEWORK; OCTREE
AB We explore the problem of decoupling color information from geometry in large scenes of voxelized surfaces and of compressing the array of colors without introducing disturbing artifacts. In this extension of our I3D paper with the same title [1], we first present a novel method for connecting each node in a sparse voxel DAG to its corresponding colors in a separate 1D array of colors, with very little additional information stored to the DAG. Then, we show that by mapping the 1D array of colors onto a 2D image using a space-filling curve, we can achieve high compression rates and good quality using conventional, modern, hardware-accelerated texture compression formats such as ASTC or BC7. We additionally explore whether this method can be used to compress voxel colors for off-line storage and network transmission using conventional off-line compression formats such as JPG and JPG2K. For real-time decompression, we suggest a novel variable bitrate block encoding that consistently outperforms previous work, often achieving two times the compression at equal quality.
C1 [Dolonius, Dan; Sintorn, Erik; Ampe, Viktor K.; Assarsson, Ulf] Chalmers Univ Technol, Dept Comp Sci & Engn, S-41258 Gothenburg, Sweden.
C3 Chalmers University of Technology
RP Dolonius, D (corresponding author), Chalmers Univ Technol, Dept Comp Sci & Engn, S-41258 Gothenburg, Sweden.
EM dolonius@chalmers.se; erik.sintorn@chalmers.se; kampe@chalmers.se;
   uffe@chalmers.se
RI Dolonius, Dan/AAD-9556-2019
OI /0000-0002-2350-4784; Assarsson, Ulf/0000-0002-5427-7406
FU Swedish Research Council [2014-4559]
FX This work was supported by the Swedish Research Council under Grant
   2014-4559. The EPIC scene is distributed with the Unreal Development Kit
   by Epic Games. The BODY scene is a freely available scene from Ten24.
   SPONZA is created by Frank Meinl at Crytek. The CAMPUS scene is courtesy
   of Jonathan Berglund, Erik Lindskog and Bjorn Johansson, and was
   obtained as described in their recent paper [26].
NR 25
TC 13
Z9 16
U1 0
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2019
VL 25
IS 2
BP 1270
EP 1282
DI 10.1109/TVCG.2017.2741480
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HG5ZY
UT WOS:000455062000003
PM 28829311
DA 2025-03-07
ER

PT J
AU Alspaugh, S
   Zokaei, N
   Liu, A
   Jin, C
   Hearst, MA
AF Alspaugh, Sara
   Zokaei, Nava
   Liu, Andrea
   Jin, Cindy
   Hearst, Marti A.
TI Futzing and Moseying: Interviews with Professional Data Analysts on
   Exploration Practices
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE EDA; exploratory data analysis; interview study; visual analytics tools
ID VISUALIZATION
AB We report the results of interviewing thirty professional data analysts working in a range of industrial, academic, and regulatory environments. This study focuses on participants' descriptions of exploratory activities and tool usage in these activities. Highlights of the findings include: distinctions between exploration as a precursor to more directed analysis versus truly open-ended exploration; confirmation that some analysts see "finding something interesting" as a valid goal of data exploration while others explicitly disavow this goal; conflicting views about the role of intelligent tools in data exploration; and pervasive use of visualization for exploration, but with only a subset using direct manipulation interfaces. These findings provide guidelines for future tool development, as well as a better understanding of the meaning of the term "data exploration" based on the words of practitioners "in the wild."
C1 [Alspaugh, Sara; Zokaei, Nava; Liu, Andrea; Jin, Cindy; Hearst, Marti A.] Univ Calif Berkeley, Berkeley, CA 94720 USA.
C3 University of California System; University of California Berkeley
RP Alspaugh, S (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.
EM alspaugh@berkeley.edu; nava.zokaei@berkeley.edu; andrealiu@berkeley.edu;
   cjin43@berkeley.edu; hearst@berkeley.edu
FU Tableau; UC Berkeley AMPLab
FX We thank our interviewees. We thank the anonymous reviewers and Melanie
   Tory for giving valuable feedback. This work supported in part by a gift
   from Tableau and by supporters of the UC Berkeley AMPLab
   (https://amplab.cs.berkeley.edu/amp-sponsors/).
NR 25
TC 64
Z9 79
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 22
EP 31
DI 10.1109/TVCG.2018.2865040
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000003
PM 30136976
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Blascheck, T
   Besançon, L
   Bezerianos, A
   Lee, B
   Isenberg, P
AF Blascheck, Tanja
   Besancon, Lonni
   Bezerianos, Anastasia
   Lee, Bongshin
   Isenberg, Petra
TI Glanceable Visualization: Studies of Data Comparison Performance on
   Smartwatches
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Glanceable visualization; smartwatch; perception; quantitative
   evaluation; data comparison
ID PERCEPTION; SCENE
AB We present the results of two perception studies to assess how quickly people can perform a simple data comparison task for small-scale visualizations on a smartwatch. The main goal of these studies is to extend our understanding of design constraints for smartwatch visualizations. Previous work has shown that a vast majority of smartwatch interactions last under 5 s. It is still unknown what people can actually perceive from visualizations during such short glances, in particular with such a limited display space of smartwatches. To shed light on this question, we conducted two perception studies that assessed the lower bounds of task time for a simple data comparison task. We tested three chart types common on smartwatches: bar charts, donut charts, and radial bar charts with three different data sizes: 7, 12, and 24 data values. In our first study, we controlled the differences of the two target bars to be compared, while the second study varied the difference randomly. For both studies, we found that participants performed the task on average in < 300 ms for the bar chart, < 220 ms for the donut chart, and in <1780 ms for the radial bar chart. Thresholds in the second study per chart type were on average 1.14-1.3x higher than in the first study. Our results show that bar and donut charts should be preferred on smartwatch displays when quick data comparisons are necessary.
C1 [Blascheck, Tanja; Isenberg, Petra] INRIA, Paris, France.
   [Besancon, Lonni; Bezerianos, Anastasia] Univ Paris Saclay, Paris, France.
   [Bezerianos, Anastasia] Univ Paris Sud, CNRS, INRIA, Paris, France.
   [Lee, Bongshin] Microsoft Res, Richmond, VA USA.
C3 Inria; Universite Paris Saclay; Universite Paris Saclay; Centre National
   de la Recherche Scientifique (CNRS); Inria
RP Blascheck, T (corresponding author), INRIA, Paris, France.
EM tanja.blascheck@inria.fr; lonni.besancon@gmail.com;
   anastasia.bezerianos@lri.fr; bongshin@microsoft.com;
   petra.isenberg@inria.fr
RI ; Besancon, Lonni/N-1856-2017
OI Blascheck, Tanja/0000-0003-4002-4499; Besancon,
   Lonni/0000-0002-7207-1276; Bezerianos, Anastasia/0000-0002-7142-2548;
   Isenberg, Petra/0000-0002-2948-6417
NR 51
TC 49
Z9 54
U1 0
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 630
EP 640
DI 10.1109/TVCG.2018.2865142
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000060
PM 30138911
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Favelier, G
   Faraj, N
   Summa, B
   Tierny, J
AF Favelier, Guillaume
   Faraj, Noura
   Summa, Brian
   Tierny, Julien
TI Persistence Atlas for Critical Point Variability in Ensembles
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Topological data analysis; scalar data; ensemble data
ID VISUALIZATION; UNCERTAINTY
AB This paper presents a new approach for the visualization and analysis of the spatial variability of features of interest represented by critical points in ensemble data. Our framework, called Persistence Atlas, enables the visualization of the dominant spatial patterns of critical points, along with statistics regarding their occurrence in the ensemble. The persistence atlas represents in the geometrical domain each dominant pattern in the form of a confidence map for the appearance of critical points. As a by-product, our method also provides 2-dimensional layouts of the entire ensemble, highlighting the main trends at a global level. Our approach is based on the new notion of Persistence Map, a measure of the geometrical density in critical points which leverages the robustness to noise of topological persistence to better emphasize salient features. We show how to leverage spectral embedding to represent the ensemble members as points in a low-dimensional Euclidean space, where distances between points measure the dissimilarities between critical point layouts and where statistical tasks, such as clustering, can be easily carried out. Further, we show how the notion of mandatory critical point can be leveraged to evaluate for each cluster confidence regions for the appearance of critical points. Most of the steps of this framework can be trivially parallelized and we show how to efficiently implement them. Extensive experiments demonstrate the relevance of our approach. The accuracy of the confidence regions provided by the persistence atlas is quantitatively evaluated and compared to a baseline strategy using an off-the-shelf clustering approach. We illustrate the importance of the persistence atlas in a variety of real-life datasets, where clear trends in feature layouts are identified and analyzed. We provide a lightweight VTK-based C++ implementation of our approach that can be used for reproduction purposes.
C1 [Favelier, Guillaume; Tierny, Julien] Sorbonne Univ, Paris, France.
   [Favelier, Guillaume; Tierny, Julien] CNRS, LIP6, Paris, France.
   [Faraj, Noura; Summa, Brian] Tulane Univ, New Orleans, LA 70118 USA.
C3 Sorbonne Universite; Sorbonne Universite; Centre National de la
   Recherche Scientifique (CNRS); Tulane University
RP Favelier, G (corresponding author), Sorbonne Univ, Paris, France.; Favelier, G (corresponding author), CNRS, LIP6, Paris, France.
EM guillaume.favelier@sorbonne-universite.fr; nfaraj@tulane.edu;
   bsumma@tulane.edu; julien.tierny@sorbonne-universite.fr
OI Summa, Brian/0000-0002-5794-3355
FU BPI grant "AVIDO" (PIA FSN2) [P112017-2661376/DOS0021427]; NSF [CRII
   1657020]; NSF/NIH [QuBBD 1664848]
FX This work is partially supported by the BPI grant "AVIDO" (PIA FSN2,
   reference P112017-2661376/DOS0021427), NSF CRII 1657020, and NSF/NIH
   QuBBD 1664848. We would like to thank the reviewers for their thoughtful
   remarks and suggestions. Julien Tierny would like to dedicate this paper
   to his son Otis.
NR 87
TC 21
Z9 22
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 1152
EP 1162
DI 10.1109/TVCG.2018.2864432
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000110
PM 30207954
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Köpp, W
   Weinkauf, T
AF Kopp, Wiebke
   Weinkauf, Tino
TI Temporal Treemaps: Static Visualization of Evolving Trees
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Terms Treemaps; Temporal trees
AB We consider temporally evolving trees with changing topology and data: tree nodes may persist for a time range, merge or split, and the associated data may change. Essentially, one can think of this as a time series of trees with a node correspondence per hierarchy level between consecutive time steps. Existing visualization approaches for such data include animated 2D treemaps, where the dynamically changing layout makes it difficult to observe the data in its entirety. We present a method to visualize this dynamic data in a static. nested, and space-filling visualization. This is based on two major contributions: First. the layout constitutes a graph drawing problem. We approach it for the entire time span at once using a combination of a heuristic and simulated annealing. Second, we propose a rendering that emphasizes the hierarchy through an adaption of the classic cushion treemaps. We showcase the wide range of applicability using data from feature tracking in time-dependent scalar fields. evolution of file system hierarchies. and world population.
C1 [Kopp, Wiebke; Weinkauf, Tino] KTH Royal Inst Technol, Stockholm, Sweden.
C3 Royal Institute of Technology
RP Köpp, W (corresponding author), KTH Royal Inst Technol, Stockholm, Sweden.
EM wiebkek@kth.se; weinkauf@kth.se
RI Koepp, Wiebke/LIX-8816-2024
FU Swedish Foundation for Strategic Research (SSF); Swedish e-Science
   Research Centre (SeRC)
FX This work was supported through grants from the Swedish Foundation for
   Strategic Research (SSF) and the Swedish e-Science Research Centre
   (SeRC). The presented concepts have been implemented in the Inviwo
   framework.
NR 38
TC 16
Z9 16
U1 1
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 534
EP 543
DI 10.1109/TVCG.2018.2865265
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000051
PM 30137007
OA Green Published
DA 2025-03-07
ER

PT J
AU Liu, SX
   Chen, CJ
   Lu, YF
   Ouyang, FX
   Wang, B
AF Liu, Shixia
   Chen, Changjian
   Lu, Yafeng
   Ouyang, Fangxin
   Wang, Bin
TI An Interactive Method to Improve Crowdsourced Annotations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Crowdsourcing; learning-from-crowds; interactive visualization; focus
   plus context
ID VISUAL ANALYTICS; VISUALIZATION; MATRIX; MACHINE
AB In order to effectively infer correct labels from noisy crowdsourced annotations, learning-from-crowds models have introduced expert validation. However, little research has been done on facilitating the validation procedure. In this paper, we propose an interactive method to assist experts in verifying uncertain instance labels and unreliable workers. Given the instance labels and worker reliability inferred from a learning-from-crowds model; candidate instances and workers are selected for expert validation. The influence of verified results is propagated to relevant instances and workers through the learning-from-crowds model. To facilitate the validation of annotations, we have developed a confusion visualization to indicate the confusing classes for further exploration, a constrained projection method to show the uncertain labels in context, and a scatter-plot-based visualization to illustrate worker reliability. The three visualizations are tightly integrated with the learning-from-crowds model to provide an iterative and progressive environment for data validation. Two case studies were conducted that demonstrate our approach offers an efficient method for validating and improving crowdsourced annotations.
C1 [Liu, Shixia; Chen, Changjian; Ouyang, Fangxin; Wang, Bin] Tsinghua Univ, Sch Software, Beijing, Peoples R China.
   [Lu, Yafeng] Arizona State Univ, Tempe, AZ 85287 USA.
C3 Tsinghua University; Arizona State University; Arizona State
   University-Tempe
RP Liu, SX (corresponding author), Tsinghua Univ, Sch Software, Beijing, Peoples R China.
EM shixia@tsinghua.edu.cn; ccj17@mails.tsinghua.edu.cn; Lu.Yafeng@asu.edu;
   oyfx15@mails.tsinghua.edu.cn; wangbins@tsinghua.edu.cn
RI Chen, Changjian/KBA-9462-2024; Liu, Shi-Xia/C-5574-2016
OI Liu, Shi-Xia/0000-0001-6104-4320; Chen, Changjian/0000-0003-2715-8839;
   Wang, Bin/0000-0002-5176-9202
FU National Key RAMP;D Program of China [SQ2018YFB100002]; National Natural
   Science Foundation of China [61761136020, 61672308]; Microsoft Research
   Asia
FX This research was funded by National Key R&D Program of China (No.
   SQ2018YFB100002), the National Natural Science Foundation of China (No.
   s 61761136020, 61672308), and Microsoft Research Asia. The authors would
   like to thank Liwang Zhou for implementing part of the
   learning-from-crowds algorithm and the anonymous reviewers for their
   valuable comments. B. Wang is the corresponding author.
NR 65
TC 49
Z9 59
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 235
EP 245
DI 10.1109/TVCG.2018.2864843
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000023
PM 30130224
DA 2025-03-07
ER

PT J
AU Srinivasan, A
   Drucker, SM
   Endert, A
   Stasko, J
AF Srinivasan, Arjun
   Drucker, Steven M.
   Endert, Alex
   Stasko, John
TI Augmenting Visualizations with Interactive Data Facts to Facilitate
   Interpretation and Communication
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Natural Language Generation; Mixed-initiative Interaction; Visualization
   Recommendation; Data-driven Communication
AB Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type. data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.
C1 [Srinivasan, Arjun; Endert, Alex; Stasko, John] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Drucker, Steven M.] Microsoft Res, Washington, DC USA.
C3 University System of Georgia; Georgia Institute of Technology; Microsoft
RP Srinivasan, A (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM arjun010@gatech.edu; sdrucker@microsoft.com; endert@gatech.edu;
   stasko@cc.gatech.edu
FU National Science Foundation [IIS-1717111]; DARPA [FA8750-17-2-0107]
FX We thank the anonymous reviewers for the detailed and helpful feedback
   on the article. This work was supported in part by the National Science
   Foundation grant IIS-1717111 and by DARPA FA8750-17-2-0107. The views
   and conclusions contained in this document are those of the authors and
   should not be interpreted as representing the official policies, either
   expressed or implied, of the U.S. Government.
NR 42
TC 116
Z9 139
U1 0
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 672
EP 681
DI 10.1109/TVCG.2018.2865145
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HD6IJ
UT WOS:000452640000064
PM 30136989
DA 2025-03-07
ER

PT J
AU Yang, YL
   Dwyer, T
   Jenny, B
   Marriott, K
   Cordeil, M
   Chen, HH
AF Yang, Yalong
   Dwyer, Tim
   Jenny, Bernhard
   Marriott, Kim
   Cordeil, Maxime
   Chen, Haohui
TI Origin-Destination Flow Maps in Immersive Environments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Origin-destination; Flow Map; Virtual Reality; Cartographic Information
   Visualisation; Immersive Analytics
ID VISUAL EXPLORATION; LAYOUT
AB Immersive virtual-and augmented-reality headsets can overlay a flat image against any surface or hang virtual objects in the space around the user. The technology is rapidly improving and may, in the long term, replace traditional flat panel displays in many situations. When displays are no longer intrinsically flat, how should we use the space around the user for abstract data visualisation? In this paper, we ask this question with respect to origin-destination flow data in a global geographic context. We report on the findings of three studies exploring different spatial encodings for flow maps. The first experiment focuses on different 2D and 3D encodings for flows on flat maps. We find that participants are significantly more accurate with raised flow paths whose height is proportional to flow distance but fastest with traditional straight line 2D flows. In our second and third experiment we compared flat maps, 3D globes and a novel interactive design we call MapsLink, involving a pair of linked flat maps. We find that participants took significantly more time with MapsLink than other flow maps while the 3D globe with raised flows was the fastest, most accurate, and most preferred method. Our work suggests that careful use of the third spatial dimension can resolve visual clutter in complex flow maps.
C1 [Yang, Yalong; Dwyer, Tim; Jenny, Bernhard; Marriott, Kim; Cordeil, Maxime] Monash Univ, Clayton, Vic, Australia.
   [Yang, Yalong; Chen, Haohui] CSIRO, Data61, Canberra, ACT, Australia.
C3 Monash University; Commonwealth Scientific & Industrial Research
   Organisation (CSIRO)
RP Yang, YL (corresponding author), Monash Univ, Clayton, Vic, Australia.; Yang, YL (corresponding author), CSIRO, Data61, Canberra, ACT, Australia.
EM yalong.yang@monash.edu; tim.dwyer@monash.edu; bernie.jenny@monash.edu;
   kim.marriott@monash.edu; max.cordeil@monash.edu; chen@data61.csiro.au
RI Chen, Haohui/T-8965-2018
OI Jenny, Bernhard/0000-0001-6101-6100; Dwyer, Tim/0000-0002-9076-9571;
   Cordeil, Maxime/0000-0002-9732-4874; Yang, Yalong/0000-0001-9414-9911
FU Australian Research Council [DP180100755]; Australian Government through
   the Department of Communications; Australian Research Council through
   the ICT Centre for Excellence Program
FX This research was supported under Australian Research Council's
   Discovery Projects funding scheme (project number DP180100755). Data61,
   CSIRO (formerly NICTA) is funded by the Australian Government through
   the Department of Communications and the Australian Research Council
   through the ICT Centre for Excellence Program. We would like to thank
   all of our user study participants for their time and feedback. We would
   also like to thank the reviewers for their valuable comments.
NR 65
TC 64
Z9 70
U1 0
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2019
VL 25
IS 1
BP 693
EP 703
DI 10.1109/TVCG.2018.2865192
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA HD6IJ
UT WOS:000452640000066
PM 30136995
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Park, D
   Drucker, SM
   Fernandez, R
   Elmqvist, N
AF Park, Deokgun
   Drucker, Steven M.
   Fernandez, Roland
   Elmqvist, Niklas
TI ATOM: A Grammar for Unit Visualizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization grammar; unit visualizations; declarative specification
ID INTERACTIVE VISUALIZATION; ANIMATED TRANSITIONS; VISUAL EXPLORATION;
   GRAPHICS; TRACKING; DESIGN; PLOTS; VEGA
AB Unit visualizations are a family of visualizations where every data item is represented by a unique visual mark-a visual unit-during visual encoding. For certain datasets and tasks, unit visualizations can provide more information, better match the user's mental model, and enable novel interactions compared to traditional aggregated visualizations. Current visualization grammars cannot fully describe the unit visualization family. In this paper, we characterize the design space of unit visualizations to derive a grammar that can express them. The resulting grammar is called ATOM, and is based on passing data through a series of layout operations that divide the output of previous operations recursively until the size and position of every data point can be determined. We evaluate the expressive power of the grammar by both using it to describe existing unit visualizations, as well as to suggest new unit visualizations.
C1 [Park, Deokgun; Elmqvist, Niklas] Univ Maryland, College Pk, MD 20742 USA.
   [Drucker, Steven M.; Fernandez, Roland] Microsoft Res, Redmond, WA 98052 USA.
C3 University System of Maryland; University of Maryland College Park;
   Microsoft
RP Park, D (corresponding author), Univ Maryland, College Pk, MD 20742 USA.
EM intuinno@umd.edu; sdrucker@microsoft.com; rfernandez@microsoft.com;
   elm@umd.edu
OI Elmqvist, Niklas/0000-0001-5805-5301; Drucker,
   Steven/0000-0002-5022-9343
FU U.S. National Institutes of Health (NIH) [R01GM114267]
FX We thank the anonymous reviewers for their helpful reviews. Deok Gun
   Park and Niklas Elmqvist were partially supported by U.S. National
   Institutes of Health (NIH) grant R01GM114267. Any opinions, findings,
   and conclusions or recommendations expressed in this article are those
   of the authors and do not necessarily reflect the views of the funding
   agencies.
NR 58
TC 53
Z9 53
U1 1
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2018
VL 24
IS 12
BP 3032
EP 3043
DI 10.1109/TVCG.2017.2785807
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA GZ0TW
UT WOS:000449079000003
PM 29990044
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Yoghourdjian, V
   Dwyer, T
   Klein, K
   Marriott, K
   Wybrow, M
AF Yoghourdjian, Vahan
   Dwyer, Tim
   Klein, Karsten
   Marriott, Kim
   Wybrow, Michael
TI Graph Thumbnails: Identifying and Comparing Multiple Graphs at a Glance
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Network visualisation; circle packing; k-core decomposition;
   k-connected; network identification; large networks
ID LAYOUT
AB We propose Graph Thumbnails, small icon-like visualisations of the high-level structure of network data. Graph Thumbnails are designed to be legible in small multiples to support rapid browsing within large graph corpora. Compared to existing graph-visualisation techniques our representation has several advantages: (1) the visualisation can be computed in linear time; (2) it is canonical in the sense that isomorphic graphs will always have identical thumbnails; and (3) it provides precise information about the graph structure. We report the results of two user studies. The first study compares Graph Thumbnails to node-link and matrix views for identifying similar graphs. The second study investigates the comprehensibility of the different representations. We demonstrate the usefulness of this representation for summarising the evolution of protein-protein interaction networks across a range of species.
C1 [Yoghourdjian, Vahan; Dwyer, Tim; Klein, Karsten; Marriott, Kim; Wybrow, Michael] Monash Univ, Clayton, Vic 3800, Australia.
   [Klein, Karsten] Univ Konstanz, D-78464 Constance, Germany.
C3 Monash University; University of Konstanz
RP Yoghourdjian, V (corresponding author), Monash Univ, Clayton, Vic 3800, Australia.
EM vahan.yoghourdjian@monash.edu; Tim.Dwyer@monash.edu;
   karsten.klein@uni-konstanz.de; Kim.Marriott@Monash.edu;
   Michael.Wybrow@Monash.edu
OI Dwyer, Tim/0000-0002-9076-9571; Wybrow, Michael/0000-0001-5536-7780
FU Australian Research Council [DP140100077]
FX This work was supported by the Australian Research Council Discovery
   Project grant DP140100077. We thank Jens Schmidt and Markus Chimani for
   sharing their knowledge on graph decompositions.
NR 53
TC 29
Z9 32
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2018
VL 24
IS 12
BP 3081
EP 3095
DI 10.1109/TVCG.2018.2790961
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GZ0TW
UT WOS:000449079000007
PM 29993949
DA 2025-03-07
ER

PT J
AU Lv, N
   Jiang, ZF
   Huang, Y
   Meng, XX
   Meenakshisundaram, G
   Peng, JL
AF Lv, Na
   Jiang, Zifei
   Huang, Yan
   Meng, Xiangxu
   Meenakshisundaram, Gopi
   Peng, Jingliang
TI Generic Content-Based Retrieval of Marker-Based Motion Capture Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Motion capture; content-based retrieval; minimal motion spanning tree;
   motion signature; biased discriminant analysis
ID PARAMETER-ESTIMATION; ACTION RECOGNITION
AB In this work, we propose an original scheme for generic content-based retrieval of marker-based motion capture data. It works on motion capture data of arbitrary subject types and arbitrary marker attachment and labelling conventions. Specifically, we propose a novel motion signature to statistically describe both the high-level and the low-level morphological and kinematic characteristics of a motion capture sequence, and conduct the content-based retrieval by computing and ordering the motion signature distance between the query and every item in the database. The distance between two motion signatures is computed by a weighted sum of differences in separate features contained in them. For maximum retrieval performance, we propose a method to pre-learn an optimal set of weights for each type of motion in the database through biased discriminant analysis, and adaptively choose a good set of weights for any given query at the run time. Excellence of the proposed scheme is experimentally demonstrated on various data sets and performance metrics.
C1 [Lv, Na] Univ Jinan, Dept Informat Sci & Technol, Jinan 250022, Shandong, Peoples R China.
   [Jiang, Zifei; Meng, Xiangxu; Peng, Jingliang] Shandong Univ, Sch Comp Sci & Technol, Jinan 250101, Shandong, Peoples R China.
   [Huang, Yan] Shenzhen Realis Multimedia Technol Co Ltd, Shenzhen 518000, Guangdong, Peoples R China.
   [Meenakshisundaram, Gopi] Univ Calif Irvine, Dept Comp Sci, Comp Sci, Irvine, CA 92697 USA.
   [Meenakshisundaram, Gopi] Univ Calif Irvine, Bren Sch Informat & Comp Sci, Irvine, CA 92697 USA.
C3 University of Jinan; Shandong University; University of California
   System; University of California Irvine; University of California
   System; University of California Irvine
RP Peng, JL (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Jinan 250101, Shandong, Peoples R China.
EM ise_lvn@ujn.edu.cn; jzf1990@qq.com; huang.yan74@gmail.com;
   mxx@sdu.edu.cn; gopi@ics.uci.edu; jingliap@gmail.com
FU National Natural Science Foundation of China [61472223, 61303083];
   Doctoral Research Foundation Project of University of Jinan [XBS1534]
FX This work is supported by the National Natural Science Foundation of
   China (Grants No.s 61472223 and 61303083) and Doctoral Research
   Foundation Project of University of Jinan (Grant No. XBS1534).
NR 44
TC 8
Z9 8
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2018
VL 24
IS 6
BP 1969
EP 1982
DI 10.1109/TVCG.2017.2702620
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE7EY
UT WOS:000431397300009
DA 2025-03-07
ER

PT J
AU Mendhurwar, K
   Gu, Q
   Mudur, S
   Popa, T
AF Mendhurwar, Kaustubha
   Gu, Qing
   Mudur, Sudhir
   Popa, Tiberiu
TI The Discriminative Power of Shape an Empirical Study in Time Series
   Matching
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Action recognition; gait authentication; shape awareness; time and shape
   correspondence; time series matching
ID MOTION; WALKING
AB Shape provides significant discriminating power in time series matching of visual or geometric data as required in many important applications in graphics and vision. The well established dynamic time warping (DTW) algorithm and its variants do this matching by determining a non-linear time mapping to minimise euclidean distances between corresponding time-warped points. However the shape of curves is not considered. In this paper, we present a new shape-aware algorithm which uses time and shape correspondence (TSC) at increasing levels of detail to define a similarity measure with an L0 norm to aggregate the results, making it robust to noise and missing data. The L0 norm is implicitly regularised using a shape-based error. Through extensive experiments we empirically show that our algorithm outperforms existing state of the art algorithms, works more effectively with high dimensional data, and handles noise and missing data better. We demonstrate its versatile applicability and comparative performance using a large in-house created gait data base, an action data base from Microsoft, exercise action data from a local company, a large public time series data base from University of California, Riverside and hand movement in quaternion stream data format.
C1 [Mendhurwar, Kaustubha; Gu, Qing; Mudur, Sudhir; Popa, Tiberiu] Concordia Univ, Dept Comp Sci & Software Engn, Montreal, PQ H4B 1R6, Canada.
C3 Concordia University - Canada
RP Mendhurwar, K (corresponding author), Concordia Univ, Dept Comp Sci & Software Engn, Montreal, PQ H4B 1R6, Canada.
EM k_mendhu@cs.concordia.ca; tsing.goo@gmail.com; mudur@cs.concordia.ca;
   tiperiu@gmail.com
OI Mendhurwar, Kaustubha/0000-0002-3022-7156
FU Natural Sciences and Engineering research Council (NSERC) of Canada;
   ENCS faculty of Concordia University, Montreal; NSERC
FX This research was funded in part by the Natural Sciences and Engineering
   research Council (NSERC) of Canada and ENCS faculty of Concordia
   University, Montreal. We acknowledge NSERC support for the project with
   the local company and thank the company for providing the data. All our
   extensive acquisition experiments were conducted at the Concordia
   PERFORM centre in the PML laboratory. We express our deep gratitude to
   Daniel Aponte and the Concordia PERFORM center for generous operational
   help and use of their Vicon Motion Capture system. We are also grateful
   to the nearly 120 anonymous subjects who let us capture their walks. We
   thank the reviewers for their constructive comments which has helped
   significantly improve the presentation of our work.
NR 42
TC 2
Z9 3
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2018
VL 24
IS 5
BP 1799
EP 1813
DI 10.1109/TVCG.2017.2691322
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA GE7EW
UT WOS:000431397100009
PM 28391198
DA 2025-03-07
ER

PT J
AU Miranda, F
   Lins, L
   Klosowski, JT
   Silva, CT
AF Miranda, Fabio
   Lins, Lauro
   Klosowski, James T.
   Silva, Claudio T.
TI TOPKUBE: A Rank-Aware Data Cube for Real-Time Exploration of
   Spatiotemporal Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Interactive visualization; data cube; top-K queries; rank merging
ID VISUALIZATION; QUERY
AB From economics to sports to entertainment and social media, ranking objects according to some notion of importance is a fundamental tool we humans use all the time to better understand our world. With the ever-increasing amount of user-generated content found online, "what's trending" is now a commonplace phrase that tries to capture the zeitgeist of the world by ranking the most popular microblogging hashtags in a given region and time. However, before we can understand what these rankings tell us about the world, we need to be able to more easily create and explore them, given the significant scale of today's data. In this paper, we describe the computational challenges in building a real-time visual exploratory tool for finding top-ranked objects; build on the recent work involving in-memory and rank-aware data cubes to propose TOPKUBE: a data structure that answers top-k queries up to one order of magnitude faster than the previous state of the art; demonstrate the usefulness of our methods using a set of real-world, publicly available datasets; and provide a new set of benchmarks for other researchers to validate their methods and compare to our own.
C1 [Miranda, Fabio; Silva, Claudio T.] NYU, 550 1St Ave, New York, NY 10003 USA.
   [Lins, Lauro; Klosowski, James T.] AT&T Labs, Florham Pk, NJ 07932 USA.
C3 New York University; AT&T
RP Miranda, F (corresponding author), NYU, 550 1St Ave, New York, NY 10003 USA.
EM fmiranda@nyu.edu; llins@research.att.com; jklosow@research.att.com;
   csilva@nyu.edu
OI Miranda, Fabio/0000-0001-8612-5805
FU Moore-Sloan Data Science Environment at NYU; NYU Tandon School of
   Engineering; NYU Center for Urban Science and Progress; ATT; IBM Faculty
   Award; NSF [CNS-1229185, CCF-1533564, CNS-1544753]; Division Of Computer
   and Network Systems; Direct For Computer & Info Scie & Enginr [1229185]
   Funding Source: National Science Foundation
FX This work was supported in part by the Moore-Sloan Data Science
   Environment at NYU, NYU Tandon School of Engineering, NYU Center for
   Urban Science and Progress, AT&T, IBM Faculty Award, NSF awards
   CNS-1229185, CCF-1533564 and CNS-1544753.
NR 34
TC 27
Z9 31
U1 0
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR
PY 2018
VL 24
IS 3
BP 1394
EP 1407
DI 10.1109/TVCG.2017.2671341
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FU0LA
UT WOS:000423541200014
PM 28221997
OA Bronze
DA 2025-03-07
ER

PT J
AU Wilkinson, L
AF Wilkinson, Leland
TI Visualizing Big Data Outliers through Distributed Aggregation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Outliers; Anomalies
ID MULTIVARIATE; MODEL
AB Visualizing outliers in massive datasets requires statistical pre-processing in order to reduce the scale of the problem to a size amenable to rendering systems like D3, Plotly or analytic systems like R or SAS. This paper presents a new algorithm, called hdoutliers, for detecting multidimensional outliers. It is unique for a) dealing with a mixture of categorical and continuous variables, b) dealing with big-p (many columns of data), c) dealing with big-n (many rows of data), d) dealing with outliers that mask other outliers, and e) dealing consistently with unidimensional and multidimensional datasets. Unlike ad hoc methods found in many machine learning papers, hdoutliers is based on a distributional model that allows outliers to be tagged with a probability. This critical feature reduces the likelihood of false discoveries.
C1 [Wilkinson, Leland] H2O Ai, Mountain View, CA 94043 USA.
   [Wilkinson, Leland] UIC, Comp Sci, Chicago, IL 60607 USA.
C3 University of Illinois System; University of Illinois Chicago
RP Wilkinson, L (corresponding author), H2O Ai, Mountain View, CA 94043 USA.; Wilkinson, L (corresponding author), UIC, Comp Sci, Chicago, IL 60607 USA.
EM leland.wilkinson@gmail.com
FU NSF/MIS [DMS-FODAVA-0808860]
FX This work was supported by NSF/MIS Supplement to grant
   DMS-FODAVA-0808860. All figures and analyses were computed using SYSTAT
   [78], Skytree Adviser [79], and R[58]. The R package hdout liers,
   developed by Chris Fraley, is available in GRAN. The distributed version
   of the algorithm, developed by Leland Wilkinson and Arno Candel, is
   available at http://www.H2O.ai
NR 78
TC 47
Z9 54
U1 3
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 256
EP 266
DI 10.1109/TVCG.2017.2744685
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400027
PM 28866555
DA 2025-03-07
ER

PT J
AU Wongsuphasawat, K
   Smilkov, D
   Wexler, J
   Wilson, J
   Mané, D
   Fritz, D
   Krishnan, D
   Viégas, FB
   Wattenberg, M
AF Wongsuphasawat, Kanit
   Smilkov, Daniel
   Wexler, James
   Wilson, Jimbo
   Mane, Dandelion
   Fritz, Doug
   Krishnan, Dilip
   Viegas, Fernanda B.
   Wattenberg, Martin
TI Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE Neural Network; Graph Visualization; Dataflow Graph; Clustered Graph
ID LAYOUT
AB We present a design study of the TensorFlow Graph Visualizer, part of the TensorFlow machine intelligence platform. This tool helps users understand complex machine learning architectures by visualizing their underlying dataflow graphs. The tool works by applying a series of graph transformations that enable standard layout techniques to produce a legible interactive diagram. To declutter the graph. we decouple non-critical nodes from the layout. To provide an overview, we build a clustered graph using the hierarchical structure annotated in the source code. To support exploration of nested structure on demand. we perform edge bundling to enable stable and responsive cluster expansion. Finally, we detect and highlight repeated structures to emphasize a model's modular composition. To demonstrate the utility of the visualizer, we describe example usage scenarios and report user feedback. Overall, users find the visualizer useful for understanding, debugging, and sharing the structures of their models.
C1 [Wongsuphasawat, Kanit] Univ Washington, Paul G Allen Sch Comp & Engn, Seattle, WA 98195 USA.
   [Smilkov, Daniel; Wexler, James; Wilson, Jimbo; Mane, Dandelion; Fritz, Doug; Krishnan, Dilip; Viegas, Fernanda B.; Wattenberg, Martin] Google Res, Mountain View, CA USA.
C3 University of Washington; University of Washington Seattle; Google
   Incorporated
RP Wongsuphasawat, K (corresponding author), Univ Washington, Paul G Allen Sch Comp & Engn, Seattle, WA 98195 USA.
EM kanitw@uw.edu; smilkov@google.com; Jiwexler@google.com;
   jimbo@google.com; dougfritz@google.com; dilipkay@google.com;
   wattenberg@google.com
OI Wattenberg, Martin/0000-0003-0904-4862
NR 52
TC 198
Z9 246
U1 0
U2 89
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 1
EP 12
DI 10.1109/TVCG.2017.2744878
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400003
PM 28866562
DA 2025-03-07
ER

PT J
AU Xia, JZ
   Ye, FJ
   Chen, W
   Wang, YS
   Chen, WF
   Ma, YX
   Tung, AKH
AF Xia, Jiazhi
   Ye, Fenjin
   Chen, Wei
   Wang, Yusi
   Chen, Weifeng
   Ma, Yuxin
   Tung, Anthony K. H.
TI LDSScanner: Exploratory Analysis of Low-Dimensional Structures in
   High-Dimensional Datasets
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 01-06, 2017
CL Phoenix, AZ
SP IEEE
DE High-dimensional data; low-dimensional structure; subspace; manifold;
   visual exploration
ID VISUAL EXPLORATION; VISUALIZATION; REDUCTION; METRICS
AB Many approaches for analyzing a high-dimensional dataset assume that the dataset contains specific structures, e.g., clusters in linear subspaces or non-linear manifolds. This yields a trial-and-error process to verify the appropriate model and parameters. This paper contributes an exploratory interface that supports visual identification of low-dimensional structures in a high-dimensional dataset, and facilitates the optimized selection of data models and configurations. Our key idea is to abstract a set of global and local feature descriptors from the neighborhood graph-based representation of the latent low-dimensional structure, such as pairwise geodesic distance (GD) among points and pairwise local tangent space divergence (LTSD) among pointwise local tangent spaces (LTS). We propose a new LTSD-GD view, which is constructed by mapping LTSD and GD to the x axis and y axis using 1D multidimensional scaling, respectively. Unlike traditional dimensionality reduction methods that preserve various kinds of distances among points, the LTSD-GD view presents the distribution of pointwise LTS (x axis) and the variation of LTS in structures (the combination of x axis and y axis). We design and implement a suite of visual tools for navigating and reasoning about intrinsic structures of a high-dimensional dataset. Three case studies verify the effectiveness of our approach.
C1 [Xia, Jiazhi; Ye, Fenjin; Wang, Yusi] Cent South Univ, Changsha, Hunan, Peoples R China.
   [Chen, Wei; Ma, Yuxin] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
   [Chen, Weifeng] Zhejiang Univ Finance & Econ, Hangzhou, Zhejiang, Peoples R China.
   [Tung, Anthony K. H.] Natl Univ Singapore, Singapore, Singapore.
C3 Central South University; Zhejiang University; Zhejiang University of
   Finance & Economics; National University of Singapore
RP Chen, W (corresponding author), Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.
EM xiajiazhi@csu.edu.cn; yefenjin@csu.edu.cn; chenwei@cad.zju.edu.cn;
   yswang@csu.edu.cn; cwf818@gmail.com; mayuxin@zju.edu.cn;
   atung@comp.nus.edu.sg
RI MA, Yuxin/AAG-8630-2020; Chen, Wei/AAR-9817-2020
OI Tung, Anthony K. H./0000-0001-7300-6196; Tung, Anthony K.
   H./0000-0002-5125-855X; /0009-0006-1337-808X; Ye,
   Fenjin/0000-0002-1478-1544
FU National Science Foundation of China [61309009, 61422211]; National 973
   Program of China [2015CB352503]; Major Program of National Natural
   Science Foundation of China [61232012]; Open Project Program of the
   State Key Lab of CADCG [A1710]
FX This research is partially supported by National Science Foundation of
   China (61309009, 61422211), National 973 Program of China
   (2015CB352503), Major Program of National Natural Science Foundation of
   China (61232012), and Open Project Program of the State Key Lab of
   CAD&CG (A1710).
NR 44
TC 56
Z9 62
U1 2
U2 25
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2018
VL 24
IS 1
BP 236
EP 245
DI 10.1109/TVCG.2017.2744098
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FQ0IM
UT WOS:000418038400025
PM 28866522
DA 2025-03-07
ER

PT J
AU Gaffary, Y
   Le Gouis, B
   Marchal, M
   Argelaguet, F
   Arnaldi, B
   Lécuyer, A
AF Gaffary, Yoren
   Le Gouis, Benoit
   Marchal, Maud
   Argelaguet, Ferran
   Arnaldi, Bruno
   Lecuyer, Anatole
TI AR Feels "Softer" than VR: Haptic Perception of Stiffness in Augmented
   versus Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY SEP 09-13, 2017
CL Nantes, FRANCE
SP IEEE
DE Augmented Reality; Virtual Reality; Haptic; Perception; Psychophysical
   Study
ID ENVIRONMENTS; INTEGRATION; VISION
AB Does it feel the same when you touch an object in Augmented Reality (AR) or in Virtual Reality (VR)? In this paper we study and compare the haptic perception of stiffness of a virtual object in two situations: (1) a purely virtual environment versus (2) a real and augmented environment. We have designed an experimental setup based on a Microsoft HoloLens and a haptic force-feedback device, enabling to press a virtual piston, and compare its stiffness successively in either Augmented Reality (the virtual piston is surrounded by several real objects all located inside a cardboard box) or in Virtual Reality (the same virtual piston is displayed in a fully virtual scene composed of the same other objects). We have conducted a psychophysical experiment with 12 participants. Our results show a surprising bias in perception between the two conditions. The virtual piston is on average perceived stiffer in the VR condition compared to the AR condition. For instance, when the piston had the same stiffness in AR and VR, participants would select the VR piston as the stiffer one in 60% of cases. This suggests a psychological effect as if objects in AR would feel "softer" than in pure VR. Taken together, our results open new perspectives on perception in AR versus VR, and pave the way to future studies aiming at characterizing potential perceptual biases.
C1 [Gaffary, Yoren; Argelaguet, Ferran; Lecuyer, Anatole] IRISA, Inria, Rennes, France.
   [Le Gouis, Benoit; Marchal, Maud; Arnaldi, Bruno] IRISA, INSA Rennes, Rennes, France.
C3 Inria; Universite de Rennes; Institut National des Sciences Appliquees
   de Rennes
RP Gaffary, Y (corresponding author), IRISA, Inria, Rennes, France.
EM yoren.gaffary@inria.fr; benoit.legouis@inria.fr; maud.marchal@inria.fr;
   ferran.argelaguet@inria.fr; bruno.arnaldi@inria.fr;
   anatole.lecuyer@inria.fr
OI Bruno, ARNALDI/0000-0002-2868-8826
NR 22
TC 38
Z9 41
U1 3
U2 76
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2017
VL 23
IS 11
BP 2372
EP 2377
DI 10.1109/TVCG.2017.2735078
PG 6
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA FI7XT
UT WOS:000412214000003
PM 28809699
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Brehmer, M
   Lee, B
   Bach, B
   Riche, NH
   Munzner, T
AF Brehmer, Matthew
   Lee, Bongshin
   Bach, Benjamin
   Riche, Nathalie Henry
   Munzner, Tamara
TI Timelines Revisited: A Design Space and Considerations for Expressive
   Storytelling
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Timelines; storytelling; narrative visualization; design space; animated
   transitions
ID ANIMATED TRANSITIONS; VISUALIZATION; SEQUENCE; TIME
AB There are many ways to visualize event sequences as timelines. In a storytelling context where the intent is to convey multiple narrative points, a richer set of timeline designs may be more appropriate than the narrow range that has been used for exploratory data analysis by the research community. Informed by a survey of 263 timelines, we present a design space for storytelling with timelines that balances expressiveness and effectiveness, identifying 14 design choices characterized by three dimensions: representation, scale, and layout. Twenty combinations of these choices are viable timeline designs that can be matched to different narrative points, while smooth animated transitions between narrative points allow for the presentation of a cohesive story, an important aspect of both interactive storytelling and data videos. We further validate this design space by realizing the full set of viable timeline designs and transitions in a proof-of-concept sandbox implementation that we used to produce seven example timeline stories. Ultimately, this work is intended to inform and inspire the design of future tools for storytelling with timelines.
C1 [Brehmer, Matthew; Lee, Bongshin; Riche, Nathalie Henry] Microsoft Res, Redmond, WA 98052 USA.
   [Bach, Benjamin] Harvard Univ, Cambridge, MA 02138 USA.
   [Munzner, Tamara] Univ British Columbia, Vancouver, BC V6T 1Z4, Canada.
C3 Microsoft; Harvard University; University of British Columbia
RP Brehmer, M (corresponding author), Microsoft Res, Redmond, WA 98052 USA.
EM mabreheme@microsoft.com; bongshin@microsoft.com;
   benj.bach@googlemail.com; nath@microsoft.com; tmm@cs.ubc.ca
RI Munzner, Tamara/HKP-2536-2023
OI Munzner, Tamara/0000-0002-3294-3869
NR 91
TC 116
Z9 143
U1 4
U2 47
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2017
VL 23
IS 9
BP 2151
EP 2164
DI 10.1109/TVCG.2016.2614803
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA FC7XA
UT WOS:000407054600009
PM 28113509
DA 2025-03-07
ER

PT J
AU Miao, HC
   Mistelbauer, G
   Karimov, A
   Alansary, A
   Davidson, A
   Lloyd, DFA
   Damodaram, M
   Story, L
   Hutter, J
   Hajnal, JV
   Rutherford, M
   Preim, B
   Kainz, B
   Gröller, ME
AF Miao, Haichao
   Mistelbauer, Gabriel
   Karimov, Alexey
   Alansary, Amir
   Davidson, Alice
   Lloyd, David F. A.
   Damodaram, Mellisa
   Story, Lisa
   Hutter, Jana
   Hajnal, Joseph V.
   Rutherford, Mary
   Preim, Bernhard
   Kainz, Bernhard
   Groeller, M. Eduard
TI Placenta Maps: In Utero Placental Health Assessment of the Human Fetus
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE Pacific Visualization Symposium (IEEE PacificVis)
CY APR 18-21, 2017
CL Seoul Natl Univ, Seoul, SOUTH KOREA
SP IEEE, IEEE Comp Soc Visualizat & Graph Tech Comm, IEEE Comp Soc
HO Seoul Natl Univ
DE Placenta; fetal; flattening; structure-aware slicing; peeling
ID DIAGNOSTIC-CRITERIA; FETAL; VISUALIZATION; AXIS
AB The human placenta is essential for the supply of the fetus. To monitor the fetal development, imaging data is acquired using (US). Although it is currently the gold-standard in fetal imaging, it might not capture certain abnormalities of the placenta. (MRI) is a safe alternative for the in utero examination while acquiring the fetus data in higher detail. Nevertheless, there is currently no established procedure for assessing the condition of the placenta and consequently the fetal health. Due to maternal respiration and inherent movements of the fetus during examination, a quantitative assessment of the placenta requires fetal motion compensation, precise placenta segmentation and a standardized visualization, which are challenging tasks. Utilizing advanced motion compensation and automatic segmentation methods to extract the highly versatile shape of the placenta, we introduce a novel visualization technique that presents the fetal and maternal side of the placenta in a standardized way. Our approach enables physicians to explore the placenta even in utero. This establishes the basis for a comparative assessment of multiple placentas to analyze possible pathologic arrangements and to support the research and understanding of this vital organ. Additionally, we propose a three-dimensional structure-aware surface slicing technique in order to explore relevant regions inside the placenta. Finally, to survey the applicability of our approach, we consulted clinical experts in prenatal diagnostics and imaging. We received mainly positive feedback, especially the applicability of our technique for research purposes was appreciated.
C1 [Miao, Haichao; Karimov, Alexey; Groeller, M. Eduard] TU Wien, Inst Comp Graph & Algorithms, A-1040 Vienna, Austria.
   [Groeller, M. Eduard] VRVis Res Ctr, A-1220 Vienna, Austria.
   [Mistelbauer, Gabriel; Preim, Bernhard] Otto Von Guericke Univ, Dept Simulat & Graph, D-39106 Magdeburg, Germany.
   [Alansary, Amir; Kainz, Bernhard] Imperial Coll, Dept Comp, London SW7 2AZ, England.
   [Davidson, Alice; Lloyd, David F. A.; Damodaram, Mellisa; Story, Lisa; Hutter, Jana; Hajnal, Joseph V.; Rutherford, Mary; Kainz, Bernhard] Kings Coll London, London WC2R 2LS, England.
C3 Technische Universitat Wien; Otto von Guericke University; Imperial
   College London; University of London; King's College London
RP Miao, HC (corresponding author), TU Wien, Inst Comp Graph & Algorithms, A-1040 Vienna, Austria.
EM miao@cg.tuwien.ac.at
RI Miao, Haichao/HNJ-6239-2023; Lloyd, David F A/JAC-4361-2023; Gröller,
   Eduard/AAH-2111-2020; Story, Lisa/M-1064-2019; Hutter, Jana/J-6413-2019;
   Mistelbauer, Gabriel/JVY-7873-2024; Preim, Bernhard/AAF-6565-2021;
   Hajnal, Joseph/P-6251-2018; Kainz, Bernhard/H-3416-2016
OI Hutter, Jana/0000-0003-3476-3500; Lloyd, David F A/0000-0003-1759-6106;
   Story, Lisa/0000-0001-9328-9592; rutherford, mary/0000-0003-3361-1337;
   Miao, Haichao/0000-0001-6580-2918; Hajnal, Joseph/0000-0002-2690-5495;
   Mistelbauer, Gabriel/0000-0003-2333-5404; Kainz,
   Bernhard/0000-0002-7813-5023
FU Vienna Science and Technology Fund (WWTF) [VRG11-010]; EC Marie Curie
   Career Integration [PCIG13-GA-2013-618680]; Nvidia; EPSRC
   [EP/N024494/1]; National Institute for Health Research (NIHR) Biomedical
   Research Centre based at Guy's and St Thomas' NHS Foundation Trust;
   King's College London; EPSRC [EP/N024494/1] Funding Source: UKRI; MRC
   [MC_U120088465, MR/K006355/1] Funding Source: UKRI
FX This project has been supported by the Vienna Science and Technology
   Fund (WWTF) through project VRG11-010, by EC Marie Curie Career
   Integration Grant through project PCIG13-GA-2013-618680, by Nvidia
   (Tesla K40 donation), by the EPSRC award EP/N024494/1, and by the
   National Institute for Health Research (NIHR) Biomedical Research Centre
   based at Guy's and St Thomas' NHS Foundation Trust and King's College
   London. The views expressed are those of the author(s) and not
   necessarily those of the NHS, the NIHR or the Department of Health.
NR 34
TC 16
Z9 17
U1 0
U2 19
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2017
VL 23
IS 6
BP 1612
EP 1623
DI 10.1109/TVCG.2017.2674938
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA ET8DC
UT WOS:000400527500005
PM 28252405
DA 2025-03-07
ER

PT J
AU Fang, H
   Walton, S
   Delahaye, E
   Harris, J
   Storchak, DA
   Chen, M
AF Fang, H.
   Walton, S.
   Delahaye, E.
   Harris, J.
   Storchak, D. A.
   Chen, M.
TI Categorical Colormap Optimization with Visualization Case Studies
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Color; categorical colormap; optimization; seismological data
   visualization; London tube map
ID VISUAL-SEARCH; UNIVARIATE; SEQUENCES; MAPS
AB Mapping a set of categorical values to different colors is an elementary technique in data visualization. Users of visualization software routinely rely on the default colormaps provided by a system, or colormaps suggested by software such as ColorBrewer. In practice, users often have to select a set of colors in a semantically meaningful way (e.g., based on conventions, color metaphors, and logological associations), and consequently would like to ensure their perceptual differentiation is optimized. In this paper, we present an algorithmic approach for maximizing the perceptual distances among a set of given colors. We address two technical problems in optimization, i.e., (i) the phenomena of local maxima that halt the optimization too soon, and (ii) the arbitrary reassignment of colors that leads to the loss of the original semantic association. We paid particular attention to different types of constraints that users may wish to impose during the optimization process. To demonstrate the effectiveness of this work, we tested this technique in two case studies. To reach out to a wider range of users, we also developed a web application called Colourmap Hospital.
C1 [Fang, H.; Walton, S.; Chen, M.] Univ Oxford, Oxford OX1 2JD, England.
   [Fang, H.; Delahaye, E.; Harris, J.; Storchak, D. A.] Int Seismol Ctr, Thatcham, England.
   [Fang, H.] Edge Hill Univ, Ormskirk, England.
C3 University of Oxford; Edge Hill University
RP Fang, H (corresponding author), Univ Oxford, Oxford OX1 2JD, England.; Fang, H (corresponding author), Int Seismol Ctr, Thatcham, England.; Fang, H (corresponding author), Edge Hill Univ, Ormskirk, England.
EM hui.fang@edgehill.ac.uk; simon.walton@oerc.ox.ac.uk; emily@isc.ac.uk;
   james@isc.ac.uk; dmitry@isc.ac.uk; min.chen@oerc.ox.ac.uk
OI Fang, Hui/0000-0001-9365-7420
NR 53
TC 23
Z9 27
U1 1
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 871
EP 880
DI 10.1109/TVCG.2016.2599214
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600090
PM 27875201
OA Green Submitted, Green Published, Green Accepted
DA 2025-03-07
ER

PT J
AU Glueck, M
   Gvozdik, A
   Chevalier, F
   Khan, A
   Brudno, M
   Wigdor, D
AF Glueck, Michael
   Gvozdik, Alina
   Chevalier, Fanny
   Khan, Azam
   Brudno, Michael
   Wigdor, Daniel
TI PhenoStacks: Cross-Sectional Cohort Phenotype Comparison Visualizations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Cross-sectional cohort analysis; Phenotypes; Human Phenotype Ontology
   (HPO)
ID ONTOLOGY; PLATFORM; TOOLS
AB Cross-sectional phenotype studies are used by genetics researchers to better understand how phenotypes vary across patients with genetic diseases, both within and between cohorts. Analyses within cohorts identify patterns between phenotypes and patients (e. g., co-occurrence) and isolate special cases (e. g., potential outliers). Comparing the variation of phenotypes between two cohorts can help distinguish how different factors affect disease manifestation (e. g., causal genes, age of onset, etc.). PhenoStacks is a novel visual analytics tool that supports the exploration of phenotype variation within and between cross-sectional patient cohorts. By leveraging the semantic hierarchy of the Human Phenotype Ontology, phenotypes are presented in context, can be grouped and clustered, and are summarized via overviews to support the exploration of phenotype distributions. The design of PhenoStacks was motivated by formative interviews with genetics researchers: we distil high-level tasks, present an algorithm for simplifying ontology topologies for visualization, and report the results of a deployment evaluation with four expert genetics researchers. The results suggest that PhenoStacks can help identify phenotype patterns, investigate data quality issues, and inform data collection design.
C1 [Glueck, Michael; Gvozdik, Alina; Brudno, Michael; Wigdor, Daniel] Univ Toronto, Toronto, ON M5S 1A1, Canada.
   [Gvozdik, Alina; Khan, Azam] Autodesk Res, San Francisco, CA 94111 USA.
   [Chevalier, Fanny] Inria, Le Chesnay, France.
   [Brudno, Michael] Hosp Sick Children Toronto, Toronto, ON, Canada.
C3 University of Toronto; Autodesk, Inc.; Inria; University of Toronto;
   Hospital for Sick Children (SickKids)
RP Glueck, M (corresponding author), Univ Toronto, Toronto, ON M5S 1A1, Canada.
EM mglueck@dgp.toronto.edu; alina.gvozdik@mail.utoronto.ca;
   fanny.chevalier@inria.fa; azam.khan@autodesk.com; brudno@cs.toronto.edu;
   daniel@dgp.toronto.edu
RI Khan, Azam/JEF-7682-2023; Glueck, Michael/AAK-3015-2020
OI Khan, Azam/0000-0002-1816-5714
FU Genome Canada; Ontario Genomics through a Bioinformatics/Computational
   Biology; CPER Nord-Pas de Calais / FEDER DATA Advanced data science and
   technologies
FX This work was partially funded by Genome Canada and Ontario Genomics
   through a Bioinformatics/Computational Biology grant to Dr. Brudno and
   by a CPER Nord-Pas de Calais / FEDER DATA Advanced data science and
   technologies grant to Inria Lille Nord-Europe. A special thanks to
   Michelle Annett, Hali Larsen, Peter Hamilton, John Hancock, the
   researchers who participated in our studies, and the anonymous reviewers
   for their thoughtful feedback.
NR 44
TC 20
Z9 25
U1 0
U2 7
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 191
EP 200
DI 10.1109/TVCG.2016.2598469
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600022
PM 27514055
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Hermosilla, P
   Estrada, J
   Guallar, V
   Ropinski, T
   Vinacua, A
   Vázquez, PP
AF Hermosilla, Pedro
   Estrada, Jorge
   Guallar, Victor
   Ropinski, Timo
   Vinacua, Alvar
   Vazquez, Pere-Pau
TI Physics-based Visual Characterization of Molecular Interaction Forces
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Molecular visualization; binding analysis
ID AMBIENT OCCLUSION; VISUALIZATION; DYNAMICS; DIAGRAMS; BINDING;
   GENERATION; CRYSTAL; SYSTEMS; SCALE
AB Molecular simulations are used in many areas of biotechnology, such as drug design and enzyme engineering. Despite the development of automatic computational protocols, analysis of molecular interactions is still a major aspect where human comprehension and intuition are key to accelerate, analyze, and propose modifications to the molecule of interest. Most visualization algorithms help the users by providing an accurate depiction of the spatial arrangement: the atoms involved in inter-molecular contacts. There are few tools that provide visual information on the forces governing molecular docking. However, these tools, commonly restricted to close interaction between atoms, do not consider whole simulation paths, long-range distances and, importantly, do not provide visual cues for a quick and intuitive comprehension of the energy functions (modeling intermolecular interactions) involved. In this paper, we propose visualizations designed to enable the characterization of interaction forces by taking into account several relevant variables such as molecule-ligand distance and the energy function, which is essential to understand binding affinities. We put emphasis on mapping molecular docking paths obtained from Molecular Dynamics or Monte Carlo simulations, and provide time-dependent visualizations for different energy components and particle resolutions: atoms, groups or residues. The presented visualizations have the potential to support domain experts in a more efficient drug or enzyme design process.
C1 [Hermosilla, Pedro] ViRVIG Grp, Barcelona, Spain.
   [Hermosilla, Pedro; Estrada, Jorge; Guallar, Victor] Barcelona Supercomp Ctr, Barcelona, Spain.
   [Ropinski, Timo] Univ Ulm, Visual Comp Grp, D-89069 Ulm, Germany.
   [Vinacua, Alvar; Vazquez, Pere-Pau] UPC Barcelona, ViRVIG Grp, Barcelona, Spain.
C3 Universitat Politecnica de Catalunya; Barcelona Supercomputer Center
   (BSC-CNS); Ulm University
RP Hermosilla, P (corresponding author), ViRVIG Grp, Barcelona, Spain.; Hermosilla, P (corresponding author), Barcelona Supercomp Ctr, Barcelona, Spain.
EM pedro.hermosilla@cgstarad.com; jorge.estrada@bsc.es;
   victorguallar@bsc.es; timo.ropinski@uni-ulm.de; pere.pau@cs.upc.edu;
   alvar@cs.upc.edu
RI Guallar, Victor/B-1579-2013; Vázquez, Pere-Pau/HTP-9691-2023; Vinacua,
   Àlvar/JAN-9485-2023; Vazquez, Pere-Pau/L-4697-2014; Vinacua,
   Alvar/K-9392-2014
OI Ropinski, Timo/0000-0002-7857-5512; Vazquez,
   Pere-Pau/0000-0003-4638-4065; Vinacua, Alvar/0000-0001-8984-4311
FU Spanish Ministerio de Economia y Competitividad [TIN2014-52211-C2-1-R];
   EU FEDER funds
FX This project has been supported by TIN2014-52211-C2-1-R by the Spanish
   Ministerio de Economia y Competitividad with EU FEDER funds. We thank
   EAPM group members (BSC) and especially Sandra Acebes, for help in data
   preparation.
NR 52
TC 17
Z9 17
U1 0
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 731
EP 740
DI 10.1109/TVCG.2016.2598825
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600076
PM 27875187
OA Green Published, Green Submitted
DA 2025-03-07
ER

PT J
AU Meuschke, M
   Voss, S
   Beuing, O
   Preim, B
   Lawonn, K
AF Meuschke, Monique
   Voss, Samuel
   Beuing, Oliver
   Preim, Bernhard
   Lawonn, Kai
TI Combined Visualization of Vessel Deformation and Hemodynamics in
   Cerebral Aneurysms
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Medical visualizations; aneurysms; blood flow; wall thickness; wall
   deformation; projections
ID BLOOD-FLOW; INTRACRANIAL ANEURYSMS; IRRAS PROJECT; RUPTURE; EXPLORATION;
   RISK; DYNAMICS; MAPS
AB We present the first visualization tool that combines patient-specific hemodynamics with information about the vessel wall deformation and wall thickness in cerebral aneurysms. Such aneurysms bear the risk of rupture, whereas their treatment also carries considerable risks for the patient. For the patient-specific rupture risk evaluation and treatment analysis, both morphological and hemodynamic data have to be investigated. Medical researchers emphasize the importance of analyzing correlations between wall properties such as the wall deformation and thickness, and hemodynamic attributes like the Wall Shear Stress and near-wall flow. Our method uses a linked 2.5D and 3D depiction of the aneurysm together with blood flow information that enables the simultaneous exploration of wall characteristics and hemodynamic attributes during the cardiac cycle. We thus offer medical researchers an effective visual exploration tool for aneurysm treatment risk assessment. The 2.5D view serves as an overview that comprises a projection of the vessel surface to a 2D map, providing an occlusion-free surface visualization combined with a glyph-based depiction of the local wall thickness. The 3D view represents the focus upon which the data exploration takes place. To support the time-dependent parameter exploration and expert collaboration, a camera path is calculated automatically, where the user can place landmarks for further exploration of the properties. We developed a GPU-based implementation of our visualizations with a flexible interactive data exploration mechanism. We designed our techniques in collaboration with domain experts, and provide details about the evaluation.
C1 [Meuschke, Monique; Voss, Samuel; Beuing, Oliver; Preim, Bernhard] Univ Magdeburg, D-39106 Magdeburg, Germany.
   [Meuschke, Monique; Voss, Samuel; Beuing, Oliver; Preim, Bernhard] Res Campus STIMULATE, D-39106 Magdeburg, Germany.
   [Lawonn, Kai] Univ Koblenz Landau, Mainz, Germany.
C3 Otto von Guericke University; University of Koblenz & Landau
RP Meuschke, M (corresponding author), Univ Magdeburg, D-39106 Magdeburg, Germany.; Meuschke, M (corresponding author), Res Campus STIMULATE, D-39106 Magdeburg, Germany.
EM meuschke@isg.cs.uni-magdeburg.de; samueLvoss@ovgu.de;
   oliverbeuing@medovgu.de; bernhard@isg.cs.uni-magdeburg.de;
   lawonn@uni-koblenz.de
RI Preim, Bernhard/AAF-6565-2021
OI Voss, Samuel/0000-0002-9757-7564
FU BMBF [STIMULATE: 13GW0095A]
FX This work was partially funded by the BMBF (STIMULATE: 13GW0095A). The
   authors like to thank Uta Preim and Philipp Berg for the fruitful
   discussions on these and related topics.
NR 47
TC 24
Z9 28
U1 0
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 761
EP 770
DI 10.1109/TVCG.2016.2598795
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600079
PM 27875190
DA 2025-03-07
ER

PT J
AU Zhao, J
   Glueck, M
   Breslav, S
   Chevalier, F
   Khan, A
AF Zhao, Jian
   Glueck, Michael
   Breslav, Simon
   Chevalier, Fanny
   Khan, Azam
TI Annotation Graphs: A Graph-Based Visualization for Meta-Analysis of Data
   based on User-Authored Annotations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE VIS Conference
CY OCT 23-28, 2016
CL Baltimore, MD
SP IEEE
DE Externalization; user-authored annotation; exploratory sequential data
   analysis; graph-based visualization
ID COMMUNICATION
AB User-authored annotations of data can support analysts in the activity of hypothesis generation and sensemaking, where it is not only critical to document key observations, but also to communicate insights between analysts. We present annotation graphs, a dynamic graph visualization that enables meta-analysis of data based on user-authored annotations. The annotation graph topology encodes annotation semantics, which describe the content of and relations between data selections, comments, and tags. We present a mixed-initiative approach to graph layout that integrates an analyst's manual manipulations with an automatic method based on similarity inferred from the annotation semantics. Various visual graph layout styles reveal different perspectives on the annotation semantics. Annotation graphs are implemented within C8, a system that supports authoring annotations during exploratory analysis of a dataset. We apply principles of Exploratory Sequential Data Analysis (ESDA) in designing C8, and further link these to an existing task typology in the visualization literature. We develop and evaluate the system through an iterative user-centered design process with three experts, situated in the domain of analyzing HCI experiment data. The results suggest that annotation graphs are effective as a method of visually extending user-authored annotations to data meta-analysis for discovery and organization of ideas.
C1 [Zhao, Jian; Glueck, Michael; Breslav, Simon; Khan, Azam] Autodesk Res, Toronto, ON, Canada.
   [Chevalier, Fanny] INRIA, Rocquencourt, France.
C3 Autodesk, Inc.; Inria
RP Zhao, J (corresponding author), Autodesk Res, Toronto, ON, Canada.
EM jian.zhao@autodesk.com; michael.glueck@autodesk.com;
   simon.breslav@autodesk.com; fanny.chevalier@inria.fr;
   azam.khan@autodesk.com
RI Glueck, Michael/AAK-3015-2020; Khan, Azam/JEF-7682-2023
OI Khan, Azam/0000-0002-1816-5714
FU Mitacs through the Mitacs Accelerate program; CPER Nord-Pas de Calais /
   FEDER DATA Advanced data science technologies
FX This work was partially supported by Mitacs through the Mitacs
   Accelerate program and CPER Nord-Pas de Calais / FEDER DATA Advanced
   data science & technologies 2015-2020.
NR 39
TC 16
Z9 27
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2017
VL 23
IS 1
BP 261
EP 270
DI 10.1109/TVCG.2016.2598543
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA EM8CA
UT WOS:000395537600029
PM 27875143
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Alcázar, JG
   Goldman, R
AF Alcazar, Juan G.
   Goldman, Ron
TI Finding the Axis of Revolution of an Algebraic Surface of Revolution
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Algebraic surfaces; surfaces of revolution; axis of revolution
ID MULTIVARIATE POLYNOMIALS
AB We present an algorithm for extracting the axis of revolution from the implicit equation of an algebraic surface of revolution based on three distinct computational methods: factoring the highest order form into quadrics, contracting the tensor of the highest order form, and using univariate resultants and gcds. We compare and contrast the advantages and disadvantages of each of these three techniques and we derive conditions under which each technique is most appropriate. In addition, we provide several necessary conditions for an implicit algebraic equation to represent a surface of revolution.
C1 [Alcazar, Juan G.] Univ Alcala, Dept Fis & Matemat, E-28871 Madrid, Spain.
   [Goldman, Ron] Rice Univ, Dept Comp Sci, Houston, TX 77005 USA.
C3 Universidad de Alcala; Rice University
RP Alcázar, JG (corresponding author), Univ Alcala, Dept Fis & Matemat, E-28871 Madrid, Spain.
EM juange.alcazar@uah.es; rng@rice.edu
RI Alcazar, Juan Gerardo/L-5698-2014
OI Alcazar, Juan Gerardo/0000-0002-1665-9710
FU Spanish Ministerio de Economia y Competitividad; European Regional
   Development Fund (ERDF) [MTM2014-54141-P]; Giner de los Rios grant from
   the Universidad de Alcala
FX The authors would like to thank Bernard Mourrain for discussing some
   aspects of the problem with us. They would also like to thank Jens
   Gravesen for some insightful conversations concerning tensors. Juan G.
   Alcazar is supported by the Spanish Ministerio de Economia y
   Competitividad and by the European Regional Development Fund (ERDF),
   under the project MTM2014-54141-P, and is a member of the Research Group
   ASYNACS (Ref. CCEE2011/R34). Ron Goldman is partially supported by a
   Giner de los Rios grant from the Universidad de Alcala.
NR 22
TC 8
Z9 8
U1 0
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEP
PY 2016
VL 22
IS 9
BP 2082
EP 2093
DI 10.1109/TVCG.2015.2498602
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DU4FD
UT WOS:000382166900002
PM 26561460
DA 2025-03-07
ER

PT J
AU Prachyabrued, M
   Borst, CW
AF Prachyabrued, Mores
   Borst, Christoph W.
TI Design and Evaluation of Visual Interpenetration Cues in Virtual
   Grasping
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Virtual grasping; virtual reality; visual feedback
ID MANIPULATION; PERFORMANCE; FEEDBACK; REALITY; OBJECTS; HAND
AB We present design and impact studies of visual feedback for virtual grasping. The studies suggest new or updated guidelines for feedback. Recent grasping techniques incorporate visual cues to help resolve undesirable visual or performance artifacts encountered after real fingers enter a virtual object. Prior guidelines about such visuals are based largely on other interaction types and provide inconsistent and potentially-misleading information when applied to grasping. We address this with a two-stage study. In the first stage, users adjusted parameters of various feedback types, including some novel aspects, to identify promising settings and to give insight into preferences regarding the parameters. In the next stage, the tuned feedback techniques were evaluated in terms of objective performance (finger penetration, release time, and precision) and subjective rankings (visual quality, perceived behavior impact, and overall preference). Additionally, subjects commented on the techniques while reviewing them in a final session. Performancewise, the most promising techniques directly reveal penetrating hand configuration in some way. Subjectively, subjects appreciated visual cues about interpenetration or grasp force, and color changes are most promising. The results enable selection of the best cues based on understanding the relevant tradeoffs and reasonable parameter values. The results also provide a needed basis for more focused studies of specific visual cues and for choosing conditions in comparisons to other feedback modes, such as haptic, audio, or multimodal. Considering results, we propose that 3D interaction guidelines must be updated to capture the importance of interpenetration cues, possible performance benefits of direct representations, and tradeoffs involved in cue selection.
C1 [Prachyabrued, Mores] Mahidol Univ, Fac Informat & Commun Technol, Bangkok 10700, Thailand.
   [Borst, Christoph W.] Univ Louisiana Lafayette, Ctr Adv Comp Studies, Lafayette, LA 70503 USA.
C3 Mahidol University; University of Louisiana Lafayette
RP Prachyabrued, M (corresponding author), Mahidol Univ, Fac Informat & Commun Technol, Bangkok 10700, Thailand.
EM mores_p@hotmail.com; cwborst@gmail.com
RI Prachyabrued, Mores/AFV-7088-2022
NR 33
TC 18
Z9 19
U1 0
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN
PY 2016
VL 22
IS 6
BP 1718
EP 1731
DI 10.1109/TVCG.2015.2456917
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DO0MX
UT WOS:000377474100009
DA 2025-03-07
ER

PT J
AU Lincoln, P
   Blate, A
   Singh, M
   Whitted, T
   State, A
   Lastra, A
   Fuchs, H
AF Lincoln, Peter
   Blate, Alex
   Singh, Montek
   Whitted, Turner
   State, Andrei
   Lastra, Anselmo
   Fuchs, Henry
TI From Motion to Photons in 80 Microseconds: Towards Minimal Latency for
   Virtual and Augmented Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE Virtual Reality Conference (IEEE VR)
CY MAR 19-23, 2016
CL Greenville, SC
SP IEEE, IEEE Comp Soc, IEEE Comp Soc Visualizat & Graph Tech Comm, Clemson Univ
DE Augmented reality; latency; display modulation
AB We describe an augmented reality, optical see-through display based on a DMD chip with an extremely fast (16 kHz) binary update rate. We combine the techniques of post-rendering 2-D offsets and just-in-time tracking updates with a novel modulation technique for turning binary pixels into perceived gray scale. These processing elements, implemented in an FPGA, are physically mounted along with the optical display elements in a head tracked rig through which users view synthetic imagery superimposed on their real environment. The combination of mechanical tracking at near-zero latency with reconfigurable display processing has given us a measured average of 80 us of end-to-end latency (from head motion to change in photons from the display) and also a versatile test platform for extremely-low-latency display systems. We have used it to examine the trade-offs between image quality and cost (i.e. power and logical complexity) and have found that quality can be maintained with a fairly simple display modulation scheme.
C1 [Lincoln, Peter; Blate, Alex; Singh, Montek; Whitted, Turner; State, Andrei; Lastra, Anselmo; Fuchs, Henry] Univ N Carolina, Chapel Hill, NC USA.
   [Whitted, Turner] NVIDIA, Durham, NC USA.
   [State, Andrei] InnerOpt Technol Inc, Hillsborough, NC USA.
C3 University of North Carolina; University of North Carolina Chapel Hill;
   Nvidia Corporation
RP Lincoln, P; Blate, A; Singh, M; Whitted, T; State, A; Lastra, A; Fuchs, H (corresponding author), Univ N Carolina, Chapel Hill, NC USA.; Whitted, T (corresponding author), NVIDIA, Durham, NC USA.; State, A (corresponding author), InnerOpt Technol Inc, Hillsborough, NC USA.
EM plincoln@cs.unc.edu; blate@cs.unc.edu; montek@cs.unc.edu;
   jtw@cs.unc.edu; andrei@cs.unc.edu; lastra@cs.unc.edu; fuchs@cs.unc.edu
FU NSF [CHS IIS-1423059]; BeingThere Centre; Eidgenossische Technische
   Hochschule (ETH) Zurich, Nanyang Technological University (NTU)
   Singapore; University of North Carolina (UNC) at Chapel Hill; Singapore
   National Research Foundation under International Research Centre @
   Singapore Funding Initiative; Div Of Information & Intelligent Systems;
   Direct For Computer & Info Scie & Enginr [1423059] Funding Source:
   National Science Foundation
FX The authors wish to thank Kurtis Keller and Jim Mahaney for their
   engineering advice and support. They also wish to thank Mary Whitton for
   her insights and suggestions. This research was supported in part by NSF
   grant CHS IIS-1423059. Minimal-Latency Tracking and Display for
   Head-Worn Augmented Reality Systems. This research was also supported in
   part by the BeingThere Centre, a collaboration between Eidgenossische
   Technische Hochschule (ETH) Zurich, Nanyang Technological University
   (NTU) Singapore, and University of North Carolina (UNC) at Chapel Hill.
   The Centre is supported by these three institutions and by the Singapore
   National Research Foundation under its International Research Centre @
   Singapore Funding Initiative and administered by the Interactive Digital
   Media Programme Office.
NR 31
TC 68
Z9 82
U1 2
U2 24
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2016
VL 22
IS 4
BP 1367
EP 1376
DI 10.1109/TVCG.2016.2518038
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA DH5RN
UT WOS:000372849600006
PM 26780797
DA 2025-03-07
ER

PT J
AU Cao, N
   Lin, YR
   Gotz, D
AF Cao, Nan
   Lin, Yu-Ru
   Gotz, David
TI UnTangle Map: Visual Analysis of Probabilistic Multi-Label Data
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; multidimensional visualization; probability vector
ID SET RELATIONS; VISUALIZATION; IDENTIFICATION
AB Data with multiple probabilistic labels are common in many situations. For example, a movie may be associated with multiple genres with different levels of confidence. Despite their ubiquity, the problem of visualizing probabilistic labels has not been adequately addressed. Existing approaches often either discard the probabilistic information, or map the data to a low-dimensional subspace where their associations with original labels are obscured. In this paper, we propose a novel visual technique, UnTangle Map, for visualizing probabilistic multi-labels. In our proposed visualization, data items are placed inside a web of connected triangles, with labels assigned to the triangle vertices such that nearby labels are more relevant to each other. The positions of the data items are determined based on the probabilistic associations between items and labels. UnTangle Map provides both (a) an automatic label placement algorithm, and (b) adaptive interactions that allow users to control the label positioning for different information needs. Our work makes a unique contribution by providing an effective way to investigate the relationship between data items and their probabilistic labels, as well as the relationships among labels. Our user study suggests that the visualization effectively helps users discover emergent patterns and compare the nuances of probabilistic information in the data labels.
C1 [Cao, Nan] IBM Corp, TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
   [Lin, Yu-Ru] Univ Pittsburgh, Sch Informat Sci, Pittsburgh, PA 15260 USA.
   [Gotz, David] Univ N Carolina, Chapel Hill Sch, Sch Informat & Lib Sci, Chapel Hill, NC 27599 USA.
C3 International Business Machines (IBM); IBM USA; Pennsylvania
   Commonwealth System of Higher Education (PCSHE); University of
   Pittsburgh; University of North Carolina; University of North Carolina
   Chapel Hill
RP Cao, N (corresponding author), IBM Corp, TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.
EM nancao@us.ibm.com; yurulin@pitt.edu; gotz@unc.edu
RI Cao, Nan/O-5397-2014
FU US Defense Advanced Research Projects Agency (DARPA) [W911NF-12-C-0028]
FX This material is partly supported by the US Defense Advanced Research
   Projects Agency (DARPA) under the Social Media in Strategic
   Communication program under Contract Number W911NF-12-C-0028.
NR 52
TC 16
Z9 16
U1 0
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2016
VL 22
IS 2
BP 1149
EP 1163
DI 10.1109/TVCG.2015.2424878
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DA3TF
UT WOS:000367721100011
PM 26731458
DA 2025-03-07
ER

PT J
AU Noguera, JM
   Jiménez, JR
AF Noguera, Jose M.
   Roberto Jimenez, J.
TI Mobile Volume Rendering: Past, Present and Future
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Mobile computing; computer graphics; volume rendering; medical imaging;
   data compression; cloud computing
ID OF-THE-ART; 3D GRAPHICS; VISUALIZATION; WIRELESS; DISPLAY
AB Volume rendering has been a relevant topic in scientific visualization for the last decades. However, the exploration of reasonably big volume datasets requires considerable computing power, which has limited this field to the desktop scenario. But the recent advances in mobile graphics hardware have motivated the research community to overcome these restrictions and to bring volume graphics to these ubiquitous handheld platforms. This survey presents the past and present work on mobile volume rendering, and is meant to serve as an overview and introduction to the field. It proposes a classification of the current efforts and covers aspects such as advantages and issues of the mobile platforms, rendering strategies, performance and user interfaces. The paper ends by highlighting promising research directions to motivate the development of new and interesting mobile volume solutions.
C1 [Noguera, Jose M.; Roberto Jimenez, J.] Univ Jaen, GGGJ, Jaen 23071, Spain.
C3 Universidad de Jaen
RP Noguera, JM (corresponding author), Univ Jaen, GGGJ, Campus Las Lagunillas S-N,Bldg A3, Jaen 23071, Spain.
EM jnoguera@ujaen.es; rjimenez@ujaen.es
RI Noguera, José/D-2746-2011
OI Noguera, Jose Maria/0000-0001-8181-0934; Jimenez-Perez,
   Juan-Roberto/0000-0002-1233-2294
FU Spanish "Ministerio de Ciencia e Innovacion"; European Union (via ERDF
   funds) [TIN2011-25259]
FX The authors would like to thank prof. Antonio J. Rueda for his valuable
   comments. This work has been partially supported by the Spanish
   "Ministerio de Ciencia e Innovacion" and the European Union (via ERDF
   funds) through the research project TIN2011-25259.
NR 60
TC 20
Z9 20
U1 0
U2 18
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2016
VL 22
IS 2
BP 1164
EP 1178
DI 10.1109/TVCG.2015.2430343
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DA3TF
UT WOS:000367721100012
PM 26731459
DA 2025-03-07
ER

PT J
AU Chen, CM
   Dutta, S
   Liu, XT
   Heinlein, G
   Shen, HW
   Chen, JP
AF Chen, Chun-Ming
   Dutta, Soumya
   Liu, Xiaotong
   Heinlein, Gregory
   Shen, Han-Wei
   Chen, Jen-Ping
TI Visualization and Analysis of Rotating Stall for Transonic Jet Engine
   Simulation
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Turbine flow visualization; vortex extraction; anomaly detection;
   juxtaposition; brushing and linking; time series
ID OUTLIER DETECTION; FLOW; INCEPTION; EXTRACTION
AB Identification of early signs of rotating stall is essential for the study of turbine engine stability. With recent advancements of high performance computing. high-resolution unsteady flow fields allow in depth exploration of rotating stall and its possible causes. Performing stall analysis, however, involves significant effort to process large amounts of simulation data, especially when investigating abnormalities across many time steps. In order to assist scientists during the exploration process, we present a visual analytics framework to identify suspected spatiotemporal regions through a comparative visualization so that scientists are able to focus on relevant data in more detail. To achieve this, we propose efficient stall analysis algorithms derived from domain knowledge and convey the analysis results through juxtaposed interactive plots. Using our integrated visualization system, scientists can visually investigate the detected regions for potential stall initiation and further explore these regions to enhance the understanding of this phenomenon. Positive feedback from scientists demonstrate the efficacy of our system in analyzing rotating stall.
C1 [Chen, Chun-Ming; Dutta, Soumya; Liu, Xiaotong; Shen, Han-Wei] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
   [Heinlein, Gregory; Chen, Jen-Ping] Ohio State Univ, Dept Mech & Aerosp Engn, Columbus, OH 43210 USA.
C3 University System of Ohio; Ohio State University; University System of
   Ohio; Ohio State University
RP Chen, CM (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.
EM chen.1701@osu.edu; dutta.33@osu.edu; liu.1952@osu.edu;
   heinlein.29@osu.edu; shen.94@osu.edu; chen.1210@osu.edu
RI Chen, Jen-Ping/F-1823-2017; Shen, Han-wei/A-4710-2012; Dutta,
   Soumya/AAN-2212-2020; Liu, Xiaotong/AAB-9458-2019
OI Dutta, Soumya/0000-0001-5030-9979
FU NSF [IIS-1250752, IIS-1065025]; US Department of Energy [DE-SC0007444,
   DE-DC0012495]
FX The authors would like to thank lames Giuliani for his great technical
   support of the TURBO simulator and constructive feedback. This work was
   supported in part by NSF grants IIS-1250752, program manager Almadena
   Chtchelkanova, IIS-1065025, and US Department of Energy grants
   DE-SC0007444, DE-DC0012495, program manager Lucy Nowell.
NR 49
TC 10
Z9 15
U1 0
U2 8
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 847
EP 856
DI 10.1109/TVCG.2015.2467952
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400090
PM 26529732
DA 2025-03-07
ER

PT J
AU Fulda, J
   Brehmer, M
   Munzner, T
AF Fulda, Johanna
   Brehmer, Matthew
   Munzner, Tamara
TI TimeLineCurator: Interactive Authoring of Visual Timelines from
   Unstructured Text
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE System; timelines; authoring environment; time-oriented data; journalism
ID DESIGN; VISUALIZATION; COLLECTIONS; REFLECTIONS; TOOL
AB We present TimeLineCurator, a browser-based authoring tool that automatically extracts event data from temporal references in unstructured text documents using natural language processing and encodes them along a visual timeline. Our goal is to facilitate the timeline creation process for journalists and others who tell temporal stories online. Current solutions involve manually extracting and formatting event data from source documents, a process that tends to be tedious and error prone. With TimeLineCurator, a prospective timeline author can quickly identify the extent of time encompassed by a document, as well as the distribution of events occurring along this timeline. Authors can speculatively browse possible documents to quickly determine whether they are appropriate sources of timeline material. TimeLineCurator provides controls for curating and editing events on a timeline, the ability to combine timelines from multiple source documents, and export curated timelines for online deployment. We evaluate TimeLineCurator through a benchmark comparison of entity extraction error against a manual timeline curation process, a preliminary evaluation of the user experience of timeline authoring, a brief qualitative analysis of its visual output. and a discussion of prospective use cases suggested by members of the target author communities following its deployment.
C1 [Fulda, Johanna] Univ Munich LMU, Munich, Germany.
   [Fulda, Johanna; Brehmer, Matthew; Munzner, Tamara] Univ British Columbia, Vancouver, BC V5Z 1M9, Canada.
C3 University of Munich; University of British Columbia
RP Fulda, J (corresponding author), Univ Munich LMU, Munich, Germany.
EM mail@johannafulda.de; brehmer@cs.ubc.ca; tmm@cs.ubc.ca
RI Munzner, Tamara/HKP-2536-2023
OI Munzner, Tamara/0000-0002-3294-3869
NR 60
TC 54
Z9 68
U1 1
U2 28
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 300
EP 309
DI 10.1109/TVCG.2015.2467531
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400035
PM 26529709
DA 2025-03-07
ER

PT J
AU Jaenicke, S
   Focht, J
   Scheuermann, G
AF Jaenicke, Stefan
   Focht, Josef
   Scheuermann, Gerik
TI Interactive Visual Profiling of Musicians
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE visual analytics. profiling system; musicians database visualization;
   digital humanities; musicology
ID TEXT; VISUALIZATION; EXPLORATION; DESIGN; TOOL
AB Determining similar objects based upon the features of an object of interest is a common task for visual analytics systems. This process is called profiling, if the object of interest is a person with individual attributes. The profiling of musicians similar to a musician of interest with the aid of visual means became an interesting research question for musicologists working with the Bavarian Musicians Encyclopedia Online. This paper illustrates the development of a visual analytics profiling system that is used to address such research questions. Taking musicological knowledge into account, we outline various steps of our collaborative digital humanities project. priority (1) the definition of various measures to determine the similarity of musicians attributes. and (2) the design of an interactive profiling system that supports musicologists in iteratively determining similar musicians. The utility of the profiling system is emphasized by various usage scenarios illustrating current research questions in musicology.
C1 [Jaenicke, Stefan; Scheuermann, Gerik] Univ Leipzig, Inst Comp Sci, Image & Signal Proc Grp, Leipzig, Germany.
   [Focht, Josef] Univ Leipzig, Inst Musicol, Museum Mus Instruments, Leipzig, Germany.
C3 Leipzig University; Leipzig University
RP Jaenicke, S (corresponding author), Univ Leipzig, Inst Comp Sci, Image & Signal Proc Grp, Leipzig, Germany.
EM stjaenicke@informatik.uni-leipzig.de; josef.focht@uni-leipzig.de;
   scheuermann@informatik.uni-leipzig.de
OI Janicke, Stefan/0000-0001-9353-5212
NR 52
TC 28
Z9 29
U1 1
U2 22
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 200
EP 209
DI 10.1109/TVCG.2015.2467620
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400025
PM 26529700
DA 2025-03-07
ER

PT J
AU Papadopoulos, C
   Gutenko, I
   Kaufman, AE
AF Papadopoulos, C.
   Gutenko, I.
   Kaufman, A. E.
TI <i>VEEVVIE</i> : Visual Explorer for Empirical Visualization, VR and
   Interaction Experiments
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 10th IEEE Conference on Visual Analytics Science and Technology (VAST) /
   IEEE VIS Conference
CY OCT 25-30, 2015
CL Chicago, IL
SP IEEE, IEEE Comp Soc, IEEE Visualizat & Graph Tech Comm, InfoVis, SciVis
DE Visual Analytics; Evaluation; User Studies; Ontology; Experiments;
   Interaction; Virtual Reality; Visualization
ID SYSTEM
AB Empirical, hypothesis-driven, experimentation is at the heart of the scientific discovery process and has become commonplace in human-factors related fields. To enable the integration of visual analytics in such experiments, we introduce VEEVVIE, the Visual Explorer for Empirical Visualization. VR and Interaction Experiments. VEEVVIE is comprised of a back-end ontology which can model several experimental designs encountered in these fields. This formalization allows VEEVVIE to capture experimental data in a query-able form and makes it accessible through a front-end interface. This front-end offers several multi-dimensional visualization widgets with built-in filtering and highlighting functionality. VEEVVIE is also expandable to support custom experimental measurements and data types through a plug-in visualization widget architecture. We demonstrate VEEVVIE through several case studies of visual analysis, performed on the design and data collected during an experiment on the scalability of high-resolution, immersive, tiled-display walls.
C1 [Papadopoulos, C.; Gutenko, I.; Kaufman, A. E.] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; Stony Brook University
RP Papadopoulos, C (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM cpapadopoulo@cs.stonybrook.edu; igutenko@cs.stonybrook.edu;
   ari@cs.stonybrook.edu
FU NSF [CNS-0959979, IIP-1069147, CNS-1302246]; New York Resiliency
   Institute for Storms and Emergencies; Direct For Computer & Info Scie &
   Enginr; Division Of Computer and Network Systems [1302246] Funding
   Source: National Science Foundation
FX This work has been supported by NSF grants CNS-0959979, IIP-1069147,
   CNS-1302246 and the New York Resiliency Institute for Storms and
   Emergencies.
NR 32
TC 8
Z9 11
U1 0
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2016
VL 22
IS 1
BP 111
EP 120
DI 10.1109/TVCG.2015.2467954
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA CV1UK
UT WOS:000364043400016
PM 26529692
DA 2025-03-07
ER

PT J
AU Zheng, CX
   Liu, BZ
   Xu, XM
   Zhang, HD
   He, SF
AF Zheng, Chenxi
   Liu, Bangzhen
   Xu, Xuemiao
   Zhang, Huaidong
   He, Shengfeng
TI Learning an Interpretable Stylized Subspace for 3D-Aware Animatable
   Artforms
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Art; Training; Painting; Image
   reconstruction; Costs; Adaptation models; 3D-aware GANs; facial
   attribute editing; stylized animation
AB Throughout history, static paintings have captivated viewers within display frames, yet the possibility of making these masterpieces vividly interactive remains intriguing. This research paper introduces 3DArtmator, a novel approach that aims to represent artforms in a highly interpretable stylized space, enabling 3D-aware animatable reconstruction and editing. Our rationale is to transfer the interpretability and 3D controllability of the latent space in a 3D-aware GAN to a stylized sub-space of a customized GAN, revitalizing the original artforms. To this end, the proposed two-stage optimization framework of 3DArtmator begins with discovering an anchor in the original latent space that accurately mimics the pose and content of a given art painting. This anchor serves as a reliable indicator of the original latent space local structure, therefore sharing the same editable predefined expression vectors. In the second stage, we train a customized 3D-aware GAN specific to the input artform, while enforcing the preservation of the original latent local structure through a meticulous style-directional difference loss. This approach ensures the creation of a stylized sub-space that remains interpretable and retains 3D control. The effectiveness and versatility of 3DArtmator are validated through extensive experiments across a diverse range of art styles. With the ability to generate 3D reconstruction and editing for artforms while maintaining interpretability, 3DArtmator opens up new possibilities for artistic exploration and engagement.
C1 [Zheng, Chenxi; Liu, Bangzhen; Xu, Xuemiao] South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.
   [Zhang, Huaidong] South China Univ Technol, Sch Future Technol, Guangzhou 510641, Guangdong, Peoples R China.
   [He, Shengfeng] Singapore Management Univ, Sch Comp & Informat Syst, Singapore 188065, Singapore.
C3 South China University of Technology; South China University of
   Technology; Singapore Management University
RP Liu, BZ; Xu, XM (corresponding author), South China Univ Technol, Sch Comp Sci & Engn, Guangzhou 510641, Guangdong, Peoples R China.
EM chansey0529@gmail.com; liubz.scut@gmail.com; xuemx@scut.edu.cn;
   huaidongz@scut.edu.cn; shengfenghe7@gmail.com
RI liu, bangzhen/LHA-3053-2024; Zhang, Huaidong/AAB-9269-2022; He,
   Shengfeng/E-5682-2016
OI He, Shengfeng/0000-0002-3802-4644; xu, xuemiao/0000-0002-8006-3663; Liu,
   Bangzhen/0000-0001-6621-0594; , Chenxi/0009-0006-0344-2439
FU Guangdong International Technology Cooperation Project
   [2022A0505050009]; China National Key RD Program [2023YFE0202700];
   Key-Area Research and Development Program of Guangzhou City
   [2023B01J0022]; Guangdong Natural Science Funds for Distinguished Young
   Scholar [2023B1515020097]; Singapore MOE Tier 1 Funds [MSS23C002];
   National Research Foundation Singapore under the AI Singapore Programme
   [AISG3-GV-2023-011]
FX This work was supported in part by Guangdong International Technology
   Cooperation Project under Grant 2022A0505050009, in part by China
   National Key R&D Program under Grant 2023YFE0202700, in part by Key-Area
   Research and Development Program of Guangzhou City under Grant
   2023B01J0022, in part by the Guangdong Natural Science Funds for
   Distinguished Young Scholar under Grant 2023B1515020097, in part by
   Singapore MOE Tier 1 Funds under Grant MSS23C002, and in part by
   National Research Foundation Singapore under the AI Singapore Programme
   under Grant AISG3-GV-2023-011.
NR 66
TC 0
Z9 0
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2025
VL 31
IS 2
BP 1465
EP 1477
DI 10.1109/TVCG.2024.3364162
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA R6W2Y
UT WOS:001392823200019
PM 38335081
OA Green Accepted
DA 2025-03-07
ER

PT J
AU Giovannangeli, L
   Lalanne, F
   Giot, R
   Bourqui, R
AF Giovannangeli, Loann
   Lalanne, Frederic
   Giot, Romain
   Bourqui, Romain
TI Overlap Removal by Stochastic Gradient Descent With(out) Shape Awareness
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Layout; Shape; Task analysis; Optimization; Stress; Data visualization;
   Stochastic processes; Layout adjustment; overlap removal; stochastic
   gradient descent; stress optimization
ID GRAPH; ALGORITHM; FRAMEWORK
AB In many 2D visualizations, data points are projected without considering their surface area, although they are often represented as shapes in visualization tools. These shapes support the display of information such as labels or encode data with size or color. However, inappropriate shape and size selections can lead to overlaps that obscure information and hinder the visualization's exploration. Overlap Removal (OR) algorithms have been developed as a layout post-processing solution to ensure that the visible graphical elements accurately represent the underlying data. As the original data layout contains vital information about its topology, it is essential for OR algorithms to preserve it as much as possible. This article presents an extension of the previously published FORBID algorithm by introducing a new approach that models OR as a joint stress and scaling optimization problem, utilizing efficient stochastic gradient descent. The goal is to produce an overlap-free layout that proposes a compromise between compactness (to ensure the encoded data is still readable) and preservation of the original layout (to preserve the structures that convey information about the data). Additionally, this article proposes SORDID, a shape-aware adaptation of FORBID that can handle the OR task on data points having any polygonal shape. Our approaches are compared against state-of-the-art algorithms, and several quality metrics demonstrate their effectiveness in removing overlaps while retaining the compactness and structures of the input layouts.
C1 [Giovannangeli, Loann; Lalanne, Frederic; Giot, Romain; Bourqui, Romain] Univ Bordeaux, CNRS, Bordeaux INP, LaBRI,UMR 5800, F-33400 Talence, France.
C3 Universite de Bordeaux; Centre National de la Recherche Scientifique
   (CNRS)
RP Giovannangeli, L (corresponding author), Univ Bordeaux, CNRS, Bordeaux INP, LaBRI,UMR 5800, F-33400 Talence, France.
EM loann.gio@hotmail.com; frederic.lalanne@u-bordeaux.fr;
   romain.giot@u-bordeaux.fr; bourqui@labri.fr
RI Giot, Romain/O-5798-2017
OI Giot, Romain/0000-0002-0638-7504; Giovannangeli,
   Loann/0000-0002-9395-6495; Lalanne, Frederic/0000-0001-9108-0955
FU French ANR project InvolvD [OPE 2020-0425]
FX This work was supported by French ANR project InvolvD OPE 2020-0425.
NR 53
TC 0
Z9 0
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2024
VL 30
IS 12
BP 7500
EP 7517
DI 10.1109/TVCG.2024.3351479
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K8D1L
UT WOS:001346124800019
PM 38194373
DA 2025-03-07
ER

PT J
AU Kageyama, Y
   Iwai, D
   Sato, K
AF Kageyama, Yuta
   Iwai, Daisuke
   Sato, Kosuke
TI Efficient Distortion-Free Neural Projector Deblurring in Dynamic
   Projection Mapping
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Degradation; Cameras; Image quality; Calibration; Real-time systems;
   Optical distortion; Training; Projector deblurring; geometric
   compensation; dynamic projection mapping; deep neural networks
ID CAMERA SYSTEM; REMOVAL; IMAGE
AB Dynamic Projection Mapping (DPM) necessitates geometric compensation of the projection image based on the position and orientation of moving objects. Additionally, the projector's shallow depth of field results in pronounced defocus blur even with minimal object movement. Achieving delay-free DPM with high image quality requires real-time implementation of geometric compensation and projector deblurring. To meet this demand, we propose a framework comprising two neural components: one for geometric compensation and another for projector deblurring. The former component warps the image by detecting the optical flow of each pixel in both the projection and captured images. The latter component performs real-time sharpening as needed. Ideally, our network's parameters should be trained on data acquired in an actual environment. However, training the network from scratch while executing DPM, which demands real-time image generation, is impractical. Therefore, the network must undergo pre-training. Unfortunately, there are no publicly available large real datasets for DPM due to the diverse image quality degradation patterns. To address this challenge, we propose a realistic synthetic data generation method that numerically models geometric distortion and defocus blur in real-world DPM. Through exhaustive experiments, we have confirmed that the model trained on the proposed dataset achieves projector deblurring in the presence of geometric distortions with a quality comparable to state-of-the-art methods.
C1 [Kageyama, Yuta; Iwai, Daisuke; Sato, Kosuke] Osaka Univ, Grad Sch Engn Sci, Osaka 5650871, Japan.
C3 Osaka University
RP Kageyama, Y (corresponding author), Osaka Univ, Grad Sch Engn Sci, Osaka 5650871, Japan.
EM kageyama@sens.sys.es.osaka-u.ac.jp; daisuke.iwai.es@osaka-u.ac.jp;
   sato@sys.es.osaka-u.ac.jp
OI Sato, Kosuke/0000-0003-1429-9990
FU JSPS KAKENHI [JP20H05958, JP23KJ1455]; Grants-in-Aid for Scientific
   Research [23KJ1455] Funding Source: KAKEN
FX This work was supported by JSPS KAKENHI under GrantsJP20H05958 and
   JP23KJ1455.
NR 62
TC 0
Z9 0
U1 4
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2024
VL 30
IS 12
BP 7544
EP 7557
DI 10.1109/TVCG.2024.3354957
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA K8D1L
UT WOS:001346124800015
PM 38227414
OA Green Submitted, hybrid
DA 2025-03-07
ER

PT J
AU Ebner, C
   Plopski, A
   Schmalstieg, D
   Kalkofen, D
AF Ebner, Christoph
   Plopski, Alexander
   Schmalstieg, Dieter
   Kalkofen, Denis
TI Gaze-Contingent Layered Optical See-Through Displays with a
   Confidence-Driven View Volume
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Gaze tracking; Estimation; Lenses; Optical imaging; Light fields;
   Three-dimensional displays; Focusing; Gaze-Contingent Layered Display;
   Optical See-Through Mixed Reality; Vergence-Accommodation Conflict
ID MIXED REALITY
AB The vergence-accommodation conflict (VAC) presents a major perceptual challenge for head-mounted displays with a fixed image plane. Varifocal and layered display designs can mitigate the VAC. However, the image quality of varifocal displays is affected by imprecise eye tracking, whereas layered displays suffer from reduced image contrast as the distance between layers increases. Combined designs support a larger workspace and tolerate some eye-tracking error. However, any layered design with a fixed layer spacing restricts the amount of error compensation and limits the in-focus contrast. We extend previous hybrid designs by introducing confidence-driven volume control, which adjusts the size of the view volume at runtime. We use the eye tracker's confidence to control the spacing of display layers and optimize the trade-off between the display's view volume and the amount of eye tracking error the display can compensate. In the case of high-quality focus point estimation, our approach provides high in-focus contrast, whereas low-quality eye tracking increases the view volume to tolerate the error. We describe our design, present its implementation as an optical-see head-mounted display using a multiplicative layer combination, and present an evaluation comparing our design with previous approaches.
C1 [Ebner, Christoph; Plopski, Alexander; Schmalstieg, Dieter; Kalkofen, Denis] Graz Univ Technol, Graz, Austria.
   [Schmalstieg, Dieter] Univ Stuttgart, Stuttgart, Germany.
   [Kalkofen, Denis] Flinders Univ S Australia, Adelaide, Australia.
C3 Graz University of Technology; University of Stuttgart; Flinders
   University South Australia
RP Ebner, C (corresponding author), Graz Univ Technol, Graz, Austria.
EM christoph.ebner@tugraz.at; alexander.plopski@tugraz.at;
   dieter.schmalstieg@visus.uni-stuttgart.de;
   denis.kalkofen@flinders.edu.au
OI Plopski, Alexander/0000-0003-1354-0279; Kalkofen,
   Denis/0000-0002-0359-206X
FU Snap, Inc.; Alexander von Humboldt Foundation-German Federal Ministry of
   Education and Research
FX This work was supported by Snap, Inc., and the Alexander von Humboldt
   Foundation funded by the German Federal Ministry of Education and
   Research.
NR 45
TC 0
Z9 0
U1 0
U2 0
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7203
EP 7213
DI 10.1109/TVCG.2024.3456184
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300022
PM 39255112
DA 2025-03-07
ER

PT J
AU Zhang, V
   Albers, A
   Saeedi-Givi, C
   Kristensson, PO
   Bohné, T
   Tadeja, S
AF Zhang, Vicky
   Albers, Alexander
   Saeedi-Givi, Christine
   Kristensson, Per Ola
   Bohne, Thomas
   Tadeja, Slawomir
TI Should I Evaluate My Augmented Reality System in an Industrial
   Environment? Investigating the Effects of Classroom and Shop Floor
   Settings on Guided Assembly
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Assembly; Manuals; Manufacturing; Laboratories; Resists; Noise;
   Industries; Augmented reality; manual assembly; user study; classroom
   setting; shop floor setting; augmented reality guidelines
ID ECOLOGICAL VALIDITY; PERFORMANCE; ASSISTANCE; CHOKING; ANXIETY; SAFETY
AB Numerous prior studies have investigated real-time assembly instructions using Augmented Reality (AR). However, most such experiments were conducted in laboratory settings with simplistic assembly tasks, failing to represent real-world industrial conditions. To ascertain to what extent results obtained in a laboratory environment may differ from studies in actual industrial environments, we carried out a user study with 32 manufacturing apprentices. We compared assembly task execution results in two settings, a classroom and an industrial workshop environment. To facilitate the experiments, we developed AR-guided manual assembly systems for simple and more complex assets. Our findings reveal a significantly improved task performance in the industrial workshop, reflected in faster task completion times, fewer errors, and subjectively perceived higher flow. This contradicted participants' subjective ratings, as they expected to perform better in the classroom environment. Our results suggest that the actual manufacturing environment is critical in evaluating AR systems for real-world industrial applications.
C1 [Zhang, Vicky; Albers, Alexander; Saeedi-Givi, Christine; Kristensson, Per Ola; Bohne, Thomas; Tadeja, Slawomir] Univ Cambridge, Cambridge, England.
   [Albers, Alexander] Karlsruhe Inst Technol, Karlsruhe, Germany.
   [Saeedi-Givi, Christine] Univ Konstanz, Constance, Germany.
C3 University of Cambridge; Helmholtz Association; Karlsruhe Institute of
   Technology; University of Konstanz
RP Tadeja, S (corresponding author), Univ Cambridge, Cambridge, England.
EM slawomir.tadeja@gmail.com
RI Zhang, Vicky/IYN-1630-2023; Dastan, Mine/LFR-9751-2024
OI Kristensson, Per Ola/0000-0002-7139-871X; Dastan,
   Mine/0000-0003-0555-155X; Bohne, Thomas/0000-0001-5986-8638
FU Engineering and Physical Sciences Research Council (EPSRC)
   [EP/V062123/1]
FX This work was supported by Engineering and Physical Sciences Research
   Council (EPSRC) grant no. EP/V062123/1.
NR 83
TC 0
Z9 0
U1 5
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2024
VL 30
IS 11
BP 7042
EP 7052
DI 10.1109/TVCG.2024.3456206
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA J7B1A
UT WOS:001338569300032
PM 39250381
OA hybrid, Green Submitted
DA 2025-03-07
ER

PT J
AU Coscia, A
   Suh, A
   Chang, RM
   Endert, A
AF Coscia, Adam
   Suh, Ashley
   Chang, Remco
   Endert, Alex
TI Preliminary Guidelines for Combining Data Integration and Visual Data
   Analysis
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Analytical behaviors; data integration; integration strategies; user
   interface design; visual analytics; Analytical behaviors; data
   integration; integration strategies; user interface design; visual
   analytics
AB Data integration is often performed to consolidate information from multiple disparate data sources during visual data analysis. However, integration operations are usually separate from visual analytics operations such as encode and filter in both interface design and empirical research. We conducted a preliminary user study to investigate whether and how data integration should be incorporated directly into the visual analytics process. We used two interface alternatives featuring contrasting approaches to the data preparation and analysis workflow: manual file-based ex-situ integration as a separate step from visual analytics operations; and automatic UI-based in-situ integration merged with visual analytics operations. Participants were asked to complete specific and free-form tasks with each interface, browsing for patterns, generating insights, and summarizing relationships between attributes distributed across multiple files. Analyzing participants' interactions and feedback, we found both task completion time and total interactions to be similar across interfaces and tasks, as well as unique integration strategies between interfaces and emergent behaviors related to satisficing and cognitive bias. Participants' time spent and interactions revealed that in-situ integration enabled users to spend more time on analysis tasks compared with ex-situ integration. Participants' integration strategies and analytical behaviors revealed differences in interface usage for generating and tracking hypotheses and insights. With these results, we synthesized preliminary guidelines for designing future visual analytics interfaces that can support integrating attributes throughout an active analysis process.
C1 [Coscia, Adam; Endert, Alex] Georgia Inst Technol, Atlanta, GA 30332 USA.
   [Suh, Ashley; Chang, Remco] Tufts Univ, Medford, MA 02155 USA.
C3 University System of Georgia; Georgia Institute of Technology; Tufts
   University
RP Coscia, A (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.
EM acoscia6@gatech.edu; ashleysuh1@gmail.com; remco@cs.tufts.edu;
   endert@gatech.edu
RI ; Coscia, Adam/GUO-9221-2022
OI Chang, Remco/0000-0002-6484-6430; Coscia, Adam/0000-0002-0429-9295; Suh,
   Ashley/0000-0001-6513-8447
FU National Science Foundation [IIS-1813281, DRL-2247790]
FX This work was supported by National Science Foundation under Grants
   IIS-1813281 and DRL-2247790.
NR 1
TC 0
Z9 0
U1 3
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT
PY 2024
VL 30
IS 10
BP 6678
EP 6690
DI 10.1109/TVCG.2023.3334513
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F0K0P
UT WOS:001306784600019
PM 37983146
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Guo, JP
   Zhang, WX
   Ye, CY
   Fu, XM
AF Guo, Jia-Peng
   Zhang, Wen-Xiang
   Ye, Chunyang
   Fu, Xiao-Ming
TI Robust Coarse Cage Construction With Small Approximation Errors
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Complexity theory; Robustness; Manifolds; Approximation error;
   Approximation algorithms; Filtering; Geometry; cage construction;
   conformal tetrahedral meshing; mesh complexity; tetrahedral subdivision
ID LOCAL REFINEMENT; SUBDIVISION
AB We propose a robust and automatic method to construct manifold cages for 3D triangular meshes. The cage contains hundreds of triangles to tightly enclose the input mesh without self-intersections. To generate such cages, our algorithm consists of two phases: (1) construct manifold cages satisfying the tightness, enclosing, and intersection-free requirements and (2) reduce mesh complexities and approximation errors without violating the enclosing and intersection-free requirements. To theoretically make the first stage have those properties, we combine the conformal tetrahedral meshing and tetrahedral mesh subdivision. The second step is a constrained remeshing process using explicit checks to ensure that the enclosing and intersection-free constraints are always satisfied. Both phases use a hybrid coordinate representation, i.e., rational numbers and floating point numbers, combined with exact arithmetic and floating point filtering techniques to guarantee the robustness of geometric predicates with a favorable speed. We extensively test our method on a data set of over 8500 models, demonstrating robustness and performance. Compared to other state-of-the-art methods, our method possesses much stronger robustness.
C1 [Guo, Jia-Peng] Univ Sci & Technol China, Sch Data Sci, Hefei 230026, Anhui, Peoples R China.
   [Zhang, Wen-Xiang; Ye, Chunyang; Fu, Xiao-Ming] Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
C3 Chinese Academy of Sciences; University of Science & Technology of
   China, CAS; Chinese Academy of Sciences; University of Science &
   Technology of China, CAS
RP Fu, XM (corresponding author), Univ Sci & Technol China, Sch Math Sci, Hefei 230026, Anhui, Peoples R China.
EM gjp171499@mail.ustc.edu.cn; zwx111@mail.ustc.edu.cn;
   yechyang@mail.ustc.edu.cn; fuxm@ustc.edu.cn
RI Fu, Xiao-Ming/V-8253-2019
OI Fu, Xiao-Ming/0000-0001-8479-0107; Guo, Jia-Peng/0009-0009-0073-7399
FU National Natural Science Foundation of China [62272429]; Major Project
   of Science and Technology of Anhui Province [202203a05020050]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grant 62272429 and in part by the Major
   Project of Science and Technology of Anhui Province under Grant
   202203a05020050.
NR 63
TC 1
Z9 1
U1 0
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 4234
EP 4245
DI 10.1109/TVCG.2023.3255207
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700074
PM 37028283
DA 2025-03-07
ER

PT J
AU Liu, XQ
   Li, JT
   Lu, GD
AF Liu, Xinqi
   Li, Jituo
   Lu, Guodong
TI Modeling Realistic Clothing From a Single Image Under Normal Guide
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Clothing mask; clothing modeling; normal map; realistic wrinkles
   distribution; single image; stylized cloth
AB We propose a robust and highly realistic clothing modeling method to generate a 3D clothing model with visually consistent clothing style and wrinkles distribution from a single RGB image. Notably, this entire process only takes a few seconds. Our high-quality clothing results benefit from the idea of combining learning and optimization, making it highly robust. First, we use the neural networks to predict the normal map, a clothing mask, and a learning-based clothing model from input images. The predicted normal map can effectively capture high-frequency clothing deformation from image observations. Then, by introducing a normal-guided clothing fitting optimization, the normal maps are used to guide the clothing model to generate realistic wrinkles details. Finally, we utilize a clothing collar adjustment strategy to stylize clothing results using predicted clothing masks. An extended multi-view version of the clothing fitting is naturally developed, which can further improve the realism of the clothing without tedious effort. Extensive experiments have proven that our method achieves state-of-the-art clothing geometric accuracy and visual realism. More importantly, it is highly adaptable and robust to in-the-wild images. Further, our method can be easily extended to multi-view inputs to improve realism. In summary, our method can provide a low-cost and user-friendly solution to achieve realistic clothing modeling.
C1 [Liu, Xinqi; Li, Jituo; Lu, Guodong] Zhejiang Univ, Inst Design Engn, Sch Mech Engn, Hangzhou 310027, Zhejiang, Peoples R China.
   [Liu, Xinqi; Li, Jituo; Lu, Guodong] Zhejiang Univ, Robot Inst, Hangzhou 310027, Zhejiang, Peoples R China.
C3 Zhejiang University; Zhejiang University
RP Li, JT (corresponding author), Zhejiang Univ, Inst Design Engn, Sch Mech Engn, Hangzhou 310027, Zhejiang, Peoples R China.
EM liuxinqi@zju.edu.cn; jituo_li@zju.edu.cn; lugd@zju.edu.cn
RI Gao, Zihao/KIL-4959-2024; li, jituo/LMP-9875-2024
OI Liu, Xinqi/0000-0002-9105-2417
FU Key Technologies Research and Development Program [2022YFB3303103];
   National Natural Science Foundation of China [52275276]; Research
   Funding of Zhejiang University Robotics Institute
FX This work was supported in part by the Key Technologies Research and
   Development Program under Grant 2022YFB3303103, in part by the National
   Natural Science Foundation of China under Grant 52275276,and in part by
   the Research Funding of Zhejiang University Robotics Institute.
NR 78
TC 0
Z9 0
U1 4
U2 6
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3995
EP 4007
DI 10.1109/TVCG.2023.3245583
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700088
PM 37027566
DA 2025-03-07
ER

PT J
AU Wang, ZY
   Wang, YY
   Yan, SQ
   Zhu, ZZ
   Zhang, KJ
   Wei, HK
AF Wang, Ziyao
   Wang, Yiye
   Yan, Shiqi
   Zhu, Zhongzheng
   Zhang, Kanjian
   Wei, Haikun
TI Redirected Walking on Omnidirectional Treadmill
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Device integration; haptic feedback; locomotion interfaces;
   omnidirectional treadmill; redirected walking
ID VIRTUAL ENVIRONMENTS; CONTROLLER
AB Redirected walking (RDW) and omnidirectional treadmill (ODT) are two effective solutions to the natural locomotion interface in virtual reality. ODT fully compresses the physical space and can be used as the integration carrier of all kinds of devices. However, the user experience varies in different directions of ODT, and the premise of interaction between users and integrated devices is a good match between virtual and real objects. RDW technology uses visual cues to guide the user's location in physical space. Based on this principle, combining RDW technology with ODT to guide the user's walking direction through visual cues can effectively improve user experience on ODT and make full use of various devices integrated on ODT. This paper explores the novel prospects of combining RDW technology with ODT and formally puts forward the concept of O-RDW (ODT-based RDW). Two baseline algorithms, i.e., OS2MD (ODT-based steer to multi-direction), and OS2MT (ODT-based steer to multi-target), are proposed to combine the merits of both RDW and ODT. With the help of the simulation environment, this paper quantitatively analyzes the applicable scenarios of the two algorithms and the influence of several main factors on the performance. Based on the conclusions of the simulation experiments, the two O-RDW algorithms are successfully applied in the practical application case of multi-target haptic feedback. Combined with the user study, the practicability and effectiveness of O-RDW technology in practical use are further verified.
C1 [Wang, Ziyao; Wang, Yiye; Yan, Shiqi; Zhu, Zhongzheng; Zhang, Kanjian; Wei, Haikun] Southeast Univ, Nanjing 211189, Jiangning, Peoples R China.
C3 Southeast University - China
RP Wei, HK (corresponding author), Southeast Univ, Nanjing 211189, Jiangning, Peoples R China.
EM zy_wang@seu.edu.cn; wangyiye@seu.edu.cn; sqyan@seu.edu.cn;
   zzzhu@seu.edu.cn; kjzhang@seu.edu.cn; hkwei@seu.edu.cn
RI Zhu, Zhongzheng/MGB-2024-2025
OI Wei, Haikun/0000-0002-6667-3166
FU National Natural Science Foundation of China [61773118, 61973083];
   Science and Technology Project of State Grid Corporation of China
   [SGTJDK00DYJS2000148]; Key Laboratory of Measurement and Control of
   Complex Systems of Engineering, Ministry of Education, Nanjing, China
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 61773118 and 61973083, in part by
   Science and Technology Project of State Grid Corporation of
   China(Intelligent operation and maintenance technology of distributed
   photovoltaic system SGTJDK00DYJS2000148), the Key Laboratory of
   Measurement and Control of Complex Systems of Engineering, Ministry of
   Education, Nanjing 210096, China.
NR 63
TC 0
Z9 0
U1 2
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3884
EP 3901
DI 10.1109/TVCG.2023.3244359
PG 18
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700012
PM 37027618
DA 2025-03-07
ER

PT J
AU Zhao, D
   Li, J
   Li, HY
   Xu, L
AF Zhao, Dong
   Li, Jia
   Li, Hongyu
   Xu, Long
TI Stripe Sensitive Convolution for Omnidirectional Image Dehazing
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Convolution; Distortion; Feature extraction; Kernel; Estimation;
   Three-dimensional displays; Layout; Omnidirectional image dehazing;
   omnidirectional image depth estimation; stripe sensitive convolution;
   virtual reality
ID SALIENCY; VR
AB The haze in a scenario may affect the 360 photo/video quality and the immersive 360(degrees) virtual reality (VR) experience. The recent single image dehazing methods, to date, have been only focused on plane images. In this work, we propose a novel neural network pipeline for single omnidirectional image dehazing. To create the pipeline, we build the first hazy omnidirectional image dataset, which contains both synthetic and real-world samples. Then, we propose a new stripe sensitive convolution (SSConv) to handle the distortion problems due to the equirectangular projections. The SSConv calibrates distortion in two steps: 1) extracting features using different rectangular filters and, 2) learning to select the optimal features by a weighting of the feature stripes (a series of rows in the feature maps). Subsequently, using SSConv, we design an end-to-end network that jointly learns haze removal and depth estimation from a single omnidirectional image. The estimated depth map is leveraged as the intermediate representation and provides global context and geometric information to the dehazing module. Extensive experiments on challenging synthetic and real-world omnidirectional image datasets demonstrate the effectiveness of SSConv, and our network attains superior dehazing performance. The experiments on practical applications also demonstrate that our method can significantly improve the 3-D object detection and 3-D layout performances for hazy omnidirectional images.
C1 [Zhao, Dong; Li, Jia; Li, Hongyu] Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.
   [Li, Jia] Peng Cheng Lab, Shenzhen 518000, Peoples R China.
   [Xu, Long] Chinese Acad Sci, Natl Space Sci Ctr, Beijing 100190, Peoples R China.
C3 Beihang University; Peng Cheng Laboratory; Chinese Academy of Sciences;
   National Space Science Center, CAS
RP Li, J (corresponding author), Beihang Univ, Sch Comp Sci & Engn, State Key Lab Virtual Real Technol & Syst, Beijing 100191, Peoples R China.; Li, J (corresponding author), Peng Cheng Lab, Shenzhen 518000, Peoples R China.
EM zhaodong@buaa.edu.cn; jiali@buaa.edu.cn; hongyuli@buaa.edu.cn;
   lxu@nao.cas.cn
RI Xu, Long/AAH-9908-2019; Li, Jia/AAB-6431-2019
OI Zhao, Dong/0000-0002-1233-3668; Zhao, Dong/0000-0003-3754-1524; Li,
   Jia/0000-0002-4346-8696; Xu, Long/0000-0002-9286-2876
FU National Natural Science Foundation of China [62132002, 11790305]; Peng
   Cheng Laboratory Cloud Brain [PCL2021A13]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62132002 and 11790305, and in part by
   Peng Cheng Laboratory Cloud Brain under Grant PCL2021A13.
NR 72
TC 0
Z9 0
U1 4
U2 10
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3516
EP 3531
DI 10.1109/TVCG.2022.3233900
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700013
PM 37018251
DA 2025-03-07
ER

PT J
AU Zhong, FH
   Xue, ML
   Zhang, J
   Zhang, F
   Ban, R
   Deussen, O
   Wang, YH
AF Zhong, Fahai
   Xue, Mingliang
   Zhang, Jian
   Zhang, Fan
   Ban, Rui
   Deussen, Oliver
   Wang, Yunhai
TI Force-Directed Graph Layouts Revisited: A New Force Based on the
   T-Distribution
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Layout; Computational modeling; Force; Stress; Springs; Scalability;
   Graphics processing units; Graph layout; force directed placement;
   student's t-distribution; FFT
ID DRAWING LARGE GRAPHS; STRESS MODEL; ALGORITHM
AB In this article, we propose the t-FDP model, a force-directed placement method based on a novel bounded short-range force (t-force) defined by Student's t-distribution. Our formulation is flexible, exerts limited repulsive forces for nearby nodes and can be adapted separately in its short- and long-range effects. Using such forces in force-directed graph layouts yields better neighborhood preservation than current methods, while maintaining low stress errors. Our efficient implementation using a Fast Fourier Transform is one order of magnitude faster than state-of-the-art methods and two orders faster on the GPU, enabling us to perform parameter tuning by globally and locally adjusting the t-force in real-time for complex graphs. We demonstrate the quality of our approach by numerical evaluation against state-of-the-art approaches and extensions for interactive exploration.
C1 [Zhong, Fahai; Xue, Mingliang; Wang, Yunhai] Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
   [Zhang, Jian] Chinese Acad Sci, Comp Network Informat Ctr, Beijing 100045, Peoples R China.
   [Zhang, Fan] Shandong Technol & Business Univ, Sch Comp Sci & Technol, Yantai 265600, Peoples R China.
   [Ban, Rui] CITC, Intelligent Network Design Inst, Beijing 100022, Peoples R China.
   [Deussen, Oliver] Univ Konstanz, Comp & Informat Sci, D-78464 Constance, Germany.
C3 Shandong University; Chinese Academy of Sciences; Computer Network
   Information Center, CAS; Shandong Technology & Business University;
   University of Konstanz
RP Wang, YH (corresponding author), Shandong Univ, Sch Comp Sci & Technol, Qingdao 266237, Peoples R China.
EM zhongfahai@gmail.com; xml95007@gmail.com; zhangjian@sccas.cn;
   zhangfan@sdtbu.edu.cn; banrui1@chinaunicom.cn;
   Oliver.Deussen@uni-konstanz.de; cloudseawang@gmail.com
RI Zhang, Fan/GLT-6231-2022; Deussen, Oliver/HKF-2004-2023
OI Zhong, Fahai/0000-0002-0971-0821; Zhang, Jian/0000-0003-1348-8124;
   Zhang, Fan/0000-0002-0343-3499; Xue, Mingliang/0000-0001-8842-1667
FU National Key Research& Development Plan of China [2019YFB1704201];
   Shandong Provincial Natural Science Foundation [ZR2022JQ32]; NSFC
   [62132017, 62141217]; Deutsche Forschungsgemeinschaft (DFG, German
   Research Foundation) under Germany's Excellence Strategy [EXC 2117 -
   422037984]
FX This work was supported in part by the National Key Research&
   Development Plan of China under Grant 2019YFB1704201, in part by the
   Shandong Provincial Natural Science Foundation under Grant
   ZR2022JQ32,and in part by the NSFC under Grants 62132017 and 62141217,
   as well as by the Deutsche Forschungsgemeinschaft (DFG, German Research
   Foundation) under Germany's Excellence Strategy - EXC 2117 - 422037984.
NR 48
TC 1
Z9 1
U1 2
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL
PY 2024
VL 30
IS 7
BP 3650
EP 3663
DI 10.1109/TVCG.2023.3238821
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XA4B5
UT WOS:001258936700017
PM 37021999
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Krüger, M
   Gerrits, T
   Römer, T
   Kuhlen, T
   Weissker, T
AF Krueger, Marcel
   Gerrits, Tim
   Roemer, Timon
   Kuhlen, Torsten
   Weissker, Tim
TI IntenSelect plus : Enhancing Score-Based Selection in Virtual Reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Visualization; Three-dimensional displays; Task analysis; Usability;
   Virtual environments; Shape; Engines; Virtual Reality; 3D User
   Interfaces; 3D Interaction; Selection; Score-Based Selection; Temporal
   Selection; IntenSelect
ID OBJECT SELECTION
AB Object selection in virtual environments is one of the most common and recurring interaction tasks. Therefore, the used technique can critically influence a system's overall efficiency and usability. IntenSelect is a scoring-based selection-by-volume technique that was shown to offer improved selection performance over conventional raycasting in virtual reality. This initial method, however, is most pronounced for small spherical objects that converge to a point-like appearance only, is challenging to parameterize, and has inherent limitations in terms of flexibility. We present an enhanced version of IntenSelect called IntenSelect+ designed to overcome multiple shortcomings of the original IntenSelect approach. In an empirical within-subjects user study with 42 participants, we compared IntenSelect+ to IntenSelect and conventional raycasting on various complex object configurations motivated by prior work. In addition to replicating the previously shown benefits of IntenSelect over raycasting, our results demonstrate significant advantages of IntenSelect+ over IntenSelect regarding selection performance, task load, and user experience. We, therefore, conclude that IntenSelect+ is a promising enhancement of the original approach that enables faster, more precise, and more comfortable object selection in immersive virtual environments.
C1 [Krueger, Marcel; Gerrits, Tim; Roemer, Timon; Kuhlen, Torsten; Weissker, Tim] Rhein Westfal TH Aachen, Visual Comp Inst, Aachen, Germany.
C3 RWTH Aachen University
RP Krüger, M (corresponding author), Rhein Westfal TH Aachen, Visual Comp Inst, Aachen, Germany.
EM krueger@vis.rwth-aachen.de; gerrits@vr.rwth-aachen.de;
   t.roemer@vr.rwth-aachen.de; kuhlen@vr.rwth-aachen.de; me@tim-weissker.de
RI ; Kuhlen, Torsten/A-1059-2017
OI Weissker, Tim/0000-0001-9119-811X; Kuhlen, Torsten/0000-0003-2144-4367;
   Kruger, Marcel/0000-0002-0085-268X
FU German Federal Ministry of Education and Research
FX No Statement Available
NR 46
TC 1
Z9 1
U1 1
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2024
VL 30
IS 5
BP 2829
EP 2838
DI 10.1109/TVCG.2024.3372077
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA OT7U2
UT WOS:001209605200003
PM 38437105
DA 2025-03-07
ER

PT J
AU Guillou, P
   Vidal, J
   Tierny, J
AF Guillou, Pierre
   Vidal, Jules
   Tierny, Julien
TI Discrete Morse Sandwich: Fast Computation of Persistence Diagrams for
   Scalar Data - An Algorithm and a Benchmark
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE discrete Morse theory; persistence diagrams; scalar data; Topological
   data analysis
ID TOPOLOGICAL SIMPLIFICATION; EFFICIENT COMPUTATION; COMPLEXES
AB This paper introduces an efficient algorithm for persistence diagram computation, given an input piecewise linear scalar field f defined on a d-dimensional simplicial complex K, with d <= 3. Our work revisits the seminal algorithm "PairSimplices" (Edelsbrunner et al. 2002), (Zomorodian, 2010) with discrete Morse theory (DMT) (Forman, 1998), (Robins et al. 2011), which greatly reduces the number of input simplices to consider. Further, we also extend to DMT and accelerate the stratification strategy described in "PairSimplices" (Edelsbrunner et al. 2002), (Zomorodian, 2010) for the fast computation of the 0th and (d - 1)th diagrams, noted D-0(f) and Dd-1(f ). Minima-saddle persistence pairs (D-0(f )) and saddle-maximum persistence pairs (Dd-1(f )) are efficiently computed by processing, with a Union-Find, the unstable sets of 1-saddles and the stable sets of (d - 1)-saddles. This fast precomputation for the dimensions 0 and (d - 1) enables an aggressive specialization of (Bauer et al. 2014) to the 3D case, which results in a drastic reduction of the number of input simplices for the computation of D-1(f ), the intermediate layer of the sandwich. Finally, we document several performance improvements via shared-memory parallelism. We provide an open-source implementation of our algorithm for reproducibility purposes. Extensive experiments indicate that our algorithm improves by two orders of magnitude the time performance of the seminal "PairSimplices" algorithm it extends. Moreover, it also improves memory footprint and time performance over a selection of 14 competing approaches, with a substantial gain over the fastest available approaches, while producing a strictly identical output.
C1 [Guillou, Pierre; Vidal, Jules; Tierny, Julien] CNRS, F-75005 Paris, France.
   [Guillou, Pierre; Vidal, Jules; Tierny, Julien] Sorbonne Univ, F-75005 Paris, France.
C3 Centre National de la Recherche Scientifique (CNRS); Sorbonne Universite
RP Tierny, J (corresponding author), CNRS, F-75005 Paris, France.
EM pierre.guillou@lip6.fr; jules.vidal@lip6.fr;
   julien.tierny@sorbonne-universite.fr
OI Vidal, Jules/0000-0002-1154-4391
FU European Commission
FX No Statement Available
NR 103
TC 6
Z9 6
U1 0
U2 3
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR
PY 2024
VL 30
IS 4
BP 1897
EP 1915
DI 10.1109/TVCG.2023.3238008
PG 19
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA JN9X1
UT WOS:001173975500001
PM 37021884
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Neuhauser, C
   Stumpfegger, J
   Westermann, R
AF Neuhauser, Christoph
   Stumpfegger, Josef
   Westermann, Ruediger
TI Adaptive Sampling of 3D Spatial Correlations for Focus plus Context
   Visualization
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Correlation; Three-dimensional displays; Meteorology; Visualization;
   Graphics processing units; Layout; Weather forecasting; Chord diagrams;
   correlation sampling; ensemble analysis
ID VISUAL ANALYSIS; ENSEMBLE; UNCERTAINTY; SENSITIVITY; TOOL
AB Visualizing spatial correlations in 3D ensembles is challenging due to the vast amounts of information that need to be conveyed. Memory and time constraints make it unfeasible to pre-compute and store the correlations between all pairs of domain points. We propose the embedding of adaptive correlation sampling into chord diagrams with hierarchical edge bundling to alleviate these constraints. Entities representing spatial regions are arranged along the circular chord layout via a space-filling curve, and Bayesian optimal sampling is used to efficiently estimate the maximum occurring correlation between any two points from different regions. Hierarchical edge bundling reduces visual clutter and emphasizes the major correlation structures. By selecting an edge, the user triggers a focus diagram in which only the two regions connected via this edge are refined and arranged in a specific way in a second chord layout. For visualizing correlations between two different variables, which are not symmetric anymore, we switch to showing a full correlation matrix. This avoids drawing the same edges twice with different correlation values. We introduce GPU implementations of both linear and non-linear correlation measures to further reduce the time that is required to generate the context and focus views, and to even enable the analysis of correlations in a 1000-member ensemble.
C1 [Neuhauser, Christoph; Stumpfegger, Josef; Westermann, Ruediger] Tech Univ Munich, D-80333 Munich, Germany.
C3 Technical University of Munich
RP Neuhauser, C (corresponding author), Tech Univ Munich, D-80333 Munich, Germany.
EM christoph.neuhauser@tum.de; ga87tux@mytum.de; westermann@tum.de
OI Westermann, Rudiger/0000-0002-3394-0731; Neuhauser,
   Christoph/0000-0002-0290-1991
NR 20
TC 0
Z9 0
U1 3
U2 5
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB
PY 2024
VL 30
IS 2
BP 1608
EP 1623
DI 10.1109/TVCG.2023.3326855
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EC6D7
UT WOS:001136746300004
PM 37874723
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Hnatyshyn, R
   Zhao, JQ
   Perez, D
   Ahrens, J
   Maciejewski, R
AF Hnatyshyn, Rostyslav
   Zhao, Jieqiong
   Perez, Danny
   Ahrens, James
   Maciejewski, Ross
TI MolSieve: A Progressive Visual Analytics System for Molecular Dynamics
   Simulations
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Trajectory; Analytical models; Biological system modeling; Visual
   analytics; Three-dimensional displays; Computational modeling; Data
   models; Molecular dynamics; time-series analysis; visual analytics
ID VISUALIZATION
AB Molecular Dynamics (MD) simulations are ubiquitous in cutting-edge physio-chemical research. They provide critical insights into how a physical system evolves over time given a model of interatomic interactions. Understanding a system's evolution is key to selecting the best candidates for new drugs, materials for manufacturing, and countless other practical applications. With today's technology, these simulations can encompass millions of unit transitions between discrete molecular structures, spanning up to several milliseconds of real time. Attempting to perform a brute-force analysis with data-sets of this size is not only computationally impractical, but would not shed light on the physically-relevant features of the data. Moreover, there is a need to analyze simulation ensembles in order to compare similar processes in differing environments. These problems call for an approach that is analytically transparent, computationally efficient, and flexible enough to handle the variety found in materials-based research. In order to address these problems, we introduce MolSieve, a progressive visual analytics system that enables the comparison of multiple long-duration simulations. Using MolSieve, analysts are able to quickly identify and compare regions of interest within immense simulations through its combination of control charts, data-reduction techniques, and highly informative visual components. A simple programming interface is provided which allows experts to fit MolSieve to their needs. To demonstrate the efficacy of our approach, we present two case studies of MolSieve and report on findings from domain collaborators.
C1 [Hnatyshyn, Rostyslav; Zhao, Jieqiong; Maciejewski, Ross] Arizona State Univ, Tempe, AZ 85287 USA.
   [Perez, Danny; Ahrens, James] Los Alamos Natl Lab, Los Alamos, NM USA.
C3 Arizona State University; Arizona State University-Tempe; United States
   Department of Energy (DOE); Los Alamos National Laboratory
RP Hnatyshyn, R (corresponding author), Arizona State Univ, Tempe, AZ 85287 USA.
EM rhnatysh@asu.edu; jzhao@asu.edu; danny_perez@lanl.gov; ahrens@lanl.gov;
   rmacieje@asu.edu
RI Zhao, Jieqiong/HLH-8586-2023; Perez, Danny/AEQ-0157-2022
OI Zhao, Jieqiong/0000-0002-4303-7722
FU Exascale Computing Project
FX No Statement Available
NR 54
TC 1
Z9 1
U1 1
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 727
EP 737
DI 10.1109/TVCG.2023.3326584
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500111
PM 37938968
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Shi, CH
   Cui, WW
   Liu, CZ
   Zheng, CB
   Zhang, HD
   Luo, Q
   Ma, XJ
AF Shi, Chuhan
   Cui, Weiwei
   Liu, Chengzhong
   Zheng, Chengbo
   Zhang, Haidong
   Luo, Qiong
   Ma, Xiaojuan
TI NL2Color: Refining Color Palettes for Charts with Natural Language
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE chart; color palette; natural language; large language model
ID VISUALIZATION
AB Choice of color is critical to creating effective charts with an engaging, enjoyable, and informative reading experience. However, designing a good color palette for a chart is a challenging task for novice users who lack related design expertise. For example, they often find it difficult to articulate their abstract intentions and translate these intentions into effective editing actions to achieve a desired outcome. In this work, we present NL2Color, a tool that allows novice users to refine chart color palettes using natural language expressions of their desired outcomes. We first collected and categorized a dataset of 131 triplets, each consisting of an original color palette of a chart, an editing intent, and a new color palette designed by human experts according to the intent. Our tool employs a large language model (LLM) to substitute the colors in original palettes and produce new color palettes by selecting some of the triplets as few-shot prompts. To evaluate our tool, we conducted a comprehensive two-stage evaluation, including a crowd-sourcing study (N=71) and a within-subjects user study (N=12). The results indicate that the quality of the color palettes revised by NL2Color has no significantly large difference from those designed by human experts. The participants who used NL2Color obtained revised color palettes to their satisfaction in a shorter period and with less effort.
C1 [Shi, Chuhan] Southeast Univ, Nanjing, Peoples R China.
   [Shi, Chuhan; Liu, Chengzhong; Zheng, Chengbo; Luo, Qiong; Ma, Xiaojuan] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Cui, Weiwei; Zhang, Haidong] Microsoft Res Asia, Beijing, Peoples R China.
   [Luo, Qiong] Hong Kong Univ Sci & Technol Guangzhou, Hong Kong, Peoples R China.
C3 Southeast University - China; Hong Kong University of Science &
   Technology; Microsoft Research Asia; Microsoft; Microsoft China
RP Shi, CH (corresponding author), Southeast Univ, Nanjing, Peoples R China.; Shi, CH (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
EM cshiag@connect.ust.hk; weiweicu@microsoft.com;
   chengzhong.liu@connect.ust.hk; cb.zheng@connect.ust.hk;
   haizhang@microsoft.com; luo@cse.ust.hk; mxj@cse.ust.hk
RI Zhang, Haidong/J-9302-2019
OI Luo, Qiong/0000-0002-2861-9492
FU HKUST 30 for 30
FX No Statement Available
NR 57
TC 3
Z9 3
U1 2
U2 2
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2024
VL 30
IS 1
BP 814
EP 824
DI 10.1109/TVCG.2023.3326522
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA HJ3Z6
UT WOS:001159106500091
PM 37871067
DA 2025-03-07
ER

PT J
AU Yang, GW
   Zhou, WY
   Peng, HY
   Liang, D
   Mu, TJ
   Hu, SM
AF Yang, Guo-Wei
   Zhou, Wen-Yang
   Peng, Hao-Yang
   Liang, Dun
   Mu, Tai-Jiang
   Hu, Shi-Min
TI Recursive-NeRF: An Efficient and Dynamically Growing NeRF
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Rendering (computer graphics); Neural networks; Complexity theory;
   Uncertainty; Training; Three-dimensional displays; Image color analysis;
   Scene representation; view synthesis; image-based rendering; volume
   rendering; 3D deep learning
ID REPRESENTATION
AB View synthesis methods using implicit continuous shape representations learned from a set of images, such as the Neural Radiance Field (NeRF) method, have gained increasing attention due to their high quality imagery and scalability to high resolution. However, the heavy computation required by its volumetric approach prevents NeRF from being useful in practice; minutes are taken to render a single image of a few megapixels. Now, an image of a scene can be rendered in a level-of-detail manner, so we posit that a complicated region of the scene should be represented by a large neural network while a small neural network is capable of encoding a simple region, enabling a balance between efficiency and quality. Recursive-NeRF is our embodiment of this idea, providing an efficient and adaptive rendering and training approach for NeRF. The core of Recursive-NeRF learns uncertainties for query coordinates, representing the quality of the predicted color and volumetric intensity at each level. Only query coordinates with high uncertainties are forwarded to the next level to a bigger neural network with a more powerful representational capability. The final rendered image is a composition of results from neural networks of all levels. Our evaluation on public datasets and a large-scale scene dataset we collected shows that Recursive-NeRF is more efficient than NeRF while providing state-of-the-art quality. The code will be available at https://github.com/Gword/Recursive-NeRF
C1 [Yang, Guo-Wei; Zhou, Wen-Yang; Peng, Hao-Yang; Liang, Dun; Mu, Tai-Jiang; Hu, Shi-Min] Tsinghua Univ, BNRist, Beijing 100084, Peoples R China.
C3 Tsinghua University
RP Hu, SM (corresponding author), Tsinghua Univ, BNRist, Beijing 100084, Peoples R China.
EM ygw19@mails.tsinghua.edu.cn; zhouwy19@mails.tsinghua.edu.cn;
   phy18@mails.tsinghua.edu.cn; randonlang@gmail.com;
   taijiang@tsinghua.edu.cn; shimin@tsinghua.edu.cn
RI Wenyang, Zhou/GSD-3239-2022; Hu, Shi-Min/AAW-1952-2020; Mu,
   Tai-Jiang/JWO-1381-2024
OI Hu, Shi-Min/0000-0001-7507-6542; Mu, Tai-Jiang/0000-0002-9197-346X
FU National Key R&D Program of China [2021ZD0112902]
FX This work was supported by the National Key R&D Program of China under
   Grant 2021ZD0112902.
NR 57
TC 9
Z9 10
U1 6
U2 27
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC
PY 2023
VL 29
IS 12
BP 5124
EP 5136
DI 10.1109/TVCG.2022.3204608
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA DE1U2
UT WOS:001130270300021
PM 36194712
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ishikawa, R
   Saito, H
   Kalkofen, D
   Mori, S
AF Ishikawa, Reina
   Saito, Hideo
   Kalkofen, Denis
   Mori, Shohei
TI Multi-Layer Scene Representation from Composed Focal Stacks
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 17th International Conference on High Speed Machining (HSM)
CY OCT 25-28, 2023
CL Nanjing, PEOPLES R CHINA
DE Multi-layered scene representation; focal stack; view synthesis;
   AR-supported imaging
AB Multi-layer images are a powerful scene representation for high-performance rendering in virtual/augmented reality (VR/AR). The major approach to generate such images is to use a deep neural network trained to encode colors and alpha values of depth certainty on each layer using registered multi-view images. A typical network is aimed at using a limited number of nearest views. Therefore, local noises in input images from a user-navigated camera deteriorate the final rendering quality and interfere with coherency over view transitions. We propose to use a focal stack composed of multi-view inputs to diminish such noises. We also provide theoretical analysis for ideal focal stacks to generate multi-layer images. Our results demonstrate the advantages of using focal stacks in coherent rendering, memory footprint, and AR-supported data capturing. We also show three applications of imaging for VR.
C1 [Ishikawa, Reina; Saito, Hideo; Mori, Shohei] Keio Univ, Minato City, Japan.
   [Kalkofen, Denis] Flinders Univ S Australia, Adelaide, Australia.
   [Kalkofen, Denis; Mori, Shohei] Graz Univ Technol, Graz, Austria.
C3 Keio University; Flinders University South Australia; Graz University of
   Technology
RP Mori, S (corresponding author), Keio Univ, Minato City, Japan.; Mori, S (corresponding author), Graz Univ Technol, Graz, Austria.
EM reina-ishikawa@keio.jp; hs@keio.jp; kalkofen@icg.tugraz.at;
   s.mori.jp@ieee.org
RI Saito, Hideo/D-6223-2014; Mori, Shohei/AAL-6642-2020; Saito,
   Hideo/ADZ-2013-2022
OI Kalkofen, Denis/0000-0002-0359-206X; Saito, Hideo/0000-0002-2421-9862;
   Mori, Shohei/0000-0003-0540-7312
FU Austrian Science Fund FWF [P33634]; Austrian Science Fund (FWF) [P33634]
   Funding Source: Austrian Science Fund (FWF)
FX This work was supported by the Austrian Science Fund FWF (grant
   no.P33634). The authors used the computational resource of AI Bridging
   Cloud Infrastructure (ABCI) provided by the National Institute of
   Advanced Industrial Science and Technology (AIST). The authors thank
   Shiori Ueda for her support on synthetic dataset generation. The authors
   also thank Thomas Layer for providing light field data inFig. 2.
NR 60
TC 1
Z9 1
U1 1
U2 1
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2023
VL 29
IS 11
BP 4719
EP 4729
DI 10.1109/TVCG.2023.3320248
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA X6ZW5
UT WOS:001099919100034
PM 37782615
OA hybrid
DA 2025-03-07
ER

PT J
AU Linhares, CDG
   Lima, DM
   Ponciano, JR
   Olivatto, MM
   Gutierrez, MA
   Poco, J
   Traina, C Jr
   Traina, AJM
AF Linhares, Claudio D. G.
   Lima, Daniel M.
   Ponciano, Jean R.
   Olivatto, Mauro M.
   Gutierrez, Marco A.
   Poco, Jorge
   Traina Jr, Caetano
   Traina, Agma J. M.
TI ClinicalPath: A Visualization Tool to Improve the Evaluation of
   Electronic Health Records in Clinical Decision-Making
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Information visualization; interactive visualizations; human-computer
   interaction; electronic health records
ID GUIDANCE
AB Physicians work at a very tight schedule and need decision-making support tools to help on improving and doing their work in a timely and dependable manner. Examining piles of sheets with test results and using systems with little visualization support to provide diagnostics is daunting, but that is still the usual way for the physicians' daily procedure, especially in developing countries. Electronic Health Records systems have been designed to keep the patients' history and reduce the time spent analyzing the patient's data. However, better tools to support decision-making are still needed. In this article, we propose ClinicalPath, a visualization tool for users to track a patient's clinical path through a series of tests and data, which can aid in treatments and diagnoses. Our proposal is focused on patient's data analysis, presenting the test results and clinical history longitudinally. Both the visualization design and the system functionality were developed in close collaboration with experts in the medical domain to ensure a right fit of the technical solutions and the real needs of the professionals. We validated the proposed visualization based on case studies and user assessments through tasks based on the physician's daily activities. Our results show that our proposed system improves the physicians' experience in decision-making tasks, made with more confidence and better usage of the physicians' time, allowing them to take other needed care for the patients.
C1 [Linhares, Claudio D. G.; Lima, Daniel M.; Traina Jr, Caetano; Traina, Agma J. M.] Univ Sao Paulo, Inst Math & Comp Sci, BR-05508070 Sao Carlos, Brazil.
   [Lima, Daniel M.; Gutierrez, Marco A.] Univ Sao Paulo, Lab Informat Biomed, Inst Coracao, Hospitaldas Clin HCFMUSP,Fac Med, BR-05508070 Sao Paulo, Brazil.
   [Ponciano, Jean R.] Getulio Vargas Fdn, Sch Appl Math, BR-22250900 Rio De Janeiro, Brazil.
   [Olivatto, Mauro M.] Fed Univ Fronteira Sul, Grad Course Med, BR-89802112 Chapeco, Brazil.
C3 Universidade de Sao Paulo; Universidade de Sao Paulo; Getulio Vargas
   Foundation; Universidade Federal da Fronteira Sul
RP Linhares, CDG (corresponding author), Univ Sao Paulo, Inst Math & Comp Sci, BR-05508070 Sao Carlos, Brazil.
EM claudiodgl@usp.br; daniel.lima@incor.usp.br; jean.ponciano@fgv.br;
   mauro_olivatto@hotmail.com; marco.gutierrez@incor.usp.br;
   jorge.poco@fgv.br; caetano@icmc.usp.br; agma@icmc.usp.br
RI Ponciano, Jean/AGE-0314-2022; Linhares, Claudio/AAJ-8869-2021; Traina,
   Caetano/E-9814-2011; Poco, Jorge/F-3344-2016; Lima, Daniel
   Mario/S-5366-2017; Gutierrez, Marco/G-6926-2012; Traina,
   Agma/F-1299-2011
OI Linhares, Claudio/0000-0001-7012-4461; Traina,
   Caetano/0000-0002-6625-6047; Ponciano, Jean Roberto/0000-0003-4629-3542;
   Poco, Jorge/0000-0001-9096-6287; Lima, Daniel Mario/0000-0002-7818-6103;
   Gutierrez, Marco/0000-0003-0964-6222; Traina, Agma/0000-0003-4929-7258
FU Sao Paulo Research Foundation (FAPESP) [2020/10049-0, 2020/07200-9,
   2016/17078-0]; Conselho Nacional de Desenvolvimento Cientifico e
   Tecnologico (CNPq); Coordenacao de Aperfeicoamento de Pessoal de Nivel
   Superior (CAPES) [312483/2018-0]; Rio de Janeiro Research Foundation
   (FAPERJ) [E-26/201.424/2021]; Foxconn Technology Group; Zerbini
   Foundation; Getulio Vargas Foundation
FX This work was supported in part by Sao Paulo Research Foundation
   (FAPESP) under Grants #2020/10049-0, #2020/07200-9, and #2016/17078-0;
   in part by the Conselho Nacional de Desenvolvimento Cientifico e
   Tecnologico (CNPq) and Coordenacao de Aperfeicoamento de Pessoal de
   Nivel Superior (CAPES) under Grant #312483/2018-0 in part by Rio de
   Janeiro Research Foundation (FAPERJ) under Grant #E-26/201.424/2021, in
   part by Foxconn Technology Group, Zerbini Foundation under Project
   AIMED-CATI 030/2007 FOXCONN-001/2019, and in part by Getulio Vargas
   Foundation.
NR 43
TC 5
Z9 5
U1 2
U2 11
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD OCT 1
PY 2023
VL 29
IS 10
BP 4031
EP 4046
DI 10.1109/TVCG.2022.3175626
PG 16
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA Q8ZW3
UT WOS:001060356200001
PM 35588413
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Ling, JW
   Wang, ZB
   Lu, M
   Wang, Q
   Qian, C
   Xu, F
AF Ling, Jingwang
   Wang, Zhibo
   Lu, Ming
   Wang, Quan
   Qian, Chen
   Xu, Feng
TI Semantically Disentangled Variational Autoencoder for Modeling 3D Facial
   Details
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Detail reconstruction; facial animation; semantic disentanglement
ID FACE RECONSTRUCTION
AB Parametric face models, such as morphable and blendshape models, have shown great potential in face representation, reconstruction, and animation. However, all these models focus on large-scale facial geometry. Facial details such as wrinkles are not parameterized in these models, impeding accuracy and realism. In this article, we propose a method to learn a Semantically Disentangled Variational Autoencoder (SDVAE) to parameterize facial details and support independent detail manipulation as an extension of an off-the-shelf large-scale face model. Our method utilizes the non-linear capability of Deep Neural Networks for detail modeling, achieving better accuracy and greater representation power compared with linear models. In order to disentangle the semantic factors of identity, expression and age, we propose to eliminate the correlation between different factors in an adversarial manner. Therefore, wrinkle-level details of various identities, expressions, and ages can be generated and independently controlled by changing latent vectors of our SDVAE. We further leverage our model to reconstruct 3D faces via fitting to facial scans and images. Benefiting from our parametric model, we achieve accurate and robust reconstruction, and the reconstructed details can be easily animated and manipulated. We evaluate our method on practical applications, including scan fitting, image fitting, video tracking, model manipulation, and expression and age animation. Extensive experiments demonstrate that the proposed method can robustly model facial details and achieve better results than alternative methods.
C1 [Ling, Jingwang; Wang, Zhibo; Xu, Feng] Tsinghua Univ, Sch Software & BNRist, Beijing 100084, Peoples R China.
   [Lu, Ming] Intel Labs, Beijing 100086, Peoples R China.
   [Wang, Quan; Qian, Chen] Sensetime Res, Beijing, Peoples R China.
C3 Tsinghua University; Intel Corporation; Intel China
RP Xu, F (corresponding author), Tsinghua Univ, Sch Software & BNRist, Beijing 100084, Peoples R China.
EM lingjw20@mails.tsinghua.edu.cn; wzb17@mails.tsinghua.edu.cn;
   lu199192@gmail.com; wangquan@sensetime.com; qianchen@sensetime.com;
   xufeng2003@gmail.com
OI Ling, Jingwang/0000-0001-8746-8578
FU Beijing Natural Science Foundation [JQ19015]; NSFC [62021002, 61727808];
   National Key Ramp;D Program of China [2018YFA0704000]; THUIBCS; Tsinghua
   University; BLBCI; Beijing Municipal Education Commission
FX This work was supported in part by Beijing Natural Science Foundation
   underGrant JQ19015, in part by the NSFC under Grants 62021002 and
   61727808,in part by the National Key R & D Program of China under Grant
   2018YFA0704000. This work was supported in part by THUIBCS, Tsinghua
   University and BLBCI, Beijing Municipal Education Commission.
NR 52
TC 6
Z9 6
U1 4
U2 17
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD AUG 1
PY 2023
VL 29
IS 8
BP 3630
EP 3641
DI 10.1109/TVCG.2022.3166666
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA L3CU5
UT WOS:001022080200013
PM 35412983
DA 2025-03-07
ER

PT J
AU Shen, LX
   Shen, EY
   Luo, YY
   Yang, XC
   Hu, XM
   Zhang, XS
   Tai, ZW
   Wang, JM
AF Shen, Leixian
   Shen, Enya
   Luo, Yuyu
   Yang, Xiaocong
   Hu, Xuming
   Zhang, Xiongshuai
   Tai, Zhiwei
   Wang, Jianmin
TI Towards Natural Language Interfaces for Data Visualization: A Survey
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Natural language processing; Task
   analysis; Human computer interaction; Software; Data mining; natural
   language interfaces; survey
ID INTERACTIVE VISUALIZATION; NARRATIVE VISUALIZATION; MULTIMODAL
   INTERACTION; EXPLORATORY ANALYSIS; TASK; GENERATION; SYSTEM; USER;
   DESIGN; GRAMMAR
AB Utilizing Visualization-oriented Natural Language Interfaces (V-NLI) as a complementary input modality to direct manipulation for visual analytics can provide an engaging user experience. It enables users to focus on their tasks rather than having to worry about how to operate visualization tools on the interface. In the past two decades, leveraging advanced natural language processing technologies, numerous V-NLI systems have been developed in academic research and commercial software, especially in recent years. In this article, we conduct a comprehensive review of the existing V-NLIs. In order to classify each article, we develop categorical dimensions based on a classic information visualization pipeline with the extension of a V-NLI layer. The following seven stages are used: query interpretation, data transformation, visual mapping, view transformation, human interaction, dialogue management, and presentation. Finally, we also shed light on several promising directions for future work in the V-NLI community.
C1 [Shen, Leixian; Shen, Enya; Luo, Yuyu; Yang, Xiaocong; Hu, Xuming; Zhang, Xiongshuai; Tai, Zhiwei; Wang, Jianmin] Tsinghua Univ, Beijing 100190, Peoples R China.
C3 Tsinghua University
RP Shen, EY (corresponding author), Tsinghua Univ, Beijing 100190, Peoples R China.
EM slx20@mails.tsinghua.edu.cn; shenenya@tsinghua.edu.cn;
   luoyy18@mails.tsinghua.edu.cn; yangxc18@mails.tsinghua.edu.cn;
   hxm19@mails.tsinghua.edu.cn; zxs21@mails.tsinghua.edu.cn;
   tzw20@mails.tsinghua.edu.cn; jimwang@tsinghua.edu.cn
OI Luo, Yuyu/0000-0001-9530-3327; Shen, Leixian/0000-0003-1084-4912
FU National Natural Science Foundation of China [71690231]; Beijing Key
   Laboratory of Industrial Bigdata System and Application; Zhejiang Lab's
   International Talent Fund for Young Professionals
FX This work was supported by the National Natural Science Foundation of
   China under Grant 71690231 and in part by the Beijing Key Laboratory of
   Industrial Bigdata System and Application. The work of Yuyu Luo was
   supported by the Zhejiang Lab's International Talent Fund for Young
   Professionals.
NR 290
TC 43
Z9 50
U1 5
U2 44
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUN 1
PY 2023
VL 29
IS 6
BP 3121
EP 3144
DI 10.1109/TVCG.2022.3148007
PG 24
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA F4DZ2
UT WOS:000981880500021
PM 35104221
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Moullec, Y
   Cogné, M
   Saint-Aubert, J
   Lécuyer, A
AF Moullec, Yann
   Cogne, Melanie
   Saint-Aubert, Justine
   Lecuyer, Anatole
TI Assisted walking-in-place: Introducing assisted motion to
   walking-by-cycling in embodied virtual reality
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Legged locomotion; Fatigue; Navigation; Avatars; Propioception; Virtual
   environments; User interfaces; Embodiment; virtual walk; avatar;
   perceived effort
ID AGENCY; SENSE; BODY; STIMULATION; SENSATION
AB In this paper, we investigate the use of a motorized bike to support the walk of a self-avatar in virtual reality (VR). While existing walking-in-place (WIP) techniques render compelling walking experiences, they can be judged repetitive and strenuous. Our approach consists in assisting a WIP technique so that the user does not have to actively move in order to reduce effort and fatigue. We chose to assist a technique called walking-by-cycling, which consists in mapping the cycling motion of a bike onto the walking of the user's self-avatar, by using a motorized bike. We expected that our approach could provide participants with a compelling walking experience while reducing the effort required to navigate. We conducted a within-subjects study where we compared "assisted walking-by-cycling" to a traditional active walking-by-cycling implementation, and to a standard condition where the user is static. In the study, we measured embodiment, including ownership and agency, walking sensation, perceived effort and fatigue. Results showed that assisted walking-by-cycling induced more ownership, agency, and walking sensation than the static simulation. Additionally, assisted walking-by-cycling induced levels of ownership and walking sensation similar to that of active walking-by-cycling, but it induced less perceived effort. Taken together, this work promotes the use of assisted walking-by-cycling in situations where users cannot or do not want to exert much effort while walking in embodied VR such as for injured or disabled users, for prolonged uses, medical rehabilitation, or virtual visits.
C1 [Moullec, Yann; Cogne, Melanie; Saint-Aubert, Justine; Lecuyer, Anatole] Univ Rennes, Inria, CNRS, IRISA, Rennes, France.
   [Cogne, Melanie] CHU Rennes, Rennes, France.
C3 Universite de Rennes; Inria; Centre National de la Recherche
   Scientifique (CNRS); CHU Rennes; Universite de Rennes
RP Moullec, Y (corresponding author), Univ Rennes, Inria, CNRS, IRISA, Rennes, France.
EM yann.moullec@inria.fr; melanie.cogne@chu-rennes.fr;
   justine.saint-aubert@inria.fr; anatole.lecuyer@inria.fr
RI Cogne, Melanie/JVN-4300-2024
OI Saint-Aubert, Justine/0000-0001-8412-653X; Moullec,
   Yann/0000-0002-1604-8864
NR 69
TC 6
Z9 6
U1 6
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2023
VL 29
IS 5
BP 2796
EP 2805
DI 10.1109/TVCG.2023.3247070
PG 10
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA C4PV6
UT WOS:000961761400001
PM 37015135
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Lin, TC
   Yang, YL
   Beyer, J
   Pfister, H
AF Lin, Tica
   Yang, Yalong
   Beyer, Johanna
   Pfister, Hanspeter
TI Labeling Out-of-View Objects in Immersive Analytics to Support Situated
   Visual Searching
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Object labeling; mixed / augmented reality; immersive analytics;
   situated analytics; data visualization
ID AUGMENTED REALITY; VISUALIZATION; STRATEGIES; MANAGEMENT; NAVIGATION
AB Augmented Reality (AR) embeds digital information into objects of the physical world. Data can be shown in-situ, thereby enabling real-time visual comparisons and object search in real-life user tasks, such as comparing products and looking up scores in a sports game. While there have been studies on designing AR interfaces for situated information retrieval, there has only been limited research on AR object labeling for visual search tasks in the spatial environment. In this article, we identify and categorize different design aspects in AR label design and report on a formal user study on labels for out-of-view objects to support visual search tasks in AR. We design three visualization techniques for out-of-view object labeling in AR, which respectively encode the relative physical position (height-encoded), the rotational direction (angle-encoded), and the label values (value-encoded) of the objects. We further implement two traditional in-view object labeling techniques, where labels are placed either next to the respective objects (situated) or at the edge of the AR FoV (boundary). We evaluate these five different label conditions in three visual search tasks for static objects. Our study shows that out-of-view object labels are beneficial when searching for objects outside the FoV, spatial orientation, and when comparing multiple spatially sparse objects. Angle-encoded labels with directional cues of the surrounding objects have the overall best performance with the highest user satisfaction. We discuss the implications of our findings for future immersive AR interface design.
C1 [Lin, Tica; Beyer, Johanna; Pfister, Hanspeter] Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
   [Yang, Yalong] Virginia Tech, Dept Comp Sci, Blacksburg, VA 24060 USA.
C3 Harvard University; Virginia Polytechnic Institute & State University
RP Lin, TC (corresponding author), Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA.
EM mlin@g.harvard.edu; yalongyang@vt.edu; jbeyer@g.harvard.edu;
   pfister@g.harvard.edu
OI Pfister, Hanspeter/0000-0002-3620-2582; Yang,
   Yalong/0000-0001-9414-9911; Lin, Tica/0000-0002-2860-0871; Beyer,
   Johanna/0000-0002-3505-9171
FU National Science Foundation (NSF) [III-2107328]; Harvard Physical
   Sciences and Engineering Accelerator Award
FX This work was supported in part by the National Science Foundation (NSF)
   under Grant III-2107328 and a Harvard Physical Sciences and Engineering
   Accelerator Award.
NR 74
TC 14
Z9 15
U1 3
U2 21
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2023
VL 29
IS 3
BP 1831
EP 1844
DI 10.1109/TVCG.2021.3133511
PG 14
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 8N3OW
UT WOS:000925059900016
PM 34882554
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Yang, J
   Gao, L
   Tan, QY
   Huang, YH
   Xia, SH
   Lai, YK
AF Yang, Jie
   Gao, Lin
   Tan, Qingyang
   Huang, Yi-Hua
   Xia, Shihong
   Lai, Yu-Kun
TI Multiscale Mesh Deformation Component Analysis With Attention-Based
   Autoencoders
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Multi-scale; shape analysis; attention mechanism; sparse regularization;
   stacked auto-encoder
ID SHAPE
AB Deformation component analysis is a fundamental problem in geometry processing and shape understanding. Existing approaches mainly extract deformation components in local regions at a similar scale while deformations of real-world objects are usually distributed in a multi-scale manner. In this article, we propose a novel method to exact multiscale deformation components automatically with a stacked attention-based autoencoder. The attention mechanism is designed to learn to softly weight multi-scale deformation components in active deformation regions, and the stacked attention-based autoencoder is learned to represent the deformation components at different scales. Quantitative and qualitative evaluations show that our method outperforms state-of-the-art methods. Furthermore, with the multiscale deformation components extracted by our method, the user can edit shapes in a coarse-to-fine fashion which facilitates effective modeling of new shapes.
C1 [Yang, Jie; Gao, Lin; Huang, Yi-Hua; Xia, Shihong] Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.
   [Yang, Jie; Gao, Lin; Huang, Yi-Hua; Xia, Shihong] Univ Chinese Acad Sci, Beijing 100864, Peoples R China.
   [Tan, Qingyang] Univ Maryland, College Pk, MD 20742 USA.
   [Lai, Yu-Kun] Cardiff Univ, Sch Comp Sci & Informat, Cardiff CF10 3AT, Wales.
C3 Chinese Academy of Sciences; Institute of Computing Technology, CAS;
   Chinese Academy of Sciences; University of Chinese Academy of Sciences,
   CAS; University System of Maryland; University of Maryland College Park;
   Cardiff University
RP Gao, L (corresponding author), Chinese Acad Sci, Inst Comp Technol, Beijing Key Lab Mobile Comp & Pervas Device, Beijing 100190, Peoples R China.; Gao, L (corresponding author), Univ Chinese Acad Sci, Beijing 100864, Peoples R China.
EM yangjie01@ict.ac.cn; gaolin@ict.ac.cn; qytan@cs.umd.edu;
   huangyihua16@mails.ucas.ac.cn; xsh@ict.ac.cn; LaiY4@cardiff.ac.uk
RI Yang, Jie/IAO-3586-2023; Gao, Lin/JNF-0375-2023; Lai,
   Yu-Kun/D-2343-2010; Tan, Qingyang/ISA-4109-2023
OI Lai, Yukun/0000-0002-2094-5680; Tan, Qingyang/0000-0002-9269-5289
FU National Natural Science Foundation of China [62061136007, 61872440];
   Beijing Municipal Natural Science Foundation [L182016]; Science and
   Technology Service Network Initiative; Chinese Academy of Sciences
   [KFJ-STS-QYZD-2021-11-001]; Royal Society Newton Advanced Fellowship
   [NAF\R2\192151]; Youth Innovation Promotion Association CAS; Zhejiang
   Lab [2021KE0AB06]
FX This work was supported by the National Natural Science Foundation
   ofChina under Grants 62061136007 and 61872440, the Beijing Municipal
   Nat-ural Science Foundation under Grant L182016, the Science and
   TechnologyService Network Initiative, Chinese Academy of Sciences under
   Grant KFJ-STS-QYZD-2021-11-001, Royal Society Newton Advanced Fellowship
   underGrant NAF\R2\192151, the Youth Innovation Promotion Association
   CASand the Open Research Projects of Zhejiang Lab under Grant
   2021KE0AB06
NR 79
TC 6
Z9 6
U1 0
U2 9
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD FEB 1
PY 2023
VL 29
IS 2
BP 1301
EP 1317
DI 10.1109/TVCG.2021.3112526
PG 17
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 7M2HO
UT WOS:000906475100002
PM 34520358
OA Green Accepted, Green Submitted
DA 2025-03-07
ER

PT J
AU Morrical, N
   Sahistan, A
   Güdükbay, U
   Wald, I
   Pascucci, V
AF Morrical, Nate
   Sahistan, Alper
   Gudukbay, Ugur
   Wald, Ingo
   Pascucci, Valerio
TI Quick Clusters: A GPU-Parallel Partitioning for Efficient Path Tracing
   of Unstructured Volumetric Grids
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Ray Tracing; Path Tracing; Volume Rendering; Scientific Visualization;
   Delta Tracking
ID AMR DATA; VISUALIZATION
AB We propose a simple yet effective method for clustering finite elements to improve preprocessing times and rendering performance of unstructured volumetric grids without requiring auxiliary connectivity data. Rather than building bounding volume hierarchies (BVHs) over individual elements, we sort elements along with a Hilbert curve and aggregate neighboring elements together, improving BVH memory consumption by over an order of magnitude. Then to further reduce memory consumption, we cluster the mesh on the fly into sub-meshes with smaller indices using a series of efficient parallel mesh re-indexing operations. These clusters are then passed to a highly optimized ray tracing API for point containment queries and ray-cluster intersection testing. Each cluster is assigned a maximum extinction value for adaptive sampling, which we rasterize into non-overlapping view-aligned bins allocated along the ray. These maximum extinction bins are then used to guide the placement of samples along the ray during visualization, reducing the number of samples required by multiple orders of magnitude (depending on the dataset), thereby improving overall visualization interactivity. Using our approach, we improve rendering performance over a competitive baseline on the NASA Mars Lander dataset from 6x (1 frame per second (fps) and 1.0 M rays per second (rps) up to now 6 fps and 12.4 M rps, now including volumetric shadows) while simultaneously reducing memory consumption by 3x (33 GB down to 11 GB) and avoiding any offline preprocessing steps, enabling high-quality interactive visualization on consumer graphics cards. Then by utilizing the full 48 GB of an RTX 8000, we improve the performance of Lander by 17x (1 fps up to 17 fps, 1.0 M rps up to 35.6 M rps).
C1 [Morrical, Nate; Pascucci, Valerio] Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.
   [Morrical, Nate; Wald, Ingo] NVIDIA, Santa Clara, CA 95051 USA.
   [Sahistan, Alper; Gudukbay, Ugur] Bilkent Univ, Ankara, Turkiye.
C3 Utah System of Higher Education; University of Utah; Nvidia Corporation;
   Ihsan Dogramaci Bilkent University
RP Morrical, N (corresponding author), Univ Utah, SCI Inst, Salt Lake City, UT 84112 USA.; Morrical, N (corresponding author), NVIDIA, Santa Clara, CA 95051 USA.
RI pascucci, Valerio/GXF-0616-2022
OI Sahistan, Alper/0000-0002-3480-7713; pascucci,
   valerio/0000-0002-8877-2042
NR 53
TC 5
Z9 5
U1 2
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2023
VL 29
IS 1
BP 537
EP 547
DI 10.1109/TVCG.2022.3209418
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA EP4T6
UT WOS:001140122900001
PM 36166550
DA 2025-03-07
ER

PT J
AU Zhou, JH
   Wang, XM
   Wong, JK
   Wang, HL
   Wang, ZW
   Yang, XY
   Yan, XR
   Feng, HZ
   Qu, HM
   Ying, HC
   Chen, W
AF Zhou, Jiehui
   Wang, Xumeng
   Wong, Jason K.
   Wang, Huanliang
   Wang, Zhongwei
   Yang, Xiaoyu
   Yan, Xiaoran
   Feng, Haozhe
   Qu, Huamin
   Ying, Haochao
   Chen, Wei
TI DPVisCreator: Incorporating Pattern Constraints to Privacy-preserving
   Visualizations via Differential Privacy
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Privacy-preserving visualization; visual analytics; differential
   privacy; tabular data
ID VISUAL ANALYSIS; EXPLORATION; METRICS; UTILITY
AB Data privacy is an essential issue in publishing data visualizations. However, it is challenging to represent multiple data patterns in privacy-preserving visualizations. The prior approaches target specific chart types or perform an anonymization model uniformly without considering the importance of data patterns in visualizations. In this paper, we propose a visual analytics approach that facilitates data custodians to generate multiple private charts while maintaining user-preferred patterns. To this end, we introduce pattern constraints to model users' preferences over data patterns in the dataset and incorporate them into the proposed Bayesian network-based Differential Privacy (DP) model PriVis. A prototype system, DPVisCreator, is developed to assist data custodians in implementing our approach. The effectiveness of our approach is demonstrated with quantitative evaluation of pattern utility under the different levels of privacy protection, case studies, and semi-structured expert interviews.
C1 [Zhou, Jiehui; Wang, Huanliang; Wang, Zhongwei; Yang, Xiaoyu; Feng, Haozhe; Chen, Wei] Zhejiang Univ, State Key Lab CAD&CG, Zhejiang, Peoples R China.
   [Chen, Wei] Zhejiang Univ, Lab Art & Archaeol Image, Minist Educ, Hangzhou, Peoples R China.
   [Wang, Xumeng] Nankai Univ, TMCC, CS, Tianjin, Peoples R China.
   [Wong, Jason K.; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Yan, Xiaoran] Zhejiang Lab, Hangzhou, Peoples R China.
   [Ying, Haochao] Zhejiang Univ, Sch Publ Hlth, Hangzhou, Peoples R China.
   [Ying, Haochao] Key Lab Intelligent Prevent Med Zhejiang Prov, Hangzhou, Peoples R China.
C3 Zhejiang University; Zhejiang University; Nankai University; Hong Kong
   University of Science & Technology; Zhejiang Laboratory; Zhejiang
   University
RP Chen, W (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Zhejiang, Peoples R China.; Chen, W (corresponding author), Zhejiang Univ, Lab Art & Archaeol Image, Minist Educ, Hangzhou, Peoples R China.; Ying, HC (corresponding author), Zhejiang Univ, Sch Publ Hlth, Hangzhou, Peoples R China.; Ying, HC (corresponding author), Key Lab Intelligent Prevent Med Zhejiang Prov, Hangzhou, Peoples R China.
EM zhoujiehui@zju.edu.cn; wangxumeng@nankai.edu.cn; kkwongar@cse.ust.hk;
   22051090@zju.edu.cn; wzw09@zju.edu.cn; 22051142@zju.edu.cn;
   xiaoran.a.yan@gmail.com; fenghz@zju.edu.cn; huamin@cse.ust.hk;
   haochaoying@zju.edu.cn; chenvis@zju.edu.cn
RI Wang, Xumeng/JBJ-0416-2023; Chen, Wei/AAR-9817-2020; yang,
   xiaoyu/AAH-9797-2020; Zhou, Jiehui/KBC-2015-2024
OI Ying, Haochao/0000-0001-7832-2518; Feng, Haozhe/0000-0002-5900-356X;
   Zhou, Jiehui/0000-0003-0709-775X; WONG, Kam Kwai/0000-0002-2813-1972
FU National Natural Science Foundation of China [62132017, 62106218];
   Zhejiang Lab [2022NF0AC01]
FX We would like to thank all the reviewers for their constructive
   comments. This work was supported by the National Natural Science
   Foundation of China (62132017 and 62106218) and Zhejiang Lab
   (2022NF0AC01).
NR 77
TC 9
Z9 10
U1 2
U2 13
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2023
VL 29
IS 1
BP 809
EP 819
DI 10.1109/TVCG.2022.3209391
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA M6EI0
UT WOS:001031124700001
PM 36166552
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Yuan, LP
   Zhou, ZQ
   Zhao, J
   Guo, YQ
   Du, F
   Qu, HM
AF Yuan, Lin-Ping
   Zhou, Ziqi
   Zhao, Jian
   Guo, Yiqiu
   Du, Fan
   Qu, Huamin
TI InfoColorizer: Interactive Recommendation of Color Palettes for
   Infographics
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Image color analysis; Tools; Visualization; Layout; Engines; Deep
   learning; Data visualization; Color palettes design; infographics;
   visualization recommendation; machine learning
ID SCHEMES; DESIGN
AB When designing infographics, general users usually struggle with getting desired color palettes using existing infographic authoring tools, which sometimes sacrifice customizability, require design expertise, or neglect the influence of elements' spatial arrangement. We propose a data-driven method that provides flexibility by considering users' preferences, lowers the expertise barrier via automation, and tailors suggested palettes to the spatial layout of elements. We build a recommendation engine by utilizing deep learning techniques to characterize good color design practices from data, and further develop InfoColorizer, a tool that allows users to obtain color palettes for their infographics in an interactive and dynamic manner. To validate our method, we conducted a comprehensive four-part evaluation, including case studies, a controlled user study, a survey study, and an interview study. The results indicate that InfoColorizer can provide compelling palette recommendations with adequate flexibility, allowing users to effectively obtain high-quality color design for input infographics with low effort.
C1 [Yuan, Lin-Ping; Qu, Huamin] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.
   [Zhou, Ziqi; Zhao, Jian] Univ Waterloo, Waterloo, ON N2L 3G1, Canada.
   [Guo, Yiqiu] Xi An Jiao Tong Univ, Xian 710061, Peoples R China.
   [Du, Fan] Adobe Res, San Jose, CA 95110 USA.
C3 Hong Kong University of Science & Technology; University of Waterloo;
   Xi'an Jiaotong University; Adobe Systems Inc.
RP Zhao, J (corresponding author), Univ Waterloo, Waterloo, ON N2L 3G1, Canada.
EM lyuanaa@cse.ust.hk; z229zhou@uwaterloo.ca; jianzhao@uwaterloo.ca;
   maxleaf@stu.xjtu.edu.cn; fdu@adobe.com; huamin@cse.ust.hk
OI Yuan, Linping/0000-0001-6268-1583; Zhou, Ziqi/0000-0002-0861-1805
FU NSERC Discovery Grant; Microsoft Research Asia; Adobe Gift Fund
FX The authors would like to thank the anonymous reviewers for their
   valuable comments. This work was supported in part by the NSERC
   Discovery Grant, in part by the Research Grant from Microsoft Research
   Asia, and in part by Adobe Gift Fund.
NR 71
TC 25
Z9 26
U1 2
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4252
EP 4266
DI 10.1109/TVCG.2021.3085327
PG 15
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400022
PM 34061743
OA Green Submitted
DA 2025-03-07
ER

PT J
AU Zhao, X
   Zhang, BW
   Wu, JJ
   Hu, RZ
   Komura, T
AF Zhao, Xi
   Zhang, Bowen
   Wu, Jinji
   Hu, Ruizhen
   Komura, Taku
TI Relationship-Based Point Cloud Completion
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Three-dimensional displays; Task analysis; Shape; Geometry; Training
   data; Semantics; Robot vision systems; Point cloud completion; spatial
   relationships
ID OBJECT DETECTION
AB We propose a partial point cloud completion approach for scenes that are composed of multiple objects. We focus on pairwise scenes where two objects are in close proximity and are contextually related to each other, such as a chair tucked in a desk, a fruit in a basket, a hat on a hook and a flower in a vase. Different from existing point cloud completion methods, which mainly focus on single objects, we design a network that encodes not only the geometry of the individual shapes, but also the spatial relations between different objects. More specifically, we complete missing parts of the objects in a conditional manner, where the partial or completed point cloud of the other object is used as an additional input to help predict missing parts. Based on the idea of conditional completion, we further propose a two-path network, which is guided by a consistency loss between different sequences of completion. Our method can handle difficult cases where the objects heavily occlude each other. Also, it only requires a small set of training data to reconstruct the interaction area compared to existing completion approaches. We evaluate our method qualitatively and quantitatively via ablation studies and in comparison to the state-of-the-art point cloud completion methods.
C1 [Zhao, Xi; Zhang, Bowen; Wu, Jinji] Xi An Jiao Tong Univ, Sch Comp Sci & Technol, Xian 710049, Peoples R China.
   [Hu, Ruizhen] Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
   [Komura, Taku] Univ Edinburgh, Sch Informat, Edinburgh EH8 9YL, Midlothian, Scotland.
C3 Xi'an Jiaotong University; Shenzhen University; University of Edinburgh
RP Hu, RZ (corresponding author), Shenzhen Univ, Coll Comp Sci & Software Engn, Shenzhen 518060, Peoples R China.
EM zhaoxi.jade@gmail.com; zbw1682123@stu.xjtu.edu.cn;
   walf568@stu.xjtu.edu.can; ruizhen.hu@gmail.com; tkomura@ed.ac.uk
RI Zhang, Bowen/AAM-8371-2020
FU National Natural Science Foundation of China [62072366, 61872250]; China
   Postdoctoral Science Foundation [2020M673407]; Fundamental Research
   Funds for the Central Universities [xzy012019048]; Guangdong Natural
   Science Foundation [2021B1515020085]; University of Hong Kong
   [182DRTAKU, 187FRTAKU, 230DRTAKU]
FX This work was supported in part by the National Natural Science
   Foundation of China under Grants 62072366 and 61872250, in part by the
   China Postdoctoral Science Foundation Funded Project under Grant
   2020M673407, in part by the Fundamental Research Funds for the Central
   Universities under Grant xzy012019048, in part by the Guangdong Natural
   Science Foundation under Grant 2021B1515020085, and in part by a
   start-up fund by The University of Hong Kong under Grants 182DRTAKU,
   187FRTAKU, and 230DRTAKU.
NR 37
TC 8
Z9 10
U1 5
U2 30
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD DEC 1
PY 2022
VL 28
IS 12
BP 4940
EP 4950
DI 10.1109/TVCG.2021.3109392
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 5Q4XK
UT WOS:000873836400070
PM 34478371
DA 2025-03-07
ER

PT J
AU Kim, YJ
   Kumaran, R
   Sayyad, E
   Milner, A
   Bullock, T
   Giesbrecht, B
   Höllerer, T
AF Kim, You-Jin
   Kumaran, Radha
   Sayyad, Ehsan
   Milner, Anne
   Bullock, Tom
   Giesbrecht, Barry
   Hollerer, Tobias
TI Investigating Search Among Physical and Virtual Objects Under Different
   Lighting Conditions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT 21st IEEE International Symposium on Mixed and Augmented Reality (ISMAR)
CY OCT 17-22, 2022
CL Singapore, SINGAPORE
SP IEEE
DE Mobile augmented reality; wide-area; user study; lighting conditions;
   perception; behavior
ID DIRECTION; SENSE
AB By situating computer-generated content in the physical world, mobile augmented reality (AR) can support many tasks that involve effective search and inspection of physical environments. Currently, there is limited information regarding the viability of using AR in realistic wide-area outdoor environments and how AR experiences affect human behavior in these environments. Here, we conducted a wide-area outdoor AR user study (n=48) using a commercially available AR headset (Microsoft Hololens 2) to compare (1) user interactions with physical and virtual objects in the environment (2) the effects of different lighting conditions on user behavior and AR experience and (3) the impact of varying cognitive load on AR task performance. Participants engaged in a treasure hunt task where they searched for and classified virtual target items (green "gems") in an augmented outdoor courtyard scene populated with physical and virtual objects. Cognitive load was manipulated so that in half the search trials users were required to monitor an audio stream and respond to specific target sounds. Walking paths, head orientation and eye gaze information were measured, and users were queried about their memory of encountered objects and provided feedback on the experience. Key findings included (1) Participants self-reported significantly lower comfort in the ambient natural light condition, with virtual objects more visible and participants more likely to walk into physical objects at night; (2) recall for physical objects was worse than for virtual objects, (3) participants discovered more gems hidden behind virtual objects than physical objects, implying higher attention on virtual objects and (4) dual-tasking modified search behavior. These results suggest there are important technical, perceptual and cognitive factors that must be considered if the full potential of "anywhere and anytime mobile AR" is to be realized.
C1 [Kim, You-Jin; Sayyad, Ehsan; Hollerer, Tobias] Univ Calif Santa Barbara, Media Arts & Technol, Santa Barbara, CA 93106 USA.
   [Kumaran, Radha; Hollerer, Tobias] Univ Calif Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA.
   [Milner, Anne; Bullock, Tom; Giesbrecht, Barry] Univ Calif Santa Barbara, Dept Psychol & Brain Sci, Santa Barbara, CA 93106 USA.
   [Milner, Anne; Bullock, Tom; Giesbrecht, Barry] Univ Calif Santa Barbara, Inst Collaborat Biotechnol, Santa Barbara, CA 93106 USA.
C3 University of California System; University of California Santa Barbara;
   University of California System; University of California Santa Barbara;
   University of California System; University of California Santa Barbara;
   University of California System; University of California Santa Barbara
RP Kim, YJ (corresponding author), Univ Calif Santa Barbara, Media Arts & Technol, Santa Barbara, CA 93106 USA.
EM yujnkm@ucsb.edu; rkumaran@ucsb.edu
RI Kumaran, Radha/LCE-5323-2024
OI Kumaran, Radha/0000-0001-7161-3048; Kim, You-Jin/0000-0003-0903-8999
FU U.S. Army Combat Capabilities Development Command Soldier Center
   Measuring and Advancing Soldier Tactical Readiness and Effectiveness
   (MASTR-E) program [W911NF-19-F-0018, W911NF-19-D-0001]; ONR
   [N00014-19-1-2553, N00174-19-1-0024]; NSF [IIS-1911230]
FX This work was funded by the U.S. Army Combat Capabilities Development
   Command Soldier Center Measuring and Advancing Soldier Tactical
   Readiness and Effectiveness (MASTR-E) program through award
   W911NF-19-F-0018 under contract W911NF-19-D-0001 for the Institute for
   Collaborative Biotechnologies. Additional funding came from ONR awards
   N00014-19-1-2553 and N00174-19-1-0024, as well as NSF award IIS-1911230.
   The authors thank Natalie Juo and Anabel Salimian for their assistance
   with data collection.
NR 47
TC 10
Z9 10
U1 2
U2 22
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD NOV
PY 2022
VL 28
IS 11
BP 3788
EP 3798
DI 10.1109/TVCG.2022.3203093
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 5M3ZS
UT WOS:000871038200023
PM 36048996
DA 2025-03-07
ER

PT J
AU Skanberg, R
   Falk, M
   Linares, M
   Ynnerman, A
   Hotz, I
AF Skanberg, Robin
   Falk, Martin
   Linares, Mathieu
   Ynnerman, Anders
   Hotz, Ingrid
TI Tracking Internal Frames of Reference for Consistent Molecular
   Distribution Functions
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Distribution functions; Trajectory; Visualization; Graphical models;
   Numerical models; Shape; Periodic structures; Molecule visualization;
   molecular dynamics; interactive exploration
ID RELATE 2 SETS; VECTOR OBSERVATIONS; DYNAMICS; BINDING; ROTATION; MOLDEN
AB In molecular analysis, spatial distribution functions (SDF) are fundamental instruments in answering questions related to spatial occurrences and relations of atomic structures over time. Given a molecular trajectory, SDFs can, for example, reveal the occurrence of water in relation to particular structures and hence provide clues of hydrophobic and hydrophilic regions. For the computation of meaningful distribution functions, the definition of molecular reference structures is essential. Therefore we introduce the concept of an internal frame of reference (IFR) for labeled point sets that represent selected molecular structures, and we propose an algorithm for tracking the IFR over time and space using a variant of Kabsch's algorithm. This approach lets us generate a consistent space for the aggregation of the SDF for molecular trajectories and molecular ensembles. We demonstrate the usefulness of the technique by applying it to temporal molecular trajectories as well as ensemble datasets. The examples include different docking scenarios with DNA, insulin, and aspirin.
C1 [Skanberg, Robin; Falk, Martin; Linares, Mathieu; Ynnerman, Anders; Hotz, Ingrid] Linkoping Univ, Sci Visualizat Grp, S-58183 Linkoping, Sweden.
   [Skanberg, Robin; Falk, Martin; Linares, Mathieu; Ynnerman, Anders; Hotz, Ingrid] Swedish E Sci Res Ctr SeRC, S-11428 Stockholm, Sweden.
   [Linares, Mathieu] Linkoping Univ, Lab Organ Elect, S-58183 Linkoping, Sweden.
C3 Linkoping University; Linkoping University
RP Skanberg, R (corresponding author), Linkoping Univ, Sci Visualizat Grp, S-58183 Linkoping, Sweden.; Skanberg, R (corresponding author), Swedish E Sci Res Ctr SeRC, S-11428 Stockholm, Sweden.
EM robin.skanberg@liu.se; martin.falk@liu.se; mathieu.linares@liu.se;
   Anders.Ynnerman@liu.se; ingrid.hotz@liu.se
RI ; linares, mathieu/C-5791-2008
OI Falk, Martin/0000-0003-1511-5006; linares, mathieu/0000-0002-9720-5429;
   Ynnerman, Anders/0000-0002-9466-9826; Hotz, Ingrid/0000-0001-7285-0483
FU Excellence Center at Linkoping and Lund in Information Technology
   (ELLIIT); Swedish e-Science Research Centre (SeRC)
FX This work was supported through grants from the Excellence Center at
   Link_oping and Lund in Information Technology (ELLIIT) and the Swedish
   e-Science Research Centre (SeRC). The authors would like to thank the
   Swedish National Infrastructure for Computing (SNIC) for providing
   computing resources.
NR 35
TC 8
Z9 8
U1 1
U2 4
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD SEPT 1
PY 2022
VL 28
IS 9
BP 3126
EP 3137
DI 10.1109/TVCG.2021.3051632
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 3K0HP
UT WOS:000833767700005
PM 33444141
OA Green Published
DA 2025-03-07
ER

PT J
AU Hu, JB
   Wang, SF
   Li, BJ
   Li, FQ
   Luo, ZX
   Liu, LG
AF Hu, Jiangbei
   Wang, Shengfa
   Li, Baojun
   Li, Fengqi
   Luo, Zhongxuan
   Liu, Ligang
TI Efficient Representation and Optimization for TPMS-Based Porous
   Structures
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Optimization; Topology; Periodic structures; Geometry; Three-dimensional
   printing; Solid modeling; Three-dimensional displays; Porous
   optimization; function description; triply periodic minimal surface
ID SCAFFOLD DESIGN; SHAPE; STIFFNESS; DENSITY
AB In this approach, we present an efficient topology and geometry optimization of triply periodic minimal surfaces (TPMS) based porous shell structures, which can be represented, analyzed, optimized and stored directly using functions. The proposed framework is directly executed on functions instead of remeshing (tetrahedral/hexahedral), and this framework substantially improves the controllability and efficiency. Specifically, a valid TPMS-based porous shell structure is first constructed by function expressions. The porous shell permits continuous and smooth changes of geometry (shell thickness) and topology (porous period). The porous structures also inherit several of the advantageous properties of TPMS, such as smoothness, full connectivity (no closed hollows), and high controllability. Then, the problem of filling an object's interior region with porous shell can be formulated into a constraint optimization problem with two control parameter functions. Finally, an efficient topology and geometry optimization scheme is presented to obtain optimized scale-varying porous shell structures. In contrast to traditional heuristic methods for TPMS, our work directly optimize both the topology and geometry of TPMS-based structures. Various experiments have shown that our proposed porous structures have obvious advantages in terms of efficiency and effectiveness.
C1 [Hu, Jiangbei] Dalian Univ Technol, Sch Math Sci, Dalian 116024, Liaoning, Peoples R China.
   [Wang, Shengfa] Dalian Univ Technol, DUT RU Int Sch Informat & Software Engn, Dalian 116024, Liaoning, Peoples R China.
   [Wang, Shengfa] Dalian Univ Technol, Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian 116024, Liaoning, Peoples R China.
   [Li, Baojun] Dalian Univ Technol, Inst Automot Digitalizat, Dalian 116024, Liaoning, Peoples R China.
   [Li, Fengqi; Luo, Zhongxuan] Dalian Univ Technol, Sch Software Technol, Dalian 116024, Liaoning, Peoples R China.
   [Luo, Zhongxuan] Guilin Univ Elect Technol, Inst Artificial Intelligence, Guilin 541004, Guangxi, Peoples R China.
   [Liu, Ligang] Univ Sci & Technol China, Sch Math Sci, Hefei 230052, Anhui, Peoples R China.
C3 Dalian University of Technology; Dalian University of Technology; Dalian
   University of Technology; Dalian University of Technology; Dalian
   University of Technology; Guilin University of Electronic Technology;
   Chinese Academy of Sciences; University of Science & Technology of
   China, CAS
RP Wang, SF (corresponding author), Dalian Univ Technol, DUT RU Int Sch Informat & Software Engn, Dalian 116024, Liaoning, Peoples R China.; Wang, SF (corresponding author), Dalian Univ Technol, Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian 116024, Liaoning, Peoples R China.
EM 915642014@qq.com; sfwang@dlut.edu.cn; bjli@dlut.edu.cn;
   lifengqi@dlut.edu.cn; zxluo@dlut.edu.cn; lgliu@ustc.edu.cn
RI Liu, Ligang/IZQ-5817-2023; Li, FengQi/HLQ-1543-2023; Li,
   Baojun/H-6254-2018; Hu, Jiangbei/JPK-7431-2023
OI li, fengqi/0000-0003-4056-548X
FU National Natural Science Foundation of China [61772104, 61720 106005,
   2017YF1103700]; Fundamental Research Funds for the Central Universities
   [DUT20JC32, DUT20TD107]
FX This work was partially supported by the National Natural Science
   Foundation of China under Grants 61772104, 61720 106005, 2017YF1103700,
   the Fundamental Research Funds for the Central Universities under Grants
   DUT20JC32, DUT20TD107.
NR 64
TC 22
Z9 27
U1 18
U2 124
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JUL 1
PY 2022
VL 28
IS 7
BP 2615
EP 2627
DI 10.1109/TVCG.2020.3037697
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA 1P2OA
UT WOS:000801853400006
PM 33180728
DA 2025-03-07
ER

PT J
AU Ebner, C
   Mori, S
   Mohr, P
   Peng, YF
   Schmalstieg, D
   Wetzstein, G
   Kalkofen, D
AF Ebner, Christoph
   Mori, Shohei
   Mohr, Peter
   Peng, Yifan
   Schmalstieg, Dieter
   Wetzstein, Gordon
   Kalkofen, Denis
TI Video See-Through Mixed Reality with Focus Cues
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article; Proceedings Paper
CT IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR)
CY MAR 12-16, 2022
CL ELECTR NETWORK
SP IEEE, IEEE Comp Soc, ChristchurchNZ, Virbela, Univ Canterbury, Immers Learning Res Network, Qualcomm, HIT Lab NZ, Appl Immers Gaming Initiat
DE Virtual reality; Rendering (computer graphics); Mixed reality; Real-time
   systems; Lenses; Additives; Pipelines; Mixed reality; Video see-through;
   Focus cues
ID DEPTH
AB This work introduces the first approach to video see-through mixed reality with full support for focus cues. By combining the flexibility to adjust the focus distance found in varifocal designs with the robustness to eye-tracking error found in multifocal designs, our novel display architecture reliably delivers focus cues over a large workspace. In particular, we introduce gaze-contingent layered displays and mixed reality focal stacks, an efficient representation of mixed reality content that lends itself to fast processing for driving layered displays in real time. We thoroughly evaluate this approach by building a complete end-to-end pipeline for capture, render, and display of focus cues in video see-through displays that uses only off-the-shelf hardware and compute components.
C1 [Ebner, Christoph; Mori, Shohei; Mohr, Peter; Schmalstieg, Dieter; Kalkofen, Denis] Graz Univ Technol, Graz, Austria.
   [Peng, Yifan; Wetzstein, Gordon] Stanford Univ, Stanford, CA 94305 USA.
C3 Graz University of Technology; Stanford University
RP Ebner, C (corresponding author), Graz Univ Technol, Graz, Austria.
EM christoph.ebner@icg.tugraz.at; gordonwz@stanford.edu
RI Peng, Yifan/M-1605-2016; Mori, Shohei/AAL-6642-2020
OI Mori, Shohei/0000-0003-0540-7312; Kalkofen, Denis/0000-0002-0359-206X;
   Ebner, Christoph/0000-0002-1449-4780
FU Austrian Science Fund FWF [P30694]; BMK; BMDW, Styria; SFG, Tyrol;
   Vienna Business Agency [879730]; Austrian Science Fund (FWF) [P30694]
   Funding Source: Austrian Science Fund (FWF)
FX This work was enabled by the Austrian Science Fund FWF (grant no.
   P30694) and the Competence Center VRVis, which is funded by BMK, BMDW,
   Styria, SFG, Tyrol and Vienna Business Agency in the scope of COMET
   -Competence Centers for Excellent Technologies (879730) which is managed
   by FFG.
NR 72
TC 12
Z9 14
U1 1
U2 30
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY 1
PY 2022
VL 28
IS 5
BP 2256
EP 2266
DI 10.1109/TVCG.2022.3150504
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Conference Proceedings Citation Index - Science (CPCI-S)
SC Computer Science
GA 1R1AK
UT WOS:000803110400037
PM 35167471
OA hybrid
DA 2025-03-07
ER

PT J
AU Peck, TC
   Good, JJ
   Erickson, A
   Bynum, I
   Bruder, G
AF Peck, Tabitha C.
   Good, Jessica J.
   Erickson, Austin
   Bynum, Isaac
   Bruder, Gerd
TI Effects of Transparency on Perceived Humanness: Implications for
   Rendering Skin Tones Using Optical See-Through Displays
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Augmented reality; optical see-through displays; additive light model;
   transparency; race; skin tone; diversity
ID DEHUMANIZATION; PERCEPTION; STEREOTYPES; EMBODIMENT; PREJUDICE; RACE;
   EYE
AB Current optical see-through displays in the field of augmented reality are limited in their ability to display colors with low lightness in the hue, saturation, lightness (HSL) color space, causing such colors to appear transparent. This hardware limitation may add unintended bias into scenarios with virtual humans. Humans have varying skin tones including HSL colors with low lightness. When virtual humans are displayed with optical see-through devices, people with low lightness skin tones may be displayed semi-transparently while those with high lightness skin tones will be displayed more opaquely. For example, a Black avatar may appear semi-transparent in the same scene as a White avatar who will appear more opaque. We present an exploratory user study (N = 160) investigating whether differing opacity levels result in dehumanizing avatar and human faces. Results support that dehumanization occurs as opacity decreases. This suggests that in similar lighting, low lightness skin tones (e.g., Black faces) will be viewed as less human than high lightness skin tones (e.g., White faces). Additionally, the perceived emotionality of virtual human faces also predicts perceived humanness. Angry faces were seen overall as less human, and at lower opacity levels happy faces were seen as more human. Our results suggest that additional research is needed to understand the effects and interactions of emotionality and opacity on dehumanization. Further, we provide evidence that unintentional racial bias may be added when developing for optical see-through devices using virtual humans. We highlight the potential bias and discuss implications and directions for future research.
C1 [Peck, Tabitha C.; Good, Jessica J.; Bynum, Isaac] Davidson Coll, Davidson, NC 28036 USA.
   [Erickson, Austin; Bruder, Gerd] Univ Cent Florida, Orlando, FL 32816 USA.
C3 Davidson College; State University System of Florida; University of
   Central Florida
RP Peck, TC (corresponding author), Davidson Coll, Davidson, NC 28036 USA.
EM tapeck@davidson.edu; jegood@davidson.edu; ericksona@knights.ucf.edu;
   isbynum@davidson.edu; Gerd.Bruder@ucf.edu
RI Erickson, Austin/AAV-9677-2020; Peck, Tabitha/AAH-2032-2021
FU Davidson College's Faculty Study and Research program; National Science
   Foundation [1800961, 1800947, 1800922]; Office of Naval Research
   [N00014-21-1-2578, N00014-17-1-2927, 34]
FX Funding for this work was supported by Davidson College's Faculty Study
   and Research program. This material includes work supported in part by
   the National Science Foundation under Collaborative Award Numbers
   1800961, 1800947, and 1800922 (Dr. Ephraim P. Glinert, IIS) to the
   University of Central Florida, University of Florida, and Stanford
   University, respectively, and the Office of Naval Research under Award
   Numbers N00014-21-1-2578 and N00014-17-1-2927 (Dr. Peter Squire, Code
   34).
NR 63
TC 8
Z9 8
U1 3
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAY
PY 2022
VL 28
IS 5
BP 2179
EP 2189
DI 10.1109/TVCG.2022.3150521
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA 0M8RI
UT WOS:000782415400006
PM 35148265
OA Bronze
DA 2025-03-07
ER

PT J
AU Huang, RP
   Li, Q
   Chen, L
   Yuan, XR
AF Huang, Renpei
   Li, Quan
   Chen, Li
   Yuan, Xiaoru
TI A Probability Density-Based Visual Analytics Approach to Forecast Bias
   Calibration
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Weather forecasting; Calibration; Data visualization; Spatiotemporal
   phenomena; Atmospheric modeling; Uncertainty; Weather forecast; pattern
   extraction; calibration; visual analytics
ID ENSEMBLE-MOS METHODS; VISUALIZATION; PRECIPITATION; VARIABILITY; TOOL;
   UNCERTAINTY; EXPLORATION
AB Biases inevitably occur in numerical weather prediction (NWP) due to an idealized numerical assumption for modeling chaotic atmospheric systems. Therefore, the rapid and accurate identification and calibration of biases is crucial for NWP in weather forecasting. Conventional approaches, such as various analog post-processing forecast methods, have been designed to aid in bias calibration. However, these approaches fail to consider the spatiotemporal correlations of forecast bias, which can considerably affect calibration efficacy. In this article, we propose a novel bias pattern extraction approach based on forecasting-observation probability density by merging historical forecasting and observation datasets. Given a spatiotemporal scope, our approach extracts and fuses bias patterns and automatically divides regions with similar bias patterns. Termed BicaVis, our spatiotemporal bias pattern visual analytics system is proposed to assist experts in drafting calibration curves on the basis of these bias patterns. To verify the effectiveness of our approach, we conduct two case studies with real-world reanalysis datasets. The feedback collected from domain experts confirms the efficacy of our approach.
C1 [Huang, Renpei; Chen, Li] Tsinghua Univ, Sch Software, BNRIST, Beijing 100084, Peoples R China.
   [Li, Quan] WeBank, AI Grp, Shenzhen, Guangdong, Peoples R China.
   [Yuan, Xiaoru] Peking Univ, Key Lab Machine Percept, Minist Educ, Beijing 100871, Peoples R China.
   [Yuan, Xiaoru] Peking Univ, Natl Engn Lab Big Data Anal & Applicat, Beijing 100871, Peoples R China.
C3 Tsinghua University; Peking University; Peking University
RP Chen, L (corresponding author), Tsinghua Univ, Sch Software, BNRIST, Beijing 100084, Peoples R China.
EM huangrp2013@gmail.com; forrestli@webank.com; chenlee@tsinghua.edu.cn;
   xiaoru.yuan@pku.edu.cn
RI Yuan, Xiaoru/E-1798-2013; Li, yuancheng/IUO-3866-2023
OI Li, Quan/0000-0003-2249-0728
FU National Natural Science Foundation of China [61972221, 61572274];
   National Numerical Windtunnel Project [NNW2018-ZT6B12]
FX The authors were grateful for the valuable feedback and comments
   provided by the anonymous reviewers. This research was partially
   supported by the National Natural Science Foundation of China (Grant
   Nos. 61972221, 61572274) and NNW2018-ZT6B12 (National Numerical
   Windtunnel Project).
NR 48
TC 1
Z9 2
U1 1
U2 12
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD APR 1
PY 2022
VL 28
IS 4
BP 1732
EP 1744
DI 10.1109/TVCG.2020.3025072
PG 13
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA ZH9CR
UT WOS:000761227900003
PM 32946394
DA 2025-03-07
ER

PT J
AU Jadhav, S
   Dmitriev, K
   Marino, J
   Barish, M
   Kaufman, AE
AF Jadhav, Shreeraj
   Dmitriev, Konstantin
   Marino, Joseph
   Barish, Matthew
   Kaufman, Arie E.
TI 3D Virtual Pancreatography
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Lesions; Pancreas; Three-dimensional displays; Ducts; Visualization;
   Computed tomography; Two dimensional displays; Visual diagnosis;
   pancreatic cancer; automatic segmentation; lesion classification; planar
   reformation
ID PLANAR REFORMATION; VISUALIZATION; PANCREAS; PERFORMANCE; PREDICTION;
   ENDOSCOPY; SUPINE
AB We present 3D virtual pancreatography (VP), a novel visualization procedure and application for non-invasive diagnosis and classification of pancreatic lesions, the precursors of pancreatic cancer. Currently, non-invasive screening of patients is performed through visual inspection of 2D axis-aligned CT images, though the relevant features are often not clearly visible nor automatically detected. VP is an end-to-end visual diagnosis system that includes: A machine learning based automatic segmentation of the pancreatic gland and the lesions, a semi-automatic approach to extract the primary pancreatic duct, a machine learning based automatic classification of lesions into four prominent types, and specialized 3D and 2D exploratory visualizations of the pancreas, lesions and surrounding anatomy. We combine volume rendering with pancreas- and lesion-centric visualizations and measurements for effective diagnosis. We designed VP through close collaboration and feedback from expert radiologists, and evaluated it on multiple real-world CT datasets with various pancreatic lesions and case studies examined by the expert radiologists.
C1 [Jadhav, Shreeraj; Dmitriev, Konstantin; Marino, Joseph; Kaufman, Arie E.] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
   [Barish, Matthew] Stony Brook Med, Dept Radiol, Stony Brook, NY 11794 USA.
C3 State University of New York (SUNY) System; Stony Brook University;
   State University of New York (SUNY) System; Stony Brook University;
   Stony Brook University Hospital
RP Jadhav, S (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.
EM sdjadhav@cs.stonybrook.edu; kdmitriev@cs.stonybrook.edu;
   jmarino@cs.stonybrook.edu; Matthew.Barish@stonybrookmedicine.edu;
   ari@cs.stonybrook.edu
OI Jadhav, Shreeraj/0000-0003-0520-4857; Dmitriev,
   Konstantin/0000-0001-9740-1334; Barish, Matthew/0000-0002-7521-8878;
   Kaufman, Arie/0000-0002-0796-6196
FU NSF [CNS1650499, OAC1919752, ICER1940302]; Marcus Foundation; National
   Heart, Lung, and Blood Institute of NIH [U01HL127522]; New York State
   Center for Advanced Technology in Biotechnology; Stony Brook University;
   Cold Spring Harbor; Brookhaven National; Feinstein Institute for Medical
   Research; New York State Economic Development Department [C14051]
FX The authors would like to thank their collaborators at Stony Brook
   Medicine, including Dr. Kevin Baker, for evaluating their system and
   providing feedback, and at Johns Hopkins, Drs. Ralph Hruban and Elliot
   Fishman, for the pancreas CT datasets. This work has been partially
   supported by NSF Grants CNS1650499, OAC1919752, ICER1940302; Marcus
   Foundation; and National Heart, Lung, and Blood Institute of NIH under
   Award U01HL127522; New York State Center for Advanced Technology in
   Biotechnology; Stony Brook University; Cold Spring Harbor; Brookhaven
   National; Feinstein Institute for Medical Research; and New York State
   Economic Development Department, Contract C14051. The content is solely
   the responsibility of the authors and does not necessarily represent the
   official views of the NIH. Shreeraj Jadhav and Konstantin Dmitriev
   contributed equally to this work.
NR 66
TC 5
Z9 5
U1 0
U2 15
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD MAR 1
PY 2022
VL 28
IS 3
BP 1457
EP 1468
DI 10.1109/TVCG.2020.3020958
PG 12
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA YP1EJ
UT WOS:000748371200001
PM 32870794
OA Green Accepted, Bronze
DA 2025-03-07
ER

PT J
AU Chundury, P
   Patnaik, B
   Reyazuddin, Y
   Tang, C
   Lazar, J
   Elmqvist, N
AF Chundury, Pramod
   Patnaik, Biswaksen
   Reyazuddin, Yasmin
   Tang, Christine
   Lazar, Jonathan
   Elmqvist, Niklas
TI Towards Understanding Sensory Substitution for Accessible Visualization:
   An Interview Study
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Visualization; Interviews; Blindness; Training;
   Navigation; Layout; Accessibility; blind users; sonification;
   visualization; spatial layouts; sound perception
ID VISUAL IMPAIRMENTS; INFORMATION; SONIFICATION; TEACHERS; STUDENTS;
   IMAGES; 2D
AB For all its potential in supporting data analysis, particularly in exploratory situations, visualization also creates barriers: accessibility for blind and visually impaired individuals. Regardless of how effective a visualization is, providing equal access for blind users requires a paradigm shift for the visualization research community. To enact such a shift, it is not sufficient to treat visualization accessibility as merely another technical problem to overcome. Instead, supporting the millions of blind and visually impaired users around the world who have equally valid needs for data analysis as sighted individuals requires a respectful, equitable, and holistic approach that includes all users from the onset. In this paper, we draw on accessibility research methodologies to make inroads towards such an approach. We first identify the people who have specific insight into how blind people perceive the world: orientation and mobility (O&M) experts, who are instructors that teach blind individuals how to navigate the physical world using non-visual senses. We interview 10 O&M experts-all of them blind-to understand how best to use sensory substitution other than the visual sense for conveying spatial layouts. Finally, we investigate our qualitative findings using thematic analysis. While blind people in general tend to use both sound and touch to understand their surroundings, we focused on auditory affordances and how they can be used to make data visualizations accessible-using sonification and auralization. However, our experts recommended supporting a combination of senses-sound and touch-to make charts accessible as blind individuals may be more familiar with exploring tactile charts. We report results on both sound and touch affordances, and conclude by discussing implications for accessible visualization for blind individuals.
C1 [Chundury, Pramod; Patnaik, Biswaksen; Lazar, Jonathan; Elmqvist, Niklas] Univ Maryland, College Pk, MD 20742 USA.
   [Reyazuddin, Yasmin] Natl Federat Blind, Baltimore, MD USA.
   [Tang, Christine] Poolesville High Sch, Poolesville, MD USA.
C3 University System of Maryland; University of Maryland College Park
RP Chundury, P (corresponding author), Univ Maryland, College Pk, MD 20742 USA.
EM pchundur@umd.edu; bpatnaik@umd.edu; yasmin81065@gmail.com;
   christinetang075@gmail.com; jlazar@umd.edu; elm@umd.edu
OI Elmqvist, Niklas/0000-0001-5805-5301
NR 82
TC 28
Z9 31
U1 2
U2 14
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 1084
EP 1094
DI 10.1109/TVCG.2021.3114829
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA XU0IB
UT WOS:000733959000103
PM 34587061
DA 2025-03-07
ER

PT J
AU Dimara, E
   Stasko, J
AF Dimara, Evanthia
   Stasko, John
TI A Critical Reflection on Visualization Research: Where Do Decision
   Making Tasks Hide?
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Decision making; Visualization; Task analysis;
   Taxonomy; Systematics; Libraries; decision making; data; visualization;
   visual analytics; taxonomies; task
ID VISUAL ANALYTICS APPROACH; INTERACTIVE VISUALIZATION; INFORMATION
   VISUALIZATION; URBAN DATA; EXPLORATION; DESIGN; UNCERTAINTY; MODEL;
   ENSEMBLES; DYNAMICS
AB It has been widely suggested that a key goal of visualization systems is to assist decision making, but is this true? We conduct a critical investigation on whether the activity of decision making is indeed central to the visualization domain. By approaching decision making as a user task, we explore the degree to which decision tasks are evident in visualization research and user studies. Our analysis suggests that decision tasks are not commonly found in current visualization task taxonomies and that the visualization field has yet to leverage guidance from decision theory domains on how to study such tasks. We further found that the majority of visualizations addressing decision making were not evaluated based on their ability to assist decision tasks. Finally, to help expand the impact of visual analytics in organizational as well as casual decision making activities, we initiate a research agenda on how decision making assistance could be elevated throughout visualization research.
C1 [Dimara, Evanthia] Univ Utrecht, Utrecht, Netherlands.
   [Dimara, Evanthia] Univ Konstanz, Constance, Germany.
   [Stasko, John] Georgia Tech, Atlanta, GA USA.
C3 Utrecht University; University of Konstanz; University System of
   Georgia; Georgia Institute of Technology
RP Dimara, E (corresponding author), Univ Utrecht, Utrecht, Netherlands.; Dimara, E (corresponding author), Univ Konstanz, Constance, Germany.
EM evanthia.dimara@gmail.com; stasko@cc.gatech.edu
OI Dimara, Evanthia/0000-0001-5212-7888
NR 190
TC 22
Z9 23
U1 3
U2 30
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 1128
EP 1138
DI 10.1109/TVCG.2021.3114813
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED)
SC Computer Science
GA XU0IB
UT WOS:000733959000107
PM 34587049
OA Green Published
DA 2025-03-07
ER

PT J
AU Hall, KW
   Kouroupis, A
   Bezerianos, A
   Szafir, DA
   Collins, C
AF Hall, Kyle Wm
   Kouroupis, Anthony
   Bezerianos, Anastasia
   Szafir, Danielle Albers
   Collins, Christopher
TI Professional Differences: A Comparative Study of Visualization Task
   Performance and Spatial Ability Across Disciplines
SO IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS
LA English
DT Article
DE Data visualization; Task analysis; Visualization; Cognition; Training;
   Three-dimensional displays; Navigation; visualization; spatial ability;
   perception; task performance; discipline; domain-specific; empirical
   evaluation
ID INDIVIDUAL-DIFFERENCES; MENTAL ROTATION; DESIGN; COMMUNICATION; GENDER
AB Problem-driven visualization work is rooted in deeply understanding the data, actors, processes, and workflows of a target domain. However, an individual's personality traits and cognitive abilities may also influence visualization use. Diverse user needs and abilities raise natural questions for specificity in visualization design: Could individuals from different domains exhibit performance differences when using visualizations? Are any systematic variations related to their cognitive abilities? This study bridges domain-specific perspectives on visualization design with those provided by cognition and perception. We measure variations in visualization task performance across chemistry, computer science, and education, and relate these differences to variations in spatial ability. We conducted an online study with over 60 domain experts consisting of tasks related to pie charts, isocontour plots, and 3D scatterplots, and grounded by a well-documented spatial ability test. Task performance (correctness) varied with profession across more complex visualizations (isocontour plots and scatterplots), but not pie charts, a comparatively common visualization. We found that correctness correlates with spatial ability, and the professions differ in terms of spatial ability. These results indicate that domains differ not only in the specifics of their data and tasks, but also in terms of how effectively their constituent members engage with visualizations and their cognitive traits. Analyzing participants' confidence and strategy comments suggests that focusing on performance neglects important nuances, such as differing approaches to engage with even common visualizations and potential skill transference. Our findings offer a fresh perspective on discipline-specific visualization with specific recommendations to help guide visualization design that celebrates the uniqueness of the disciplines and individuals we seek to serve.
C1 [Hall, Kyle Wm] Temple Univ, Philadelphia, PA 19122 USA.
   [Hall, Kyle Wm] Layer 6, Philadelphia, PA 19122 USA.
   [Kouroupis, Anthony; Collins, Christopher] Ontario Tech Univ, Oshawa, ON, Canada.
   [Bezerianos, Anastasia] Univ Paris Saclay, LRI, Gif Sur Yvette, France.
   [Szafir, Danielle Albers] Univ Colorado Boulder, Boulder, CO USA.
C3 Pennsylvania Commonwealth System of Higher Education (PCSHE); Temple
   University; Universite Paris Saclay; University of Colorado System;
   University of Colorado Boulder
RP Hall, KW (corresponding author), Temple Univ, Philadelphia, PA 19122 USA.; Hall, KW (corresponding author), Layer 6, Philadelphia, PA 19122 USA.
EM kyle@layer6.ai; anthony.kouroupis@ontariotechu.net; anab@lri.fr;
   danielle.szafir@colorado.edu; christopher.collins@ontariotechu.ca
OI Bezerianos, Anastasia/0000-0002-7142-2548
FU NSF [1764089, 2046725]; NSERC [RGPIN-2015-03916]; Direct For Computer &
   Info Scie & Enginr; Div Of Information & Intelligent Systems [1764089]
   Funding Source: National Science Foundation; Div Of Information &
   Intelligent Systems; Direct For Computer & Info Scie & Enginr [2046725]
   Funding Source: National Science Foundation
FX We acknowledge the support of NSF #1764089 & 2046725 and NSERC
   [RGPIN-2015-03916]. We thank Dr. So Yoon Yoon (University of Cincinnati)
   for providing the revised PSVT:R materials.
NR 76
TC 10
Z9 10
U1 8
U2 26
PU IEEE COMPUTER SOC
PI LOS ALAMITOS
PA 10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA
SN 1077-2626
EI 1941-0506
J9 IEEE T VIS COMPUT GR
JI IEEE Trans. Vis. Comput. Graph.
PD JAN
PY 2022
VL 28
IS 1
BP 654
EP 664
DI 10.1109/TVCG.2021.3114805
PG 11
WC Computer Science, Software Engineering
WE Science Citation Index Expanded (SCI-EXPANDED); Social Science Citation Index (SSCI)
SC Computer Science
GA XU0IB
UT WOS:000733959000073
PM 34648448
OA Green Published, Green Submitted
DA 2025-03-07
ER

EF