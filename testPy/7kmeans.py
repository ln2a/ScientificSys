from sklearn.feature_extraction.text import TfidfVectorizer

# import matplotlib.pyplot as plt
import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.metrics import euclidean_distances
from pandas import DataFrame
import json


def wordsToVector(dataset):
    # dataset=corpus
    vectorizer = TfidfVectorizer(
        max_df=0.5, max_features=1000, min_df=2, use_idf=True, stop_words="english"
    )
    # vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(dataset)
    """
    max_df=0.5	å¿½ç•¥é«˜é¢‘è¯ï¼ˆå¦‚æœæŸä¸ªå•è¯å‡ºç°åœ¨ 50% ä»¥ä¸Šçš„æ–‡æ¡£ä¸­ï¼Œå°±è®¤ä¸ºå®ƒæ²¡åŒºåˆ†åº¦ï¼‰
    min_df=2	å¿½ç•¥ä½é¢‘è¯ï¼ˆæŸä¸ªè¯è‡³å°‘è¦å‡ºç°åœ¨ 2 ä¸ªæ–‡æ¡£ä¸­ï¼Œå¦åˆ™å°±ä¸¢å¼ƒï¼‰
    max_features=1000	æœ€å¤šä¿ç•™ 1000 ä¸ªé‡è¦ç‰¹å¾
    use_idf=True è®¡ç®—IDF
    """
    # è¾“å‡ºè¯è¢‹[word1,word2,word3,...]
    # print(vectorizer.get_feature_names_out())
    # è¾“å‡ºTF_IDFçŸ©é˜µ [word1 in paper1,word2 in paper1 , ...]
    print(X.toarray())
    return X, vectorizer


def find_best_K(X):
    """æµ‹è¯•é€‰æ‹©æœ€ä¼˜å‚æ•°"""
    # dataset = loadDataset()
    # print("%d documents" % len(dataset))
    # X, vectorizer = transform(dataset, n_features=500)
    scores = []
    k_values = range(2, 10)
    for k in k_values:
        km = KMeans(
            n_clusters=k,
            init="k-means++",
            max_iter=300,
            verbose=False,
            random_state=42,
        )
        # èšç±»æ•°ï¼›åˆå§‹ä¸­å¿ƒé€‰æ‹©æ–¹æ³•ï¼›æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼›ä¸æ˜¾ç¤ºè¿­ä»£è¯¦ç»†æ—¥å¿—ï¼›å›ºå®šéšæœºæ•°
        cluster_labels = km.fit_predict(X)
        # æ¯ä¸€ä¸ªæ ·æœ¬è¢«åˆ†é…åˆ°çš„K
        silhouette_avg = silhouette_score(X, cluster_labels)
        # è®¡ç®—è½®å»“ç³»æ•°:è½®å»“ç³»æ•°ğ‘ (ğ‘–)å€¼è¶Šæ¥è¿‘1è¡¨ç¤ºæ ·æœ¬ğ‘–èšç±»è¶Šåˆç†
        scores.append(silhouette_avg)
    # plt.figure(figsize=(8, 4))
    # plt.plot(k_values, scores, label="error", color="red", linewidth=1)
    # plt.xlabel("k_features")
    # plt.ylabel("error")
    # plt.legend()
    # plt.show()

    # æ‰¾åˆ°æœ€ä½³ k
    best_k = k_values[np.argmax(scores)]
    # è¿”å›æœ€ä¼˜kï¼›np.argmaxæ˜¯æœ€å¤§å€¼çš„ç´¢å¼•
    print(f"æœ€ä½³ k å€¼: {best_k}ï¼Œæœ€å¤§è½®å»“ç³»æ•°: {max(scores):.4f}")
    return best_k


def find_topic_words_ab(X, final_k, commNum):
    km = KMeans(
        n_clusters=final_k,
        init="k-means++",
        max_iter=300,
        verbose=False,
        random_state=42,
    )
    cluster_labels = km.fit_predict(X)
    # æ‰€æœ‰æ ·æœ¬æ‰€å±çš„ä¸»é¢˜
    order_centroids = km.cluster_centers_.argsort()[:, ::-1]
    # æ¯ä¸ªç°‡çš„ä¸­å¿ƒç‚¹ï¼šç›¸å¯¹äºæ‰€æœ‰ç‰¹å¾çš„å‡å€¼ï¼ˆç›¸å¯¹äºè¯è¢‹ï¼‰
    # æ¯ä¸€è¡Œè¡¨ç¤ºä¸€ä¸ªç°‡çš„ä¸­å¿ƒç‚¹ï¼Œå³è¯¥ç°‡æ‰€æœ‰æ ·æœ¬åœ¨æ¯ä¸ªç‰¹å¾ç»´åº¦ï¼ˆå•è¯ï¼‰çš„å‡å€¼ ä»å¤§åˆ°å°æ’åˆ—ï¼Œå€¼ä¸ºå…¶åºå·
    feature_words = vectorizer.get_feature_names_out()
    topicWords = {}  # æ¯ä¸€ä¸ªä¸»é¢˜çš„ä¸»é¢˜è¯
    for i in range(final_k):
        topicWords[i] = []
        # print("Topic %d:" % i, end="")
        # è¾“å‡ºå‰ä¸‰åä¸ªç‰¹å¾è¯[ç¬¬iä¸ªç°‡,å‰ä¸‰åä¸ªç‰¹å¾ç´¢å¼•]
        for ind in order_centroids[i, :30]:
            topicWords[i].append(feature_words[ind])
    #  ---------------------------
    # è®¡ç®—æ‰€æœ‰æ ·æœ¬åˆ°ç°‡ä¸­å¿ƒçš„è·ç¦»    sample1:[dis cluster1,dis cluster2 ,...]
    distances = euclidean_distances(X, km.cluster_centers_)

    # å­˜å‚¨æ¯ä¸ªç°‡æœ€å…·ä»£è¡¨æ€§çš„ 5 ä¸ªæ ·æœ¬
    topic_top_ab = {}

    for cluster in range(final_k):
        # æ‰¾å‡ºå±äºå½“å‰ç°‡çš„æ ·æœ¬ç´¢å¼•
        cluster_indices = np.where(cluster_labels == cluster)[0]

        # æŒ‰ç…§è·ç¦»ä»å°åˆ°å¤§æ’åº
        sorted_indices = cluster_indices[
            np.argsort(distances[cluster_indices, cluster])
        ]

        # å–å‰ 5 ä¸ªæœ€æ¥è¿‘ä¸­å¿ƒçš„æ ·æœ¬
        top_5_indices = sorted_indices[:5]

        # å­˜å‚¨ç»“æœ
        ls = []
        for i in range(len(top_5_indices)):
            ls.append(paperNodes[commNum][top_5_indices[i]]["ab"])
        topic_top_ab[cluster] = ls

    return topic_top_ab, topicWords, cluster_labels


# è¯»å– JSON æ–‡ä»¶
with open("./output_data/jsonCluster_All_Coop10.json", "r", encoding="utf-8") as f:
    jsonData = json.load(f)  # è§£æ JSON æ–‡ä»¶

commNums = 11  # ç¤¾åŒºæ•°
paperNodes = [[], [], [], [], [], [], [], [], [], [], []]
dataset = [[], [], [], [], [], [], [], [], [], [], []]  # å…³é”®è¯åˆ—è¡¨
# è·å–æ¯ä¸€ä¸ªç¤¾åŒºçš„papersçš„å…³é”®è¯
for node in jsonData["nodes"]:
    if node["group"] == 1:
        c = node["cluster"]
        paperNodes[c].append(node)
        dataset[c].append(node["keywords"])

for i in range(commNums):
    print(len(paperNodes[i]))
# print(paperNodes[1][1]["keywords"])
# print(dataset[1][1])

df = DataFrame(columns=("clusterNum", "topics", "topicNum", "topicWords", "topicAB"))
commNums = 11  # ç¤¾åŒºæ•°
for commNum in range(1, commNums):
    print(len(dataset[commNum]))
    X, vectorizer = wordsToVector(dataset[commNum])
    final_k = find_best_K(X)
    topic_top_ab, topicWords, topic_labels = find_topic_words_ab(X, final_k, commNum)
    for i in range(len(topic_labels)):
        paperNodes[commNum][i]["topic"] = int(topic_labels[i])
    for k in range(final_k):
        n = len(df)
        df.loc[n, "clusterNum"] = commNum
        df.loc[n, "topics"] = final_k
        df.loc[n, "topicNum"] = k
        df.loc[n, "topicWords"] = topicWords[k]
        df.loc[n, "topicAB"] = topic_top_ab[k]

# df.to_excel("./output_data/kmeans_Info.xlsx")


jsonDataNew = []
for nodes in paperNodes:
    for node in nodes:
        if node["cluster"] == 0:
            continue
        jsonDataNew.append(node)
# with open("./output_data/papersToTopicInfo.json", "w", encoding="utf-8") as f:
#     json.dump(jsonDataNew, f, indent=4, ensure_ascii=False)
