{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2989\n",
      "11585\n",
      "4864\n",
      "19732\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_excel(\"../output_data/nodes_10y_clean.xlsx\")\n",
    "\n",
    "n = len(df)\n",
    "# n = 500\n",
    "\n",
    "papersLs = {}\n",
    "nodesLs = []\n",
    "nodesLs_all = []\n",
    "linksLs = []\n",
    "linksLs_all = []\n",
    "# 获取最有影响力的作者和机构\n",
    "auRank = {}\n",
    "instRank = {}\n",
    "auInst = {}\n",
    "for i in range(n):\n",
    "    # 用字典存一行\n",
    "    dc = {}\n",
    "    dc = df.loc[i]\n",
    "    ls = eval(dc[\"AF\"])\n",
    "    for j in ls:\n",
    "        for k, v in j.items():\n",
    "            # auRank[(k, v)] = auRank.setdefault((k, v), 0) + 1\n",
    "            auRank[k] = auRank.setdefault(k, 0) + 1\n",
    "            instRank[v] = instRank.setdefault(v, 0) + 1\n",
    "            auInst[k] = v\n",
    "\n",
    "# 取发文数量前100的作者和机构\n",
    "auRankls = [(k, v) for k, v in auRank.items()]\n",
    "auRankls.sort(key=lambda x: x[1], reverse=True)\n",
    "auLsMax = [i[0] for i in auRankls[:100]]  # 前100作者\n",
    "for index, author in enumerate(auRankls):\n",
    "    if index < 100:\n",
    "        nodesLs.append(\n",
    "            {\n",
    "                \"id\": author[0],\n",
    "                \"group\": 2,\n",
    "                \"paperCount\": author[1],\n",
    "                \"inst\": auInst[author[0]],\n",
    "            }\n",
    "        )\n",
    "    nodesLs_all.append(\n",
    "        {\n",
    "            \"id\": author[0],\n",
    "            \"group\": 2,\n",
    "            \"paperCount\": author[1],\n",
    "            \"inst\": auInst[author[0]],\n",
    "        }\n",
    "    )\n",
    "\n",
    "instRankls = [(k, v) for k, v in instRank.items()]\n",
    "instRankls.sort(key=lambda x: x[1], reverse=True)\n",
    "instLsMax = [i[0] for i in instRankls[:100]]  # 前100机构\n",
    "for index, inst in enumerate(instRankls):\n",
    "    if index < 100:\n",
    "        nodesLs.append({\"id\": inst[0], \"group\": 3, \"paperCount\": inst[1]})\n",
    "    nodesLs_all.append({\"id\": inst[0], \"group\": 3, \"paperCount\": inst[1]})\n",
    "\n",
    "# print(len(instLs))\n",
    "# print(len(auLs))\n",
    "\n",
    "# 添加paper节点\n",
    "for i in range(n):\n",
    "    # 存一行\n",
    "    dc = df.loc[i]\n",
    "    # doi和标题的对应关系\n",
    "    papersLs[dc[\"DI\"]] = dc[\"TI\"]\n",
    "    # 添加paper节点\n",
    "    paperInfo = {\n",
    "        \"id\": dc[\"TI\"],\n",
    "        \"group\": 1,\n",
    "        \"doi\": dc[\"DI\"],\n",
    "        \"ab\": dc[\"AB\"],\n",
    "        \"keywords\": \",\".join(eval(dc[\"DE\"])),\n",
    "        \"year\": str(dc[\"PY\"]),\n",
    "    }\n",
    "    auInst = eval(dc[\"AF\"])\n",
    "    auInstInfo = []\n",
    "    aus = []  # 存这篇paper的所有 作者-机构\n",
    "    insts = []  # 存这篇paper的所有机构\n",
    "    for item in auInst:\n",
    "        for k, v in item.items():\n",
    "            auInstInfo.append(k + \"-\" + v)\n",
    "            aus.append(k)\n",
    "            insts.append(v)\n",
    "    paperInfo[\"auInst\"] = \";\".join(auInstInfo)\n",
    "\n",
    "    nodesLs.append(paperInfo)\n",
    "    nodesLs_all.append(paperInfo)\n",
    "\n",
    "    # 去重复\n",
    "    aus = list(set(aus))\n",
    "    insts = list(set(insts))\n",
    "    # 存储P-A;P-I\n",
    "    for k in aus:\n",
    "        linksLs_all.append({\"source\": dc[\"TI\"], \"target\": k, \"value\": 1})\n",
    "        if k in auLsMax:\n",
    "            linksLs.append({\"source\": dc[\"TI\"], \"target\": k, \"value\": 1})\n",
    "    for v in insts:\n",
    "        linksLs_all.append({\"source\": dc[\"TI\"], \"target\": v, \"value\": 1})\n",
    "        if v in instLsMax:\n",
    "            linksLs.append({\"source\": dc[\"TI\"], \"target\": v, \"value\": 1})\n",
    "\n",
    "\n",
    "print(len(nodesLs))\n",
    "print(len(nodesLs_all))\n",
    "print(len(linksLs))\n",
    "print(len(linksLs_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8703\n"
     ]
    }
   ],
   "source": [
    "# 添加论文合作连边\n",
    "count = 0\n",
    "papersDOI = [name for name in papersLs.keys()]\n",
    "for i in range(n):\n",
    "    # 用列表存引文列表\n",
    "    \n",
    "    quotLs = eval(df.loc[i, \"CR\"])  # 引文DOI\n",
    "    for j in range(len(quotLs)):\n",
    "        if quotLs[j] in papersDOI:\n",
    "            # 引文在文章节点中存在\n",
    "            count += 1\n",
    "            edge = {\n",
    "                \"source\": df.loc[i, \"TI\"],\n",
    "                \"target\": papersLs[quotLs[j]],\n",
    "                \"value\": 1,\n",
    "            }\n",
    "            linksLs.append(edge)\n",
    "            linksLs_all.append(edge)\n",
    "    \n",
    "\n",
    "print(count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13567\n",
      "28435\n"
     ]
    }
   ],
   "source": [
    "print(len(linksLs))\n",
    "print(len(linksLs_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonData = {\"nodes\": nodesLs, \"links\": linksLs}\n",
    "jsonData_all = {\"nodes\": nodesLs_all, \"links\": linksLs_all}\n",
    "\n",
    "\n",
    "# 导出为 JSON 文件\n",
    "# with open(\"./output_data/sourceJsonTest.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(jsonData, f, indent=4, ensure_ascii=False)\n",
    "with open(\"../output_data/sourceJson_Draw.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(jsonData, f, indent=4, ensure_ascii=False)\n",
    "with open(\"../output_data/sourceJson_All.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(jsonData_all, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
